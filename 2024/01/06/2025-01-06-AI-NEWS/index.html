<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2025년 01월 06일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="➡️ Google은 제너레이티브 AI 모델의 기능을 확장하는 ‘Agent’ 개념과 아키텍처를 상세히 설명하는 백서를 발표했습니다. 이 백서는 에이전트의 핵심 구성 요소인 모델, 도구, 오케스트레이션 레이어를 설명하고, LangChain 및 Vertex AI를 이용한 에"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2025년 01월 06일 AI 소식</a></h3></div><div class="post-content"><p>➡️ Google은 제너레이티브 AI 모델의 기능을 확장하는 ‘Agent’ 개념과 아키텍처를 상세히 설명하는 백서를 발표했습니다. 이 백서는 에이전트의 핵심 구성 요소인 모델, 도구, 오케스트레이션 레이어를 설명하고, LangChain 및 Vertex AI를 이용한 에이전트 구축 사례를 제시합니다.</p>
<p>➡️ Hugging Face에서는 LLM에 에이전트 기능을 통합하는 간단한 라이브러리인 ‘smolagents’를 출시했습니다. smolagents는 코드 에이전트를 위한 강력한 지원을 제공하며, 다양한 LLM과 도구를 허브를 통해 통합할 수 있도록 설계되었습니다.</p>
<p>➡️ Meta FAIR에서는 가상 에이전트 제어, 비디오 워터마킹, 새로운 언어 모델링 방식 등 다양한 AI 연구 결과와 오픈소스 모델을 공개했습니다.</p>
<p>➡️ NVIDIA는 문서에서 콘텐츠와 메타데이터를 추출하는 마이크로서비스인 ‘NVIDIA Ingest’를 발표했습니다. 이 서비스는 다양한 문서 형식을 지원하며, LLM 애플리케이션에 활용될 수 있도록 설계되었습니다.</p>
<p>➡️ 이 외에도 웹 크롤링 라이브러리, PDF 번역 도구, LLM 성능 향상 기법 등 다양한 AI 관련 소식이 있었습니다.</p>
<h3 id="Google-Agents"><a href="#Google-Agents" class="headerlink" title="Google, Agents"></a>Google, Agents</h3><p><a target="_blank" rel="noopener" href="https://media.licdn.com/dms/document/media/v2/D561FAQH8tt1cvunj0w/feedshare-document-pdf-analyzed/B56ZQq.TtsG8AY-/0/1735887787265?e=1736985600&v=beta&t=pLuArcKyUcxE9B1Her1QWfMHF_UxZL9Q-Y0JTDuSn38">링크</a>, September 2024</p>
<ul>
<li>제너레이티브 AI 에이전트는 목표 달성을 위해 외부 세계를 관찰하고 도구를 사용하여 행동하는 애플리케이션으로 정의됨.</li>
<li>에이전트는 자율적으로 작동하며, 명시적인 지시 없이도 목표 달성을 위해 추론 가능.</li>
<li>에이전트의 핵심 구성 요소는 의사 결정 역할을 하는 모델, 외부 세계와의 상호 작용을 가능하게 하는 도구, 정보 처리 및 의사 결정을 관리하는 오케스트레이션 레이어임.</li>
<li>모델은 명령어 기반 추론 및 논리 프레임워크(ReAct, CoT, ToT)를 따르는 LM을 활용하며, 특정 에이전트 아키텍처의 요구 사항에 따라 다양한 크기 및 방식으로 구성 가능.</li>
<li>도구는 에이전트가 외부 데이터 및 서비스와 상호 작용하도록 지원하며, 웹 API 메소드(GET, POST 등)와 유사한 형태로 제공됨.</li>
<li>확장 프로그램(Extensions), 함수(Functions), 데이터 저장소(Data Stores)는 Google 모델이 상호 작용할 수 있는 주요 도구 유형임.</li>
<li>확장 프로그램은 API와 에이전트 간의 간극을 표준화된 방식으로 연결하여 에이전트가 API의 구현 방식에 관계없이 원활하게 실행하도록 함.</li>
<li>함수는 특정 작업을 수행하는 재사용 가능한 코드 모듈로, 모델이 함수 호출 시점과 필요한 인수를 결정함. 함수는 클라이언트 측에서 실행됨.</li>
<li>데이터 저장소는 에이전트가 최신 정보에 접근하도록 지원하며, 벡터 데이터베이스 형태로 구현되어 RAG(Retrieval Augmented Generation) 애플리케이션에 활용됨.</li>
<li>에이전트의 응답 품질은 모델의 추론 능력, 올바른 도구 선택 능력, 도구 정의의 정확성에 따라 결정됨.</li>
<li>모델 성능 향상을 위해 인-컨텍스트 학습, 검색 기반 인-컨텍스트 학습, 파인튜닝 기반 학습 등 다양한 타겟 학습 방식이 활용될 수 있음.</li>
<li>LangChain 및 LangGraph 라이브러리를 사용하여 실제 에이전트 프로토타입을 구축하는 예시 제시.</li>
<li>Vertex AI 플랫폼은 에이전트 개발, 테스트, 평가, 배포를 위한 완전 관리형 환경을 제공함.</li>
</ul>
<h3 id="Hugging-Face-Introducing-smolagents-a-simple-library-to-build-agents"><a href="#Hugging-Face-Introducing-smolagents-a-simple-library-to-build-agents" class="headerlink" title="Hugging Face, Introducing smolagents, a simple library to build agents"></a>Hugging Face, Introducing smolagents, a simple library to build agents</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/smolagents">링크</a>, December 31, 2024</p>
<ul>
<li>smolagents는 LLM에 에이전트 기능을 통합하는 간단한 Python 라이브러리임.</li>
<li>에이전트는 LLM 출력이 워크플로우를 제어하는 프로그램으로 정의되며, 에이전시 수준은 LLM이 워크플로우에 미치는 영향력에 따라 연속적인 스펙트럼으로 표현됨.</li>
<li>멀티 스텝 에이전트는 루프를 통해 작업을 수행하며, 각 단계에서 LLM은 외부 도구를 호출하는 액션을 작성하고, 관찰 결과를 바탕으로 다음 단계를 결정함.</li>
<li>에이전트는 LLM이 앱의 워크플로우를 결정해야 할 때 유용하지만, 미리 결정된 워크플로우로 충분한 경우 과도한 설정일 수 있음.</li>
<li>코드 에이전트는 액션을 JSON 대신 코드로 작성하는 방식으로, 코드의 표현력과 LLM 학습 데이터와의 연관성을 활용하여 성능 향상을 기대할 수 있음.</li>
<li>smolagents는 단순성, 코드 에이전트에 대한 강력한 지원, 허브 통합, 다양한 LLM 지원을 목표로 개발됨.</li>
<li>에이전트 구축을 위해서는 도구 목록과 LLM 모델이 필요하며, 도구는 타입 힌트와 독스트링을 포함한 함수로 정의하고 <code>@tool</code> 데코레이터를 사용하여 만들 수 있음.</li>
<li>허브를 통해 도구를 공유하고 로드하는 기능 지원.</li>
<li>오픈 소스 모델이 에이전트 워크플로우에서 최고의 성능을 보이는 클로즈드 모델에 필적할 수 있음을 보여주는 벤치마크 결과 제시.</li>
</ul>
<h3 id="Thytu-Agentarium"><a href="#Thytu-Agentarium" class="headerlink" title="Thytu, Agentarium"></a>Thytu, Agentarium</h3><p><a target="_blank" rel="noopener" href="https://github.com/Thytu/Agentarium">링크</a>, 2025&#x2F;1&#x2F;2</p>
<ul>
<li>Agentarium은 AI 에이전트 관리 및 오케스트레이션을 위한 새로운 Python 프레임워크임.</li>
<li>주요 기능으로는 고급 에이전트 관리, 강력한 상호 작용 관리, 체크포인트 시스템, 데이터 생성, 성능 최적화, 유연한 환경 구성 (YAML), 확장 가능한 아키텍처 등이 있음.</li>
<li>다양한 역할과 기능을 가진 여러 AI 에이전트를 생성하고 오케스트레이션 가능.</li>
<li>에이전트 간의 복잡한 상호 작용을 조정.</li>
<li>에이전트 상태 및 상호 작용을 저장하고 복원하는 기능 제공.</li>
<li>에이전트 상호 작용을 통해 합성 데이터 생성 가능.</li>
<li>효율성과 확장성을 고려하여 구축됨.</li>
<li>YAML 구성 파일을 사용하여 사용자 정의 환경 정의 가능.</li>
<li>특정 요구 사항에 맞게 확장 및 사용자 정의가 용이한 아키텍처 제공.</li>
</ul>
<h3 id="Byaidu-PDFMathTranslate"><a href="#Byaidu-PDFMathTranslate" class="headerlink" title="Byaidu, PDFMathTranslate"></a>Byaidu, PDFMathTranslate</h3><p><a target="_blank" rel="noopener" href="https://github.com/Byaidu/PDFMathTranslate">링크</a>, 2024&#x2F;12&#x2F;1</p>
<ul>
<li>PDF 과학 논문 번역 및 이중 언어 비교 도구.</li>
<li>수식, 차트, 목차 및 주석 유지 (미리보기).</li>
<li>다양한 언어 및 번역 서비스 지원.</li>
<li>명령줄 도구, 대화형 사용자 인터페이스 및 Docker 제공.</li>
<li>GitHub Issues, Telegram Group 또는 QQ Group을 통해 피드백 제공 가능.</li>
<li>기여 방법은 Contribution Guide 참조.</li>
</ul>
<h3 id="Meta-FAIR-Memory-Layers-at-Scale"><a href="#Meta-FAIR-Memory-Layers-at-Scale" class="headerlink" title="Meta FAIR, Memory Layers at Scale"></a>Meta FAIR, Memory Layers at Scale</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.09764">링크</a>, December 23, 2024</p>
<ul>
<li>Meta FAIR에서 메모리 레이어를 활용하여 모델의 매개변수 수를 늘리지 않고도 정보를 저장하고 검색하는 새로운 기술을 개발하고 관련 연구 결과 및 모델을 오픈소스로 공개함.</li>
<li>메모리 레이어는 훈련 가능한 키-값 조회 메커니즘을 사용하여 FLOPs 증가 없이 모델에 추가 매개변수를 추가함.</li>
<li>희소하게 활성화되는 메모리 레이어는 연산 집약적인 밀집 피드포워드 레이어를 보완하여 정보를 저렴하게 저장하고 검색할 수 있는 전용 용량을 제공함.</li>
<li>개선된 메모리 레이어로 강화된 언어 모델은 다운스트림 작업에서 두 배 이상의 연산 예산을 가진 밀집 모델보다 성능이 뛰어나며, 연산량과 매개변수 수가 일치할 때 MoE 모델보다도 우수한 성능을 보임.</li>
<li>특히 사실적인 작업에서 성능 향상이 두드러짐.</li>
<li>최대 1280억 개의 메모리 매개변수를 사용하여 확장 법칙을 보여주는 완전 병렬화 가능한 메모리 레이어 구현을 제공하며, 1조 개의 토큰으로 사전 훈련하여 최대 80억 개의 매개변수를 가진 기본 모델과 비교함.</li>
<li>Meta FAIR에서 에이전트, 견고성 및 안전성, 머신 러닝을 용이하게 하는 아키텍처 개발에 대한 최근 혁신을 강조하는 여러 새로운 연구 결과 공개.</li>
<li>Meta Motivo (가상 구현 에이전트의 동작 제어를 위한 파운데이션 모델) 및 Meta Video Seal (오픈 소스 비디오 워터마킹 모델) 포함.</li>
</ul>
<h3 id="Meta-FAIR-Sharing-new-research-models-and-datasets-from-Meta-FAIR"><a href="#Meta-FAIR-Sharing-new-research-models-and-datasets-from-Meta-FAIR" class="headerlink" title="Meta FAIR, Sharing new research, models, and datasets from Meta FAIR"></a>Meta FAIR, Sharing new research, models, and datasets from Meta FAIR</h3><p><a target="_blank" rel="noopener" href="https://ai.meta.com/blog/fair-new-research-agents-video-watermarking-large-concept-models/">링크</a>, December 12, 2024</p>
<ul>
<li>Meta FAIR에서 더욱 강력한 에이전트 구축, 견고성 및 안전성 확보, 모델이 새로운 정보를 보다 효과적으로 학습하고 현재 한계를 뛰어넘어 확장할 수 있도록 지원하는 아키텍처 혁신에 중점을 둔 최신 연구, 코드, 모델 및 데이터 세트 공개.</li>
<li>Meta Video Seal (신경 비디오 워터마킹을 위한 최첨단 포괄적인 프레임워크 데모 및 코드), Meta Omni Seal Bench (신경 워터마킹 전용 리더보드), Meta Watermark Anything 모델 (허용 라이선스로 재출시) 공개.</li>
<li>Meta Motivo (가상 구현 휴머노이드 에이전트의 움직임을 제어하는 행동 파운데이션 모델), Flow Matching 가이드 및 코드베이스, Meta Explore Theory-of-Mind (ToM 추론을 위한 프로그램 기반 적대적 데이터 생성), Meta Large Concept Models (LCM, 새로운 언어 모델링 패러다임), Meta Dynamic Byte Latent Transformer (계층적 바이트 레벨 모델), Meta Memory Layers at Scale 연구 결과 및 코드, Meta Image Diversity Modeling 연구 업데이트 및 텍스트-이미지 생성 모델 평가 툴박스, Meta CLIP 1.2 공개.</li>
</ul>
<h3 id="NVIDIA-NVIDIA-Ingest"><a href="#NVIDIA-NVIDIA-Ingest" class="headerlink" title="NVIDIA, NVIDIA-Ingest"></a>NVIDIA, NVIDIA-Ingest</h3><p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/nv-ingest">링크</a>, 2025&#x2F;3&#x2F;1</p>
<ul>
<li>NVIDIA Ingest는 PDF, Word, PowerPoint 문서 구문 분석을 지원하는 확장 가능하고 성능 지향적인 문서 콘텐츠 및 메타데이터 추출 마이크로서비스임.</li>
<li>다운스트림 생성 애플리케이션에 사용하기 위해 특수화된 NVIDIA NIM 마이크로서비스를 사용하여 텍스트, 표, 차트 및 이미지를 찾고 컨텍스트화하고 추출함.</li>
<li>문서를 페이지로 분할하는 프로세스를 병렬화하여 콘텐츠를 분류하고 (표, 차트, 이미지, 텍스트), 개별 콘텐츠로 추출하고, 광학 문자 인식 (OCR)을 통해 컨텍스트화하여 잘 정의된 JSON 스키마로 변환함.</li>
<li>추출된 콘텐츠에 대한 임베딩 계산을 선택적으로 관리하고, 벡터 데이터베이스 Milvus에 저장을 선택적으로 관리할 수 있음.</li>
<li>JSON 작업 설명, 문서 페이로드 및 해당 페이로드에 수행할 수집 작업을 허용하고 작업 결과를 검색할 수 있도록 지원하며, 결과는 기본 문서에서 추출된 객체에 대한 메타데이터 목록과 처리 주석 및 타이밍&#x2F;추적 데이터를 포함하는 JSON 딕셔너리임.</li>
<li>PDF, Docx, pptx 및 이미지를 지원하며, 처리량과 정확성 간의 균형을 맞추기 위해 각 문서 유형에 대한 여러 추출 방법을 지원함 (예: PDF 문서의 경우 pdfium, Unstructured.io 및 Adobe Content Extraction Services를 통한 추출 지원).</li>
<li>텍스트 분할 및 청킹, 변환 및 필터링, 임베딩 생성, 이미지 스토리지를 포함한 다양한 유형의 사전 및 사후 처리 작업을 지원함.</li>
</ul>
<h3 id="unclecode-crawl4ai"><a href="#unclecode-crawl4ai" class="headerlink" title="unclecode, crawl4ai"></a>unclecode, crawl4ai</h3><p><a target="_blank" rel="noopener" href="https://github.com/unclecode/crawl4ai">링크</a>, 2024&#x2F;12&#x2F;15</p>
<ul>
<li>Crawl4AI는 웹 크롤링 및 데이터 추출을 간소화하여 LLM 및 AI 애플리케이션에 즉시 사용할 수 있도록 지원하는 무료 오픈소스 라이브러리임.</li>
<li>빠른 성능 (유료 서비스보다 뛰어남), LLM 친화적인 출력 형식 (JSON, 정리된 HTML, 마크다운), 여러 URL 동시 크롤링 지원, 모든 미디어 태그 (이미지, 오디오, 비디오) 추출, 외부 및 내부 링크 추출 기능 제공.</li>
<li>페이지에서 메타데이터 추출, 인증, 헤더 및 페이지 수정을 위한 사용자 정의 훅, 사용자 에이전트 사용자 정의, 페이지 스크린샷 캡처, 크롤링 전에 사용자 정의 JavaScript 실행 기능 지원.</li>
<li>실시간, 비용 효율적인 성능으로 6배 빠른 결과 제공.</li>
<li>세션 관리, 프록시 및 원활한 데이터 액세스를 위한 사용자 정의 훅 제공.</li>
<li>비용이 많이 드는 모델에 대한 의존도를 줄이기 위해 고급 알고리즘 사용.</li>
<li>Docker 및 클라우드 통합에 적합한 완전한 오픈 소스 라이브러리.</li>
</ul>
<h3 id="Jio-Oh-외-Better-Think-with-Tables-Leveraging-Tables-to-Enhance-Large-Language-Model-Comprehension"><a href="#Jio-Oh-외-Better-Think-with-Tables-Leveraging-Tables-to-Enhance-Large-Language-Model-Comprehension" class="headerlink" title="Jio Oh 외, Better Think with Tables: Leveraging Tables to Enhance Large Language Model Comprehension"></a>Jio Oh 외, Better Think with Tables: Leveraging Tables to Enhance Large Language Model Comprehension</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.17189">링크</a>, 2024&#x2F;12&#x2F;22</p>
<ul>
<li>LLM은 복잡한 쿼리 (특히 여러 조건이 포함된 현실 세계 시나리오)에 어려움을 겪음.</li>
<li>테이블을 활용하여 중간 사고를 수행하도록 LLM을 지원하는 “Thinking with Tables” 기술 제안.</li>
<li>LLM이 정보를 테이블로 구성하도록 유도하는 사전 지시를 통해 평균 40.29%의 상대적 성능 향상, 더 높은 견고성 및 다양한 요청, 조건 또는 시나리오에 대한 일반화 가능성을 달성함.</li>
<li>데이터 구조화 수준이 모델에 미치는 영향을 비교하기 위해 네 가지의 서로 다른 구조화 수준을 소개하고 결과를 비교함.</li>
</ul>
<h3 id="Brian-J-Chan-외-Don’t-Do-RAG-When-Cache-Augmented-Generation-is-All-You-Need-for-Knowledge-Tasks"><a href="#Brian-J-Chan-외-Don’t-Do-RAG-When-Cache-Augmented-Generation-is-All-You-Need-for-Knowledge-Tasks" class="headerlink" title="Brian J Chan 외, Don’t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks"></a>Brian J Chan 외, Don’t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.15605">링크</a>, 2024&#x2F;12&#x2F;20</p>
<ul>
<li>검색 증강 생성 (RAG)은 외부 지식 소스를 통합하여 언어 모델을 향상시키는 강력한 접근 방식으로 주목받고 있지만, 검색 지연 시간, 문서 선택 오류 가능성, 시스템 복잡성 증가와 같은 문제가 있음.</li>
<li>긴 컨텍스트 창을 특징으로 하는 대규모 언어 모델 (LLM)의 등장으로 실시간 검색을 우회하는 대안적인 패러다임인 캐시 증강 생성 (CAG)을 제안함.</li>
<li>제한적이고 관리 가능한 크기의 문서 또는 지식과 같은 관련 리소스를 LLM의 확장된 컨텍스트에 미리 로드하고 런타임 매개변수를 캐싱하는 방식임.</li>
<li>추론 중에 모델은 추가 검색 단계 없이 이러한 미리 로드된 매개변수를 활용하여 쿼리에 응답함.</li>
<li>CAG는 검색 지연 시간을 제거하고 컨텍스트 관련성을 유지하면서 검색 오류를 최소화함.</li>
<li>여러 벤치마크에 대한 성능 평가 결과, 특히 제한된 지식 기반을 가진 특정 애플리케이션의 경우 CAG가 기존 RAG 파이프라인을 능가하거나 보완하는 시나리오를 강조함.</li>
<li>CAG는 RAG에 대한 간소화되고 효율적인 대안을 제공하며, 복잡성을 줄이면서 유사하거나 우수한 결과를 달성할 수 있음을 시사함.</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">Agent by Google</span><br><span class="line"></span><br><span class="line">Agents</span><br><span class="line">Authors: Julia Wiesinger, Patrick Marlow</span><br><span class="line">and Vladimir Vuskovic</span><br><span class="line">Agents</span><br><span class="line">September 2024 2</span><br><span class="line">Acknowledgements</span><br><span class="line">Reviewers and Contributors</span><br><span class="line">Evan Huang</span><br><span class="line">Emily Xue</span><br><span class="line">Olcan Sercinoglu</span><br><span class="line">Sebastian Riedel</span><br><span class="line">Satinder Baveja</span><br><span class="line">Antonio Gulli</span><br><span class="line">Anant Nawalgaria</span><br><span class="line">Curators and Editors</span><br><span class="line">Antonio Gulli</span><br><span class="line">Anant Nawalgaria</span><br><span class="line">Grace Mollison</span><br><span class="line">Technical Writer</span><br><span class="line">Joey Haymaker</span><br><span class="line">Designer</span><br><span class="line">Michael Lanning</span><br><span class="line">Introduction 4</span><br><span class="line">What is an agent? 5</span><br><span class="line">The model 6</span><br><span class="line">The tools 7</span><br><span class="line">The orchestration layer 7</span><br><span class="line">Agents vs. models 8</span><br><span class="line">Cognitive architectures: How agents operate 8</span><br><span class="line">Tools: Our keys to the outside world 12</span><br><span class="line">Extensions 13</span><br><span class="line"> Sample Extensions 15</span><br><span class="line">Functions 18</span><br><span class="line"> Use cases 21</span><br><span class="line"> Function sample code 24</span><br><span class="line">Data stores 27</span><br><span class="line"> Implementation and application 28</span><br><span class="line">Tools recap 32</span><br><span class="line">Enhancing model performance with targeted learning 33</span><br><span class="line">Agent quick start with LangChain 35</span><br><span class="line">Production applications with Vertex AI agents 38</span><br><span class="line">Summary 40</span><br><span class="line">Endnotes 42</span><br><span class="line">Table of contents</span><br><span class="line">Agents</span><br><span class="line">September 2024 4</span><br><span class="line">Introduction</span><br><span class="line">Humans are fantastic at messy pattern recognition tasks. However, they often rely on tools</span><br><span class="line">- like books, Google Search, or a calculator - to supplement their prior knowledge before</span><br><span class="line">arriving at a conclusion. Just like humans, Generative AI models can be trained to use tools</span><br><span class="line">to access real-time information or suggest a real-world action. For example, a model can</span><br><span class="line">leverage a database retrieval tool to access specific information, like a customer&#x27;s purchase</span><br><span class="line">history, so it can generate tailored shopping recommendations. Alternatively, based on a</span><br><span class="line">user&#x27;s query, a model can make various API calls to send an email response to a colleague</span><br><span class="line">or complete a financial transaction on your behalf. To do so, the model must not only have</span><br><span class="line">access to a set of external tools, it needs the ability to plan and execute any task in a selfdirected fashion. This combination of reasoning, logic, and access to external information</span><br><span class="line">that are all connected to a Generative AI model invokes the concept of an agent, or a</span><br><span class="line">program that extends beyond the standalone capabilities of a Generative AI model. This</span><br><span class="line">whitepaper dives into all these and associated aspects in more detail.</span><br><span class="line">This combination of reasoning,</span><br><span class="line">logic, and access to external</span><br><span class="line">information that are all connected</span><br><span class="line">to a Generative AI model invokes</span><br><span class="line">the concept of an agent.</span><br><span class="line">Agents</span><br><span class="line">September 2024 5</span><br><span class="line">What is an agent?</span><br><span class="line">In its most fundamental form, a Generative AI agent can be defined as an application that</span><br><span class="line">attempts to achieve a goal by observing the world and acting upon it using the tools that it</span><br><span class="line">has at its disposal. Agents are autonomous and can act independently of human intervention,</span><br><span class="line">especially when provided with proper goals or objectives they are meant to achieve. Agents</span><br><span class="line">can also be proactive in their approach to reaching their goals. Even in the absence of</span><br><span class="line">explicit instruction sets from a human, an agent can reason about what it should do next to</span><br><span class="line">achieve its ultimate goal. While the notion of agents in AI is quite general and powerful, this</span><br><span class="line">whitepaper focuses on the specific types of agents that Generative AI models are capable of</span><br><span class="line">building at the time of publication.</span><br><span class="line">In order to understand the inner workings of an agent, let’s first introduce the foundational</span><br><span class="line">components that drive the agent’s behavior, actions, and decision making. The combination</span><br><span class="line">of these components can be described as a cognitive architecture, and there are many</span><br><span class="line">such architectures that can be achieved by the mixing and matching of these components.</span><br><span class="line">Focusing on the core functionalities, there are three essential components in an agent’s</span><br><span class="line">cognitive architecture as shown in Figure 1.</span><br><span class="line">Agents</span><br><span class="line">September 2024 6</span><br><span class="line">Figure 1. General agent architecture and components</span><br><span class="line">The model</span><br><span class="line">In the scope of an agent, a model refers to the language model (LM) that will be utilized as</span><br><span class="line">the centralized decision maker for agent processes. The model used by an agent can be one</span><br><span class="line">or multiple LM’s of any size (small / large) that are capable of following instruction based</span><br><span class="line">reasoning and logic frameworks, like ReAct, Chain-of-Thought, or Tree-of-Thoughts. Models</span><br><span class="line">can be general purpose, multimodal or fine-tuned based on the needs of your specific agent</span><br><span class="line">architecture. For best production results, you should leverage a model that best fits your</span><br><span class="line">desired end application and, ideally, has been trained on data signatures associated with the</span><br><span class="line">tools that you plan to use in the cognitive architecture. It’s important to note that the model is</span><br><span class="line">typically not trained with the specific configuration settings (i.e. tool choices, orchestration/</span><br><span class="line">reasoning setup) of the agent. However, it’s possible to further refine the model for the</span><br><span class="line">agent’s tasks by providing it with examples that showcase the agent’s capabilities, including</span><br><span class="line">instances of the agent using specific tools or reasoning steps in various contexts.</span><br><span class="line">Agents</span><br><span class="line">September 2024 7</span><br><span class="line">The tools</span><br><span class="line">Foundational models, despite their impressive text and image generation, remain constrained</span><br><span class="line">by their inability to interact with the outside world. Tools bridge this gap, empowering agents</span><br><span class="line">to interact with external data and services while unlocking a wider range of actions beyond</span><br><span class="line">that of the underlying model alone. Tools can take a variety of forms and have varying</span><br><span class="line">depths of complexity, but typically align with common web API methods like GET, POST,</span><br><span class="line">PATCH, and DELETE. For example, a tool could update customer information in a database</span><br><span class="line">or fetch weather data to influence a travel recommendation that the agent is providing to</span><br><span class="line">the user. With tools, agents can access and process real-world information. This empowers</span><br><span class="line">them to support more specialized systems like retrieval augmented generation (RAG),</span><br><span class="line">which significantly extends an agent’s capabilities beyond what the foundational model can</span><br><span class="line">achieve on its own. We’ll discuss tools in more detail below, but the most important thing</span><br><span class="line">to understand is that tools bridge the gap between the agent’s internal capabilities and the</span><br><span class="line">external world, unlocking a broader range of possibilities.</span><br><span class="line">The orchestration layer</span><br><span class="line">The orchestration layer describes a cyclical process that governs how the agent takes in</span><br><span class="line">information, performs some internal reasoning, and uses that reasoning to inform its next</span><br><span class="line">action or decision. In general, this loop will continue until an agent has reached its goal or a</span><br><span class="line">stopping point. The complexity of the orchestration layer can vary greatly depending on the</span><br><span class="line">agent and task it’s performing. Some loops can be simple calculations with decision rules,</span><br><span class="line">while others may contain chained logic, involve additional machine learning algorithms, or</span><br><span class="line">implement other probabilistic reasoning techniques. We’ll discuss more about the detailed</span><br><span class="line">implementation of the agent orchestration layers in the cognitive architecture section.</span><br><span class="line">Agents</span><br><span class="line">September 2024 8</span><br><span class="line">Agents vs. models</span><br><span class="line">To gain a clearer understanding of the distinction between agents and models, consider the</span><br><span class="line">following chart:</span><br><span class="line">Models Agents</span><br><span class="line">Knowledge is limited to what is available in their</span><br><span class="line">training data.</span><br><span class="line">Knowledge is extended through the connection</span><br><span class="line">with external systems via tools</span><br><span class="line">Single inference / prediction based on the</span><br><span class="line">user query. Unless explicitly implemented for</span><br><span class="line">the model, there is no management of session</span><br><span class="line">history or continuous context. (i.e. chat history)</span><br><span class="line">Managed session history (i.e. chat history) to</span><br><span class="line">allow for multi turn inference / prediction based</span><br><span class="line">on user queries and decisions made in the</span><br><span class="line">orchestration layer. In this context, a ‘turn’ is</span><br><span class="line">defined as an interaction between the interacting</span><br><span class="line">system and the agent. (i.e. 1 incoming event/</span><br><span class="line">query and 1 agent response)</span><br><span class="line">No native tool implementation. Tools are natively implemented in agent</span><br><span class="line">architecture.</span><br><span class="line">No native logic layer implemented. Users can</span><br><span class="line">form prompts as simple questions or use</span><br><span class="line">reasoning frameworks (CoT, ReAct, etc.) to</span><br><span class="line">form complex prompts to guide the model in</span><br><span class="line">prediction.</span><br><span class="line">Native cognitive architecture that uses reasoning</span><br><span class="line">frameworks like CoT, ReAct, or other pre-built</span><br><span class="line">agent frameworks like LangChain.</span><br><span class="line">Cognitive architectures: How agents operate</span><br><span class="line">Imagine a chef in a busy kitchen. Their goal is to create delicious dishes for restaurant</span><br><span class="line">patrons which involves some cycle of planning, execution, and adjustment.</span><br><span class="line">Agents</span><br><span class="line">September 2024 9</span><br><span class="line">• They gather information, like the patron’s order and what ingredients are in the pantry</span><br><span class="line">and refrigerator.</span><br><span class="line">• They perform some internal reasoning about what dishes and flavor profiles they can</span><br><span class="line">create based on the information they have just gathered.</span><br><span class="line">• They take action to create the dish: chopping vegetables, blending spices, searing meat.</span><br><span class="line">At each stage in the process the chef makes adjustments as needed, refining their plan as</span><br><span class="line">ingredients are depleted or customer feedback is received, and uses the set of previous</span><br><span class="line">outcomes to determine the next plan of action. This cycle of information intake, planning,</span><br><span class="line">executing, and adjusting describes a unique cognitive architecture that the chef employs to</span><br><span class="line">reach their goal.</span><br><span class="line">Just like the chef, agents can use cognitive architectures to reach their end goals by</span><br><span class="line">iteratively processing information, making informed decisions, and refining next actions</span><br><span class="line">based on previous outputs. At the core of agent cognitive architectures lies the orchestration</span><br><span class="line">layer, responsible for maintaining memory, state, reasoning and planning. It uses the rapidly</span><br><span class="line">evolving field of prompt engineering and associated frameworks to guide reasoning and</span><br><span class="line">planning, enabling the agent to interact more effectively with its environment and complete</span><br><span class="line">tasks. Research in the area of prompt engineering frameworks and task planning for</span><br><span class="line">language models is rapidly evolving, yielding a variety of promising approaches. While not an</span><br><span class="line">exhaustive list, these are a few of the most popular frameworks and reasoning techniques</span><br><span class="line">available at the time of this publication:</span><br><span class="line">• ReAct, a prompt engineering framework that provides a thought process strategy for</span><br><span class="line">language models to Reason and take action on a user query, with or without in-context</span><br><span class="line">examples. ReAct prompting has shown to outperform several SOTA baselines and improve</span><br><span class="line">human interoperability and trustworthiness of LLMs.</span><br><span class="line">Agents</span><br><span class="line">September 2024 10</span><br><span class="line">• Chain-of-Thought (CoT), a prompt engineering framework that enables reasoning</span><br><span class="line">capabilities through intermediate steps. There are various sub-techniques of CoT including</span><br><span class="line">self-consistency, active-prompt, and multimodal CoT that each have strengths and</span><br><span class="line">weaknesses depending on the specific application.</span><br><span class="line">• Tree-of-thoughts (ToT),</span><br><span class="line">,</span><br><span class="line"> a prompt engineering framework that is well suited for</span><br><span class="line">exploration or strategic lookahead tasks. It generalizes over chain-of-thought prompting</span><br><span class="line">and allows the model to explore various thought chains that serve as intermediate steps</span><br><span class="line">for general problem solving with language models.</span><br><span class="line">Agents can utilize one of the above reasoning techniques, or many other techniques, to</span><br><span class="line">choose the next best action for the given user request. For example, let’s consider an agent</span><br><span class="line">that is programmed to use the ReAct framework to choose the correct actions and tools for</span><br><span class="line">the user query. The sequence of events might go something like this:</span><br><span class="line">1. User sends query to the agent</span><br><span class="line">2. Agent begins the ReAct sequence</span><br><span class="line">3. The agent provides a prompt to the model, asking it to generate one of the next ReAct</span><br><span class="line">steps and its corresponding output:</span><br><span class="line">a. Question: The input question from the user query, provided with the prompt</span><br><span class="line">b. Thought: The model’s thoughts about what it should do next</span><br><span class="line">c. Action: The model’s decision on what action to take next</span><br><span class="line">i. This is where tool choice can occur</span><br><span class="line">ii. For example, an action could be one of [Flights, Search, Code, None], where the first</span><br><span class="line">3 represent a known tool that the model can choose, and the last represents “no</span><br><span class="line">tool choice”</span><br><span class="line">Agents</span><br><span class="line">September 2024 11</span><br><span class="line">d. Action input: The model’s decision on what inputs to provide to the tool (if any)</span><br><span class="line">e. Observation: The result of the action / action input sequence</span><br><span class="line">i. This thought / action / action input / observation could repeat N-times as needed</span><br><span class="line">f. Final answer: The model’s final answer to provide to the original user query</span><br><span class="line">4. The ReAct loop concludes and a final answer is provided back to the user</span><br><span class="line">Figure 2. Example agent with ReAct reasoning in the orchestration layer</span><br><span class="line">As shown in Figure 2, the model, tools, and agent configuration work together to provide</span><br><span class="line">a grounded, concise response back to the user based on the user’s original query. While</span><br><span class="line">the model could have guessed at an answer (hallucinated) based on its prior knowledge,</span><br><span class="line">it instead used a tool (Flights) to search for real-time external information. This additional</span><br><span class="line">information was provided to the model, allowing it to make a more informed decision based</span><br><span class="line">on real factual data and to summarize this information back to the user.</span><br><span class="line">Agents</span><br><span class="line">September 2024 12</span><br><span class="line">In summary, the quality of agent responses can be tied directly to the model’s ability to</span><br><span class="line">reason and act about these various tasks, including the ability to select the right tools, and</span><br><span class="line">how well that tools has been defined. Like a chef crafting a dish with fresh ingredients and</span><br><span class="line">attentive to customer feedback, agents rely on sound reasoning and reliable information to</span><br><span class="line">deliver optimal results. In the next section, we’ll dive into the various ways agents connect</span><br><span class="line">with fresh data.</span><br><span class="line">Tools: Our keys to the outside world</span><br><span class="line">While language models excel at processing information, they lack the ability to directly</span><br><span class="line">perceive and influence the real world. This limits their usefulness in situations requiring</span><br><span class="line">interaction with external systems or data. This means that, in a sense, a language model</span><br><span class="line">is only as good as what it has learned from its training data. But regardless of how much</span><br><span class="line">data we throw at a model, they still lack the fundamental ability to interact with the outside</span><br><span class="line">world. So how can we empower our models to have real-time, context-aware interaction with</span><br><span class="line">external systems? Functions, Extensions, Data Stores and Plugins are all ways to provide this</span><br><span class="line">critical capability to the model.</span><br><span class="line">While they go by many names, tools are what create a link between our foundational models</span><br><span class="line">and the outside world. This link to external systems and data allows our agent to perform a</span><br><span class="line">wider variety of tasks and do so with more accuracy and reliability. For instance, tools can</span><br><span class="line">enable agents to adjust smart home settings, update calendars, fetch user information from</span><br><span class="line">a database, or send emails based on a specific set of instructions.</span><br><span class="line">As of the date of this publication, there are three primary tool types that Google models are</span><br><span class="line">able to interact with: Extensions, Functions, and Data Stores. By equipping agents with tools,</span><br><span class="line">we unlock a vast potential for them to not only understand the world but also act upon it,</span><br><span class="line">opening doors to a myriad of new applications and possibilities.</span><br><span class="line">Agents</span><br><span class="line">September 2024 13</span><br><span class="line">Extensions</span><br><span class="line">The easiest way to understand Extensions is to think of them as bridging the gap between</span><br><span class="line">an API and an agent in a standardized way, allowing agents to seamlessly execute APIs</span><br><span class="line">regardless of their underlying implementation. Let’s say that you’ve built an agent with a goal</span><br><span class="line">of helping users book flights. You know that you want to use the Google Flights API to retrieve</span><br><span class="line">flight information, but you’re not sure how you’re going to get your agent to make calls to this</span><br><span class="line">API endpoint.</span><br><span class="line">Figure 3. How do Agents interact with External APIs?</span><br><span class="line">One approach could be to implement custom code that would take the incoming user query,</span><br><span class="line">parse the query for relevant information, then make the API call. For example, in a flight</span><br><span class="line">booking use case a user might state “I want to book a flight from Austin to Zurich.” In this</span><br><span class="line">scenario, our custom code solution would need to extract “Austin” and “Zurich” as relevant</span><br><span class="line">entities from the user query before attempting to make the API call. But what happens if the</span><br><span class="line">user says “I want to book a flight to Zurich” and never provides a departure city? The API call</span><br><span class="line">would fail without the required data and more code would need to be implemented in order</span><br><span class="line">to catch edge and corner cases like this. This approach is not scalable and could easily break</span><br><span class="line">in any scenario that falls outside of the implemented custom code.</span><br><span class="line">Agents</span><br><span class="line">September 2024 14</span><br><span class="line">A more resilient approach would be to use an Extension. An Extension bridges the gap</span><br><span class="line">between an agent and an API by:</span><br><span class="line">1. Teaching the agent how to use the API endpoint using examples.</span><br><span class="line">2. Teaching the agent what arguments or parameters are needed to successfully call the</span><br><span class="line">API endpoint.</span><br><span class="line">Figure 4. Extensions connect Agents to External APIs</span><br><span class="line">Extensions can be crafted independently of the agent, but should be provided as part of the</span><br><span class="line">agent’s configuration. The agent uses the model and examples at run time to decide which</span><br><span class="line">Extension, if any, would be suitable for solving the user’s query. This highlights a key strength</span><br><span class="line">of Extensions, their built-in example types, that allow the agent to dynamically select the</span><br><span class="line">most appropriate Extension for the task.</span><br><span class="line">Figure 5. 1-to-many relationship between Agents, Extensions and APIs</span><br><span class="line">Agents</span><br><span class="line">September 2024 15</span><br><span class="line">Think of this the same way that a software developer decides which API endpoints to use</span><br><span class="line">while solving and solutioning for a user’s problem. If the user wants to book a flight, the</span><br><span class="line">developer might use the Google Flights API. If the user wants to know where the nearest</span><br><span class="line">coffee shop is relative to their location, the developer might use the Google Maps API. In</span><br><span class="line">this same way, the agent / model stack uses a set of known Extensions to decide which one</span><br><span class="line">will be the best fit for the user’s query. If you’d like to see Extensions in action, you can try</span><br><span class="line">them out on the Gemini application by going to Settings &gt; Extensions and then enabling any</span><br><span class="line">you would like to test. For example, you could enable the Google Flights extension then ask</span><br><span class="line">Gemini “Show me flights from Austin to Zurich leaving next Friday.”</span><br><span class="line">Sample Extensions</span><br><span class="line">To simplify the usage of Extensions, Google provides some out of the box extensions that</span><br><span class="line">can be quickly imported into your project and used with minimal configurations. For example,</span><br><span class="line">the Code Interpreter extension in Snippet 1 allows you to generate and run Python code from</span><br><span class="line">a natural language description.</span><br><span class="line">Agents</span><br><span class="line">September 2024 16</span><br><span class="line">Python</span><br><span class="line">import vertexai</span><br><span class="line">import pprint</span><br><span class="line">PROJECT_ID = &quot;YOUR_PROJECT_ID&quot;</span><br><span class="line">REGION = &quot;us-central1&quot;</span><br><span class="line">vertexai.init(project=PROJECT_ID, location=REGION)</span><br><span class="line">from vertexai.preview.extensions import Extension</span><br><span class="line">extension_code_interpreter = Extension.from_hub(&quot;code_interpreter&quot;)</span><br><span class="line">CODE_QUERY = &quot;&quot;&quot;Write a python method to invert a binary tree in O(n) time.&quot;&quot;&quot;</span><br><span class="line">response = extension_code_interpreter.execute(</span><br><span class="line"> operation_id = &quot;generate_and_execute&quot;,</span><br><span class="line"> operation_params = &#123;&quot;query&quot;: CODE_QUERY&#125;</span><br><span class="line"> )</span><br><span class="line">print(&quot;Generated Code:&quot;)</span><br><span class="line">pprint.pprint(&#123;response[&#x27;generated_code&#x27;]&#125;)</span><br><span class="line"># The above snippet will generate the following code.</span><br></pre></td></tr></table></figure>

<p>Generated Code:<br>class TreeNode:<br>def <strong>init</strong>(self, val&#x3D;0, left&#x3D;None, right&#x3D;None):<br>self.val &#x3D; val<br>self.left &#x3D; left<br>self.right &#x3D; right<br>Continues next page…<br>Agents<br>September 2024 17<br>Python<br>def invert_binary_tree(root):<br>“””<br>Inverts a binary tree.<br>Args:<br>root: The root of the binary tree.<br>Returns:<br>The root of the inverted binary tree.<br>“””<br>if not root:<br>return None</p>
<h1 id="Swap-the-left-and-right-children-recursively"><a href="#Swap-the-left-and-right-children-recursively" class="headerlink" title="Swap the left and right children recursively"></a>Swap the left and right children recursively</h1><p>root.left, root.right &#x3D;<br>invert_binary_tree(root.right), invert_binary_tree(root.left)<br>return root</p>
<h1 id="Example-usage"><a href="#Example-usage" class="headerlink" title="Example usage:"></a>Example usage:</h1><h1 id="Construct-a-sample-binary-tree"><a href="#Construct-a-sample-binary-tree" class="headerlink" title="Construct a sample binary tree"></a>Construct a sample binary tree</h1><p>root &#x3D; TreeNode(4)<br>root.left &#x3D; TreeNode(2)<br>root.right &#x3D; TreeNode(7)<br>root.left.left &#x3D; TreeNode(1)<br>root.left.right &#x3D; TreeNode(3)<br>root.right.left &#x3D; TreeNode(6)<br>root.right.right &#x3D; TreeNode(9)</p>
<h1 id="Invert-the-binary-tree"><a href="#Invert-the-binary-tree" class="headerlink" title="Invert the binary tree"></a>Invert the binary tree</h1><p>inverted_root &#x3D; invert_binary_tree(root)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br><span class="line">936</span><br><span class="line">937</span><br><span class="line">938</span><br><span class="line">939</span><br><span class="line">940</span><br><span class="line">941</span><br><span class="line">942</span><br><span class="line">943</span><br><span class="line">944</span><br><span class="line">945</span><br><span class="line">946</span><br><span class="line">947</span><br><span class="line">948</span><br><span class="line">949</span><br><span class="line">950</span><br><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br><span class="line">954</span><br><span class="line">955</span><br><span class="line">956</span><br><span class="line">957</span><br></pre></td><td class="code"><pre><span class="line">Snippet 1. Code Interpreter Extension can generate and run Python code</span><br><span class="line">Agents</span><br><span class="line">September 2024 18</span><br><span class="line">To summarize, Extensions provide a way for agents to perceive, interact, and influence the</span><br><span class="line">outside world in a myriad of ways. The selection and invocation of these Extensions is guided</span><br><span class="line">by the use of Examples, all of which are defined as part of the Extension configuration.</span><br><span class="line">Functions</span><br><span class="line">In the world of software engineering, functions are defined as self-contained modules</span><br><span class="line">of code that accomplish a specific task and can be reused as needed. When a software</span><br><span class="line">developer is writing a program, they will often create many functions to do various tasks.</span><br><span class="line">They will also define the logic for when to call function_a versus function_b, as well as the</span><br><span class="line">expected inputs and outputs.</span><br><span class="line">Functions work very similarly in the world of agents, but we can replace the software</span><br><span class="line">developer with a model. A model can take a set of known functions and decide when to use</span><br><span class="line">each Function and what arguments the Function needs based on its specification. Functions</span><br><span class="line">differ from Extensions in a few ways, most notably:</span><br><span class="line">1. A model outputs a Function and its arguments, but doesn’t make a live API call.</span><br><span class="line">2. Functions are executed on the client-side, while Extensions are executed on</span><br><span class="line">the agent-side.</span><br><span class="line">Using our Google Flights example again, a simple setup for functions might look like the</span><br><span class="line">example in Figure 7.</span><br><span class="line">Agents</span><br><span class="line">September 2024 19</span><br><span class="line">Figure 7. How do functions interact with external APIs?</span><br><span class="line">Note that the main difference here is that neither the Function nor the agent interact directly</span><br><span class="line">with the Google Flights API. So how does the API call actually happen?</span><br><span class="line">With functions, the logic and execution of calling the actual API endpoint is offloaded away</span><br><span class="line">from the agent and back to the client-side application as seen in Figure 8 and Figure 9 below.</span><br><span class="line">This offers the developer more granular control over the flow of data in the application. There</span><br><span class="line">are many reasons why a Developer might choose to use functions over Extensions, but a few</span><br><span class="line">common use cases are:</span><br><span class="line">• API calls need to be made at another layer of the application stack, outside of the direct</span><br><span class="line">agent architecture flow (e.g. a middleware system, a front end framework, etc.)</span><br><span class="line">• Security or Authentication restrictions that prevent the agent from calling an API directly</span><br><span class="line">(e.g API is not exposed to the internet, or non-accessible by agent infrastructure)</span><br><span class="line">• Timing or order-of-operations constraints that prevent the agent from making API calls in</span><br><span class="line">real-time. (i.e. batch operations, human-in-the-loop review, etc.)</span><br><span class="line">Agents</span><br><span class="line">September 2024 20</span><br><span class="line">• Additional data transformation logic needs to be applied to the API Response that the</span><br><span class="line">agent cannot perform. For example, consider an API endpoint that doesn’t provide a</span><br><span class="line">filtering mechanism for limiting the number of results returned. Using Functions on the</span><br><span class="line">client-side provides the developer additional opportunities to make these transformations.</span><br><span class="line">• The developer wants to iterate on agent development without deploying additional</span><br><span class="line">infrastructure for the API endpoints (i.e. Function Calling can act like “stubbing” of APIs)</span><br><span class="line">While the difference in internal architecture between the two approaches is subtle as seen in</span><br><span class="line">Figure 8, the additional control and decoupled dependency on external infrastructure makes</span><br><span class="line">Function Calling an appealing option for the Developer.</span><br><span class="line">Figure 8. Delineating client vs. agent side control for extensions and function calling</span><br><span class="line">Agents</span><br><span class="line">September 2024 21</span><br><span class="line">Use cases</span><br><span class="line">A model can be used to invoke functions in order to handle complex, client-side execution</span><br><span class="line">flows for the end user, where the agent Developer might not want the language model to</span><br><span class="line">manage the API execution (as is the case with Extensions). Let’s consider the following</span><br><span class="line">example where an agent is being trained as a travel concierge to interact with users that want</span><br><span class="line">to book vacation trips. The goal is to get the agent to produce a list of cities that we can use</span><br><span class="line">in our middleware application to download images, data, etc. for the user’s trip planning. A</span><br><span class="line">user might say something like:</span><br><span class="line">I’d like to take a ski trip with my family but I’m not sure where to go.</span><br><span class="line">In a typical prompt to the model, the output might look like the following:</span><br><span class="line">Sure, here’s a list of cities that you can consider for family ski trips:</span><br><span class="line">• Crested Butte, Colorado, USA</span><br><span class="line">• Whistler, BC, Canada</span><br><span class="line">• Zermatt, Switzerland</span><br><span class="line">While the above output contains the data that we need (city names), the format isn’t ideal</span><br><span class="line">for parsing. With Function Calling, we can teach a model to format this output in a structured</span><br><span class="line">style (like JSON) that’s more convenient for another system to parse. Given the same input</span><br><span class="line">prompt from the user, an example JSON output from a Function might look like Snippet</span><br><span class="line">5 instead.</span><br><span class="line">Agents</span><br><span class="line">September 2024 22</span><br><span class="line">Unset</span><br><span class="line">function_call &#123;</span><br><span class="line"> name: &quot;display_cities&quot;</span><br><span class="line"> args: &#123;</span><br><span class="line"> &quot;cities&quot;: [&quot;Crested Butte&quot;, &quot;Whistler&quot;, &quot;Zermatt&quot;],</span><br><span class="line"> &quot;preferences&quot;: &quot;skiing&quot;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line">Snippet 5. Sample Function Call payload for displaying a list of cities and user preferences</span><br><span class="line">This JSON payload is generated by the model, and then sent to our Client-side server to do</span><br><span class="line">whatever we would like to do with it. In this specific case, we’ll call the Google Places API to</span><br><span class="line">take the cities provided by the model and look up Images, then provide them as formatted</span><br><span class="line">rich content back to our User. Consider this sequence diagram in Figure 9 showing the above</span><br><span class="line">interaction in step by step detail.</span><br><span class="line">Agents</span><br><span class="line">September 2024 23</span><br><span class="line">Figure 9. Sequence diagram showing the lifecycle of a Function Call</span><br><span class="line">The result of the example in Figure 9 is that the model is leveraged to “fill in the blanks” with</span><br><span class="line">the parameters required for the Client side UI to make the call to the Google Places API. The</span><br><span class="line">Client side UI manages the actual API call using the parameters provided by the model in the</span><br><span class="line">returned Function. This is just one use case for Function Calling, but there are many other</span><br><span class="line">scenarios to consider like:</span><br><span class="line">• You want a language model to suggest a function that you can use in your code, but you</span><br><span class="line">don&#x27;t want to include credentials in your code. Because function calling doesn&#x27;t run the</span><br><span class="line">function, you don&#x27;t need to include credentials in your code with the function information.</span><br><span class="line">Agents</span><br><span class="line">September 2024 24</span><br><span class="line">• You are running asynchronous operations that can take more than a few seconds. These</span><br><span class="line">scenarios work well with function calling because it&#x27;s an asynchronous operation.</span><br><span class="line">• You want to run functions on a device that&#x27;s different from the system producing the</span><br><span class="line">function calls and their arguments.</span><br><span class="line">One key thing to remember about functions is that they are meant to offer the developer</span><br><span class="line">much more control over not only the execution of API calls, but also the entire flow of data</span><br><span class="line">in the application as a whole. In the example in Figure 9, the developer chose to not return</span><br><span class="line">API information back to the agent as it was not pertinent for future actions the agent might</span><br><span class="line">take. However, based on the architecture of the application, it may make sense to return the</span><br><span class="line">external API call data to the agent in order to influence future reasoning, logic, and action</span><br><span class="line">choices. Ultimately, it is up to the application developer to choose what is right for the</span><br><span class="line">specific application.</span><br><span class="line">Function sample code</span><br><span class="line">To achieve the above output from our ski vacation scenario, let’s build out each of the</span><br><span class="line">components to make this work with our gemini-1.5-flash-001 model.</span><br><span class="line">First, we’ll define our display_cities function as a simple Python method.</span><br><span class="line">Agents</span><br><span class="line">September 2024 25</span><br><span class="line">Python</span><br><span class="line">def display_cities(cities: list[str], preferences: Optional[str] = None):</span><br><span class="line">&quot;&quot;&quot;Provides a list of cities based on the user&#x27;s search query and preferences.</span><br><span class="line">Args:</span><br><span class="line"> preferences (str): The user&#x27;s preferences for the search, like skiing,</span><br><span class="line"> beach, restaurants, bbq, etc.</span><br><span class="line"> cities (list[str]): The list of cities being recommended to the user.</span><br><span class="line">Returns:</span><br><span class="line"> list[str]: The list of cities being recommended to the user.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">return cities</span><br><span class="line">Snippet 6. Sample python method for a function that will display a list of cities.</span><br><span class="line">Next, we’ll instantiate our model, build the Tool, then pass in our user’s query and tools to</span><br><span class="line">the model. Executing the code below would result in the output as seen at the bottom of the</span><br><span class="line">code snippet.</span><br><span class="line">Agents</span><br><span class="line">September 2024 26</span><br><span class="line">Python</span><br><span class="line">from vertexai.generative_models import GenerativeModel, Tool, FunctionDeclaration</span><br><span class="line">model = GenerativeModel(&quot;gemini-1.5-flash-001&quot;)</span><br><span class="line">display_cities_function = FunctionDeclaration.from_func(display_cities)</span><br><span class="line">tool = Tool(function_declarations=[display_cities_function])</span><br><span class="line">message = &quot;I’d like to take a ski trip with my family but I’m not sure where</span><br><span class="line">to go.&quot;</span><br><span class="line">res = model.generate_content(message, tools=[tool])</span><br><span class="line">print(f&quot;Function Name: &#123;res.candidates[0].content.parts[0].function_call.name&#125;&quot;)</span><br><span class="line">print(f&quot;Function Args: &#123;res.candidates[0].content.parts[0].function_call.args&#125;&quot;)</span><br><span class="line">&gt; Function Name: display_cities</span><br><span class="line">&gt; Function Args: &#123;&#x27;preferences&#x27;: &#x27;skiing&#x27;, &#x27;cities&#x27;: [&#x27;Aspen&#x27;, &#x27;Vail&#x27;,</span><br><span class="line">&#x27;Park City&#x27;]&#125;</span><br><span class="line">Snippet 7. Building a Tool, sending to the model with a user query and allowing the function call to take place</span><br><span class="line">In summary, functions offer a straightforward framework that empowers application</span><br><span class="line">developers with fine-grained control over data flow and system execution, while effectively</span><br><span class="line">leveraging the agent/model for critical input generation. Developers can selectively choose</span><br><span class="line">whether to keep the agent “in the loop” by returning external data, or omit it based on</span><br><span class="line">specific application architecture requirements.</span><br><span class="line">Agents</span><br><span class="line">September 2024 27</span><br><span class="line">Data stores</span><br><span class="line">Imagine a language model as a vast library of books, containing its training data. But unlike</span><br><span class="line">a library that continuously acquires new volumes, this one remains static, holding only the</span><br><span class="line">knowledge it was initially trained on. This presents a challenge, as real-world knowledge is</span><br><span class="line">constantly evolving. Data Stores address this limitation by providing access to more dynamic</span><br><span class="line">and up-to-date information, and ensuring a model’s responses remain grounded in factuality</span><br><span class="line">and relevance.</span><br><span class="line">Consider a common scenario where a developer might need to provide a small amount of</span><br><span class="line">additional data to a model, perhaps in the form of spreadsheets or PDFs.</span><br><span class="line">Figure 10. How can Agents interact with structured and unstructured data?</span><br><span class="line">Agents</span><br><span class="line">September 2024 28</span><br><span class="line">Data Stores allow developers to provide additional data in its original format to an agent,</span><br><span class="line">eliminating the need for time-consuming data transformations, model retraining, or finetuning. The Data Store converts the incoming document into a set of vector database</span><br><span class="line">embeddings that the agent can use to extract the information it needs to supplement its next</span><br><span class="line">action or response to the user.</span><br><span class="line">Figure 11. Data Stores connect Agents to new real-time data sources of various types.</span><br><span class="line">Implementation and application</span><br><span class="line">In the context of Generative AI agents, Data Stores are typically implemented as a vector</span><br><span class="line">database that the developer wants the agent to have access to at runtime. While we won’t</span><br><span class="line">cover vector databases in depth here, the key point to understand is that they store data</span><br><span class="line">in the form of vector embeddings, a type of high-dimensional vector or mathematical</span><br><span class="line">representation of the data provided. One of the most prolific examples of Data Store usage</span><br><span class="line">with language models in recent times has been the implementation of Retrieval Augmented</span><br><span class="line">Agents</span><br><span class="line">September 2024 29</span><br><span class="line">Generation (RAG) based applications. These applications seek to extend the breadth and</span><br><span class="line">depth of a model’s knowledge beyond the foundational training data by giving the model</span><br><span class="line">access to data in various formats like:</span><br><span class="line">• Website content</span><br><span class="line">• Structured Data in formats like PDF, Word Docs, CSV, Spreadsheets, etc.</span><br><span class="line">• Unstructured Data in formats like HTML, PDF, TXT, etc.</span><br><span class="line">Figure 12. 1-to-many relationship between agents and data stores, which can represent various types of</span><br><span class="line">pre-indexed data</span><br><span class="line">The underlying process for each user request and agent response loop is generally modeled</span><br><span class="line">as seen in Figure 13.</span><br><span class="line">1. A user query is sent to an embedding model to generate embeddings for the query</span><br><span class="line">2. The query embeddings are then matched against the contents of the vector database</span><br><span class="line">using a matching algorithm like SCaNN</span><br><span class="line">3. The matched content is retrieved from the vector database in text format and sent back to</span><br><span class="line">the agent</span><br><span class="line">4. The agent receives both the user query and retrieved content, then formulates a response</span><br><span class="line">or action</span><br><span class="line">Agents</span><br><span class="line">September 2024 30</span><br><span class="line">5. A final response is sent to the user</span><br><span class="line">Figure 13. The lifecycle of a user request and agent response in a RAG based application</span><br><span class="line">The end result is an application that allows the agent to match a user’s query to a known data</span><br><span class="line">store through vector search, retrieve the original content, and provide it to the orchestration</span><br><span class="line">layer and model for further processing. The next action might be to provide a final answer to</span><br><span class="line">the user, or perform an additional vector search to further refine the results.</span><br><span class="line">A sample interaction with an agent that implements RAG with ReAct reasoning/planning can</span><br><span class="line">be seen in Figure 14.</span><br><span class="line">Agents</span><br><span class="line">September 2024 31</span><br><span class="line">Figure 14. Sample RAG based application w/ ReAct reasoning/planning</span><br><span class="line">Agents</span><br><span class="line">September 2024 32</span><br><span class="line">Tools recap</span><br><span class="line">To summarize, extensions, functions and data stores make up a few different tool types</span><br><span class="line">available for agents to use at runtime. Each has their own purpose and they can be used</span><br><span class="line">together or independently at the discretion of the agent developer.</span><br><span class="line">Extensions Function Calling Data Stores</span><br><span class="line">Execution Agent-Side Execution Client-Side Execution Agent-Side Execution</span><br><span class="line">Use Case • Developer wants</span><br><span class="line">agent to control</span><br><span class="line">interactions with the</span><br><span class="line">API endpoints</span><br><span class="line">• Useful when</span><br><span class="line">leveraging native prebuilt Extensions (i.e.</span><br><span class="line">Vertex Search, Code</span><br><span class="line">Interpreter, etc.)</span><br><span class="line">• Multi-hop planning</span><br><span class="line">and API calling</span><br><span class="line">(i.e. the next agent</span><br><span class="line">action depends on</span><br><span class="line">the outputs of the</span><br><span class="line">previous action /</span><br><span class="line">API call)</span><br><span class="line">• Security or</span><br><span class="line">Authentication</span><br><span class="line">restrictions prevent the</span><br><span class="line">agent from calling an</span><br><span class="line">API directly</span><br><span class="line">• Timing constraints or</span><br><span class="line">order-of-operations</span><br><span class="line">constraints that</span><br><span class="line">prevent the agent</span><br><span class="line">from making API calls</span><br><span class="line">in real-time. (i.e. batch</span><br><span class="line">operations, human-inthe-loop review, etc.)</span><br><span class="line">• API that is not exposed</span><br><span class="line">to the internet, or</span><br><span class="line">non-accessible by</span><br><span class="line">Google systems</span><br><span class="line">Developer wants to</span><br><span class="line">implement Retrieval</span><br><span class="line">Augmented Generation</span><br><span class="line">(RAG) with any of the</span><br><span class="line">following data types:</span><br><span class="line">• Website Content from</span><br><span class="line">pre-indexed domains</span><br><span class="line">and URLs</span><br><span class="line">• Structured Data in</span><br><span class="line">formats like PDF,</span><br><span class="line">Word Docs, CSV,</span><br><span class="line">Spreadsheets, etc.</span><br><span class="line">• Relational / NonRelational Databases</span><br><span class="line">• Unstructured Data in</span><br><span class="line">formats like HTML, PDF,</span><br><span class="line">TXT, etc.</span><br><span class="line">Agents</span><br><span class="line">September 2024 33</span><br><span class="line">Enhancing model performance with</span><br><span class="line">targeted learning</span><br><span class="line">A crucial aspect of using models effectively is their ability to choose the right tools when</span><br><span class="line">generating output, especially when using tools at scale in production. While general training</span><br><span class="line">helps models develop this skill, real-world scenarios often require knowledge beyond the</span><br><span class="line">training data. Imagine this as the difference between basic cooking skills and mastering</span><br><span class="line">a specific cuisine. Both require foundational cooking knowledge, but the latter demands</span><br><span class="line">targeted learning for more nuanced results.</span><br><span class="line">To help the model gain access to this type of specific knowledge, several approaches exist:</span><br><span class="line">• In-context learning: This method provides a generalized model with a prompt, tools, and</span><br><span class="line">few-shot examples at inference time which allows it to learn ‘on the fly&#x27; how and when to</span><br><span class="line">use those tools for a specific task. The ReAct framework is an example of this approach in</span><br><span class="line">natural language.</span><br><span class="line">• Retrieval-based in-context learning: This technique dynamically populates the model</span><br><span class="line">prompt with the most relevant information, tools, and associated examples by retrieving</span><br><span class="line">them from external memory. An example of this would be the ‘Example Store’ in Vertex AI</span><br><span class="line">extensions or the data stores RAG based architecture mentioned previously.</span><br><span class="line">• Fine-tuning based learning: This method involves training a model using a larger dataset</span><br><span class="line">of specific examples prior to inference. This helps the model understand when and how to</span><br><span class="line">apply certain tools prior to receiving any user queries.</span><br><span class="line">To provide additional insights on each of the targeted learning approaches, let’s revisit our</span><br><span class="line">cooking analogy.</span><br><span class="line">Agents</span><br><span class="line">September 2024 34</span><br><span class="line">• Imagine a chef has received a specific recipe (the prompt), a few key ingredients (relevant</span><br><span class="line">tools) and some example dishes (few-shot examples) from a customer. Based on this</span><br><span class="line">limited information and the chef’s general knowledge of cooking, they will need to figure</span><br><span class="line">out how to prepare the dish ‘on the fly’ that most closely aligns with the recipe and the</span><br><span class="line">customer’s preferences. This is in-context learning.</span><br><span class="line">• Now let’s imagine our chef in a kitchen that has a well-stocked pantry (external data</span><br><span class="line">stores) filled with various ingredients and cookbooks (examples and tools). The chef is now</span><br><span class="line">able to dynamically choose ingredients and cookbooks from the pantry and better align</span><br><span class="line">to the customer’s recipe and preferences. This allows the chef to create a more informed</span><br><span class="line">and refined dish leveraging both existing and new knowledge. This is retrieval-based</span><br><span class="line">in-context learning.</span><br><span class="line">• Finally, let’s imagine that we sent our chef back to school to learn a new cuisine or set of</span><br><span class="line">cuisines (pre-training on a larger dataset of specific examples). This allows the chef to</span><br><span class="line">approach future unseen customer recipes with deeper understanding. This approach is</span><br><span class="line">perfect if we want the chef to excel in specific cuisines (knowledge domains). This is finetuning based learning.</span><br><span class="line">Each of these approaches offers unique advantages and disadvantages in terms of speed,</span><br><span class="line">cost, and latency. However, by combining these techniques in an agent framework, we can</span><br><span class="line">leverage the various strengths and minimize their weaknesses, allowing for a more robust and</span><br><span class="line">adaptable solution.</span><br><span class="line">Agents</span><br><span class="line">September 2024 35</span><br><span class="line">Agent quick start with LangChain</span><br><span class="line">In order to provide a real-world executable example of an agent in action, we’ll build a quick</span><br><span class="line">prototype with the LangChain and LangGraph libraries. These popular open source libraries</span><br><span class="line">allow users to build customer agents by “chaining” together sequences of logic, reasoning,</span><br><span class="line">and tool calls to answer a user’s query. We’ll use our gemini-1.5-flash-001 model and</span><br><span class="line">some simple tools to answer a multi-stage query from the user as seen in Snippet 8.</span><br><span class="line">The tools we are using are the SerpAPI (for Google Search) and the Google Places API. After</span><br><span class="line">executing our program in Snippet 8, you can see the sample output in Snippet 9.</span><br><span class="line">Agents</span><br><span class="line">September 2024 36</span><br><span class="line">Python</span><br><span class="line">from langgraph.prebuilt import create_react_agent</span><br><span class="line">from langchain_core.tools import tool</span><br><span class="line">from langchain_community.utilities import SerpAPIWrapper</span><br><span class="line">from langchain_community.tools import GooglePlacesTool</span><br><span class="line">os.environ[&quot;SERPAPI_API_KEY&quot;] = &quot;XXXXX&quot;</span><br><span class="line">os.environ[&quot;GPLACES_API_KEY&quot;] = &quot;XXXXX&quot;</span><br><span class="line">@tool</span><br><span class="line">def search(query: str):</span><br><span class="line">&quot;&quot;&quot;Use the SerpAPI to run a Google Search.&quot;&quot;&quot;</span><br><span class="line">search = SerpAPIWrapper()</span><br><span class="line">return search.run(query)</span><br><span class="line">@tool</span><br><span class="line">def places(query: str):</span><br><span class="line">&quot;&quot;&quot;Use the Google Places API to run a Google Places Query.&quot;&quot;&quot;</span><br><span class="line">places = GooglePlacesTool()</span><br><span class="line">return places.run(query)</span><br><span class="line">model = ChatVertexAI(model=&quot;gemini-1.5-flash-001&quot;)</span><br><span class="line">tools = [search, places]</span><br><span class="line">query = &quot;Who did the Texas Longhorns play in football last week? What is the</span><br><span class="line">address of the other team&#x27;s stadium?&quot;</span><br><span class="line">agent = create_react_agent(model, tools)</span><br><span class="line">input = &#123;&quot;messages&quot;: [(&quot;human&quot;, query)]&#125;</span><br><span class="line">for s in agent.stream(input, stream_mode=&quot;values&quot;):</span><br><span class="line">message = s[&quot;messages&quot;][-1]</span><br><span class="line">if isinstance(message, tuple):</span><br><span class="line"> print(message)</span><br><span class="line">else:</span><br><span class="line"> message.pretty_print()</span><br><span class="line">Snippet 8. Sample LangChain and LangGraph based agent with tools</span><br><span class="line">Agents</span><br><span class="line">September 2024 37</span><br><span class="line">Unset</span><br><span class="line">=============================== Human Message ================================</span><br><span class="line">Who did the Texas Longhorns play in football last week? What is the address</span><br><span class="line">of the other team&#x27;s stadium?</span><br><span class="line">================================= Ai Message =================================</span><br><span class="line">Tool Calls: search</span><br><span class="line">Args:</span><br><span class="line">query: Texas Longhorns football schedule</span><br><span class="line">================================ Tool Message ================================</span><br><span class="line">Name: search</span><br><span class="line">&#123;...Results: &quot;NCAA Division I Football, Georgia, Date...&quot;&#125;</span><br><span class="line">================================= Ai Message =================================</span><br><span class="line">The Texas Longhorns played the Georgia Bulldogs last week.</span><br><span class="line">Tool Calls: places</span><br><span class="line">Args:</span><br><span class="line">query: Georgia Bulldogs stadium</span><br><span class="line">================================ Tool Message ================================</span><br><span class="line">Name: places</span><br><span class="line">&#123;...Sanford Stadium Address: 100 Sanford...&#125;</span><br><span class="line">================================= Ai Message =================================</span><br><span class="line">The address of the Georgia Bulldogs stadium is 100 Sanford Dr, Athens, GA</span><br><span class="line">30602, USA.</span><br><span class="line">Snippet 9. Output from our program in Snippet 8</span><br><span class="line">While this is a fairly simple agent example, it demonstrates the foundational components</span><br><span class="line">of Model, Orchestration, and tools all working together to achieve a specific goal. In the</span><br><span class="line">final section, we’ll explore how these components come together in Google-scale managed</span><br><span class="line">products like Vertex AI agents and Generative Playbooks.</span><br><span class="line">Agents</span><br><span class="line">September 2024 38</span><br><span class="line">Production applications with Vertex</span><br><span class="line">AI agents</span><br><span class="line">While this whitepaper explored the core components of agents, building production-grade</span><br><span class="line">applications requires integrating them with additional tools like user interfaces, evaluation</span><br><span class="line">frameworks, and continuous improvement mechanisms. Google’s Vertex AI platform</span><br><span class="line">simplifies this process by offering a fully managed environment with all the fundamental</span><br><span class="line">elements covered earlier. Using a natural language interface, developers can rapidly</span><br><span class="line">define crucial elements of their agents - goals, task instructions, tools, sub-agents for task</span><br><span class="line">delegation, and examples - to easily construct the desired system behavior. In addition, the</span><br><span class="line">platform comes with a set of development tools that allow for testing, evaluation, measuring</span><br><span class="line">agent performance, debugging, and improving the overall quality of developed agents. This</span><br><span class="line">allows developers to focus on building and refining their agents while the complexities of</span><br><span class="line">infrastructure, deployment and maintenance are managed by the platform itself.</span><br><span class="line">In Figure 15 we’ve provided a sample architecture of an agent that was built on the Vertex</span><br><span class="line">AI platform using various features such as Vertex Agent Builder, Vertex Extensions, Vertex</span><br><span class="line">Function Calling and Vertex Example Store to name a few. The architecture includes many of</span><br><span class="line">the various components necessary for a production ready application.</span><br><span class="line">Agents</span><br><span class="line">September 2024 39</span><br><span class="line">Figure 15. Sample end-to-end agent architecture built on Vertex AI platform</span><br><span class="line">You can try a sample of this prebuilt agent architecture from our official documentation.</span><br><span class="line">Agents</span><br><span class="line">September 2024 40</span><br><span class="line">Summary</span><br><span class="line">In this whitepaper we’ve discussed the foundational building blocks of Generative AI</span><br><span class="line">agents, their compositions, and effective ways to implement them in the form of cognitive</span><br><span class="line">architectures. Some key takeaways from this whitepaper include:</span><br><span class="line">1. Agents extend the capabilities of language models by leveraging tools to access realtime information, suggest real-world actions, and plan and execute complex tasks</span><br><span class="line">autonomously. agents can leverage one or more language models to decide when and</span><br><span class="line">how to transition through states and use external tools to complete any number of</span><br><span class="line">complex tasks that would be difficult or impossible for the model to complete on its own.</span><br><span class="line">2. At the heart of an agent’s operation is the orchestration layer, a cognitive architecture that</span><br><span class="line">structures reasoning, planning, decision-making and guides its actions. Various reasoning</span><br><span class="line">techniques such as ReAct, Chain-of-Thought, and Tree-of-Thoughts, provide a framework</span><br><span class="line">for the orchestration layer to take in information, perform internal reasoning, and generate</span><br><span class="line">informed decisions or responses.</span><br><span class="line">3. Tools, such as Extensions, Functions, and Data Stores, serve as the keys to the outside</span><br><span class="line">world for agents, allowing them to interact with external systems and access knowledge</span><br><span class="line">beyond their training data. Extensions provide a bridge between agents and external APIs,</span><br><span class="line">enabling the execution of API calls and retrieval of real-time information. functions provide</span><br><span class="line">a more nuanced control for the developer through the division of labor, allowing agents</span><br><span class="line">to generate Function parameters which can be executed client-side. Data Stores provide</span><br><span class="line">agents with access to structured or unstructured data, enabling data-driven applications.</span><br><span class="line">The future of agents holds exciting advancements and we’ve only begun to scratch the</span><br><span class="line">surface of what is possible. As tools become more sophisticated and reasoning capabilities</span><br><span class="line">are enhanced, agents will be empowered to solve increasingly complex problems.</span><br><span class="line">Furthermore, the strategic approach of ‘agent chaining’ will continue to gain momentum. By</span><br><span class="line">Agents</span><br><span class="line">September 2024 41</span><br><span class="line">combining specialized agents - each excelling in a particular domain or task - we can create</span><br><span class="line">a ‘mixture of agent experts’ approach, capable of delivering exceptional results across</span><br><span class="line">various industries and problem areas.</span><br><span class="line">It’s important to remember that building complex agent architectures demands an iterative</span><br><span class="line">approach. Experimentation and refinement are key to finding solutions for specific business</span><br><span class="line">cases and organizational needs. No two agents are created alike due to the generative nature</span><br><span class="line">of the foundational models that underpin their architecture. However, by harnessing the</span><br><span class="line">strengths of each of these foundational components, we can create impactful applications</span><br><span class="line">that extend the capabilities of language models and drive real-world value.</span><br><span class="line">Agents</span><br><span class="line">September 2024 42</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/blog/smolagents</span><br><span class="line">Introducing smolagents, a simple library to build agents</span><br><span class="line">Published December 31, 2024</span><br><span class="line">Aymeric Roucher&#x27;s avatar</span><br><span class="line">m-ric</span><br><span class="line">Aymeric Roucher</span><br><span class="line">Merve Noyan&#x27;s avatar</span><br><span class="line">merve</span><br><span class="line">Merve Noyan</span><br><span class="line">Thomas Wolf&#x27;s avatar</span><br><span class="line">thomwolf</span><br><span class="line">Thomas Wolf</span><br><span class="line"></span><br><span class="line">Today we are launching smolagents, a very simple library that unlocks agentic capabilities for language models. Here’s a glimpse:</span><br><span class="line">from smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel</span><br><span class="line"></span><br><span class="line">agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=HfApiModel())</span><br><span class="line"></span><br><span class="line">agent.run(&quot;How many seconds would it take for a leopard at full speed to run through Pont des Arts?&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Table of Contents</span><br><span class="line">🤔 What are agents?</span><br><span class="line">✅ When to use agents / ⛔ when to avoid them</span><br><span class="line">Code agents</span><br><span class="line">Introducing smolagents: making agents simple 🥳</span><br><span class="line">Building an agent</span><br><span class="line">How strong are open models for agentic workflows?</span><br><span class="line">Next steps 🚀</span><br><span class="line">🤔 What are agents?</span><br><span class="line">Any efficient system using AI will need to provide LLMs some kind of access to the real world: for instance the possibility to call a search tool to get external information, or to act on certain programs in order to solve a task. In other words, LLMs should have agency. Agentic programs are the gateway to the outside world for LLMs.</span><br><span class="line"></span><br><span class="line">AI Agents are programs where LLM outputs control the workflow.</span><br><span class="line"></span><br><span class="line">Any system leveraging LLMs will integrate the LLM outputs into code. The influence of the LLM&#x27;s input on the code workflow is the level of agency of LLMs in the system.</span><br><span class="line"></span><br><span class="line">Note that with this definition, &quot;agent&quot; is not a discrete, 0 or 1 definition: instead, &quot;agency&quot; evolves on a continuous spectrum, as you give more or less power to the LLM on your workflow.</span><br><span class="line"></span><br><span class="line">The table below illustrates how agency varies across systems:</span><br><span class="line"></span><br><span class="line">Agency Level	Description	How that&#x27;s called	Example Pattern</span><br><span class="line">☆☆☆	LLM output has no impact on program flow	Simple processor	process_llm_output(llm_response)</span><br><span class="line">★☆☆	LLM output determines basic control flow	Router	if llm_decision(): path_a() else: path_b()</span><br><span class="line">★★☆	LLM output determines function execution	Tool call	run_function(llm_chosen_tool, llm_chosen_args)</span><br><span class="line">★★★	LLM output controls iteration and program continuation	Multi-step Agent	while llm_should_continue(): execute_next_step()</span><br><span class="line">★★★	One agentic workflow can start another agentic workflow	Multi-Agent	if llm_trigger(): execute_agent()</span><br><span class="line">The multi-step agent has this code structure:</span><br><span class="line"></span><br><span class="line">memory = [user_defined_task]</span><br><span class="line">while llm_should_continue(memory): # this loop is the multi-step part</span><br><span class="line">    action = llm_get_next_action(memory) # this is the tool-calling part</span><br><span class="line">    observations = execute_action(action)</span><br><span class="line">    memory += [action, observations]</span><br><span class="line"></span><br><span class="line">So this system runs in a loop, executing a new action at each step (the action can involve calling some pre-determined tools that are just functions), until its observations make it apparent that a satisfactory state has been reached to solve the given task. Here’s an example of how a multi-step agent can solve a simple math question:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">✅ When to use agents / ⛔ when to avoid them</span><br><span class="line">Agents are useful when you need an LLM to determine the workflow of an app. But they’re often overkill. The question is: do I really need flexibility in the workflow to efficiently solve the task at hand? If the pre-determined workflow falls short too often, that means you need more flexibility. Let&#x27;s take an example: say you&#x27;re making an app that handles customer requests on a surfing trip website.</span><br><span class="line"></span><br><span class="line">You could know in advance that the requests will belong to either of 2 buckets (based on user choice), and you have a predefined workflow for each of these 2 cases.</span><br><span class="line"></span><br><span class="line">Want some knowledge on the trips? ⇒ give them access to a search bar to search your knowledge base</span><br><span class="line">Wants to talk to sales? ⇒ let them type in a contact form.</span><br><span class="line">If that deterministic workflow fits all queries, by all means just code everything! This will give you a 100% reliable system with no risk of error introduced by letting unpredictable LLMs meddle in your workflow. For the sake of simplicity and robustness, it&#x27;s advised to regularize towards not using any agentic behaviour.</span><br><span class="line"></span><br><span class="line">But what if the workflow can&#x27;t be determined that well in advance?</span><br><span class="line"></span><br><span class="line">For instance, a user wants to ask : &quot;I can come on Monday, but I forgot my passport so risk being delayed to Wednesday, is it possible to take me and my stuff to surf on Tuesday morning, with a cancellation insurance?&quot; This question hinges on many factors, and probably none of the predetermined criteria above will suffice for this request.</span><br><span class="line"></span><br><span class="line">If the pre-determined workflow falls short too often, that means you need more flexibility.</span><br><span class="line"></span><br><span class="line">That is where an agentic setup helps.</span><br><span class="line"></span><br><span class="line">In the above example, you could just make a multi-step agent that has access to a weather API for weather forecasts, Google Maps API to compute travel distance, an employee availability dashboard and a RAG system on your knowledge base.</span><br><span class="line"></span><br><span class="line">Until recently, computer programs were restricted to pre-determined workflows, trying to handle complexity by piling up if/else switches. They focused on extremely narrow tasks, like &quot;compute the sum of these numbers&quot; or &quot;find the shortest path in this graph&quot;. But actually, most real-life tasks, like our trip example above, do not fit in pre-determined workflows. Agentic systems open up the vast world of real-world tasks to programs!</span><br><span class="line"></span><br><span class="line">Code agents</span><br><span class="line">In a multi-step agent, at each step, the LLM can write an action, in the form of some calls to external tools. A common format (used by Anthropic, OpenAI, and many others) for writing these actions is generally different shades of &quot;writing actions as a JSON of tools names and arguments to use, which you then parse to know which tool to execute and with which arguments&quot;.</span><br><span class="line"></span><br><span class="line">Multiple research papers have shown that having the tool calling LLMs in code is much better.</span><br><span class="line"></span><br><span class="line">The reason for this simply that we crafted our code languages specifically to be the best possible way to express actions performed by a computer. If JSON snippets were a better expression, JSON would be the top programming language and programming would be hell on earth.</span><br><span class="line"></span><br><span class="line">The figure below, taken from Executable Code Actions Elicit Better LLM Agents, illustrate some advantages of writing actions in code:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Writing actions in code rather than JSON-like snippets provides better:</span><br><span class="line"></span><br><span class="line">Composability: could you nest JSON actions within each other, or define a set of JSON actions to re-use later, the same way you could just define a python function?</span><br><span class="line">Object management: how do you store the output of an action like generate_image in JSON?</span><br><span class="line">Generality: code is built to express simply anything you can have a computer do.</span><br><span class="line">Representation in LLM training data: plenty of quality code actions is already included in LLMs’ training data which means they’re already trained for this!</span><br><span class="line">Introducing smolagents: making agents simple 🥳</span><br><span class="line">We built smolagents with these objectives:</span><br><span class="line"></span><br><span class="line">✨ Simplicity: the logic for agents fits in ~thousand lines of code (see this file). We kept abstractions to their minimal shape above raw code!</span><br><span class="line"></span><br><span class="line">🧑‍💻 First-class support for Code Agents, i.e. agents that write their actions in code (as opposed to &quot;agents being used to write code&quot;). To make it secure, we support executing in sandboxed environments via E2B.</span><br><span class="line"></span><br><span class="line">On top of this CodeAgent class, we still support the standard ToolCallingAgent that writes actions as JSON/text blobs.</span><br><span class="line">🤗 Hub integrations: you can share and load tools to/from the Hub, and more is to come!</span><br><span class="line"></span><br><span class="line">🌐 Support for any LLM: it supports models hosted on the Hub loaded in their transformers version or through our inference API, but also supports models from OpenAI, Anthropic and many others via our LiteLLM integration.</span><br><span class="line"></span><br><span class="line">smolagents is the successor to transformers.agents, and will be replacing it as transformers.agents gets deprecated in the future.</span><br><span class="line"></span><br><span class="line">Building an agent</span><br><span class="line">To build an agent, you need at least two elements:</span><br><span class="line"></span><br><span class="line">tools: a list of tools the agent has access to</span><br><span class="line">model: an LLM that will be the engine of your agent.</span><br><span class="line">For the model, you can use any LLM, either open models using our HfApiModel class, that leverages Hugging Face&#x27;s free inference API (as shown in the leopard example above), or you can use LiteLLMModel to leverage litellm and pick from a list of 100+ different cloud LLMs.</span><br><span class="line"></span><br><span class="line">For the tool, you can just make a function with type hints on inputs and outputs, and docstrings giving descriptions for inputs, and use the @tool decorator to make it a tool.</span><br><span class="line"></span><br><span class="line">Here’s how to make a custom tool that gets travel times from Google Maps, and how to use it into a travel planner agent:</span><br><span class="line"></span><br><span class="line">from typing import Optional</span><br><span class="line">from smolagents import CodeAgent, HfApiModel, tool</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_travel_duration(start_location: str, destination_location: str, departure_time: Optional[int] = None) -&gt; str:</span><br><span class="line">    &quot;&quot;&quot;Gets the travel time in car between two places.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        start_location: the place from which you start your ride</span><br><span class="line">        destination_location: the place of arrival</span><br><span class="line">        departure_time: the departure time, provide only a `datetime.datetime` if you want to specify this</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    import googlemaps # All imports are placed within the function, to allow for sharing to Hub.</span><br><span class="line">    import os</span><br><span class="line"></span><br><span class="line">    gmaps = googlemaps.Client(os.getenv(&quot;GMAPS_API_KEY&quot;))</span><br><span class="line"></span><br><span class="line">    if departure_time is None:</span><br><span class="line">        from datetime import datetime</span><br><span class="line">        departure_time = datetime(2025, 1, 6, 11, 0)</span><br><span class="line"></span><br><span class="line">    directions_result = gmaps.directions(</span><br><span class="line">        start_location,</span><br><span class="line">        destination_location,</span><br><span class="line">        mode=&quot;transit&quot;,</span><br><span class="line">        departure_time=departure_time</span><br><span class="line">    )</span><br><span class="line">    return directions_result[0][&quot;legs&quot;][0][&quot;duration&quot;][&quot;text&quot;]</span><br><span class="line"></span><br><span class="line">agent = CodeAgent(tools=[get_travel_duration], model=HfApiModel(), additional_authorized_imports=[&quot;datetime&quot;])</span><br><span class="line"></span><br><span class="line">agent.run(&quot;Can you give me a nice one-day trip around Paris with a few locations and the times? Could be in the city or outside, but should fit in one day. I&#x27;m travelling only via public transportation.&quot;)</span><br><span class="line"></span><br><span class="line">After a few steps of gathering travel times and running calculations, the agent returns this final proposition:</span><br><span class="line"></span><br><span class="line">Out - Final answer: Here&#x27;s a suggested one-day itinerary for Paris:</span><br><span class="line">Visit Eiffel Tower at 9:00 AM - 10:30 AM</span><br><span class="line">Visit Louvre Museum at 11:00 AM - 12:30 PM</span><br><span class="line">Visit Notre-Dame Cathedral at 1:00 PM - 2:30 PM</span><br><span class="line">Visit Palace of Versailles at 3:30 PM - 5:00 PM</span><br><span class="line">Note: The travel time to the Palace of Versailles is approximately 59</span><br><span class="line">minutes from Notre-Dame Cathedral, so be sure to plan your day accordingly.</span><br><span class="line"></span><br><span class="line">After building a tool, sharing it to the Hub is as simple as:</span><br><span class="line"></span><br><span class="line">get_travel_duration.push_to_hub(&quot;&#123;your_username&#125;/get-travel-duration-tool&quot;)</span><br><span class="line"></span><br><span class="line">You can see the result under this space. You can check the logic for the tool under the file tool.py in the space. As you can see, the tool was actually exported to a class inheriting from class Tool, which is the underlying structure for all our tools.</span><br><span class="line"></span><br><span class="line">How strong are open models for agentic workflows?</span><br><span class="line">We&#x27;ve created CodeAgent instances with some leading models, and compared them on this benchmark that gathers questions from a few different benchmarks to propose a varied blend of challenges.</span><br><span class="line"></span><br><span class="line">Find the benchmark here for more detail on the agentic setup used, and see a comparison of code agents versus tool calling agents (spoilers: code works better).</span><br><span class="line"></span><br><span class="line">benchmark of different models on agentic workflows</span><br><span class="line"></span><br><span class="line">This comparison shows that open source models can now take on the best closed models!</span><br><span class="line"></span><br><span class="line">Next steps 🚀</span><br><span class="line">Start with the guided tour to familiarize yourself with the library.</span><br><span class="line">Study more in-depth tutorials to learn more on tools or general best practices.</span><br><span class="line">Dive into examples to set up specific systems: text-to-SQL, agentic RAG or multi-agent orchestration.</span><br><span class="line">Read more on agents:</span><br><span class="line">This excellent blog post by Anthropic gives solid general knowledge.</span><br><span class="line">This collection gathers the most impactful research papers on agents.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/Thytu/Agentarium</span><br><span class="line">25/1/2</span><br><span class="line">Agentarium is a new Python framework for managing and orchestrating AI agents.</span><br><span class="line">Features include (from the repo):</span><br><span class="line">• 🤖 Advanced Agent Management: Create and orchestrate multiple AI agents with different roles and capabilities</span><br><span class="line">• 🔄 Robust Interaction Management: Coordinate complex interactions between agents</span><br><span class="line">• 💾 Checkpoint System: Save and restore agent states and interactions</span><br><span class="line">• 📊 Data Generation: Generate synthetic data through agent interactions</span><br><span class="line">• ⚡ Performance Optimized: Built for efficiency and scalability</span><br><span class="line">• 🌍 Flexible Environment Configuration: Define custom environments with YAML configuration files</span><br><span class="line">• 🛠️ Extensible Architecture: Easy to extend and customize for your specific needs</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/Byaidu/PDFMathTranslate</span><br><span class="line">24/12/1</span><br><span class="line">PDF scientific paper translation and bilingual comparison.</span><br><span class="line"></span><br><span class="line">📊 Preserve formulas, charts, table of contents, and annotations (preview).</span><br><span class="line">🌐 Support multiple languages, and diverse translation services.</span><br><span class="line">🤖 Provides commandline tool, interactive user interface, and Docker</span><br><span class="line">Feel free to provide feedback in GitHub Issues, Telegram Group or QQ Group.</span><br><span class="line"></span><br><span class="line">For details on how to contribute, please consult the Contribution Guide.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2412.09764</span><br><span class="line">Memory Layers at Scale</span><br><span class="line">Vincent-Pierre Berges∗</span><br><span class="line">, Barlas Oğuz∗</span><br><span class="line">, Daniel Haziza, Wen-tau Yih, Luke Zettlemoyer, Gargi Ghosh</span><br><span class="line">Meta FAIR</span><br><span class="line">∗Main authors</span><br><span class="line">Memory layers use a trainable key-value lookup mechanism to add extra parameters to a model without</span><br><span class="line">increasing FLOPs. Conceptually, sparsely activated memory layers complement compute-heavy dense</span><br><span class="line">feed-forward layers, providing dedicated capacity to store and retrieve information cheaply. This</span><br><span class="line">work takes memory layers beyond proof-of-concept, proving their utility at contemporary scale. On</span><br><span class="line">downstream tasks, language models augmented with our improved memory layer outperform dense</span><br><span class="line">models with more than twice the computation budget, as well as mixture-of-expert models when</span><br><span class="line">matched for both compute and parameters. We find gains are especially pronounced for factual tasks.</span><br><span class="line">We provide a fully parallelizable memory layer implementation, demonstrating scaling laws with up to</span><br><span class="line">128B memory parameters, pretrained to 1 trillion tokens, comparing to base models with up to 8B</span><br><span class="line">parameters.</span><br><span class="line">Date: December 23, 2024</span><br><span class="line">Open Source</span><br><span class="line">Sharing new research, models, and datasets from Meta FAIR</span><br><span class="line">December 12, 2024</span><br><span class="line">•</span><br><span class="line">11 minute read</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Takeaways</span><br><span class="line"></span><br><span class="line">Today, Meta FAIR is releasing several new research artifacts that highlight our recent innovations in developing agents, robustness and safety, and architectures that facilitate machine learning.</span><br><span class="line">The work we’re sharing advances our goal of achieving advanced machine intelligence and includes Meta Motivo, a foundation model for controlling the behavior of virtual embodied agents, and Meta Video Seal, an open source model for video watermarking.</span><br><span class="line">We aim to democratize access to state-of-the-art technologies that transform our interaction with the physical world, which is why we&#x27;re committed to fostering a collaborative and open ecosystem that accelerates progress and discovery.</span><br><span class="line">As we continue to work towards our goal of achieving advanced machine intelligence, we want to share our progress with the research community so they can build upon our work. Today, we’re excited to release some of the latest research, code, models, and datasets from Meta Fundamental AI Research (FAIR). The artifacts we’re sharing today focus on building more capable agents, robustness and safety, and architecture innovations that enable models to learn new information more effectively and scale beyond current limits.</span><br><span class="line"></span><br><span class="line">In this release, we’re sharing a demo and code for Meta Video Seal, an open source model work video watermarking that builds on the popular Meta Audio Seal work we shared last year. We’re also sharing a variety of other artifacts, including a foundation model for controlling the behavior of virtual embodied agents, a method for scaling memory layers that will enable more factual information, and code to help models become more socially intelligent. There’s plenty more to explore in this post with nine total projectsand artifacts ready for people to download and start using today.</span><br><span class="line"></span><br><span class="line">This work supports our long and proven track record of sharing open reproducible science with the community. By publicly sharing our early research work, we hope to inspire iterations and ultimately help advance AI in a responsible way. As always, we look forward to seeing what the community will build using these new releases and continuing the dialogue about how we can all advance AI together responsibly and build for the greater good.</span><br><span class="line"></span><br><span class="line">Meta Motivo</span><br><span class="line"></span><br><span class="line">Unsupervised reinforcement learning involves pre-training models to solve a wide range of downstream tasks in complex environments. Most methods require highly curated interaction datasets and often rely on unsupervised losses that lead to policies that may not align well with target tasks. Today, we’re sharing Meta Motivo, a first-of-its-kind behavioral foundation model that controls the movements of a virtual embodied humanoid agent to perform complex tasks.</span><br><span class="line"></span><br><span class="line">Meta Motivo is trained with a novel algorithm that leverages an unlabeled dataset of motions to ground unsupervised reinforcement learning towards learning human-like behaviors while retaining zero-shot inference capabilities. The key technical novelty of our algorithm is to learn a representation that can be used to embed states, motions, and rewards into the same latent space. As a result, Meta Motivo is able to solve a wide range of whole-body control tasks, including motion tracking, goal pose reaching, and reward optimization, without any additional training or planning.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Meta Motivo achieves competitive performance compared to task-specific methods and outperforms state-of-the-art unsupervised reinforcement learning and model-based baselines, while exhibiting more human-like behaviors. The model also displays a surprising level of robustness to changes in the environment, such as gravity, wind, or direct perturbations, despite not being trained for them.</span><br><span class="line"></span><br><span class="line">In the future, we believe this research could pave the way for fully embodied agents in the Metaverse, leading to more lifelike NPCs, democratization of character animation, and new types of immersive experiences.</span><br><span class="line"></span><br><span class="line">Read the paper</span><br><span class="line"></span><br><span class="line">Try the demo</span><br><span class="line"></span><br><span class="line">Download the code and model</span><br><span class="line"></span><br><span class="line">Meta Video Seal</span><br><span class="line"></span><br><span class="line">While AI tools can help bring the world closer together, it’s important that we implement safeguards to mitigate the risks of imitation, manipulation, and other forms of misuse that can undermine their benefits. Post-hoc watermarking is a crucial step towards better traceability for content and AI models.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Today, we’re releasing Meta Video Seal, a state-of-the art comprehensive framework for neural video watermarking. Video Seal adds a watermark (with an optional hidden message) into videos that is imperceptible to the naked eye and can later be uncovered to determine a video’s origin. The watermark has proven resilience against common video editing efforts like blurring or cropping, as well as compression algorithms commonly used when sharing content online. We’re publicly releasing the Video Seal model under a permissive license, along with a research paper, training code, and inference code. A demo is also available to try the model out interactively.</span><br><span class="line"></span><br><span class="line">Along with Video Seal, we’re also releasing Meta Omni Seal Bench, a leaderboard dedicated to neural watermarking covering several modalities, enabling the research community to easily test and add their own work in the field. We’re also re-releasing our Meta Watermark Anything model under a permissive license and will organize a workshop on watermarking at ICLR in 2025.</span><br><span class="line"></span><br><span class="line">This research is a testimony to our commitment to responsible AI. We hope that other researchers and developers will join our efforts by integrating watermarking capabilities when building generative AI models. Watermark Anything, Video Seal, and Audio Seal—our previous work on post-hoc audio watermarking—are now all available for download and ready to be integrated.</span><br><span class="line"></span><br><span class="line">Read the paper</span><br><span class="line"></span><br><span class="line">Try the demo</span><br><span class="line"></span><br><span class="line">Download the Video Seal code and model</span><br><span class="line"></span><br><span class="line">Download the Watermark Anything code and model</span><br><span class="line"></span><br><span class="line">View the Omni Seal Bench leaderboard</span><br><span class="line"></span><br><span class="line">Flow Matching guide and codebase, a Meta FAIR release</span><br><span class="line"></span><br><span class="line">Flow Matching is a state-of-the-art generative paradigm for many modalities including generation of images, videos, audio, music, 3D structures like proteins, and more. Our method has already replaced classical diffusion in many generative applications at Meta, including Meta Movie Gen, Meta Audiobox, and Meta Melody Flow, and across the industry in works such as Stable-Diffusion-3, Flux, Fold-Flow, and Physical Intelligence Pi_0. Flow Matching provides a simple yet flexible generative AI framework, improving performance and efficiency while allowing easy generalization to complex data. Today, we’re sharing a paper and code, including core implementations of both continuous and discrete Flow Matching, alongside state-of-the-art training scripts to enable the research community to easily use and iterate on the Flow Matching method. By publicly sharing this work, we hope to inspire wider adoption of Flow Matching and enable people to use it in their own generative projects.</span><br><span class="line"></span><br><span class="line">Read the paper</span><br><span class="line"></span><br><span class="line">Download the code</span><br><span class="line"></span><br><span class="line">Meta Explore Theory-of-Mind</span><br><span class="line"></span><br><span class="line">A key aspect of our social intelligence enables us to reason about the thoughts and beliefs of other agents, both human and artificial. Existing Theory-of-Mind (ToM) datasets have limitations, focusing solely on evaluation and depicting only a narrow range of interactions. To address this and move closer to achieving advanced machine intelligence, we introduce Meta Explore Theory-of-Mind, a program-guided adversarial data generation for theory of mind reasoning. Our novel framework enables the generation of diverse, challenging, and scalable ToM reasoning data for both training and evaluation, which will help accelerate progress in this critical area of research.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Explore Theory-of-Mind generates robust and reliable stories that push the limits of large language models (LLMs), making it ideal for evaluating frontier models or fine-tuning data, resulting in significant improvements on classic theory of mind benchmarks. Our first-of-its-kind approach led to a 27-point accuracy improvement on the commonly used ToMi benchmark when fine-tuning a Llama-3.1 7B model, which means unprecedented accuracy in evaluating the theory of mind training data. Explore Theory-of-Mind can be used to generate datasets for improving LLMs, enhance goal-oriented scenarios, and collect interaction datasets, while also serving as a benchmark for evaluating LLM performance.</span><br><span class="line"></span><br><span class="line">Read the paper</span><br><span class="line"></span><br><span class="line">Download the code</span><br><span class="line"></span><br><span class="line">Download the dataset</span><br><span class="line"></span><br><span class="line">Meta Large Concept Models</span><br><span class="line"></span><br><span class="line">As we work toward advanced machine intelligence, models will need to be able to reason across languages and modalities and to excel at long-form generational capabilities that require explicit hierarchical thinking, such as writing an essay. Current mainstream language modeling approaches typically operate at the token level and don’t explicitly reason in a hierarchical manner.</span><br><span class="line"></span><br><span class="line">Today, we’re introducing a fundamentally different training paradigm for language modeling: the Large Concept Model (LCM). The core idea of the LCM is to decouple reasoning from language representation, and it’s inspired by how humans can plan high-level thoughts to communicate. For example, when giving a presentation multiple times, a presenter always has the same series of ideas they want to convey (materialized by their slides projected on screen), but their exact choice of words might vary from one run to the other.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Guided by that principle, the LCM is a significant departure from a typical LLM. Rather than predicting the next token, the LCM is trained to predict the next concept or high-level idea, represented by a full sentence in a multimodal and multilingual embedding space. Our work explores how predictions can be made for text in such a continuous space. Overall, the LCM outperforms or matches recent LLMs in the pure generative task of summarization, offers strong zero-shot generalization to unseen languages, and is more computationally efficient as input context grows. We hope the research community uses this work to improve language models that can operate on any modality or language, in an explicit hierarchical manner.</span><br><span class="line"></span><br><span class="line">Read the paper</span><br><span class="line"></span><br><span class="line">Download the code</span><br><span class="line"></span><br><span class="line">Meta Dynamic Byte Latent Transformer</span><br><span class="line"></span><br><span class="line">Language models assume text has been tokenized in a heuristic preprocessing step, breaking words into smaller units that are easier to process. This limits end-to-end learning, is difficult to optimize in practice, and can hurt performance on rare text sequences. To address this, we’re introducing Dynamic Byte Latent Transformer, a hierarchical byte-level (tokenizer-free) model with dynamic patching schemes that are able to operate over bytes—without any tokenization heuristics—while also improving efficiency for long sequences during training and inference.</span><br><span class="line"></span><br><span class="line">Dynamic Byte Latent Transformer outperforms tokenizer-based models across the board in terms of robustness, with a seven point advantage on average, and excels at processing longtail and rare sequences of unseen symbols. By sharing this work, we hope to accelerate advancements that will enable us to better reason over a variety of domains that are important to advanced machine intelligence, including low resource languages, coding, and factuality.</span><br><span class="line"></span><br><span class="line">Read the paper</span><br><span class="line"></span><br><span class="line">Download the code</span><br><span class="line"></span><br><span class="line">Meta Memory Layers</span><br><span class="line"></span><br><span class="line">Parametric memory, the repository of factual information stored in the weights on a neural network during pretraining, enables LLMs to understand complex concepts and linguistic nuances. As current scaling methods approach their limit of efficient scaling, new architectures that enable models to learn information more effectively must be explored. Today, we’re sharing a research paper and code for Meta Memory Layers at Scale, a method for scaling memory layers that enables an increase in factuality against commonly used benchmarks as we work toward achieving advanced machine intelligence.</span><br><span class="line"></span><br><span class="line">Memory Layers use a trainable key-value lookup mechanism to add extra parameters to a model without increasing FLOPs. Sparsely activated memory layers complement the compute-heavy nature of dense feed-forward layers, providing dedicated capacity to store and retrieve information cheaply. On downstream tasks, language models augmented with our improved memory layer outperform dense models with more than twice the computation budget, as well as MoE models when matched for both compute and parameters.</span><br><span class="line"></span><br><span class="line">Contrary to the prevailing perception in the field that sparse memory architectures cannot be scaled competitively, we demonstrate efficient scaling of sparse memory layers up to 128 billion parameters and 8B base models, with significant improvements at comparable compute across the board for commonly used factuality benchmarks.</span><br><span class="line"></span><br><span class="line">Read the paper</span><br><span class="line"></span><br><span class="line">Download the code</span><br><span class="line"></span><br><span class="line">Meta Image Diversity Modeling</span><br><span class="line"></span><br><span class="line">This year, FAIR has focused on research to better understand and develop new methods for the safe development of image generation models. Today, we’re announcing updates on this research and releasing a comprehensive evaluation toolbox for text-to-image generative models. The image generation model we’ve developed through the course of this research builds on our prior research on generative models’ architectures and losses and prioritizes generating images that are representative of the physical world while maintaining competitive image quality with state-of-the-art models.</span><br><span class="line"></span><br><span class="line">To further the research into new methods and techniques for responsible development, we’re collaborating with external experts, whom we’re inviting to use our model to carry out research in areas that can help us to improve the safety and responsibility across image diversity modeling. This initiative highlights our commitment to collaborating with the wider AI research community to collectively advance AI responsibility.</span><br><span class="line"></span><br><span class="line">Additionally, we will be open sourcing a comprehensive evaluation toolbox for text-to-image generative models to improve the ease and reproducibility of image generation benchmarking while promoting interpretable takeaways that inform future responsible text-to-image research.</span><br><span class="line"></span><br><span class="line">Through our continued work, we hope to better understand and offer new methods for responsible development of image generative models that can be adopted by the broader research community.</span><br><span class="line"></span><br><span class="line">Read the paper</span><br><span class="line"></span><br><span class="line">Download the code</span><br><span class="line"></span><br><span class="line">Meta CLIP 1.2</span><br><span class="line"></span><br><span class="line">We’re excited to release Meta CLIP 1.2, a milestone in our ongoing efforts to develop a high-performance vision-language encoder. We have been working on advanced algorithms to effectively curate and align vast amounts of image-text data, unlocking the learning of human knowledge about the world. This enables our models to learn efficiently and accurately, capturing the nuances of fine-grained mapping between image and language semantics.</span><br><span class="line"></span><br><span class="line">Large-scale, high-quality, and diverse datasets are essential for building foundation models that can learn about the world. Meta CLIP is our effort towards building such datasets and foundation models. To ensure a high-quality and safe vision-language encoder foundation model, we’ve developed algorithms to effectively curate and align data with human knowledge from vast data pools, enabling our models to learn efficiently and cover all possibilities. We also conducted rigorous data research while applying robust integrity and privacy-protective measures.</span><br><span class="line"></span><br><span class="line">By releasing our data algorithms, training recipes, and foundation models trained on our curated dataset, we’re providing researchers and developers with the tools they need to advance the field of vision-language understanding. These foundation models can be used as vision encoding for MLLM, multi-modal embedding for retrieval, and zero-shot classification, while serving as a starting point for research on data quality. Additionally, our algorithms and training methods can also be used to create high-quality, large-scale, CLIP-like datasets from scratch, which can help with new research or production use cases.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/NVIDIA/nv-ingest</span><br><span class="line">1/3/25</span><br><span class="line">NVIDIA-Ingest: Multi-modal data extraction</span><br><span class="line">NVIDIA-Ingest is a scalable, performance-oriented document content and metadata extraction microservice. Including support for parsing PDFs, Word and PowerPoint documents, it uses specialized NVIDIA NIM microservices to find, contextualize, and extract text, tables, charts and images for use in downstream generative applications.</span><br><span class="line"></span><br><span class="line">NVIDIA Ingest enables parallelization of the process of splitting documents into pages where contents are classified (as tables, charts, images, text), extracted into discrete content, and further contextualized via optical character recognition (OCR) into a well defined JSON schema. From there, NVIDIA Ingest can optionally manage computation of embeddings for the extracted content, and also optionally manage storing into a vector database Milvus.</span><br><span class="line"></span><br><span class="line">Table of Contents</span><br><span class="line">Introduction</span><br><span class="line">Prerequisites</span><br><span class="line">Quickstart</span><br><span class="line">Repo Structure</span><br><span class="line">Notices</span><br><span class="line">Introduction</span><br><span class="line">What NVIDIA-Ingest is ✔️</span><br><span class="line">A microservice that:</span><br><span class="line"></span><br><span class="line">Accepts a JSON Job description, containing a document payload, and a set of ingestion tasks to perform on that payload.</span><br><span class="line">Allows the results of a Job to be retrieved; the result is a JSON dictionary containing a list of Metadata describing objects extracted from the base document, as well as processing annotations and timing/trace data.</span><br><span class="line">Supports PDF, Docx, pptx, and images.</span><br><span class="line">Supports multiple methods of extraction for each document type in order to balance trade-offs between throughput and accuracy. For example, for PDF documents we support extraction via pdfium, Unstructured.io, and Adobe Content Extraction Services.</span><br><span class="line">Supports various types of pre and post processing operations, including text splitting and chunking; transform, and filtering; embedding generation, and image offloading to storage.</span><br><span class="line">What NVIDIA-Ingest is not ✖️</span><br><span class="line">A service that:</span><br><span class="line"></span><br><span class="line">Runs a static pipeline or fixed set of operations on every submitted document.</span><br><span class="line">Acts as a wrapper for any specific document parsing library.</span><br><span class="line"></span><br><span class="line">👏 NVIDIA released ingest, a microservice designed for extracting content and metadata efficiently from thousands of documents like PDFs, Word, and PowerPoint. It uses NVIDIA NIM microservices to efficiently parse and extract text, tables, charts, and images, making it ideal for downstream applications.</span><br><span class="line">NVIDIA Ingest splits documents into pages, classifies content, and uses optical character recognition (OCR) to convert it into a structured JSON format. It can also compute embeddings for the extracted content and store them in a vector database like Milvus.</span><br><span class="line">The service supports various extraction methods to balance throughput and accuracy, such as pdfium, Unstructured.io, and Adobe Content Extraction Services for PDFs. It also offers pre and post-processing operations, including text splitting, filtering, and embedding generation.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/unclecode/crawl4ai</span><br><span class="line">12/15/24</span><br><span class="line">Web scraping will never be the same!</span><br><span class="line"></span><br><span class="line">Crawl4AI simplifies web crawling and data extraction, making it ready to use for LLMs and AI applications.</span><br><span class="line"></span><br><span class="line">Here’s why it’s a game-changer:</span><br><span class="line"></span><br><span class="line">🆓 Completely free and open-source</span><br><span class="line"></span><br><span class="line">🚀 Blazing fast performance, outperforming many paid services</span><br><span class="line"></span><br><span class="line">🤖 LLM-friendly output formats (JSON, cleaned HTML, markdown)</span><br><span class="line"></span><br><span class="line">🌍 Supports crawling multiple URLs simultaneously</span><br><span class="line"></span><br><span class="line">🎨 Extracts all media tags (Images, Audio, Video)</span><br><span class="line"></span><br><span class="line">🔗 Extracts all external and internal links</span><br><span class="line"></span><br><span class="line">But that’s not all:</span><br><span class="line"></span><br><span class="line">📚 Extracts metadata from pages</span><br><span class="line"></span><br><span class="line">🔄 Custom hooks for auth, headers, and page modifications</span><br><span class="line"></span><br><span class="line">🕵️ User-agent customization</span><br><span class="line"></span><br><span class="line">🖼️ Takes screenshots of pages</span><br><span class="line"></span><br><span class="line">📜 Executes custom JavaScript before crawling</span><br><span class="line"></span><br><span class="line">🚀🤖 Crawl4AI: Open-source LLM Friendly Web Crawler &amp; Scraper.</span><br><span class="line">unclecode%2Fcrawl4ai | Trendshift</span><br><span class="line"></span><br><span class="line">GitHub Stars GitHub Forks</span><br><span class="line"></span><br><span class="line">PyPI version Python Version Downloads</span><br><span class="line"></span><br><span class="line">License Code style: black Security: bandit</span><br><span class="line"></span><br><span class="line">Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for LLMs, AI agents, and data pipelines. Open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease.</span><br><span class="line"></span><br><span class="line">✨ Check out latest update v0.4.24x</span><br><span class="line"></span><br><span class="line">🎉 Version 0.4.24x is out! Major improvements in extraction strategies with enhanced JSON handling, SSL security, and Amazon product extraction. Plus, a completely revamped content filtering system! Read the release notes →</span><br><span class="line"></span><br><span class="line">🧐 Why Crawl4AI?</span><br><span class="line">Built for LLMs: Creates smart, concise Markdown optimized for RAG and fine-tuning applications.</span><br><span class="line">Lightning Fast: Delivers results 6x faster with real-time, cost-efficient performance.</span><br><span class="line">Flexible Browser Control: Offers session management, proxies, and custom hooks for seamless data access.</span><br><span class="line">Heuristic Intelligence: Uses advanced algorithms for efficient extraction, reducing reliance on costly models.</span><br><span class="line">Open Source &amp; Deployable: Fully open-source with no API keys—ready for Docker and cloud integration.</span><br><span class="line">Thriving Community: Actively maintained by a vibrant community and the #1 trending GitHub repository.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/pdf/2412.17189</span><br><span class="line">[Submitted on 22 Dec 2024]</span><br><span class="line"></span><br><span class="line">Better Think with Tables: Leveraging Tables to Enhance Large Language</span><br><span class="line">Model Comprehension</span><br><span class="line">Jio Oh*‡1</span><br><span class="line">, Geon Heo*1</span><br><span class="line">, Seungjun Oh1</span><br><span class="line">, Jindong Wang2</span><br><span class="line">, Xing Xie2</span><br><span class="line">, Steven Euijong Whang†1</span><br><span class="line">1KAIST, 2Microsoft Research Asia</span><br><span class="line">Abstract</span><br><span class="line">Despite the recent advancement of Large Langauge Models (LLMs), they struggle with complex queries often involving multiple conditions, common in real-world scenarios. We</span><br><span class="line">propose Thinking with Tables, a technique that</span><br><span class="line">assists LLMs to leverage tables for intermediate thinking aligning with human cognitive</span><br><span class="line">behavior. By introducing a pre-instruction that</span><br><span class="line">triggers an LLM to organize information in</span><br><span class="line">tables, our approach achieves a 40.29% average relative performance increase, higher robustness, and show generalizability to different</span><br><span class="line">requests, conditions, or scenarios. We additionally show the influence of data structuredness</span><br><span class="line">for the model by comparing results from four</span><br><span class="line">distinct structuring levels that we introduce.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2412.15605</span><br><span class="line">Don’t Do RAG:</span><br><span class="line">When Cache-Augmented Generation is All You Need for</span><br><span class="line">Knowledge Tasks</span><br><span class="line">Brian J Chan∗</span><br><span class="line">Chao-Ting Chen∗</span><br><span class="line">Jui-Hung Cheng∗</span><br><span class="line">Department of Computer Science</span><br><span class="line">National Chengchi University</span><br><span class="line">Taipei, Taiwan</span><br><span class="line">&#123;110703065,110703038,110703007&#125;@nccu.edu.tw</span><br><span class="line">Hen-Hsen Huang</span><br><span class="line">Insititue of Information Science</span><br><span class="line">Academia Sinica</span><br><span class="line">Taipei, Taiwan</span><br><span class="line">hhhuang@iis.sinica.edu.tw</span><br><span class="line">[Submitted on 20 Dec 2024]</span><br><span class="line"></span><br><span class="line">Abstract</span><br><span class="line">Retrieval-augmented generation (RAG) has gained traction as a</span><br><span class="line">powerful approach for enhancing language models by integrating</span><br><span class="line">external knowledge sources. However, RAG introduces challenges</span><br><span class="line">such as retrieval latency, potential errors in document selection,</span><br><span class="line">and increased system complexity. With the advent of large language models (LLMs) featuring significantly extended context windows, this paper proposes an alternative paradigm, cache-augmented</span><br><span class="line">generation (CAG) that bypasses real-time retrieval. Our method involves preloading all relevant resources, especially when the documents or knowledge for retrieval are of a limited and manageable</span><br><span class="line">size, into the LLM’s extended context and caching its runtime parameters. During inference, the model utilizes these preloaded parameters to answer queries without additional retrieval steps. Comparative analyses reveal that CAG eliminates retrieval latency and</span><br><span class="line">minimizes retrieval errors while maintaining context relevance. Performance evaluations across multiple benchmarks highlight scenarios where long-context LLMs either outperform or complement</span><br><span class="line">traditional RAG pipelines. These findings suggest that, for certain</span><br><span class="line">applications, particularly those with a constrained knowledge base,</span><br><span class="line">CAG provide a streamlined and efficient alternative to RAG, achieving comparable or superior results with reduced complexity</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>기술적으로 최대한 자세하게 적어. 9개의 기사가 있고 하나도 빼먹지 말고 적어.</p>
</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-01-06</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/01/06/2025-01-06-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2025년 01월 06일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/05/17/AI-%E1%84%82%E1%85%B2%E1%84%89%E1%85%B3-for-2024%E1%84%82%E1%85%A7%E1%86%AB-5%E1%84%8B%E1%85%AF%E1%86%AF-17%E1%84%8B%E1%85%B5%E1%86%AF/" title="2024년 5월 17일 AI 소식">Previous</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>