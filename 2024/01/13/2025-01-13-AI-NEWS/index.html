<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2025년 01월 13일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="➡️ Microsoft에서는 대규모 STEM QA에 강점을 보이는 새로운 14B 파라미터 모델 Phi-4를 Apache 2.0 라이선스로 공개하였습니다.
➡️ Google에서는 500M 파라미터의 시계열 예측 모델 TimesFM-2.0을 선보였습니다.
➡️ NVIDIA"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2025년 01월 13일 AI 소식</a></h3></div><div class="post-content"><p>➡️ Microsoft에서는 대규모 STEM QA에 강점을 보이는 새로운 14B 파라미터 모델 Phi-4를 Apache 2.0 라이선스로 공개하였습니다.</p>
<p>➡️ Google에서는 500M 파라미터의 시계열 예측 모델 TimesFM-2.0을 선보였습니다.</p>
<p>➡️ NVIDIA는 물리적 AI 개발 가속화를 위한 Cosmos 플랫폼과 함께 ‘VILA’라는 비전-언어 모델 패밀리를 공개하였습니다.</p>
<p>➡️ ‘moondream’ 프로젝트에서는 초경량 2B·0.5B 파라미터 VLM을 제안하였습니다.</p>
<p>➡️ 1.58-bit FLUX 연구는 1.58비트 양자화로 텍스트-투-이미지 모델의 용량 감소 및 추론 효율을 대폭 높이는 기법을 발표했습니다.</p>
<p>➡️ 소프트웨어 공학 분야에서는 Agentless라는 간단한 접근법이 복잡한 LLM 에이전트보다 뛰어난 성능을 보일 수 있음을 보고하였고, 검색 기반 강화 프레임워크 ‘Search-o1’ 역시 LLM의 한계를 보완하는 방식을 제시했습니다.</p>
<p>➡️ KaLM-Embedding은 고품질 다국어 임베딩 모델로서, 적절한 데이터 필터링 기법과 프리트레인 아키텍처 변화를 통해 우수한 성능을 선보였습니다.</p>
<p>➡️ ProTracker 연구는 영상 내 포인트를 추적하는 고효율 방법을 내놓았습니다.</p>
<p>➡️ ‘Long Context vs. RAG’ 논문은 초장문 맥락 사용 vs. Retrieval-Augmented Generation 간의 장단점을 정밀 비교하였습니다.</p>
<p>➡️ Chip Huyen은 에이전트(Agents)의 개념과 구축 방안, 그리고 실패 모드 및 평가 방식을 심도 있게 분석하는 글을 통해 LLM 기반 에이전트 설계의 가이드라인을 제시했습니다.</p>
<hr>
<h3 id="Microsoft-Phi-4"><a href="#Microsoft-Phi-4" class="headerlink" title="Microsoft, Phi-4"></a>Microsoft, Phi-4</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/microsoft/phi-4">링크</a>, 1&#x2F;9&#x2F;25</p>
<ul>
<li>14B 파라미터 규모의 STEM 특화 대규모 언어 모델 발표</li>
<li>GPT-4를 능가하는 STEM QA 성능을 보였으며, reasoning, math, code generation 능력이 뛰어남</li>
<li>9.8T 토큰에 달하는 고품질 데이터와 멀티에이전트·self-revision 기반 대규모 합성 데이터 활용</li>
<li>1920대의 H100-80G GPU로 21일간 학습</li>
<li>16K 토큰 컨텍스트 및 안전성 확보를 위한 SFT·DPO 기법 적용</li>
<li>English 중심 최적화로 reasoning-focused benchmark에서 우수한 결과 달성</li>
</ul>
<h3 id="Google-TimesFM-2-0"><a href="#Google-TimesFM-2-0" class="headerlink" title="Google, TimesFM-2.0"></a>Google, TimesFM-2.0</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/google/timesfm-2.0-500m-jax">링크</a></p>
<ul>
<li>500M 파라미터 Time Series Foundation Model(TimesFM-2.0) 공개</li>
<li>4배 더 긴 최대 컨텍스트(2048 시계열 포인트)로 시계열 예측 정확도 향상</li>
<li>새로운 버전(v2.0)은 v1.0 대비 최대 25% 높은 정확도를 보임</li>
<li>GIFT-Eval 리더보드에서 MASE·CRPS 측면에서 최고 성능 기록</li>
<li>Fine-tuning 및 zero-shot covariate 지원 등을 통한 유연한 활용 가능</li>
<li>Google Research에서 개발, ICML 2024 논문 발표 예정</li>
</ul>
<h3 id="NVIDIA-Cosmos"><a href="#NVIDIA-Cosmos" class="headerlink" title="NVIDIA, Cosmos"></a>NVIDIA, Cosmos</h3><p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/Cosmos">링크</a>, 2025년 1월 6일 공개</p>
<ul>
<li>Physical AI 개발을 가속화하기 위한 ‘Cosmos’ 플랫폼 공개</li>
<li>Text2World·Video2World 등 영상·텍스트 기반 시뮬레이션 모델(확산&#x2F;오토리그레시브) 제공</li>
<li>실제 AV·로보틱스 동영상을 통해 학습된 물리 기반 환경 예측·생성 가능</li>
<li>선행 모델들 대비 오픈 라이선스(NVIDIA Open Model License)로 기업·연구자 활용에 유리</li>
<li>NVIDIA NeMo 프레임워크를 이용해 후속 학습 및 파인튜닝 가능</li>
<li>AV·로보틱스 업계(1X, Agility Robotics, XPENG, Uber, Waabi 등)에서 이미 사용 중</li>
</ul>
<h3 id="moondream-프로젝트"><a href="#moondream-프로젝트" class="headerlink" title="moondream 프로젝트"></a>moondream 프로젝트</h3><p><a target="_blank" rel="noopener" href="https://github.com/vikhyat/moondream">링크</a>, 1&#x2F;9&#x2F;25</p>
<ul>
<li>초경량 비전 언어 모델(VLM)인 “Moondream” 공개 (2B·0.5B 파라미터 버전)</li>
<li>이미지 캡셔닝, VQA, 객체 검출, 포인트 추론 등 다양한 비전 태스크 지원</li>
<li>8비트(int8) 및 심지어 4비트(int4) 양자화를 활용, 적은 메모리로도 구동 가능</li>
<li>Edge 환경, 모바일 기기 등 제한된 자원에서 동작 가능하도록 설계</li>
<li>Apache 2.0 라이선스 하에 오픈소스로 제공, PyPI 패키지로 간편 설치 가능</li>
</ul>
<h3 id="1-58-bit-FLUX"><a href="#1-58-bit-FLUX" class="headerlink" title="1.58-bit FLUX"></a>1.58-bit FLUX</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.18653">링크</a>, (논문 업로드일: 2025년 1월 초)</p>
<ul>
<li>텍스트-투-이미지 모델 FLUX.1-dev를 약 1.58비트(±1, 0)로 양자화한 연구</li>
<li>11.9B 파라미터 중 99.5%를 1.58비트로 표현하면서도 높은 이미지 생성 성능 유지</li>
<li>7.7배의 모델 스토리지 절감, 5.1배의 GPU 메모리 사용량 절감, 추론 지연시간 개선</li>
<li>Image data 없이도 self-supervision 기반 양자화가 가능함을 시사</li>
<li>합성능력과 효율성을 함께 잡은 새로운 저비트 양자화 기법</li>
</ul>
<h3 id="Fudan-University-외-“Agentless-Demystifying-LLM-based-Software-Engineering-Agents”"><a href="#Fudan-University-외-“Agentless-Demystifying-LLM-based-Software-Engineering-Agents”" class="headerlink" title="Fudan University 외, “Agentless: Demystifying LLM-based Software Engineering Agents”"></a>Fudan University 외, “Agentless: Demystifying LLM-based Software Engineering Agents”</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2407.01489">링크</a>, 2024년 7월 2일(출판일)</p>
<ul>
<li>SWE-bench Lite 벤치마크에서 복잡한 소프트웨어 개발 업무를 에이전트 없이 해결하는 방식 제안</li>
<li>“Agentless” 접근법이 다양한 툴을 사용하는 복잡한 LLM 에이전트보다 단순하면서도 비용 및 성능 면에서 우수</li>
<li>Localization-Repair 2단계 프로세스로 이루어진 간단한 모델이, 복잡한 에이전트 대비 성공률 및 경제성이 뛰어남</li>
<li>오픈소스 소프트웨어 에이전트 대비 27.33%의 높은 성능과 $0.34의 낮은 비용 달성</li>
</ul>
<h3 id="NVIDIA-VILA"><a href="#NVIDIA-VILA" class="headerlink" title="NVIDIA, VILA"></a>NVIDIA, VILA</h3><p><a target="_blank" rel="noopener" href="https://github.com/NVlabs/VILA">링크</a></p>
<ul>
<li>비전·언어 모델(VLM)을 효율·정확도 균형 있게 설계한 “VILA” 계열 발표</li>
<li>“Cosmos Nemotron VLMs”의 일부로 출시되어, 영상·다중이미지 처리 효율성 개선</li>
<li>구조적 개선(Scale-then-Compress)으로 고해상도 이미지와 긴 동영상 처리에도 효율적</li>
<li>학습·추론·파인튜닝 전 과정에서 4.5배~2.8배 효율 향상, 오픈 및 상용 VLM들과 경쟁</li>
<li>다양한 이미지·동영상 벤치마크에서 상위권 성능 기록</li>
</ul>
<h3 id="HIT-TMG-KaLM-Embedding"><a href="#HIT-TMG-KaLM-Embedding" class="headerlink" title="HIT-TMG, KaLM-Embedding"></a>HIT-TMG, KaLM-Embedding</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/collections/HIT-TMG/kalm-embedding-67316afa4c56f4fc1f58764b">링크</a>, 2025년 1월 2일 발표</p>
<ul>
<li>Qwen2-0.5B 기반으로 구축한 오픈 라이선스(MIT) 다국어 임베딩 모델</li>
<li>MTEB 벤치마크에서 평균 64.53점 달성(C-MTEB 64.13, MTEB 64.94)</li>
<li>고품질·다양화된 훈련 데이터를 확보하기 위해 ranking consistency filtering 기법 도입</li>
<li>Matryoshka Representation Learning으로 임베딩 차원을 유연하게 지원</li>
<li>&lt;1B 파라미터임에도 여러 언어에서 높은 성능 보임, Sentence-Transformers로 통합 사용 가능</li>
</ul>
<h3 id="Tsinghua-University-외-“Search-o1-Agentic-Search-Enhanced-Large-Reasoning-Models”"><a href="#Tsinghua-University-외-“Search-o1-Agentic-Search-Enhanced-Large-Reasoning-Models”" class="headerlink" title="Tsinghua University 외, “Search-o1: Agentic Search-Enhanced Large Reasoning Models”"></a>Tsinghua University 외, “Search-o1: Agentic Search-Enhanced Large Reasoning Models”</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.05366">링크</a>, 2025년 1월 9일(논문 제출일)</p>
<ul>
<li>OpenAI-o1 스타일의 긴 단계 추론(Large Reasoning Model)에 검색(검색 에이전트)을 접목한 프레임워크인 “Search-o1” 발표</li>
<li>불확실한 지식 포인트에서 외부 정보를 동적으로 검색하고, Reason-in-Documents 모듈로 노이즈를 최소화</li>
<li>수학·과학·코딩 등 복잡한 reasoning 태스크 및 6가지 오픈 QA 벤치마크에서 우수한 성능</li>
<li>LLM의 지식 부족을 보완하여, 추론 신뢰성과 정확성을 높임</li>
<li>Agentic RAG 메커니즘을 통해 외부 문서 검색 및 문서 재정리 후 reasoning에 반영</li>
</ul>
<h3 id="Long-Context-vs-RAG-for-LLMs-An-Evaluation-and-Revisits"><a href="#Long-Context-vs-RAG-for-LLMs-An-Evaluation-and-Revisits" class="headerlink" title="Long Context vs. RAG for LLMs: An Evaluation and Revisits"></a>Long Context vs. RAG for LLMs: An Evaluation and Revisits</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.03220">링크</a>, 2025년 1월 9일(논문 제출일)</p>
<ul>
<li>초장문(Long Context, LC)와 Retrieval-Augmented Generation(RAG)을 통한 외부 정보 활용 방안을 비교 연구</li>
<li>Wikipedia 기반 QA에서는 LC가 RAG 대비 전반적으로 더 우수한 성능을 보이는 반면, 대화형 질의 등에서는 RAG가 유리</li>
<li>Summarization 기반 Retrieval이 Chunk-based Retrieval보다 성능이 높음을 검증</li>
<li>LC와 RAG를 혼합하거나 적절히 선택하는 전략이 과제별로 중요함을 제안</li>
<li>기존 연구들이 놓친 ‘맥락 적합성’ 문제가 실제 성능에 매우 큰 영향을 준다고 지적</li>
</ul>
<h3 id="Agents-by-Chip-Huyen"><a href="#Agents-by-Chip-Huyen" class="headerlink" title="Agents (by Chip Huyen)"></a>Agents (by Chip Huyen)</h3><p><a target="_blank" rel="noopener" href="https://huyenchip.com/2025/01/07/agents.html">링크</a>, 2025년 1월 7일</p>
<ul>
<li>에이전트의 개념 정의, 도구 사용, 계획(Planning) 방식, 실패 모드, 평가 등을 체계적으로 정리</li>
<li>Planning과 Execution을 분리하고, Multi-Agent 시스템 설계를 통해 복잡도를 분산시키는 방법 제시</li>
<li>에이전트가 사용할 툴을 신중히 고르는 것이 중요하며, 툴이 많아질수록 혼선이 생길 수 있음을 지적</li>
<li>Reflection(자기 평가) 기법을 적용해 에이전트가 스스로 에러를 수정하고 성능을 향상할 수 있는 가능성 언급</li>
<li>Anthropic의 “Building effective agents”와 유사하면서도 실행 흐름, 실패 모드 구체화에 초점</li>
</ul>
<h3 id="ProTracker-Probabilistic-Integration-for-Robust-and-Accurate-Point-Tracking"><a href="#ProTracker-Probabilistic-Integration-for-Robust-and-Accurate-Point-Tracking" class="headerlink" title="ProTracker: Probabilistic Integration for Robust and Accurate Point Tracking"></a>ProTracker: Probabilistic Integration for Robust and Accurate Point Tracking</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.01880">링크</a>, 2025년 1월 초(논문 제출일)</p>
<ul>
<li>영상 내 임의의 포인트를 장기간 추적하기 위한 새 프레임워크 “ProTracker” 제안</li>
<li>Optical Flow와 semantic feature 기반 예측을 확률적으로 통합해 정확도 및 견고성 향상</li>
<li>Occlusion이나 비슷한 영역이 많은 영상에서도 드리프트 없이 지속적으로 포인트 추적</li>
<li>TAP-Vid-DAVIS 등 다양한 벤치마크에서 최고 수준의 성능 달성</li>
<li>Geometry-aware feature filtering, long-term keypoint relocalization 등으로 잡음 제거 및 안정성 극대화</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://huggingface.co/microsoft/phi-4</span><br><span class="line">1/9/25</span><br><span class="line">It&#x27;s here! Phi-4 on Hugging Face with MIT License! Microsoft Phi-4 is a 14B LLM that outperforms OpenAI GPT-4o on STEM-focused QA. Built using large-scale, high-quality synthetic data created by multi-agent, self-revision workflows. 👀</span><br><span class="line">TL;DR:</span><br><span class="line">🧠 14B parameters, but performs on par with 70B models</span><br><span class="line">📚 Trained on 9.8T tokens of high-quality data</span><br><span class="line">⚡ 16K token context length</span><br><span class="line">🎯 Outperforms previous version (Phi-3) across all benchmarks</span><br><span class="line">🔬 21 days of training on 1920 H100-80G GPUs</span><br><span class="line">🛡️ Comprehensive safety alignment using SFT and DPO</span><br><span class="line">🌐 Optimized for English language tasks</span><br><span class="line">🎓 Particularly strong in reasoning, math, and code generation</span><br><span class="line"></span><br><span class="line">Phi-4 Technical Report</span><br><span class="line">Marah Abdin Jyoti Aneja Harkirat Behl S´ebastien Bubeck</span><br><span class="line">Ronen Eldan Suriya Gunasekar Michael Harrison Russell J. Hewett</span><br><span class="line">Mojan Javaheripi Piero Kauffmann James R. Lee Yin Tat Lee</span><br><span class="line">Yuanzhi Li Weishung Liu Caio C. T. Mendes Anh Nguyen</span><br><span class="line">Eric Price Gustavo de Rosa Olli Saarikivi Adil Salim</span><br><span class="line">Shital Shah Xin Wang Rachel Ward Yue Wu</span><br><span class="line">Dingli Yu Cyril Zhang Yi Zhang</span><br><span class="line">Microsoft Research</span><br><span class="line">Abstract</span><br><span class="line">We present phi-4, a 14-billion parameter language model developed with a training recipe that</span><br><span class="line">is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic</span><br><span class="line">data throughout the training process. While previous models in the Phi family largely distill the</span><br><span class="line">capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model</span><br><span class="line">on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong</span><br><span class="line">performance relative to its size – especially on reasoning-focused benchmarks – due to improved data,</span><br><span class="line">training curriculum, and innovations in the post-training scheme.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/google/timesfm-2.0-500m-jax</span><br><span class="line">Google</span><br><span class="line">We just released the weights of TimesFM-2.0 (jax: https://goo.gle/4agXX3w, and pytorch: https://goo.gle/3WdOjZy) on Hugging Face. This checkpoint has 500M parameters and can be better than v1.0 by up to 25% on leading benchmarks. It also has a 4x longer maximum context length.</span><br><span class="line">TimesFM-2.0 takes the top spot on the GIFT-Eval (https://goo.gle/4aeiA0f) leaderboard in terms of point forecasting accuracy measured by MASE as well as probabilistic forecasting accuracy measured by CRPS. It is better than the next best model by about 6% in terms of aggregated MASE.</span><br><span class="line">Instructions for using this model are in our repository (https://goo.gle/4h6ocM7). It should work with our prior examples of forecasting with covariates and fine tuning.</span><br><span class="line"></span><br><span class="line">TimesFM</span><br><span class="line">TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.</span><br><span class="line"></span><br><span class="line">Resources and Technical Documentation:</span><br><span class="line"></span><br><span class="line">Paper: A decoder-only foundation model for time-series forecasting, ICML 2024.</span><br><span class="line">Google Research blog</span><br><span class="line">GitHub repo</span><br><span class="line">Authors: Google Research</span><br><span class="line"></span><br><span class="line">This is not an officially supported Google product.</span><br><span class="line"></span><br><span class="line">Checkpoint timesfm-2.0-500m</span><br><span class="line">timesfm-2.0-500m is the second open model checkpoint:</span><br><span class="line"></span><br><span class="line">It performs univariate time series forecasting for context lengths up to 2048 time points and any horizon lengths, with an optional frequency indicator. Note that it can go even beyond 2048 context even though it was trained with that as the maximum context.</span><br><span class="line">It focuses on point forecasts. We experimentally offer 10 quantile heads but they have not been calibrated after pretraining.</span><br><span class="line">It ideally requires the context to be contiguous (i.e. no &quot;holes&quot;), and the context and the horizon to be of the same frequency. In case there are nans we fill in the missing values with linear interpolation before calling the model.</span><br><span class="line"></span><br><span class="line">TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.</span><br><span class="line"></span><br><span class="line">Paper: A decoder-only foundation model for time-series forecasting, to appear in ICML 2024.</span><br><span class="line">Google Research blog</span><br><span class="line">Hugging Face release</span><br><span class="line">This repo contains the code to load public TimesFM checkpoints and run model inference. Please visit our Hugging Face release to download model checkpoints.</span><br><span class="line"></span><br><span class="line">This is not an officially supported Google product.</span><br><span class="line"></span><br><span class="line">We recommend at least 32GB RAM to load TimesFM dependencies.</span><br><span class="line"></span><br><span class="line">Update - Dec. 30, 2024</span><br><span class="line">We are launching a 500m checkpoint as a part of TimesFM-2.0 release. This new checkpoint can be upto 25% better than v1.0 on leading benchmarks and also has a 4 times longer max. context length.</span><br><span class="line">Launched finetuning support that lets you finetune the weights of the pretrained TimesFM model on your own data.</span><br><span class="line">Launched ~zero-shot covariate support with external regressors. More details here.</span><br><span class="line">Checkpoint timesfm-1.0-200m (-pytorch)</span><br><span class="line">timesfm-1.0-200m is our first open model checkpoint:</span><br><span class="line"></span><br><span class="line">It performs univariate time series forecasting for context lengths up to 512 timepoints and any horizon lengths, with an optional frequency indicator.</span><br><span class="line">It focuses on point forecasts, and does not support probabilistic forecasts. We experimentally offer quantile heads but they have not been calibrated after pretraining.</span><br><span class="line">Checkpoint timesfm-2.0-500m (-jax/-pytorch)</span><br><span class="line">timesfm-2.0-500m is our second open model checkpoint:</span><br><span class="line"></span><br><span class="line">It performs univariate time series forecasting for context lengths up to 2048 timepoints and any horizon lengths, with an optional frequency indicator.</span><br><span class="line">It focuses on point forecasts. We experimentally offer 10 quantile heads but they have not been calibrated after pretraining.</span><br><span class="line">This new checkpoint can be upto 25% better than v1.0 on leading benchmarks and also has a 4 times longer max. context length.</span><br><span class="line">Benchmarking</span><br><span class="line">TimesFM 2.0 has been added to GIFT-Eval which is one of the most comprehensive time-series bechmarks available. It takes the top spot in terms of aggregated MASE and CRPS, where it is 6% better than the next best model in terms of aggregated MASE.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/NVIDIA/Cosmos</span><br><span class="line">NVIDIA Cosmos is a developer-first world foundation model platform designed to help Physical AI developers build their Physical AI systems better and faster. Cosmos contains</span><br><span class="line"></span><br><span class="line">pre-trained models, available via Hugging Face under the NVIDIA Open Model License that allows commercial use of the models for free</span><br><span class="line">training scripts under the Apache 2 License, offered through NVIDIA Nemo Framework for post-training the models for various downstream Physical AI applications</span><br><span class="line">Details of the platform is described in the Cosmos paper. Preview access is avaiable at build.nvidia.com.</span><br><span class="line"></span><br><span class="line">Key Features</span><br><span class="line">Pre-trained Diffusion-based world foundation models for Text2World and Video2World generation where a user can generate visual simulation based on text prompts and video prompts.</span><br><span class="line">Pre-trained Autoregressive-based world foundation models for Video2World generation where a user can generate visual simulation based on video prompts and optional text prompts.</span><br><span class="line">Video tokenizers for tokenizing videos into continuous tokens (latent vectors) and discrete tokens (integers) efficiently and effectively.</span><br><span class="line">Video curation pipeline for building your own video dataset. [Coming soon]</span><br><span class="line">Post-training scripts via NeMo Framework to post-train the pre-trained world foundation models for various Physical AI setup.</span><br><span class="line">Pre-training scripts via NeMo Framework for building your own world foundation model. [Diffusion] [Autoregressive] [Tokenizer].</span><br><span class="line">Model Family</span><br><span class="line">Model name	Description	Try it out</span><br><span class="line">Cosmos-1.0-Diffusion-7B-Text2World	Text to visual world generation	Inference</span><br><span class="line">Cosmos-1.0-Diffusion-14B-Text2World	Text to visual world generation	Inference</span><br><span class="line">Cosmos-1.0-Diffusion-7B-Video2World	Video + Text based future visual world generation	Inference</span><br><span class="line">Cosmos-1.0-Diffusion-14B-Video2World	Video + Text based future visual world generation	Inference</span><br><span class="line">Cosmos-1.0-Autoregressive-4B	Future visual world generation	Inference</span><br><span class="line">Cosmos-1.0-Autoregressive-12B	Future visual world generation	Inference</span><br><span class="line">Cosmos-1.0-Autoregressive-5B-Video2World	Video + Text based future visual world generation	Inference</span><br><span class="line">Cosmos-1.0-Autoregressive-13B-Video2World	Video + Text based future visual world generation	Inference</span><br><span class="line">Cosmos-1.0-Guardrail	Guardrail contains pre-Guard and post-Guard for safe use	Embedded in model inference scripts</span><br><span class="line"></span><br><span class="line">NVIDIA Makes Cosmos World Foundation Models Openly Available to Physical AI Developer Community</span><br><span class="line">State-of-the-art models trained on millions of hours of driving and robotics videos to democratize physical AI development, available under open model license.</span><br><span class="line">January 6, 2025 by Ming-Yu Liu</span><br><span class="line"></span><br><span class="line"> Share</span><br><span class="line"></span><br><span class="line">NVIDIA Cosmos, a platform for accelerating physical AI development, introduces a family of world foundation models — neural networks that can predict and generate physics-aware videos of the future state of a virtual environment — to help developers build next-generation robots and autonomous vehicles (AVs).</span><br><span class="line"></span><br><span class="line">World foundation models, or WFMs, are as fundamental as large language models. They use input data, including text, image, video and movement, to generate and simulate virtual worlds in a way that accurately models the spatial relationships of objects in the scene and their physical interactions.</span><br><span class="line"></span><br><span class="line">Announced today at CES, NVIDIA is making available the first wave of Cosmos WFMs for physics-based simulation and synthetic data generation — plus state-of-the-art tokenizers, guardrails, an accelerated data processing and curation pipeline, and a framework for model customization and optimization.</span><br><span class="line"></span><br><span class="line">Researchers and developers, regardless of their company size, can freely use the Cosmos models under NVIDIA’s permissive open model license that allows commercial usage. Enterprises building AI agents can also use new open NVIDIA Llama Nemotron and Cosmos Nemotron models, unveiled at CES.</span><br><span class="line"></span><br><span class="line">The openness of Cosmos’ state-of-the-art models unblocks physical AI developers building robotics and AV technology and enables enterprises of all sizes to more quickly bring their physical AI applications to market. Developers can use Cosmos models directly to generate physics-based synthetic data, or they can harness the NVIDIA NeMo framework to fine-tune the models with their own videos for specific physical AI setups.</span><br><span class="line"></span><br><span class="line">Physical AI leaders — including robotics companies 1X, Agility Robotics and XPENG, and AV developers Uber and Waabi  — are already working with Cosmos to accelerate and enhance model development.</span><br><span class="line"></span><br><span class="line">Developers can preview the first Cosmos autoregressive and diffusion models on the NVIDIA API catalog, and download the family of models and fine-tuning framework from the NVIDIA NGC catalog and Hugging Face.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">World Foundational Models for Physical AI</span><br><span class="line">Cosmos world foundation models are a suite of open diffusion and autoregressive transformer models for physics-aware video generation. The models have been trained on 9,000 trillion tokens from 20 million hours of real-world human interactions, environment, industrial, robotics and driving data.</span><br><span class="line"></span><br><span class="line">The models come in three categories: Nano, for models optimized for real-time, low-latency inference and edge deployment; Super, for highly performant baseline models; and Ultra, for maximum quality and fidelity, best used for distilling custom models.</span><br><span class="line"></span><br><span class="line">When paired with NVIDIA Omniverse 3D outputs, the diffusion models generate controllable, high-quality synthetic video data to bootstrap training of robotic and AV perception models. The autoregressive models predict what should come next in a sequence of video frames based on input frames and text. This enables real-time next-token prediction, giving physical AI models the foresight to predict their next best action.</span><br><span class="line"></span><br><span class="line">Developers can use Cosmos’ open models for text-to-world and video-to-world generation. Versions of the diffusion and autoregressive models, with between 4 and 14 billion parameters each, are available now on the NGC catalog and Hugging Face.</span><br><span class="line"></span><br><span class="line">Also available are a 12-billion-parameter upsampling model for refining text prompts, a 7-billion-parameter video decoder optimized for augmented reality, and guardrail models to ensure responsible, safe use.</span><br><span class="line"></span><br><span class="line">To demonstrate opportunities for customization, NVIDIA is also releasing fine-tuned model samples for vertical applications, such as generating multisensor views for AVs.</span><br><span class="line"></span><br><span class="line">Advancing Robotics, Autonomous Vehicle Applications</span><br><span class="line">Cosmos world foundation models can enable synthetic data generation to augment training datasets, simulation to test and debug physical AI models before they’re deployed in the real world, and reinforcement learning in virtual environments to accelerate AI agent learning.</span><br><span class="line"></span><br><span class="line">Developers can generate massive amounts of controllable, physics-based synthetic data by conditioning Cosmos with composed 3D scenes from NVIDIA Omniverse.</span><br><span class="line"></span><br><span class="line">Waabi, a company pioneering generative AI for the physical world, starting with autonomous vehicles, is evaluating the use of Cosmos for the search and curation of video data for AV software development and simulation. This will further accelerate the company’s industry-leading approach to safety, which is based on Waabi World, a generative AI simulator that can create any situation a vehicle might encounter with the same level of realism as if it happened in the real world.</span><br><span class="line"></span><br><span class="line">In robotics, WFMs can generate synthetic virtual environments or worlds to provide a less expensive, more efficient and controlled space for robot learning. Embodied AI startup Hillbot is boosting its data pipeline by using Cosmos to generate terabytes of high-fidelity 3D environments. This AI-generated data will help the company refine its robotic training and operations, enabling faster, more efficient robotic skilling and improved performance for industrial and domestic tasks.</span><br><span class="line"></span><br><span class="line">In both industries, developers can use NVIDIA Omniverse and Cosmos as a multiverse simulation engine, allowing a physical AI policy model to simulate every possible future path it could take to execute a particular task — which in turn helps the model select the best of these paths.</span><br><span class="line"></span><br><span class="line">Data curation and the training of Cosmos models relied on thousands of NVIDIA GPUs through NVIDIA DGX Cloud, a high-performance, fully managed AI platform that provides accelerated computing clusters in every leading cloud.</span><br><span class="line"></span><br><span class="line">Developers adopting Cosmos can use DGX Cloud for an easy way to deploy Cosmos models, with further support available through the NVIDIA AI Enterprise software platform.</span><br><span class="line"></span><br><span class="line">Customize and Deploy With NVIDIA Cosmos</span><br><span class="line">In addition to foundation models, the Cosmos platform includes a data processing and curation pipeline powered by NVIDIA NeMo Curator and optimized for NVIDIA data center GPUs.</span><br><span class="line"></span><br><span class="line">Robotics and AV developers collect millions or billions of hours of real-world recorded video, resulting in petabytes of data. Cosmos enables developers to process 20 million hours of data in just 40 days on NVIDIA Hopper GPUs, or as little as 14 days on NVIDIA Blackwell GPUs. Using unoptimized pipelines running on a CPU system with equivalent power consumption, processing the same amount of data would take over three years.</span><br><span class="line"></span><br><span class="line">The platform also features a suite of powerful video and image tokenizers that can convert videos into tokens at different video compression ratios for training various transformer models.</span><br><span class="line"></span><br><span class="line">The Cosmos tokenizers deliver 8x more total compression than state-of-the-art methods and 12x faster processing speed, which offers superior quality and reduced computational costs in both training and inference. Developers can access these tokenizers, available under NVIDIA’s open model license, via Hugging Face and GitHub.</span><br><span class="line"></span><br><span class="line">Developers using Cosmos can also harness model training and fine-tuning capabilities offered by NeMo framework, a GPU-accelerated framework that enables high-throughput AI training.</span><br><span class="line"></span><br><span class="line">Developing Safe, Responsible AI Models</span><br><span class="line">Now available to developers under the NVIDIA Open Model License Agreement, Cosmos was developed in line with NVIDIA’s trustworthy AI principles, which include nondiscrimination, privacy, safety, security and transparency.</span><br><span class="line"></span><br><span class="line">The Cosmos platform includes Cosmos Guardrails, a dedicated suite of models that, among other capabilities, mitigates harmful text and image inputs during preprocessing and screens generated videos during postprocessing for safety. Developers can further enhance these guardrails for their custom applications.</span><br><span class="line"></span><br><span class="line">Cosmos models on the NVIDIA API catalog also feature an inbuilt watermarking system that enables identification of AI-generated sequences.</span><br><span class="line"></span><br><span class="line">NVIDIA Cosmos was developed by NVIDIA Research. Read the research paper, “Cosmos World Foundation Model Platform for Physical AI,” for more details on model development and benchmarks. Model cards providing additional information are available on Hugging Face.</span><br><span class="line"></span><br><span class="line">Learn more about world foundation models in an AI Podcast episode that features Ming-Yu Liu, vice president of research at NVIDIA.</span><br><span class="line"></span><br><span class="line">Get started with NVIDIA Cosmos and join NVIDIA at CES. Watch the Cosmos demo and Huang’s keynote below:</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/vikhyat/moondream</span><br><span class="line">1/9/25</span><br><span class="line">🌔 moondream</span><br><span class="line">a tiny vision language model that kicks ass and runs anywhere</span><br><span class="line"></span><br><span class="line">Website | Demo</span><br><span class="line"></span><br><span class="line">Examples</span><br><span class="line">Image	Example</span><br><span class="line">	What is the girl doing?</span><br><span class="line">The girl is sitting at a table and eating a large hamburger.</span><br><span class="line"></span><br><span class="line">What color is the girl&#x27;s hair?</span><br><span class="line">The girl&#x27;s hair is white.</span><br><span class="line">	What is this?</span><br><span class="line">This is a computer server rack, which is a device used to store and manage multiple computer servers. The rack is filled with various computer servers, each with their own dedicated space and power supply. The servers are connected to the rack via multiple cables, indicating that they are part of a larger system. The rack is placed on a carpeted floor, and there is a couch nearby, suggesting that the setup is in a living or entertainment area.</span><br><span class="line"></span><br><span class="line">What is behind the stand?</span><br><span class="line">Behind the stand, there is a brick wall.</span><br><span class="line">About</span><br><span class="line">Moondream is a highly efficient open-source vision language model that combines powerful image understanding capabilities with a remarkably small footprint. It&#x27;s designed to be versatile and accessible, capable of running on a wide range of devices and platforms.</span><br><span class="line"></span><br><span class="line">The project offers two model variants:</span><br><span class="line"></span><br><span class="line">Moondream 2B: The primary model with 2 billion parameters, offering robust performance for general-purpose image understanding tasks including captioning, visual question answering, and object detection.</span><br><span class="line">Moondream 0.5B: A compact 500 million parameter model specifically optimized as a distillation target for edge devices, enabling efficient deployment on resource-constrained hardware while maintaining impressive capabilities.</span><br><span class="line">Getting Started</span><br><span class="line">Latest Model Checkpoints</span><br><span class="line">These are the latest bleeding-edge versions of both models, with all new features and improvements:</span><br><span class="line"></span><br><span class="line">Model	Precision	Download Size	Memory Usage	Best For	Download Link</span><br><span class="line">Moondream 2B	int8	1,733 MiB	2,624 MiB	General use, best quality	Download</span><br><span class="line">Moondream 0.5B	int8	593 MiB	996 MiB	Edge devices, faster speed	Download</span><br><span class="line">Announcement: New Moondream 2B model update!</span><br><span class="line"></span><br><span class="line">Moondream</span><br><span class="line">The Open Source VLM That Runs Everywhere.</span><br><span class="line"></span><br><span class="line">Get Started</span><br><span class="line"></span><br><span class="line">Try it out!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Over 6 million downloads!</span><br><span class="line"></span><br><span class="line">Explore the lineup.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Moondream 2B</span><br><span class="line">Powerful and fast</span><br><span class="line"></span><br><span class="line">1.9B</span><br><span class="line"></span><br><span class="line">Parameters</span><br><span class="line"></span><br><span class="line">fp16, int8, int4</span><br><span class="line"></span><br><span class="line">Quantized</span><br><span class="line"></span><br><span class="line">2GiB</span><br><span class="line"></span><br><span class="line">Memory</span><br><span class="line"></span><br><span class="line">Quantized Aware Training</span><br><span class="line"></span><br><span class="line">Training</span><br><span class="line"></span><br><span class="line">Servers, PC, Mobile</span><br><span class="line"></span><br><span class="line">Target Devices</span><br><span class="line"></span><br><span class="line">GPU, CPU-Optimized</span><br><span class="line"></span><br><span class="line">Inference</span><br><span class="line"></span><br><span class="line">Apache 2.0</span><br><span class="line"></span><br><span class="line">License</span><br><span class="line"></span><br><span class="line">New</span><br><span class="line"></span><br><span class="line">Moondream 0.5B</span><br><span class="line">Tiny and speedy.</span><br><span class="line"></span><br><span class="line">0.5B</span><br><span class="line"></span><br><span class="line">Parameters</span><br><span class="line"></span><br><span class="line">int8, int4</span><br><span class="line"></span><br><span class="line">Quantized</span><br><span class="line"></span><br><span class="line">1GiB</span><br><span class="line"></span><br><span class="line">Memory</span><br><span class="line"></span><br><span class="line">Quantized Aware Training</span><br><span class="line"></span><br><span class="line">Training</span><br><span class="line"></span><br><span class="line">Mobile, Edge</span><br><span class="line"></span><br><span class="line">Target Devices</span><br><span class="line"></span><br><span class="line">GPU, CPU-Optimized</span><br><span class="line"></span><br><span class="line">Inference</span><br><span class="line"></span><br><span class="line">Apache 2.0</span><br><span class="line"></span><br><span class="line">License</span><br><span class="line"></span><br><span class="line">Discover the capabilities.</span><br><span class="line">Query</span><br><span class="line">Get human-like answers from any prompt.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">List all the food shown in this image.</span><br><span class="line"></span><br><span class="line">A halved avocado, cherry tomatoes, green onions, spinach, mushrooms, and a few peppers. There are also two eggs on the board.</span><br><span class="line"></span><br><span class="line">Caption</span><br><span class="line">Generate detailed descriptions of any scene.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The image depicts a clownfish, a type of sea anemone, swimming in a vibrant underwater scene. The clownfish has a distinctive red body and black stripes on its sides, with a white stripe running along its back. It is positioned near a cluster of purple and white anemones, which provide a striking contrast against the deep blue background. The clownfish&#x27;s large eyes are visible, and it appears to be looking towards the right side of the image. The anemones have a textured appearance with many small bumps and protrusions.</span><br><span class="line"></span><br><span class="line">Object Detection</span><br><span class="line">Get bounding boxes from a prompt.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Detect: Drone</span><br><span class="line"></span><br><span class="line">4 objects detected.</span><br><span class="line"></span><br><span class="line">Point</span><br><span class="line">Get X, Y locations for any items.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Point: Sign in with Apple button</span><br><span class="line"></span><br><span class="line">1 point detected.</span><br><span class="line"></span><br><span class="line">Get started in 5 minutes.</span><br><span class="line">Our clients are optimized for CPU and GPU inference, and are a snap to learn.</span><br><span class="line"></span><br><span class="line">pip install moondream</span><br><span class="line"></span><br><span class="line">import moondream as md</span><br><span class="line"></span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># initialize with a downloaded model</span><br><span class="line"></span><br><span class="line">model = md.vl(model=&quot;./moondream-2b-int8.mf&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># open an image</span><br><span class="line"></span><br><span class="line">image = Image.open(&quot;./image.jpg&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># query the image</span><br><span class="line"></span><br><span class="line">result = model.query(image, &quot;Is this a hot dog?&quot;)</span><br><span class="line"></span><br><span class="line">print(&quot;Answer: &quot;, result[&quot;answer&quot;])</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/pdf/2412.18653</span><br><span class="line">1.58-bit FLUX</span><br><span class="line">Chenglin Yang1</span><br><span class="line">, Celong Liu1</span><br><span class="line">, Xueqing Deng1</span><br><span class="line">, Dongwon Kim2</span><br><span class="line">,</span><br><span class="line">Xing Mei1</span><br><span class="line">, Xiaohui Shen1</span><br><span class="line">, Liang-Chieh Chen1</span><br><span class="line">1ByteDance 2POSTECH</span><br><span class="line"></span><br><span class="line">We present 1.58-bit FLUX, the first successful approach to quantizing the state-of-the-art text-to-image generation model, FLUX.1-dev, using 1.58-bit weights (i.e., values in &#123;-1, 0, +1&#125;) while maintaining comparable performance for generating 1024 x 1024 images. Notably, our quantization method operates without access to image data, relying solely on self-supervision from the FLUX.1-dev model. Additionally, we develop a custom kernel optimized for 1.58-bit operations, achieving a 7.7x reduction in model storage, a 5.1x reduction in inference memory, and improved inference latency. Extensive evaluations on the GenEval and T2I Compbench benchmarks demonstrate the effectiveness of 1.58-bit FLUX in maintaining generation quality while significantly enhancing computational efficiency.</span><br><span class="line"></span><br><span class="line">Figure 1. Visual comparisons between FLUX and 1.58-bit FLUX. 1.58-bit FLUX demonstrates comparable generation quality to FLUX</span><br><span class="line">while employing 1.58-bit quantization, where 99.5% of the 11.9B parameters in the vision transformer are constrained to the values +1,</span><br><span class="line">-1, or 0. For consistency, all images in each comparison are generated using the same latent noise input. 1.58-bit FLUX utilizes a custom</span><br><span class="line">1.58-bit kernel. Additional visual comparisons are provided in Fig. 3 and Fig</span><br><span class="line"></span><br><span class="line">Figure 2. Efficiency measurements on the vision transformer component of FLUX and 1.58-bit FLUX. The measurements are based</span><br><span class="line">on generating a single image with 50 inference steps. (a) 1.58-bit FLUX reduces checkpoint storage by 7.7× compared to FLUX. (b)</span><br><span class="line">1.58-bit FLUX achieves a 5.1× reduction in inference memory usage across various GPU types. The x-axis labels, m-nG, represent GPU</span><br><span class="line">type m with a maximum memory capacity of n Gigabytes (G)</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2407.01489</span><br><span class="line">Agentless: Demystifying LLM-based Software Engineering Agents</span><br><span class="line">Published on Jul 2, 2024</span><br><span class="line">·</span><br><span class="line">Submitted by</span><br><span class="line">nevetsaix</span><br><span class="line">on Jul 3, 2024</span><br><span class="line">#2 Paper of the day</span><br><span class="line">Authors:</span><br><span class="line"></span><br><span class="line">Chunqiu Steven Xia</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Yinlin Deng</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Soren Dunn</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Lingming Zhang</span><br><span class="line">Abstract</span><br><span class="line">Recent advancements in large language models (LLMs) have significantly advanced the automation of software development tasks, including code synthesis, program repair, and test generation. More recently, researchers and industry practitioners have developed various autonomous LLM agents to perform end-to-end software development tasks. These agents are equipped with the ability to use tools, run commands, observe feedback from the environment, and plan for future actions. However, the complexity of these agent-based approaches, together with the limited abilities of current LLMs, raises the following question: Do we really have to employ complex autonomous software agents? To attempt to answer this question, we build Agentless -- an agentless approach to automatically solve software development problems. Compared to the verbose and complex setup of agent-based approaches, Agentless employs a simplistic two-phase process of localization followed by repair, without letting the LLM decide future actions or operate with complex tools. Our results on the popular SWE-bench Lite benchmark show that surprisingly the simplistic Agentless is able to achieve both the highest performance (27.33%) and lowest cost (\$0.34) compared with all existing open-source software agents! Furthermore, we manually classified the problems in SWE-bench Lite and found problems with exact ground truth patch or insufficient/misleading issue descriptions. As such, we construct SWE-bench Lite-S by excluding such problematic issues to perform more rigorous evaluation and comparison. Our work highlights the current overlooked potential of a simple, interpretable technique in autonomous software development. We hope Agentless will help reset the baseline, starting point, and horizon for autonomous software agents, and inspire future work along this crucial direction.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/NVlabs/VILA</span><br><span class="line">NVIDIA&#x27;s VILA: Optimized Vision Language Models</span><br><span class="line">💡VILA is a part of the new Cosmos Nemotron VLMs</span><br><span class="line">💡A family of open VLMs designed to optimize efficiency and accuracy for efficient video and multi-image understanding</span><br><span class="line">💡Trending on GitHub</span><br><span class="line">💡Built with Gradio</span><br><span class="line">VILA is a family of open VLMs designed to optimize both efficiency and accuracy for efficient video understanding and multi-image understanding.</span><br><span class="line">NVILA: Efficient Frontier Visual Language Models</span><br><span class="line">Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, Yao Lu</span><br><span class="line">Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and accuracy. Building on top of VILA, we improve its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens. This &quot;scale-then-compress&quot; approach enables NVILA to efficiently process high-resolution images and long videos. We also conduct a systematic investigation to enhance the efficiency of NVILA throughout its entire lifecycle, from training and fine-tuning to deployment. NVILA matches or surpasses the accuracy of many leading open and proprietary VLMs across a wide range of image and video benchmarks. At the same time, it reduces training costs by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by 1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code and models available to facilitate reproducibility.</span><br><span class="line"></span><br><span class="line">💡 News</span><br><span class="line">[2025/1] As of January 6, 2025 VILA is now part of the new Cosmos Nemotron vision language models.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/collections/HIT-TMG/kalm-embedding-67316afa4c56f4fc1f58764b</span><br><span class="line">New open multilingual embedding models released! KaLM-Embedding is a series of embedding models built on Qwen 2 0.5B and released under MIT. 👀</span><br><span class="line">TL;DR:</span><br><span class="line">🚀 Built on Qwen2-0.5B trained on 550k synthetic data released under MIT</span><br><span class="line">🧹 Implements ranking consistency filtering to remove noisy and false negative samples</span><br><span class="line">📊 Achieves 64.53 average score on MTEB benchmark (64.13 C-MTEB, 64.94 MTEB)</span><br><span class="line">🎯 Supports flexible dimension embedding through Matryoshka Representation Learning</span><br><span class="line">🌍 Strong multilingual performance outperforms other open models</span><br><span class="line">🤗 Integrated into sentence-transformers available on Hugging Face</span><br><span class="line">KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model</span><br><span class="line">Published on Jan 2</span><br><span class="line">Authors:</span><br><span class="line"></span><br><span class="line">Xinshuo Hu</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Zifei Shan</span><br><span class="line">,</span><br><span class="line">Xinping Zhao</span><br><span class="line">,</span><br><span class="line">Zetian Sun</span><br><span class="line">,</span><br><span class="line">Zhenyu Liu</span><br><span class="line">,</span><br><span class="line">Dongfang Li</span><br><span class="line">,</span><br><span class="line">Shaolin Ye</span><br><span class="line">,</span><br><span class="line">Xinyuan Wei</span><br><span class="line">,</span><br><span class="line">Qian Chen</span><br><span class="line">,</span><br><span class="line">Baotian Hu</span><br><span class="line">,</span><br><span class="line">Min Zhang</span><br><span class="line">Abstract</span><br><span class="line">As retrieval-augmented generation prevails in large language models, embedding models are becoming increasingly crucial. Despite the growing number of general embedding models, prior work often overlooks the critical role of training data quality. In this work, we introduce KaLM-Embedding, a general multilingual embedding model that leverages a large quantity of cleaner, more diverse, and domain-specific training data. Our model has been trained with key techniques proven to enhance performance: (1) persona-based synthetic data to create diversified examples distilled from LLMs, (2) ranking consistency filtering to remove less informative samples, and (3) semi-homogeneous task batch sampling to improve training efficacy. Departing from traditional BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model, facilitating the adaptation of auto-regressive language models for general embedding tasks. Extensive evaluations of the MTEB benchmark across multiple languages show that our model outperforms others of comparable size, setting a new standard for multilingual embedding models with &lt;1B parameters.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/pdf/2501.05366</span><br><span class="line">[Submitted on 9 Jan 2025]</span><br><span class="line">Search-o1: Agentic Search-Enhanced Large Reasoning Models</span><br><span class="line">Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, Zhicheng Dou</span><br><span class="line">Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive long stepwise reasoning capabilities through large-scale reinforcement learning. However, their extended reasoning processes often suffer from knowledge insufficiency, leading to frequent uncertainties and potential errors. To address this limitation, we introduce \textbf&#123;Search-o1&#125;, a framework that enhances LRMs with an agentic retrieval-augmented generation (RAG) mechanism and a Reason-in-Documents module for refining retrieved documents. Search-o1 integrates an agentic search workflow into the reasoning process, enabling dynamic retrieval of external knowledge when LRMs encounter uncertain knowledge points. Additionally, due to the verbose nature of retrieved documents, we design a separate Reason-in-Documents module to deeply analyze the retrieved information before injecting it into the reasoning chain, minimizing noise and preserving coherent reasoning flow. Extensive experiments on complex reasoning tasks in science, mathematics, and coding, as well as six open-domain QA benchmarks, demonstrate the strong performance of Search-o1. This approach enhances the trustworthiness and applicability of LRMs in complex reasoning tasks, paving the way for more reliable and versatile intelligent systems. The code is available at \url&#123;this https URL&#125;.</span><br><span class="line"></span><br><span class="line">We propose Search-o1, the first framework that integrates the agentic search workflow into the</span><br><span class="line">o1-like reasoning process of LRM for achieving autonomous knowledge supplementation.</span><br><span class="line">• To effectively integrate external knowledge during reasoning, Search-o1 combines the reasoning</span><br><span class="line">process with an agentic RAG mechanism and a knowledge refinement module. This design enables</span><br><span class="line">the LRM to retrieve external knowledge on demand, seamlessly incorporating it into the reasoning</span><br><span class="line">chain while maintaining the original logical flow.</span><br><span class="line">• With five complex reasoning domains and six open-domain QA benchmarks, we demonstrate that</span><br><span class="line">Search-o1 achieves remarkable performance in the reasoning field while maintaining substantial</span><br><span class="line">improvements in the general knowledge. Further quantitative analysis confirms its efficiency and</span><br><span class="line">scalability, offering practical guidance for trustworthy reasoning in LRMs.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/pdf/2501.03220</span><br><span class="line">ProTracker: Probabilistic Integration for Robust and Accurate Point Tracking</span><br><span class="line">Tingyang Zhang1,2 Chen Wang1 Zhiyang Dou1,3 Qingzhe Gao4</span><br><span class="line">Jiahui Lei1 Baoquan Chen2 Lingjie Liu1</span><br><span class="line">1University of Pennsylvania 2Peking University</span><br><span class="line">3The University of Hong Kong 4Shandong University</span><br><span class="line">&#123;tyzh,chenw30,zydou,leijh,lingjie.liu&#125;@seas.upenn.edu;</span><br><span class="line">gaoqingzhe97@gmail.com; baoquan@pku.edu.cn</span><br><span class="line">Figure 1. Visualization of tracking trajectories in various videos. Our method robustly recovers each point’s complete trajectory without</span><br><span class="line">drifting over time, even in challenging scenarios such as occlusions and multiple similar regions.</span><br><span class="line">Abstract</span><br><span class="line">In this paper, we propose ProTracker, a novel framework</span><br><span class="line">for robust and accurate long-term dense tracking of arbitrary points in videos. The key idea of our method is incorporating probabilistic integration to refine multiple predictions from both optical flow and semantic features for</span><br><span class="line">robust short-term and long-term tracking. Specifically, we</span><br><span class="line">integrate optical flow estimations in a probabilistic manner,</span><br><span class="line">producing smooth and accurate trajectories by maximizing</span><br><span class="line">the likelihood of each prediction. To effectively re-localize</span><br><span class="line">challenging points that disappear and reappear due to occlusion, we further incorporate long-term feature correspondence into our flow predictions for continuous trajectory generation. Extensive experiments show that ProTracker achieves the state-of-the-art performance among</span><br><span class="line">unsupervised and self-supervised approaches, and even outperforms supervised methods on several benchmarks. Our</span><br><span class="line">code and model will be publicly available upon publication.</span><br><span class="line">Project page: https://michaelszj.github.io/</span><br><span class="line">protracker</span><br><span class="line">Pipeline</span><br><span class="line"></span><br><span class="line">Pipeline</span><br><span class="line"></span><br><span class="line">Pipeline overview of our proposed method. (1) Sample &amp; Chain: Key points are initially sampled and linked through optical flow chaining to produce preliminary trajectory predictions. (2) Long-term Correspondence: Key points are re-localized over longer time spans to maintain continuity, even for points that temporarily disappear. (3) Dual-Stage Filter: Masks and feature filters are applied to remove incorrect predictions, reducing noise for subsequent steps. (4) Probabilistic Integration: Filtered flow predictions across frames are first integrated and then combined with long-term keypoint to produce the final prediction, producing smoother and more consistent trajectories.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TAP-Vid-DAVIS Comparisons</span><br><span class="line"></span><br><span class="line">Qualitative comparisons to DINO-Tracker [1], CaDex++ [2] and LocoTrack [3] on TAP-Vid-DAVIS [7].</span><br><span class="line">Our method is able to capture finer details and recover the full trajectory of less distinctive points.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PreviousNext</span><br><span class="line"></span><br><span class="line">Qualitative comparisons to TAPTR [4], SpaTrack [5] and Co-Tracker [6] on TAP-Vid-DAVIS [7].</span><br><span class="line">While these sliding window based trackers are prone to drift and vulnerable to occlusions, our method reliably maintains accurate tracking of the same point.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PreviousNext</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Comparisons on challenging videos</span><br><span class="line"></span><br><span class="line">To further illustrate our method&#x27;s robustness, we conduct experiments on challenging videos from the web.Some of the previous methods relies on computing heatmap between the query point and the target frame. However, the per-frame heatmap lacks temporal-awareness and may confuse between different objects. We address this issue by leveraging mask and combining the heatmap with optical flow. By comparing the results of our method with DINO-Tracker[1] and TAPIR[8], we show that although our method also relies on per-frame heatmap to extract keypoints, our method has strong temporal-awareness and is able to tell between similar objects.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PreviousNext</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">To further demonstrate the robustness of our method, we conduct experiments on extended videos from TAP-Vid-DAVIS, simulating high frame-rate videos by repeating each frame three times. In contrast to typical sliding-window or flow-based trackers (such as TAPTR [4], SpatialTracker [5] and Co-Tracker [6]), which tend to accumulate errors and drift over time, our integration of long-term key points with short-term optical flow enables continuous, drift-free tracking of the same point through occlusions. Experiments are conducted in full resolution.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PreviousNext</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Ablations</span><br><span class="line"></span><br><span class="line">We conduct ablation study on different components of our method. w/o key indicates directly using the results from the flow integration as output without the joint integration with long-term key points. w/o geo removes the process of filtering by the geometry-aware feature. w/o mask uses the rough flow prediction without object-level filtering. w/o pro replaces the probabilistic integration by choosing the prediction of the lowest σ as the final results. We visualize the results on libby, parkour, horsejump-high, shooting and car-roundabout, respectively.</span><br><span class="line">The results shows that without long-term keypoints, the methods cannot locate some point when they appear after occlusion (e.g. libby, parkour);</span><br><span class="line">without geometry-aware feature, the methods may drift to other parts (e.g. car-roundabout,shooting);</span><br><span class="line">without mask, the methods may confuse between different objects (e.g. parkour, shooting);</span><br><span class="line">without probabilistic integration, the methods can be less accurate (e.g. car-roundabout, horsejump-high).</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/pdf/2501.01880</span><br><span class="line">Long Context vs. RAG for LLMs: An Evaluation and Revisits</span><br><span class="line">Xinze Li1</span><br><span class="line">, Yixin Cao2†</span><br><span class="line">, Yubo Ma1</span><br><span class="line">, Aixin Sun1†</span><br><span class="line">1 S-Lab, Nanyang Technological University</span><br><span class="line">2 School of Computer Science, Fudan University</span><br><span class="line">&#123;xinze002, yubo001&#125;@e.ntu.edu.sg axsun@ntu.edu.sg</span><br><span class="line">yxcao@fudan.edu.cn</span><br><span class="line">Abstract</span><br><span class="line">Extending context windows (i.e., Long Context, LC) and using retrievers to selectively</span><br><span class="line">access relevant information (i.e., RetrievalAugmented Generation, RAG) are the two main</span><br><span class="line">strategies to enable LLMs to incorporate extremely long external contexts. This paper revisits recent studies on this topic, highlighting their key insights and discrepancies. We</span><br><span class="line">then provide a more comprehensive evaluation by filtering out questions answerable without external context, identifying the most effective retrieval methods, and expanding the</span><br><span class="line">datasets. We show that LC generally outperforms RAG in question-answering benchmarks, especially for Wikipedia-based questions. Summarization-based retrieval performs</span><br><span class="line">comparably to LC, while chunk-based retrieval</span><br><span class="line">lags behind. However, RAG has advantages in</span><br><span class="line">dialogue-based and general question queries.</span><br><span class="line">These insights underscore the trade-offs between RAG and LC strategies, offering guidance for future optimization of LLMs with external knowledge sources. We also provide an</span><br><span class="line">in-depth discussion on this topic, highlighting</span><br><span class="line">the overlooked importance of context relevance</span><br><span class="line">in existing studies.</span><br><span class="line"></span><br><span class="line">Our key contributions in this paper are as follows:</span><br><span class="line">(i) Providing a comprehensive survey of existing</span><br><span class="line">studies on LC and RAG, analyzing their implementations and key insights. (ii) Proposing a fair and</span><br><span class="line">systematic evaluation framework, and performing</span><br><span class="line">detailed analyses to understand the strengths and</span><br><span class="line">limitations of LC and RAG. (iii) Discussing chal1The experiment code and expanded datasets are available</span><br><span class="line">at https://github.com/lixinze777/LC_VS_RAG</span><br><span class="line">lenges for comparing and combining LC and RAG,</span><br><span class="line">reflecting on the key points that researchers tend to</span><br><span class="line">overlook in this field. Evaluation results indicate</span><br><span class="line">that LC models generally outperform RAG when</span><br><span class="line">processing self-contained information like stories,</span><br><span class="line">while RAG excels at handling fragmented information, particularly in dialogue-based contexts.</span><br><span class="line">These experiments deepen our understanding of the</span><br><span class="line">strengths and limitations of LC and RAG, offering</span><br><span class="line">valuable insights into optimizing retrieval strategies and effectively integrating these approaches to</span><br><span class="line">enhance performance in open-domain question answering. These findings also based on a systematic</span><br><span class="line">survey of existing studies on this topic (see § 2).</span><br><span class="line">Additionally, we discuss key aspects of comparing</span><br><span class="line">LC and RAG in § 6, highlighting areas that have</span><br><span class="line">been underexplored in prior research.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huyenchip.com/2025/01/07/agents.html</span><br><span class="line">Agents</span><br><span class="line">Jan 7, 2025 • Chip Huyen</span><br><span class="line"></span><br><span class="line">Intelligent agents are considered by many to be the ultimate goal of AI. The classic book by Stuart Russell and Peter Norvig, Artificial Intelligence: A Modern Approach (Prentice Hall, 1995), defines the field of AI research as “the study and design of rational agents.”</span><br><span class="line"></span><br><span class="line">The unprecedented capabilities of foundation models have opened the door to agentic applications that were previously unimaginable. These new capabilities make it finally possible to develop autonomous, intelligent agents to act as our assistants, coworkers, and coaches. They can help us create a website, gather data, plan a trip, do market research, manage a customer account, automate data entry, prepare us for interviews, interview our candidates, negotiate a deal, etc. The possibilities seem endless, and the potential economic value of these agents is enormous.</span><br><span class="line"></span><br><span class="line">This section will start with an overview of agents and then continue with two aspects that determine the capabilities of an agent: tools and planning. Agents, with their new modes of operations, have new modes of failure. This section will end with a discussion on how to evaluate agents to catch these failures.</span><br><span class="line"></span><br><span class="line">This post is adapted from the Agents section of AI Engineering (2025) with minor edits to make it a standalone post.</span><br><span class="line"></span><br><span class="line">Notes:</span><br><span class="line"></span><br><span class="line">AI-powered agents are an emerging field with no established theoretical frameworks for defining, developing, and evaluating them. This section is a best-effort attempt to build a framework from the existing literature, but it will evolve as the field does. Compared to the rest of the book, this section is more experimental. I received helpful feedback from early reviewers, and I hope to get feedback from readers of this blog post, too.</span><br><span class="line">Just before this book came out, Anthropic published a blog post on Building effective agents (Dec 2024). I’m glad to see that Anthropic’s blog post and my agent section are conceptually aligned, though with slightly different terminologies. However, Anthropic’s post focuses on isolated patterns, whereas my post covers why and how things work. I also focus more on planning, tool selection, and failure modes.</span><br><span class="line">The post contains a lot of background information. Feel free to skip ahead if it feels a little too in the weeds!</span><br><span class="line">Agents Overview</span><br><span class="line">Great write-up on Agents by Chip.</span><br><span class="line">Here are my takeaways:</span><br><span class="line">🤖 Agents Overview</span><br><span class="line">An AI agent is made up of both the environment it operates in (e.g., a game, the internet, or computer system) and the set of actions it can perform through its available tools. This dual definition is fundamental to understanding how agents work.</span><br><span class="line">👨‍💻 Agent Example</span><br><span class="line">The figure shows an example of an agent built on top of GPT-4. The environment is the computer which has access to a terminal and filesystem. The set of action include navigate, searching files, viewing files, etc.</span><br><span class="line">🧰 Importance of Tools</span><br><span class="line">Tools allow agents to both perceive their environment (through read actions) and modify it (through write actions). Adding appropriate tools can dramatically expand what an agent can do, from performing calculations to accessing real-time information.</span><br><span class="line">💡 Tool Selection</span><br><span class="line">More tools give agents more capabilities but also make it harder for them to use them effectively. Finding the right tool inventory requires careful experimentation and analysis of usage patterns.</span><br><span class="line">🧩 Planning</span><br><span class="line">Effective agents require robust planning capabilities to break down complex tasks into manageable steps. This planning should ideally be decoupled from execution to allow for validation before running potentially costly or time-consuming operations.</span><br><span class="line">📍 Foundation Models Can Act as Planners</span><br><span class="line">While there&#x27;s debate about whether LLMs can truly plan, they can be effective components of planning systems, especially when augmented with appropriate tools and reflection capabilities.</span><br><span class="line">⛓️ Multi-Agent Systems</span><br><span class="line">Most practical agent implementations are multi-agent systems, with different components handling plan generation, validation, and execution. This separation of concerns allows for better specialization and error handling.</span><br><span class="line">🎛️ Control Flows</span><br><span class="line">Agent plans can involve various control flows beyond simple sequential execution, including parallel execution, conditional statements, and loops. However, more complex control flows are harder to generate and execute correctly.</span><br><span class="line">💭 Reflection and Error Correction</span><br><span class="line">While not strictly required, reflection capabilities (the ability to evaluate progress and correct mistakes) significantly improve agent performance. This can be implemented through self-critique or separate evaluation components.</span><br><span class="line">❌ Failure Modes</span><br><span class="line">Agents can fail in multiple ways, including planning failures (invalid tools or parameters), tool execution failures (incorrect outputs), and efficiency failures (taking too long or using too many resources).</span><br><span class="line">📈 Evaluation</span><br><span class="line">Proper agent evaluation needs to consider multiple metrics, including success rate, efficiency, cost, and time taken. This should be done across different tasks and compared against appropriate baselines.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>기술적으로 최대한 자세하게 적어. 12개의 기사가 있고 하나도 빼먹지 말고 적어.</p>
</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-01-13</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/01/13/2025-01-13-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2025년 01월 13일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/05/17/AI-%E1%84%82%E1%85%B2%E1%84%89%E1%85%B3-for-2024%E1%84%82%E1%85%A7%E1%86%AB-5%E1%84%8B%E1%85%AF%E1%86%AF-17%E1%84%8B%E1%85%B5%E1%86%AF/" title="2024년 5월 17일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/01/06/2025-01-06-AI-NEWS/" title="2025년 01월 06일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>