<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 5월 17일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="SummaryOpenAI는 ChatGPT의 데이터 분석 기능을 개선하고 Google Drive 및 Microsoft OneDrive와의 통합을 통해 사용자 경험을 향상시켰습니다. 또한 OpenAI와 Reddit은 새로운 파트너십을 통해 ChatGPT에서 Reddit의 "><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 5월 17일 AI 소식</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>OpenAI는 ChatGPT의 데이터 분석 기능을 개선하고 Google Drive 및 Microsoft OneDrive와의 통합을 통해 사용자 경험을 향상시켰습니다. 또한 OpenAI와 Reddit은 새로운 파트너십을 통해 ChatGPT에서 Reddit의 콘텐츠 접근성을 강화하고, Apple은 Eye Tracking, Music Haptics 등의 새로운 접근성 기능을 발표했습니다. Vision-language models(VLMs)의 설계 결정을 분석한 연구와 Google의 Data Science Agent 실험도 주목받고 있습니다.</p>
<h2 id="Improvements-to-Data-Analysis-in-ChatGPT"><a href="#Improvements-to-Data-Analysis-in-ChatGPT" class="headerlink" title="Improvements to Data Analysis in ChatGPT"></a>Improvements to Data Analysis in ChatGPT</h2><ul>
<li><strong>Link</strong>: <a target="_blank" rel="noopener" href="https://openai.com/index/improvements-to-data-analysis-in-chatgpt/">Improvements to Data Analysis in ChatGPT</a></li>
<li><strong>Date</strong>: 2024년 5월 16일</li>
<li>ChatGPT의 데이터 분석 기능 강화</li>
<li>Google Drive와 Microsoft OneDrive에서 파일 직접 업로드 가능</li>
<li>테이블과 차트를 실시간으로 상호작용하고 확장 가능한 뷰 제공</li>
<li>GPT-4o 모델을 통해 데이터 분석 기능 제공</li>
<li>파이썬 코드를 작성 및 실행하여 데이터 파일 분석</li>
<li>파일을 데스크탑에 다운로드하지 않고 바로 ChatGPT에 업로드 가능</li>
<li>대화 중 차트를 사용자 맞춤화하고 다운로드 가능</li>
</ul>
<h2 id="OpenAI-and-Reddit-Partnership"><a href="#OpenAI-and-Reddit-Partnership" class="headerlink" title="OpenAI and Reddit Partnership"></a>OpenAI and Reddit Partnership</h2><ul>
<li><strong>Link</strong>: <a target="_blank" rel="noopener" href="https://openai.com/index/openai-and-reddit-partnership/">OpenAI and Reddit Partnership</a></li>
<li><strong>Date</strong>: 2024년 5월 16일</li>
<li>Reddit의 콘텐츠를 ChatGPT에 통합</li>
<li>Reddit의 Data API를 활용하여 실시간 콘텐츠 제공</li>
<li>OpenAI의 AI 도구들이 Reddit 콘텐츠를 더 잘 이해하고 보여줄 수 있게 됨</li>
<li>Reddit은 OpenAI 플랫폼을 활용하여 AI 기능 강화</li>
<li>OpenAI, Reddit의 광고 파트너가 됨</li>
</ul>
<h2 id="Apple-Announces-New-Accessibility-Features-Including-Eye-Tracking"><a href="#Apple-Announces-New-Accessibility-Features-Including-Eye-Tracking" class="headerlink" title="Apple Announces New Accessibility Features, Including Eye Tracking"></a>Apple Announces New Accessibility Features, Including Eye Tracking</h2><ul>
<li><strong>Link</strong>: <a target="_blank" rel="noopener" href="https://www.apple.com/newsroom/2024/05/apple-announces-new-accessibility-features-including-eye-tracking/">Apple Announces New Accessibility Features, Including Eye Tracking</a></li>
<li><strong>Date</strong>: 2024년 5월 15일</li>
<li>Eye Tracking을 통해 사용자가 눈으로 iPad와 iPhone을 제어 가능</li>
<li>Music Haptics는 청각 장애인 사용자에게 음악을 촉각적으로 경험할 수 있게 함</li>
<li>Vocal Shortcuts는 사용자가 사용자 정의 소리를 만들어 작업을 수행할 수 있게 함</li>
<li>Vehicle Motion Cues는 차량 이동 중 사용자가 멀미를 줄일 수 있게 도와줌</li>
<li>visionOS에 다양한 접근성 기능 추가</li>
</ul>
<h2 id="What-Matters-When-Building-Vision-Language-Models"><a href="#What-Matters-When-Building-Vision-Language-Models" class="headerlink" title="What Matters When Building Vision-Language Models?"></a>What Matters When Building Vision-Language Models?</h2><ul>
<li><strong>Link</strong>: <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2405.02246">What Matters When Building Vision-Language Models?</a></li>
<li><strong>Date</strong>: 2024년 5월 4일</li>
<li>VLMs 설계 결정이 종종 근거 없이 이루어지는 문제 지적</li>
<li>Idefics2라는 80억 파라미터의 VLM 개발 및 성능 입증</li>
<li>다양한 멀티모달 벤치마크에서 동급 최고 성능 달성</li>
</ul>
<h2 id="Data-Science-Agent-Experiment"><a href="#Data-Science-Agent-Experiment" class="headerlink" title="Data Science Agent Experiment"></a>Data Science Agent Experiment</h2><ul>
<li><strong>Link</strong>: <a target="_blank" rel="noopener" href="https://labs.google.com/code/dsa">Data Science Agent</a></li>
<li><strong>Date</strong>: 2024년 5월 16일</li>
<li>AI를 이용한 Google Colab 노트북 생성 실험</li>
<li>데이터 청소, 탐색, 시각화, 예측 모델링 등의 작업 자동화</li>
<li>사용자가 데이터셋을 업로드하고 원하는 분석 목표를 설명하면 AI가 노트북 생성</li>
<li>피드백 시스템을 통해 사용자의 요구에 맞춘 커스터마이징 가능</li>
<li>Google 계정 필요 및 미국 내 사용자만 사용 가능</li>
</ul>
<details>
  <summary>Sources</summary>
  This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is: # AI News for (today's date), ## Summary (overall short summary), ## Link1 Title, link, date - detailed summary1, - detailed summary2, - detailed summary..N, ## Link2 Title, link, date - detailed summary1, - detailed summary2, - detailed point..N, etc. The report should be written in Korean and use the 개조식 문체 style. give the very deep details for each link as much as possible.

<p><a target="_blank" rel="noopener" href="https://openai.com/index/improvements-to-data-analysis-in-chatgpt/">https://openai.com/index/improvements-to-data-analysis-in-chatgpt/</a><br>May 16, 2024</p>
<p>Improvements to data analysis in ChatGPT<br>Interact with tables and charts and add files directly from Google Drive and Microsoft OneDrive.</p>
<p>ChatGPT Charts Blog Summary v3<br>Today, we’re starting to roll out enhancements to data analysis:</p>
<p>Upload the latest file versions directly from Google Drive and Microsoft OneDrive</p>
<p>Interact with tables and charts in a new expandable view</p>
<p>Customize and download charts for presentations and documents</p>
<p>Data analysis improvements will be available in our new flagship model, GPT-4o, for ChatGPT Plus, Team, and Enterprise users over the coming weeks.</p>
<p>How data analysis works in ChatGPT<br>These improvements build on ChatGPT’s ability to understand datasets and complete tasks in natural language. To start, upload one or more data files, and ChatGPT will analyze your data by writing and running Python code on your behalf. It can handle a range of data tasks, like merging and cleaning large datasets, creating charts, and uncovering insights. This makes it easier for beginners to perform in-depth analyses and saves experts time on routine data-cleaning tasks.</p>
<p>“ChatGPT is part of my toolkit for analyzing customer data, which has become too large and complex for Excel. It helps me sift through massive datasets, allowing me to conduct more data exploration on my own and reduce the time it takes to reach valuable insights.”<br>David Vaughn, VP, The Carlyle Group<br>Add files directly from Google Drive and Microsoft OneDrive<br>Instead of downloading files to your desktop and then uploading them to ChatGPT, you can now add various file types directly from your Google Drive or Microsoft OneDrive. This allows ChatGPT to understand your Google Sheets, Docs, Slides, and Microsoft Excel, Word, and PowerPoint files more quickly.</p>
<p>Work on tables in real-time<br>When you add a dataset, ChatGPT will create an interactive table that you can expand to a full-screen view so you can follow along as it updates during your analysis. Click on specific areas to ask follow-up questions, or choose from one of ChatGPT’s suggested prompts to go deeper into your analysis.</p>
<p>For example, you can ask ChatGPT to combine spreadsheets of monthly expenses and create a pivot table categorized by expense type.</p>
<p>“ChatGPT walks me through data analysis and helps me better understand insights. It makes my job more fulfilling, helps me learn, and frees up my time to focus on more strategic parts of my job.”<br>Lauren Nowak, Marketing Manager, Afterpay<br>Customize presentation-ready charts<br>You can now customize and interact with bar, line, pie, and scatter plot charts in the conversation. Hover over chart elements, ask additional questions, or select colors. When ready, download to use in presentations or documents.</p>
<p>For example, you can select a Google Sheet with your company’s latest user data directly from Google Drive and ask ChatGPT to create a chart showing retention rates by cohort.</p>
<p>These new interactive features cover many chart types. ChatGPT will generate a static version for charts that aren’t supported.</p>
<p>Comprehensive security and privacy<br>As with any feature in ChatGPT, trust and data privacy are at the core of OpenAI’s mission. We don’t train on data from ChatGPT Team and Enterprise customers, and ChatGPT Plus users can opt out of training through their Data Controls(opens in a new window). Learn more about our privacy and security policies, including SAML SSO, compliance, and data encryption for ChatGPT Enterprise.</p>
<p>Announcements</p>
<p><a target="_blank" rel="noopener" href="https://openai.com/index/openai-and-reddit-partnership/">https://openai.com/index/openai-and-reddit-partnership/</a><br>May 16, 2024</p>
<p>OpenAI and Reddit Partnership<br>We’re bringing Reddit’s content to ChatGPT and our products.</p>
<p>Media: OpenAI and Reddit Partnership<br>Editor’s Note: This post was originally published by Reddit(opens in a new window).</p>
<p>Keeping the internet open is crucial, and part of being open means Reddit content needs to be accessible to those fostering human learning and researching ways to build community, belonging, and empowerment online. Reddit is a uniquely large and vibrant community that has long been an important space for conversation on the internet. Additionally, using LLMs, ML, and AI allow Reddit to improve the user experience for everyone.</p>
<p>In line with this, Reddit and OpenAI today announced a partnership to benefit both the Reddit and OpenAI user communities in a number of ways:</p>
<p>OpenAI will bring enhanced Reddit content to ChatGPT and new products, helping users discover and engage with Reddit communities. To do so, OpenAI will access Reddit’s Data API, which provides real-time, structured, and unique content from Reddit. This will enable OpenAI’s AI tools to better understand and showcase Reddit content, especially on recent topics.</p>
<p>This partnership will also enable Reddit to bring new AI-powered features to redditors and mods. Reddit will be building on OpenAI’s platform of AI models to bring its powerful vision to life.</p>
<p>Lastly, OpenAI will become a Reddit advertising partner.</p>
<p>“We are thrilled to partner with Reddit to enhance ChatGPT with uniquely timely and relevant information, and to explore the possibilities to enrich the Reddit experience with AI-powered features.”<br>Brad Lightcap, OpenAI COO<br>“Reddit has become one of the internet’s largest open archives of authentic, relevant, and always up to date human conversations about anything and everything. Including it in ChatGPT upholds our belief in a connected internet, helps people find more or what they’re looking for, and helps new audiences find community on Reddit.”<br>Steve Huffman, Reddit Co-Founder and CEO<br>OpenAI Disclosure: Sam Altman is a shareholder in Reddit. This partnership was led by OpenAI’s COO and approved by its independent Board of Directors.</p>
<p><a target="_blank" rel="noopener" href="https://www.apple.com/newsroom/2024/05/apple-announces-new-accessibility-features-including-eye-tracking/">https://www.apple.com/newsroom/2024/05/apple-announces-new-accessibility-features-including-eye-tracking/</a></p>
<p>PRESS RELEASE<br>May 15, 2024<br>Apple announces new accessibility features, including Eye Tracking, Music Haptics, and Vocal Shortcuts</p>
<p><a target="_blank" rel="noopener" href="https://nr.apple.com/dP9x0E3hu7">https://nr.apple.com/dP9x0E3hu7</a></p>
<p>Coming later this year, Apple’s new accessibility features include Eye Tracking, a way for users to navigate iPad and iPhone with just their eyes.<br>CUPERTINO, CALIFORNIA Apple today announced new accessibility features coming later this year, including Eye Tracking, a way for users with physical disabilities to control iPad or iPhone with their eyes. Additionally, Music Haptics will offer a new way for users who are deaf or hard of hearing to experience music using the Taptic Engine in iPhone; Vocal Shortcuts will allow users to perform tasks by making a custom sound; Vehicle Motion Cues can help reduce motion sickness when using iPhone or iPad in a moving vehicle; and more accessibility features will come to visionOS. These features combine the power of Apple hardware and software, harnessing Apple silicon, artificial intelligence, and machine learning to further Apple’s decades-long commitment to designing products for everyone.<br>“We believe deeply in the transformative power of innovation to enrich lives,” said Tim Cook, Apple’s CEO. “That’s why for nearly 40 years, Apple has championed inclusive design by embedding accessibility at the core of our hardware and software. We’re continuously pushing the boundaries of technology, and these new features reflect our long-standing commitment to delivering the best possible experience to all of our users.”<br>“Each year, we break new ground when it comes to accessibility,” said Sarah Herrlinger, Apple’s senior director of Global Accessibility Policy and Initiatives. “These new features will make an impact in the lives of a wide range of users, providing new ways to communicate, control their devices, and move through the world.”<br>Eye Tracking Comes to iPad and iPhone<br>Powered by artificial intelligence, Eye Tracking gives users a built-in option for navigating iPad and iPhone with just their eyes. Designed for users with physical disabilities, Eye Tracking uses the front-facing camera to set up and calibrate in seconds, and with on-device machine learning, all data used to set up and control this feature is kept securely on device, and isn’t shared with Apple.<br>Eye Tracking works across iPadOS and iOS apps, and doesn’t require additional hardware or accessories. With Eye Tracking, users can navigate through the elements of an app and use Dwell Control to activate each element, accessing additional functions such as physical buttons, swipes, and other gestures solely with their eyes.<br>Music Haptics Makes Songs More Accessible<br>Music Haptics is a new way for users who are deaf or hard of hearing to experience music on iPhone. With this accessibility feature turned on, the Taptic Engine in iPhone plays taps, textures, and refined vibrations to the audio of the music. Music Haptics works across millions of songs in the Apple Music catalog, and will be available as an API for developers to make music more accessible in their apps.<br>Pause playback of video: Music Haptics on iPhone 15 Pro<br>Music Haptics is a new way for users who are deaf or hard of hearing to experience music on iPhone.<br>New Features for a Wide Range of Speech<br>With Vocal Shortcuts, iPhone and iPad users can assign custom utterances that Siri can understand to launch shortcuts and complete complex tasks. Listen for Atypical Speech, another new feature, gives users an option for enhancing speech recognition for a wider range of speech. Listen for Atypical Speech uses on-device machine learning to recognize user speech patterns. Designed for users with acquired or progressive conditions that affect speech, such as cerebral palsy, amyotrophic lateral sclerosis (ALS), or stroke, these features provide a new level of customization and control, building on features introduced in iOS 17 for users who are nonspeaking or at risk of losing their ability to speak.<br>On iPhone 15 Pro, a screen reads “Set Up Vocal Shortcuts” and prompts the user to choose an action and record a phrase to teach their iPhone how to recognize their voice.<br>On iPhone 15 Pro, a screen reads “Say ‘Rings’ One Last Time,” and prompts the user to teach iPhone to recognize the phrase by saying it three times.<br>On iPhone 15 Pro, a user gets an alert from Vocal Shortcuts that says “Open Activity Rings.”<br>On iPhone 15 Pro, a screen reads “Set Up Vocal Shortcuts” and prompts the user to choose an action and record a phrase to teach their iPhone how to recognize their voice.<br>With Vocal Shortcuts, iPhone and iPad users can assign custom utterances that Siri can understand to launch shortcuts and complete complex tasks.<br>previous<br>next<br>“Artificial intelligence has the potential to improve speech recognition for millions of people with atypical speech, so we are thrilled that Apple is bringing these new accessibility features to consumers,” said Mark Hasegawa-Johnson, the Speech Accessibility Project at the Beckman Institute for Advanced Science and Technology at the University of Illinois Urbana-Champaign’s principal investigator. “The Speech Accessibility Project was designed as a broad-based, community-supported effort to help companies and universities make speech recognition more robust and effective, and Apple is among the accessibility advocates who made the Speech Accessibility Project possible.”<br>Vehicle Motion Cues Can Help Reduce Motion Sickness<br>Vehicle Motion Cues is a new experience for iPhone and iPad that can help reduce motion sickness for passengers in moving vehicles. Research shows that motion sickness is commonly caused by a sensory conflict between what a person sees and what they feel, which can prevent some users from comfortably using iPhone or iPad while riding in a moving vehicle. With Vehicle Motion Cues, animated dots on the edges of the screen represent changes in vehicle motion to help reduce sensory conflict without interfering with the main content. Using sensors built into iPhone and iPad, Vehicle Motion Cues recognizes when a user is in a moving vehicle and responds accordingly. The feature can be set to show automatically on iPhone, or can be turned on and off in Control Center.<br>Pause playback of video: Vehicle Motion Cues on iPhone 15 Pro<br>Vehicle Motion Cues is a new experience for iPhone and iPad that can help reduce motion sickness for passengers in moving vehicles.<br>CarPlay Gets Voice Control, More Accessibility Updates<br>Accessibility features coming to CarPlay include Voice Control, Color Filters, and Sound Recognition. With Voice Control, users can navigate CarPlay and control apps with just their voice. With Sound Recognition, drivers or passengers who are deaf or hard of hearing can turn on alerts to be notified of car horns and sirens. For users who are colorblind, Color Filters make the CarPlay interface visually easier to use, with additional visual accessibility features including Bold Text and Large Text.<br>The new Sound Recognition feature in CarPlay alerts a user of a potential siren sound.<br>Updates to CarPlay include Sound Recognition, which allows drivers or passengers who are deaf or hard of hearing to turn on alerts to be notified of car horns and sirens.<br>Accessibility Features Coming to visionOS<br>This year, accessibility features coming to visionOS will include systemwide Live Captions to help everyone — including users who are deaf or hard of hearing — follow along with spoken dialogue in live conversations and in audio from apps. With Live Captions for FaceTime in visionOS, more users can easily enjoy the unique experience of connecting and collaborating using their Persona. Apple Vision Pro will add the capability to move captions using the window bar during Apple Immersive Video, as well as support for additional Made for iPhone hearing devices and cochlear hearing processors. Updates for vision accessibility will include the addition of Reduce Transparency, Smart Invert, and Dim Flashing Lights for users who have low vision, or those who want to avoid bright lights and frequent flashing.<br>The Live Captions experience in visionOS is shown from an Apple Vision Pro user’s point of view.<br>visionOS will offer Live Captions, so users who are deaf or hard of hearing can follow along with spoken dialogue in live conversations and in audio from apps.<br>These features join the dozens of accessibility features already available in Apple Vision Pro, which offers a flexible input system and an intuitive interface designed with a wide range of users in mind. Features such as VoiceOver, Zoom, and Color Filters can also provide users who are blind or have low vision access to spatial computing, while features such as Guided Access can support users with cognitive disabilities. Users can control Vision Pro with any combination of their eyes, hands, or voice, with accessibility features including Switch Control, Sound Actions, and Dwell Control that can also help those with physical disabilities.<br>“Apple Vision Pro is without a doubt the most accessible technology I’ve ever used,” said Ryan Hudson-Peralta, a Detroit-based product designer, accessibility consultant, and cofounder of Equal Accessibility LLC. “As someone born without hands and unable to walk, I know the world was not designed with me in mind, so it’s been incredible to see that visionOS just works. It’s a testament to the power and importance of accessible and inclusive design.”<br>Additional Updates<br>For users who are blind or have low vision, VoiceOver will include new voices, a flexible Voice Rotor, custom volume control, and the ability to customize VoiceOver keyboard shortcuts on Mac.<br>Magnifier will offer a new Reader Mode and the option to easily launch Detection Mode with the Action button.<br>Braille users will get a new way to start and stay in Braille Screen Input for faster control and text editing; Japanese language availability for Braille Screen Input; support for multi-line braille with Dot Pad; and the option to choose different input and output tables.<br>For users with low vision, Hover Typing shows larger text when typing in a text field, and in a user’s preferred font and color.<br>For users at risk of losing their ability to speak, Personal Voice will be available in Mandarin Chinese. Users who have difficulty pronouncing or reading full sentences will be able to create a Personal Voice using shortened phrases.<br>For users who are nonspeaking, Live Speech will include categories and simultaneous compatibility with Live Captions.<br>For users with physical disabilities, Virtual Trackpad for AssistiveTouch allows users to control their device using a small region of the screen as a resizable trackpad.<br>Switch Control will include the option to use the cameras in iPhone and iPad to recognize finger-tap gestures as switches.<br>Voice Control will offer support for custom vocabularies and complex words.<br>The new Reader Mode in Magnifier is shown on iPhone 15 Pro.<br>The Hover Typing experience is shown on iPhone 15 Pro.<br>The Personal Voice experience is shown in Mandarin Chinese on Mac.<br>The new Reader Mode in Magnifier is shown on iPhone 15 Pro.<br>Additional accessibility updates include a new Reader Mode in Magnifier.<br>previous<br>next<br>Celebrate Global Accessibility Awareness Day with Apple<br>This week, Apple is introducing new features, curated collections, and more in celebration of Global Accessibility Awareness Day:<br>Throughout the month of May, select Apple Store locations will host free sessions to help customers explore and discover accessibility features built into the products they love. Apple Piazza Liberty in Milan will feature the talent behind “Assume that I can,” the viral campaign for World Down Syndrome Day. And available year-round at Apple Store locations globally, Today at Apple group reservations are a place where friends, families, schools, and community groups can learn about accessibility features together.<br>Shortcuts adds Calming Sounds, which plays ambient soundscapes to minimize distractions, helping users focus or rest.<br>Visit the App Store to discover incredible apps and games that promote access and inclusion for all, including the accessible App Store Award-winning game Unpacking, apps as tools for augmentative and alternative communication (AAC), and more.<br>The Apple TV app will honor trailblazing creators, performers, and activists who passionately share the experiences of people with disabilities. This year’s theme is Remaking the World, and each story invites viewers to envision a reality where everyone is empowered to add their voice to the greater human story.<br>Apple Books will spotlight lived experiences of disability through curated collections of first-person narratives by disabled writers in ebook and audiobook formats.<br>Apple Fitness+ workouts, meditations, and trainer tips welcome users who are deaf or hard of hearing with American Sign Language, and Time to Walk now includes transcripts in the Apple Podcasts app. Fitness+ workouts always include Audio Hints to support users who are blind or have low vision, as well as modifiers so that users of all levels can participate.<br>Users can visit Apple Support to learn how their Apple devices can be customized using built-in accessibility features. From adapting the gestures to customizing how information is presented on a device’s screen, the Apple Accessibility playlist will help users learn how to personalize Apple Vision Pro, iPhone, iPad, Apple Watch, and Mac to work best for them.</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2405.02246">https://huggingface.co/papers/2405.02246</a><br>What matters when building vision-language models?<br>Published on May 4<br>·<br>Featured in Daily Papers on May 14<br>Authors:</p>
<p>Hugo Laurençon<br>,</p>
<p>Léo Tronchon<br>,<br>Matthieu Cord<br>,</p>
<p>Victor Sanh<br>Abstract<br>The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods. Our consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. We release the model (base, instructed, and chat) along with the datasets created for its training.</p>
<p><a target="_blank" rel="noopener" href="https://labs.google.com/code/dsa">https://labs.google.com/code/dsa</a><br>&#x2F;code</p>
<p>Code Experiments<br>Experiment logo<br>Data Science Agent<br>An experiment to build an AI generated Colab notebook that handles data cleaning, data exploration, plotting, Q&amp;A on data, and predictive modeling.<br>README.md<br>Playground<br>📊🤖 Welcome to Data Science Agent!</p>
<p>Data Science Agent is an experiment designed to streamline your data workflow by using AI to help you generate Google Colab notebooks for various data analysis tasks.</p>
<p>Data Science animation</p>
<p>Why did we build it?</p>
<p>We want to simplify and automate common data science tasks like predictive modeling, data preprocessing, and visualization. The goal is to help data scientists focus on more complex tasks and strategic questions.</p>
<p>How it works</p>
<p>Give the agent a description of what you are looking to learn from your data, and upload up to three datasets. Then, the agent will construct a notebook with the necessary code and explanations. Follow along as it plans, reasons, and writes a Colab notebook for your review.</p>
<p>How we built it</p>
<p>We are using a Gemini model for our agentic flows. Data Science Agent achieves the end goal you’ve set for it by orchestrating a composite flow that mimics a typical data scientist’s workflow, using the Large Language Model (LLM) for task decomposition and planning.</p>
<p>A flow consists of individual atomic flows (or subflows) that specialize in concrete data science tasks such as data cleaning, data exploration, and data plotting. Each atomic flow is a sequence of individual steps, using code execution and execution output feedback to complete a subtask and communicate downstream.</p>
<p>Our framework integrates high-quality natural language to code, the planning and reasoning capabilities of LLMs and code execution as feedback for allowing for agentic capabilities such as self-refinement, error fixing&#x2F;correction, summarization, etc.</p>
<p>Notebooks are generated dynamically by AI, including generated code, code outputs (e.g., plots), and text cells, reflecting language the agent is using to solve the task, for improved notebook readability and functionality</p>
<p>Data Scientist Loop</p>
<p>Features</p>
<p>Code Generation: Create well-structured and documented Python code within a notebook<br>Support a variety of data science tasks including:<br>Data Cleaning: Generate code for handling missing values, outliers, inconsistencies, and formatting issues.<br>Exploratory Data Analysis (EDA): Create visualizations and summary statistics to understand the distribution, relationships, and characteristics of your data. Generate descriptive statistics, histograms, scatter plots, correlation matrices, and other visualizations.<br>Statistical Analysis: Implement hypothesis testing (e.g., t-tests, ANOVA, Chi squared test), correlation analysis, and other statistical techniques to draw meaningful insights from your data.<br>Predictive Modeling: Build and evaluate machine learning models for regression or classification tasks based on your data and objective. Supports modeling tasks such as classification, regression, clustering etc<br>Model Evaluation: Provides metrics such as accuracy, precision, recall, F1-score, and AUC-ROC for model performance assessment.<br>Customization: Tailor the generated notebooks by specifying the libraries, visualization types, algorithms, and&#x2F;or evaluation metrics using natural language in the task description.<br>Getting started</p>
<p>Open the Data Science Agent by navigating to the Playground tab where you can upload your dataset from your local machine or from Google Drive. You can add up to three datasets.<br>If you don’t have a dataset in mind, there are lots of open datasets available to explore publicly on sites like Kaggle. Here’s one about tracking bird migration, or this one about solar and lunar eclipses! We’ve also added an example project for you in the right column if you want to start there. Click on an example to see a dataset and prompt already loaded up in Data Science Agent!<br>After you input your question, hit continue and the model will begin crafting a plan. The next step is where things get fun. Data Science Agent will pause, allowing you to interact with it for the first time. It’ll wait for you to read the plan before executing it, so you’ll have to do that and then click “create Colab.” If you don’t like the plan, you can click “refresh plan” to generate a new one. You’ll start over again with the same prompt, but a generating new plan. Then you can move onto the next step, generating and executing a Colab notebook, by clicking “create notebook.”<br>At this point, Data Science Agent will take some time to execute the plan one step at a time. You can follow along on the plan card and watch it as the results stream into view in real time. Open up each card to see the results as it completes each stage. Once it’s done executing the full plan, you’ll be able to download or view the notebook in Colab.<br>Send us feedback! This project is a WIP, and we would love to know what you think. We’re learning as we go, and we want to make Data Science Agent work well for your use case. After you create your notebook, you’ll see a prompt box to send feedback to the product and engineering team here at Google Labs. We really appreciate your response and will adjust our roadmap based on your feedback!<br>Known limitations</p>
<p>The agent can be verbose in its output which can be overwhelming for a human to review. We are working on finding the right balance between showing the inner workings of the agent and getting the user to the output they desire.<br>The generated notebooks may require further fine-tuning and adjustments depending on the complexity of your data and analysis goals.<br>The generated notebooks might rely on libraries that are assumed to be part of the execution kernel. Depending on your kernel you might need to ‘pip install’ any missing dependencies and import them before execution. We recommend opening the notebooks with Google Colab.<br>The agent currently supports common data science tasks for tabular data, but may not cover highly-specialized queries, domain-specific analyses or uncommon libraries. This can include complex neural network architectures or libraries (e.g., TensorFlow, PyTorch) or non-tabular data, across multiple files, such as time-series, audio or images.<br>All limitations applicable to Gemini models, apply to our agent as well.<br>Future development</p>
<p>Interactive elements for user feedback and customization within the notebook generation process.<br>Enhancing natural language understanding for more flexible input descriptions.<br>Supporting additional data types for more complex tasks.<br>Expanding the range of supported data science tasks and algorithms.<br>Larger file size upload support.<br>Things to know</p>
<p>You must have a Google account and be located in the United States of America in order to use Data Science Agent.<br>&#x2F;code is experimental and may sometimes give inaccurate or inappropriate information, so double-check its output and use code with caution.<br>Don’t rely on &#x2F;code for medical, legal, financial, or other professional advice.<br>Your prompts, datasets, related usage information, and feedback will help make &#x2F;code better. Please do not include sensitive (e.g., confidential) or personal information that can be used to identify you or others in your prompts or feedback. Learn more.<br>The Google Terms of Service, the Generative AI Additional Terms of Service, and the Generative AI Prohibited Use Policy apply.</p>
</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-05-17</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/05/17/AI-뉴스-for-2024년-5월-17일/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 5월 17일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/05/18/AI-%E1%84%82%E1%85%B2%E1%84%89%E1%85%B3-for-2024%E1%84%82%E1%85%A7%E1%86%AB-5%E1%84%8B%E1%85%AF%E1%86%AF-18%E1%84%8B%E1%85%B5%E1%86%AF/" title="2024년 5월 18일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/01/06/2025-01-06-AI-NEWS/" title="2025년 01월 06일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>