<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 5월 21일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="Summary오늘의 소식은 인공지능 모델 CogVLM2의 출시, 마이크로소프트의 새로운 AI 칩 탑재 PC 발표, GitHub Copilot의 엔터프라이즈에서의 영향, Yi-1.5 모델의 업그레이드, 그리고 ChatGPT의 음성 기능 선택 과정에 대한 소식을 다룹니다."><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 5월 21일 AI 소식</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>오늘의 소식은 인공지능 모델 CogVLM2의 출시, 마이크로소프트의 새로운 AI 칩 탑재 PC 발표, GitHub Copilot의 엔터프라이즈에서의 영향, Yi-1.5 모델의 업그레이드, 그리고 ChatGPT의 음성 기능 선택 과정에 대한 소식을 다룹니다.</p>
<h2 id="CogVLM2-새로운-세대의-인공지능-모델-출시"><a href="#CogVLM2-새로운-세대의-인공지능-모델-출시" class="headerlink" title="CogVLM2: 새로운 세대의 인공지능 모델 출시"></a>CogVLM2: 새로운 세대의 인공지능 모델 출시</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/cogvlm2-llama3-chat-19B">링크</a> | 2024년 5월 20일</p>
<ul>
<li>새로운 CogVLM2 시리즈 모델 출시, Meta-Llama-3-8B-Instruct 기반.</li>
<li>주요 벤치마크(TextVQA, DocVQA 등)에서 성능 향상.</li>
<li>8K 콘텐츠 길이 및 최대 1344 * 1344 이미지 해상도 지원.</li>
<li>중국어와 영어를 지원하는 모델 버전 제공.</li>
<li>CogVLM2 모델은 비공개 모델과 경쟁할 수 있는 성능을 보여줌.</li>
<li>모델 크기는 19B로, 이미지 이해 및 대화 모델로 사용 가능.</li>
<li>TextVQA에서 85.0, DocVQA에서 92.3 등의 성과를 기록.</li>
</ul>
<h2 id="마이크로소프트-새로운-AI-칩-탑재-PC-발표"><a href="#마이크로소프트-새로운-AI-칩-탑재-PC-발표" class="headerlink" title="마이크로소프트: 새로운 AI 칩 탑재 PC 발표"></a>마이크로소프트: 새로운 AI 칩 탑재 PC 발표</h2><p><a target="_blank" rel="noopener" href="https://www.cnbc.com/amp/2024/05/20/microsoft-qualcomm-ai-pcs-snapdragon-arm-processors.html">링크</a> | 2024년 5월 20일</p>
<ul>
<li>마이크로소프트는 Qualcomm 칩을 탑재한 새로운 Surface Laptop 및 Surface Pro 태블릿을 발표.</li>
<li>이러한 PC는 인터넷 연결 없이도 일부 AI 작업을 수행할 수 있음.</li>
<li>Lenovo, Dell, HP, Asus, Acer, Samsung과 같은 다른 제조사들도 AI 준비된 PC를 출시.</li>
<li>Snapdragon X Elite 및 X Plus 프로세서 사용으로 배터리 수명 연장.</li>
<li>Satya Nadella, 마이크로소프트 CEO가 새로운 AI 기능 소개.</li>
</ul>
<h2 id="GitHub-Copilot-엔터프라이즈에서의-영향-연구"><a href="#GitHub-Copilot-엔터프라이즈에서의-영향-연구" class="headerlink" title="GitHub Copilot: 엔터프라이즈에서의 영향 연구"></a>GitHub Copilot: 엔터프라이즈에서의 영향 연구</h2><p><a target="_blank" rel="noopener" href="https://github.blog/2024-05-13-research-quantifying-github-copilots-impact-in-the-enterprise-with-accenture/">링크</a> | 2024년 5월 13일</p>
<ul>
<li>Accenture와 함께 GitHub Copilot의 실제 기업 환경에서의 영향 연구.</li>
<li>개발자 효율성 최대 55% 향상, 85%가 코드 품질에 자신감을 가짐.</li>
<li>90%의 개발자가 Copilot 사용 후 직무 만족도 증가.</li>
<li>80% 이상의 Accenture 개발자가 GitHub Copilot을 빠르게 채택.</li>
<li>Pull request 수 8.69% 증가, merge rate 15% 증가.</li>
<li>84%의 빌드 성공률 증가로 코드 품질 향상.</li>
<li>Copilot 사용 후 개발자들이 일관된 flow 상태를 유지, 번아웃 감소.</li>
</ul>
<h2 id="Yi-1-5-모델-업그레이드-및-공개"><a href="#Yi-1-5-모델-업그레이드-및-공개" class="headerlink" title="Yi-1.5: 모델 업그레이드 및 공개"></a>Yi-1.5: 모델 업그레이드 및 공개</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5">링크</a> | 2024년 5월 13일</p>
<ul>
<li>Yi-1.5 모델은 9B 및 34B 크기의 모델로 업그레이드, 16K 및 32K 컨텍스트 지원.</li>
<li>500B 토큰으로 지속적 사전 학습, 3M의 다양한 fine-tuning 샘플 사용.</li>
<li>코드, 수학, 추론 및 지시 수행 능력 향상.</li>
<li>Apache 2.0 라이선스 하에 공개.</li>
<li>Hugging Face, ModelScope, WiseModel에서 다운로드 가능.</li>
<li>A800 (80G)에서 로컬 실행 튜토리얼 제공.</li>
<li>vLLM 서버를 통해 채팅 API 사용 가능.</li>
</ul>
<h2 id="ChatGPT-음성-기능-선택-과정"><a href="#ChatGPT-음성-기능-선택-과정" class="headerlink" title="ChatGPT 음성 기능 선택 과정"></a>ChatGPT 음성 기능 선택 과정</h2><p><a target="_blank" rel="noopener" href="https://openai.com/index/how-the-voices-for-chatgpt-were-chosen/">링크</a> | 2024년 5월 19일</p>
<ul>
<li>ChatGPT의 음성 기능을 위해 5개의 목소리 선택 과정 공개.</li>
<li>400개 이상의 제출물 중에서 최종 5개의 목소리 선택.</li>
<li>독립적이고 유명한 캐스팅 디렉터 및 프로듀서와 협력하여 음성 선정 기준 설정.</li>
<li>다양한 배경의 배우들 참여, 다국어 구사 가능.</li>
<li>신뢰감을 주는 따뜻하고 매력적인 목소리 선정.</li>
<li>배우들은 샌프란시스코에서 녹음 세션을 거쳐 ChatGPT에 목소리를 제공.</li>
<li>GPT-4o의 새로운 음성 모드가 곧 ChatGPT Plus 사용자에게 제공될 예정.</li>
</ul>
<details>
  <summary>Sources</summary>
  This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is: # AI News for (today's date), ## Summary (overall short summary), ## Link1 Title, link, date - detailed summary1, - detailed summary2, - detailed summary..N, ## Link2 Title, link, date - detailed summary1, - detailed summary2, - detailed point..N, etc. The report should be written in Korean and use the 개조식 문체 style. give the very deep details for each link as much as possible.

<p><a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/cogvlm2-llama3-chat-19B">https://huggingface.co/THUDM/cogvlm2-llama3-chat-19B</a></p>
<h1 id="CogVLM2"><a href="#CogVLM2" class="headerlink" title="CogVLM2"></a>CogVLM2</h1><div align="center">
<img src=https://raw.githubusercontent.com/THUDM/CogVLM2/53d5d5ea1aa8d535edffc0d15e31685bac40f878/resources/logo.svg width="40%"/>
</div>
<p align="center">
    👋 <a href="resources/WECHAT.md" target="_blank">Wechat</a> · 💡<a href="http://36.103.203.44:7861/" target="_blank">Online Demo</a> · 🎈<a href="https://github.com/THUDM/CogVLM2" target="_blank">Github Page</a>
</p>
<p align="center">
📍Experience the larger-scale CogVLM model on the <a target="_blank" rel="noopener" href="https://open.bigmodel.cn/dev/api#super-humanoid">ZhipuAI Open Platform</a>.
</p>

<h2 id="Model-introduction"><a href="#Model-introduction" class="headerlink" title="Model introduction"></a>Model introduction</h2><p>We launch a new generation of <strong>CogVLM2</strong> series of models and open source two models built with <a target="_blank" rel="noopener" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">Meta-Llama-3-8B-Instruct</a>. Compared with the previous generation of CogVLM open source models, the CogVLM2 series of open source models have the following improvements:</p>
<ol>
<li>Significant improvements in many benchmarks such as <code>TextVQA</code>, <code>DocVQA</code>.</li>
<li>Support <strong>8K</strong> content length.</li>
<li>Support image resolution up to <strong>1344 * 1344</strong>.</li>
<li>Provide an open source model version that supports both <strong>Chinese and English</strong>.</li>
</ol>
<p>You can see the details of the CogVLM2 family of open source models in the table below:</p>
<table>
<thead>
<tr>
<th>Model name</th>
<th>cogvlm2-llama3-chat-19B</th>
<th>cogvlm2-llama3-chinese-chat-19B</th>
</tr>
</thead>
<tbody><tr>
<td>Base Model</td>
<td>Meta-Llama-3-8B-Instruct</td>
<td>Meta-Llama-3-8B-Instruct</td>
</tr>
<tr>
<td>Language</td>
<td>English</td>
<td>Chinese, English</td>
</tr>
<tr>
<td>Model size</td>
<td>19B</td>
<td>19B</td>
</tr>
<tr>
<td>Task</td>
<td>Image understanding, dialogue model</td>
<td>Image understanding, dialogue model</td>
</tr>
<tr>
<td>Text length</td>
<td>8K</td>
<td>8K</td>
</tr>
<tr>
<td>Image resolution</td>
<td>1344 * 1344</td>
<td>1344 * 1344</td>
</tr>
</tbody></table>
<h2 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h2><p>Our open source models have achieved good results in many lists compared to the previous generation of CogVLM open source models. Its excellent performance can compete with some non-open source models, as shown in the table below:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Open Source</th>
<th>LLM Size</th>
<th>TextVQA</th>
<th>DocVQA</th>
<th>ChartQA</th>
<th>OCRbench</th>
<th>MMMU</th>
<th>MMVet</th>
<th>MMBench</th>
</tr>
</thead>
<tbody><tr>
<td>CogVLM1.1</td>
<td>✅</td>
<td>7B</td>
<td>69.7</td>
<td>-</td>
<td>68.3</td>
<td>590</td>
<td>37.3</td>
<td>52.0</td>
<td>65.8</td>
</tr>
<tr>
<td>LLaVA-1.5</td>
<td>✅</td>
<td>13B</td>
<td>61.3</td>
<td>-</td>
<td>-</td>
<td>337</td>
<td>37.0</td>
<td>35.4</td>
<td>67.7</td>
</tr>
<tr>
<td>Mini-Gemini</td>
<td>✅</td>
<td>34B</td>
<td>74.1</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>48.0</td>
<td>59.3</td>
<td>80.6</td>
</tr>
<tr>
<td>LLaVA-NeXT-LLaMA3</td>
<td>✅</td>
<td>8B</td>
<td>-</td>
<td>78.2</td>
<td>69.5</td>
<td>-</td>
<td>41.7</td>
<td>-</td>
<td>72.1</td>
</tr>
<tr>
<td>LLaVA-NeXT-110B</td>
<td>✅</td>
<td>110B</td>
<td>-</td>
<td>85.7</td>
<td>79.7</td>
<td>-</td>
<td>49.1</td>
<td>-</td>
<td>80.5</td>
</tr>
<tr>
<td>InternVL-1.5</td>
<td>✅</td>
<td>20B</td>
<td>80.6</td>
<td>90.9</td>
<td><strong>83.8</strong></td>
<td>720</td>
<td>46.8</td>
<td>55.4</td>
<td><strong>82.3</strong></td>
</tr>
<tr>
<td>QwenVL-Plus</td>
<td>❌</td>
<td>-</td>
<td>78.9</td>
<td>91.4</td>
<td>78.1</td>
<td>726</td>
<td>51.4</td>
<td>55.7</td>
<td>67.0</td>
</tr>
<tr>
<td>Claude3-Opus</td>
<td>❌</td>
<td>-</td>
<td>-</td>
<td>89.3</td>
<td>80.8</td>
<td>694</td>
<td><strong>59.4</strong></td>
<td>51.7</td>
<td>63.3</td>
</tr>
<tr>
<td>Gemini Pro 1.5</td>
<td>❌</td>
<td>-</td>
<td>73.5</td>
<td>86.5</td>
<td>81.3</td>
<td>-</td>
<td>58.5</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT-4V</td>
<td>❌</td>
<td>-</td>
<td>78.0</td>
<td>88.4</td>
<td>78.5</td>
<td>656</td>
<td>56.8</td>
<td><strong>67.7</strong></td>
<td>75.0</td>
</tr>
<tr>
<td>CogVLM1.1</td>
<td>✅</td>
<td>7B</td>
<td>69.7</td>
<td>-</td>
<td>68.3</td>
<td>590</td>
<td>37.3</td>
<td>52.0</td>
<td>65.8</td>
</tr>
<tr>
<td>CogVLM2-LLaMA3 (Ours)</td>
<td>✅</td>
<td>8B</td>
<td>84.2</td>
<td><strong>92.3</strong></td>
<td>81.0</td>
<td>756</td>
<td>44.3</td>
<td>60.4</td>
<td>80.5</td>
</tr>
<tr>
<td>CogVLM2-LLaMA3-Chinese (Ours)</td>
<td>✅</td>
<td>8B</td>
<td><strong>85.0</strong></td>
<td>88.4</td>
<td>74.7</td>
<td><strong>780</strong></td>
<td>42.8</td>
<td>60.5</td>
<td>78.9</td>
</tr>
</tbody></table>
<p>All reviews were obtained without using any external OCR tools (“pixel only”).</p>
<p><a target="_blank" rel="noopener" href="https://www-cnbc-com.cdn.ampproject.org/c/s/www.cnbc.com/amp/2024/05/20/microsoft-qualcomm-ai-pcs-snapdragon-arm-processors.html">https://www-cnbc-com.cdn.ampproject.org/c/s/www.cnbc.com/amp/2024/05/20/microsoft-qualcomm-ai-pcs-snapdragon-arm-processors.html</a><br>TECH<br>Microsoft announces new PCs with AI chips from Qualcomm<br>PUBLISHED MON, MAY 20 2024 2:01 PM EDT<br>UPDATED MON, MAY 20 2024 4:03 PM EDT<br>Jordan Novet<br>@JORDANNOVET<br>WATCH LIVE<br>KEY POINTS<br>Microsoft will bring out new Surface PCs that adhere to its Copilot+ standard for running artificial intelligence models.<br>These PCs and others use Arm-based Qualcomm chips to deliver longer battery life, and PCs with AMD and Intel chips will also become available.<br>Microsoft Chairman and Chief Executive Officer Satya Nadella speaks during the Microsoft May 20 Briefing event at Microsoft in Redmond, Washington, on May 20, 2024.<br>Microsoft Chairman and Chief Executive Officer Satya Nadella speaks during the Microsoft May 20 Briefing event at Microsoft in Redmond, Washington, on May 20, 2024.<br>Jason Redmond | AFP | Getty Images<br>Microsoft is touting new computers with advanced chips designed to run artificial intelligence features of software for Windows, without quickly using up battery life.</p>
<p>The company on Monday announced a Surface Laptop and a Surface Pro tablet with a Qualcomm chip that can run some AI tasks without an internet connection. Other computer makers like Lenovo, Dell, HP, Asus, Acer and Samsung are also launching AI-ready PCs powered by Qualcomm’s Snapdragon X Elite and X Plus processors, which promise longer battery life and will run Microsoft’s Copilot AI chatbot.</p>
<p><a target="_blank" rel="noopener" href="https://github.blog/2024-05-13-research-quantifying-github-copilots-impact-in-the-enterprise-with-accenture/">https://github.blog/2024-05-13-research-quantifying-github-copilots-impact-in-the-enterprise-with-accenture/</a><br>Research: Quantifying GitHub Copilot’s impact in the enterprise with Accenture<br>We conducted research with developers at Accenture to understand GitHub Copilot’s real-world impact in enterprise organizations.</p>
<p>An image showing the Accenture and GitHub logos with text that reads “The enterprise impact of GitHub Copilot”<br>Authors<br>Ya GaoYa Gao<br>GitHub Customer ResearchGitHub Customer Research<br>May 13, 2024<br>Since bringing GitHub Copilot to market, we’ve conducted several lab studies to discover its impact on developer efficiency, developer satisfaction, and overall code quality. We found that our AI pair programmer helps developers code up to 55% faster and that it made 85% of developers feel more confident in their code quality. With the introduction of our first GitHub Copilot offering for businesses and organizations in 2023—and more recently GitHub Copilot Enterprise—it’s become increasingly important for us to measure the impact of GitHub Copilot across real-world, large engineering organizations.</p>
<p>To learn more, we partnered with Accenture to study how developers integrated GitHub Copilot into their daily workflows, and we found significant improvements in several areas, including:</p>
<p>Improved developer satisfaction. 90% of developers found they were more fulfilled with their job when using GitHub Copilot, and 95% said they enjoyed coding more with Copilot’s help.<br>Quickly adopted by developers. Over 80% of Accenture participants successfully adopted GitHub Copilot with a 96% success rate among initial users. 43% found it “extremely easy to use.” Additionally, 67% of total participants used GitHub Copilot at least 5 days per week, averaging 3.4 days of usage weekly.<br>Methodology<br>In this study, we collaborated with Accenture to conduct an extensive, randomized controlled trial (RCT). Participants included developers who engage in a variety of software development tasks daily, including engineering, design, and testing across a spectrum of software products and services. They hold various positions within their organizations from entry-level roles to team management positions, and may work collaboratively or independently depending on the project and team dynamics.</p>
<p>For the trial, developers were randomly assigned to two groups. One group of developers was given access to GitHub Copilot, while the other group was not. Our objective was to assess the influence of GitHub Copilot on developers’ experience within the enterprise setting, where they collaborate on multifaceted projects. We collected DevOps telemetry on several output performance metrics that reflect insights into developers’ regular coding activity.</p>
<p>Beyond the initial experiment, we conducted a company-wide adoption analysis, which explored installation rates, generated code acceptance rates, and the time it took developers to accept GitHub Copilot’s first coding suggestion. Success was determined by whether they accepted a suggestion from GitHub Copilot or not.</p>
<p>In addition, we surveyed the GitHub Copilot users at Accenture to gain a better understanding of how developers perceived the impact of GitHub Copilot on their workflows. Not only did this survey uncover insights into how and when developers are using GitHub Copilot, but it also indicated an overwhelming improvement in developer satisfaction, which we know to be a key component of the developer experience (DevEx). The combination of both telemetry data and the information from the survey provides a full picture for us to understand GitHub Copilot’s impact at the enterprise level.</p>
<p>Our findings<br>Developers quickly found value in GitHub Copilot and adopted it as part of their daily toolkit<br>More than 50,000 organizations have adopted GitHub Copilot so far, but we haven’t yet had a clear view into what those adoption rates look like on the individual level. When we dug deeper into the usage patterns of GitHub Copilot among Accenture developers, 67% of respondents reported utilizing GitHub Copilot at least 5 days per week, with an average usage frequency of 3.4 days per week. Moreover, a substantial 70% of respondents relied on GitHub Copilot for coding tasks in a familiar programming language. This indicates a high level of integration of GitHub Copilot into developers’ daily workflows, highlighting its importance as a valuable engineering tool and resource.</p>
<p>We also observed that developers were excited to use GitHub Copilot. 81.4% of developers installed the GitHub Copilot IDE extension on the same day that they received a license. And not only were they excited to use it, but getting started was simple and did not provide a barrier to entry.</p>
<p>In fact, 96% of those who installed the IDE extension started receiving and accepting suggestions on the same day. On average, developers took just one minute from seeing their first suggestion to accepting one, too. This was further validated in user surveys, with 43% finding GitHub Copilot “extremely easy to use” and 51% rating it as “extremely useful.”</p>
<p>As part of the GitHub Copilot service, we provide the measurement capabilities for our customers to determine gains from Copilot themselves. To produce many of the insights in this report, we leveraged public APIs available via GitHub and Azure DevOps. Among them, GitHub offers the GitHub Copilot Metrics API, designed to provide users with information about Copilot usage within your organization. You can also explore the Copilot Learning Pathways to learn more about what GitHub Copilot can help your business achieve. Keep reading for more information on conducting your own studies on GitHub Copilot’s impact.</p>
<p>A chart showing how developers at Accenture gauged the ease of using GitHub Copilot. 42.8% of developers at Accenture deem it “extremely easy to use.”</p>
<p>A chart showing how developers at Accenture gauged the usefulness of GitHub Copilot. 50.9% of developers at Accenture deem it “extremely useful.”</p>
<p>Developers improved code quality using GitHub Copilot<br>By convention, pull requests represent a ready-to-deploy code change (for example, a new feature, bug fix, or code refactoring). When measured in aggregate, the number of pull requests per developer can be used to measure a team’s throughput or velocity. Ultimately, an increase in pull requests represents an increase in value delivered, and Accenture developers saw an 8.69% increase in pull requests. Because each pull request must pass through a code review, the pull request merge rate is an excellent measure of code quality as seen through the eyes of a maintainer or coworker. Accenture saw a 15% increase to the pull request merge rate, which means that as the volume of pull requests increased, so did the number of pull requests passing code review.</p>
<p>But we don’t want to just shift issues downstream and overburden the system with low-quality code. It’s one thing for a teammate to assess quality and yet another for new code to successfully complete CI runs where test automation evaluates code quality against deterministic measures. At Accenture, we saw an 84% increase in successful builds suggesting not only that more pull requests were passing through the system, but they were also of higher quality as assessed by both human reviewers and test automation.</p>
<p>By enabling developers to maintain focus and stay in the flow, GitHub Copilot doesn’t sacrifice quality for speed. And our findings provide evidence for exactly that.</p>
<p>In our study, developers accepted around 30% of GitHub Copilot’s suggestions. And 90% of the developers reported that they committed code suggested by GitHub Copilot, while 91% of the developers reported that their teams had merged pull requests containing code suggested by GitHub Copilot. Analysis also showed high usage rates with the accepted code—for example, developers retained 88% of GitHub Copilot-generated characters in their editor.</p>
<p>A chart showing how developers at Accenture gauged GitHub Copilot’s impact on production code, also outlined in the preceding paragraph.</p>
<p>By experiencing improved success rates in builds, developers can reduce the likelihood of errors.</p>
<p>GitHub Copilot improved the overall developer experience<br>Our survey among Accenture developers unveiled compelling findings indicating a significant boost in overall developer satisfaction with GitHub Copilot. An impressive 90% of developers expressed feeling more fulfilled with their jobs when utilizing GitHub Copilot, and a staggering 95% of developers reported enjoying coding more when leveraging GitHub Copilot’s capabilities.</p>
<p>This enhancement in job satisfaction could allow developers to allocate their focus toward tasks most fulfilling to them, like solutions design or collaboration. Furthermore, our analysis revealed that developers’ heightened fulfillment correlated directly with their engagement with GitHub Copilot. When using GitHub Copilot less than two days per week, fulfillment only increased “a little.” But when using GitHub Copilot more than 2 days per week, fulfillment increases “quite a bit.”</p>
<p>70% of developers also reported quite a bit less mental effort was expended on repetitive tasks, and 54% spent less time searching for information or examples when utilizing GitHub Copilot. This reduction in cognitive load could enable developers to allocate their cognitive resources more efficiently, reducing burnout. GitHub Copilot also allowed developers to maintain uninterrupted focus, with a majority indicating that they could maintain flow state while using the tool, a hallmark of good DevEx. These impacts extend beyond mere task optimization, which offers enterprises a competitive edge by maximizing developer resources and fostering a conducive environment for innovation and growth.</p>
<p>A chart showing how developers at Accenture grew more fulfilled the more they used GitHub Copilot.</p>
<p>How to evaluate the impact of GitHub Copilot in your organization</p>
<p>Organizations seeking to conduct studies on the impact of GitHub Copilot can follow a methodological approach focusing on collecting and analyzing three types of data: quantitative, qualitative, and operational. To ensure readiness for data collection, organizations are advised to streamline their DevOps platform telemetry infrastructure in alignment with their specific goals and workflows.</p>
<p>It’s essential to note that success metrics should be tailored to reflect the unique processes and operations of each organization. By adopting this methodology and customizing metrics accordingly, organizations can effectively gauge the impact of GitHub Copilot.</p>
<p>Learn how to measure the impact of GitHub Copilot in your organization &gt;</p>
<p>From the lab to the real world<br>After conducting multiple lab studies on the impact of GitHub Copilot, we are now working to understand how GitHub Copilot affected developers’ workdays in real-world environments—and that’s been made possible by the tremendous adoption we’ve seen among businesses and enterprise organizations alike.</p>
<p>With this study, we have uncovered compelling evidence that GitHub Copilot significantly enhances developer experience, satisfaction, and overall job fulfillment in real-world enterprise settings. With GitHub Copilot in their toolkits, developers can also enhance their skill sets and gain greater proficiency in their organization’s codebase, which ultimately leads to heightened contribution levels across teams, all without sacrificing the quality of code.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5">https://github.com/01-ai/Yi-1.5</a><br>Let’s goo! Yi release 1.5 models 9B and 34B now with 16K &amp; 32K context! Apache 2.0 licensed! 🔥</p>
<blockquote>
<p>Continuous pre-training on 500B (total 3.6T) tokens<br>3M carefully curated instruction tuning set<br>Better at code, math, reasoning and instruction following<br>Chat checkpoints go up to 16K<br>Base checkpoints go up to 32K<br>Congrats, and thanks to the 01 AI team for open-sourcing such brilliant checkpoints!</p>
</blockquote>
<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#intro"></a></p>
<p>Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.</p>
<p>Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension.</p>
<p>Yi-1.5 comes in 3 model sizes: 34B, 9B, and 6B. For model details and benchmarks, see <a target="_blank" rel="noopener" href="https://huggingface.co/collections/01-ai/yi-15-2024-05-663f3ecab5f815a3eaca7ca8">Model Card</a>.</p>
<h2 id="News"><a href="#News" class="headerlink" title="News"></a>News</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#news"></a></p>
<ul>
<li>2024-05-13: The Yi-1.5 series models are open-sourced, further improving coding, math, reasoning, and instruction-following abilities.</li>
</ul>
<h2 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#requirements"></a></p>
<ul>
<li><p>Make sure Python 3.10 or a later version is installed.</p>
</li>
<li><p>Set up the environment and install the required packages.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>Download the Yi-1.5 model from <a target="_blank" rel="noopener" href="https://huggingface.co/01-ai">Hugging Face</a>, <a target="_blank" rel="noopener" href="https://www.modelscope.cn/organization/01ai/">ModelScope</a>, or <a target="_blank" rel="noopener" href="https://wisemodel.cn/organization/01.AI">WiseModel</a>.</p>
</li>
</ul>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#quick-start"></a></p>
<p>This tutorial runs Yi-1.5-34B-Chat locally on an A800 (80G).</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from transformers import AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_path = &#x27;&lt;your-model-path&gt;&#x27;</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)</span><br><span class="line"></span><br><span class="line"># Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_path,</span><br><span class="line">    device_map=&quot;auto&quot;,</span><br><span class="line">    torch_dtype=&#x27;auto&#x27;</span><br><span class="line">).eval()</span><br><span class="line"></span><br><span class="line"># Prompt content: &quot;hi&quot;</span><br><span class="line">messages = [</span><br><span class="line">    &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;hi&quot;&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, return_tensors=&#x27;pt&#x27;)</span><br><span class="line">output_ids = model.generate(input_ids.to(&#x27;cuda&#x27;), eos_token_id=tokenizer.eos_token_id)</span><br><span class="line">response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)</span><br><span class="line"></span><br><span class="line"># Model response: &quot;Hello! How can I assist you today?&quot;</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure>

<h2 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#deployment"></a></p>
<p>Prerequisites: Before deploying Yi-1.5 models, make sure you meet the <a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi/tree/main?tab=readme-ov-file#software-requirements">software and hardware requirements</a>.</p>
<h3 id="vLLM"><a href="#vLLM" class="headerlink" title="vLLM"></a>vLLM</h3><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#vllm"></a></p>
<p>Prerequisites: Download the latest version of <a target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/getting_started/installation.html">vLLM</a>.</p>
<ol>
<li><p>Start the server with a chat model.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m vllm.entrypoints.openai.api_server  --model 01-ai/Yi-1.5-9B-Chat  --served-model-name Yi-1.5-9B-Chat</span><br></pre></td></tr></table></figure>
</li>
<li><p>Use the chat API.</p>
</li>
</ol>
<ul>
<li><p>HTTP</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:8000/v1/chat/completions\</span><br><span class="line">    -H &quot;Content-Type: application/json&quot;\</span><br><span class="line">    -d &#x27;&#123;</span><br><span class="line">        &quot;model&quot;: &quot;Yi-1.5-9B-Chat&quot;,</span><br><span class="line">        &quot;messages&quot;: [</span><br><span class="line">            &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;,</span><br><span class="line">            &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;&#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;&#x27;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Python client</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from openai import OpenAI</span><br><span class="line"># Set OpenAI&#x27;s API key and API base to use vLLM&#x27;s API server.</span><br><span class="line">openai_api_key = &quot;EMPTY&quot;</span><br><span class="line">openai_api_base = &quot;http://localhost:8000/v1&quot;</span><br><span class="line"></span><br><span class="line">client = OpenAI(</span><br><span class="line">    api_key=openai_api_key,</span><br><span class="line">    base_url=openai_api_base,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">chat_response = client.chat.completions.create(</span><br><span class="line">    model=&quot;Yi-1.5-9B-Chat&quot;,</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;,</span><br><span class="line">        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me a joke.&quot;&#125;,</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">print(&quot;Chat response:&quot;, chat_response)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Web-Demo"><a href="#Web-Demo" class="headerlink" title="Web Demo"></a>Web Demo</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#web-demo"></a></p>
<p>We have deployed a <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/01-ai/Yi-1.5-34B-Chat">Yi-1.5-34B-Chat Space</a> on Huggingface.</p>
<p>Or you can build it locally by yourself, as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python demo/web_demo.py -c &lt;your-model-path&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a>Fine-tuning</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#fine-tuning"></a></p>
<p>You can use <a target="_blank" rel="noopener" href="https://github.com/hiyouga/LLaMA-Factory">LLaMA-Factory</a>, <a target="_blank" rel="noopener" href="https://github.com/modelscope/swift">Swift</a>, <a target="_blank" rel="noopener" href="https://github.com/InternLM/xtuner">XTuner</a>, and <a target="_blank" rel="noopener" href="https://github.com/yangjianxin1/Firefly">Firefly</a> for fine-tuning. These frameworks all support fine-tuning the Yi series models.</p>
<h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#api"></a></p>
<p>Yi APIs are OpenAI-compatible and provided at <a target="_blank" rel="noopener" href="https://platform.lingyiwanwu.com/">Yi Platform</a>. Sign up to get free tokens, and you can also pay-as-you-go at a competitive price. Additionally, Yi APIs are also deployed on <a target="_blank" rel="noopener" href="https://replicate.com/search?query=01+ai">Replicate</a> and <a target="_blank" rel="noopener" href="https://openrouter.ai/models?q=01%20ai">OpenRouter</a>.</p>
<h2 id="License"><a href="#License" class="headerlink" title="License"></a>License</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#license"></a></p>
<p>The code and weights of the Yi-1.5 series models are distributed under the <a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi/blob/main/LICENSE">Apache 2.0 license</a>.</p>
<p><a target="_blank" rel="noopener" href="https://openai.com/index/how-the-voices-for-chatgpt-were-chosen/">https://openai.com/index/how-the-voices-for-chatgpt-were-chosen/</a><br>May 19, 2024</p>
<p>How the voices for ChatGPT were chosen<br>We worked with industry-leading casting and directing professionals to narrow down over 400 submissions before selecting the 5 voices.</p>
<p>Asset &gt; How the voices for ChatGPT were chosen<br>Voice Mode is one of the most beloved features in ChatGPT. Each of the five distinct voices you hear has been carefully selected through an extensive process spanning five months involving professional voice actors, talent agencies, casting directors, and industry advisors. We’re sharing more on how the voices were chosen.</p>
<p>In September of 2023, we introduced voice capabilities to give users another way to interact with ChatGPT. Since then, we are encouraged by the way users have responded to the feature and the individual voices. Each of the voices—Breeze, Cove, Ember, Juniper and Sky—are sampled from voice actors we partnered with to create them.</p>
<p>We support the creative community and collaborated with the voice acting industry<br>We support the creative community and worked closely with the voice acting industry to ensure we took the right steps to cast ChatGPT’s voices. Each actor receives compensation above top-of-market rates, and this will continue for as long as their voices are used in our products.</p>
<p>We believe that AI voices should not deliberately mimic a celebrity’s distinctive voice—Sky’s voice is not an imitation of Scarlett Johansson but belongs to a different professional actress using her own natural speaking voice. To protect their privacy, we cannot share the names of our voice talents.</p>
<p>We partnered with award-winning casting directors and producers to create the criteria for voices<br>In early 2023, to identify our voice actors, we had the privilege of partnering with independent, well-known, award-winning casting directors and producers. We worked with them to create a set of criteria for ChatGPT’s voices, carefully considering the unique personality of each voice and their appeal to global audiences.</p>
<p>Some of these characteristics included:</p>
<p>Actors from diverse backgrounds or who could speak multiple languages</p>
<p>A voice that feels timeless</p>
<p>An approachable voice that inspires trust</p>
<p>A warm, engaging, confidence-inspiring, charismatic voice with rich tone</p>
<p>Natural and easy to listen to</p>
<p>We received over 400 submissions from voice and screen actors<br>In May of 2023, the casting agency and our casting directors issued a call for talent. In under a week, they received over 400 submissions from voice and screen actors. To audition, actors were given a script of ChatGPT responses and were asked to record them. These samples ranged from answering questions about mindfulness to brainstorming travel plans, and even engaging in conversations about a user’s day.</p>
<p>We selected five final voices and discussed our vision for human-AI interactions and the goals of Voice Mode with the actors<br>Through May 2023, the casting team independently reviewed and hand-selected an initial list of 14 actors. They further refined their list before presenting their top voices for the project to OpenAI.</p>
<p>We spoke with each actor about the vision for human-AI voice interactions and OpenAI, and discussed the technology’s capabilities, limitations, and the risks involved, as well as the safeguards we have implemented. It was important to us that each actor understood the scope and intentions of Voice Mode before committing to the project.</p>
<p>An internal team at OpenAI reviewed the voices from a product and research perspective, and after careful consideration, the voices for Breeze, Cove, Ember, Juniper and Sky were finally selected.</p>
<p>Each actor flew to San Francisco for recording sessions and their voices were launched into ChatGPT in September 2023<br>During June and July, we flew the actors to San Francisco for recording sessions and in-person meetings with the OpenAI product and research teams.</p>
<p>On September 25, 2023, we launched their voices into ChatGPT.</p>
<p>This entire process involved extensive coordination with the actors and the casting team, taking place over five months. We are continuing to collaborate with the actors, who have contributed additional work for audio research and new voice capabilities in GPT-4o.</p>
<p>New Voice Mode coming to GPT-4o for paid users, and adding new voices<br>We plan to give access to a new Voice Mode for GPT-4o(opens in a new window) in alpha to ChatGPT Plus users in the coming weeks. With GPT-4o, using your voice to interact with ChatGPT is much more natural. GPT-4o handles interruptions smoothly, manages group conversations effectively, filters out background noise, and adapts to tone.</p>
<p>Looking ahead, you can expect even more options as we plan to introduce additional voices in ChatGPT to better match the diverse interests and preferences of users.</p>
</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-05-20</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/05/20/2024년-5월-21일-AI-뉴스/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 5월 21일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/05/22/2024-5-23-AI-NEWS/" title="2024년 5월 23일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/05/20/README-md/" title="README.md">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>