<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 5ì›” 21ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="Summaryì˜¤ëŠ˜ì˜ ì†Œì‹ì€ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ CogVLM2ì˜ ì¶œì‹œ, ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ì˜ ìƒˆë¡œìš´ AI ì¹© íƒ‘ì¬ PC ë°œí‘œ, GitHub Copilotì˜ ì—”í„°í”„ë¼ì´ì¦ˆì—ì„œì˜ ì˜í–¥, Yi-1.5 ëª¨ë¸ì˜ ì—…ê·¸ë ˆì´ë“œ, ê·¸ë¦¬ê³  ChatGPTì˜ ìŒì„± ê¸°ëŠ¥ ì„ íƒ ê³¼ì •ì— ëŒ€í•œ ì†Œì‹ì„ ë‹¤ë£¹ë‹ˆë‹¤."><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 5ì›” 21ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>ì˜¤ëŠ˜ì˜ ì†Œì‹ì€ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ CogVLM2ì˜ ì¶œì‹œ, ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ì˜ ìƒˆë¡œìš´ AI ì¹© íƒ‘ì¬ PC ë°œí‘œ, GitHub Copilotì˜ ì—”í„°í”„ë¼ì´ì¦ˆì—ì„œì˜ ì˜í–¥, Yi-1.5 ëª¨ë¸ì˜ ì—…ê·¸ë ˆì´ë“œ, ê·¸ë¦¬ê³  ChatGPTì˜ ìŒì„± ê¸°ëŠ¥ ì„ íƒ ê³¼ì •ì— ëŒ€í•œ ì†Œì‹ì„ ë‹¤ë£¹ë‹ˆë‹¤.</p>
<h2 id="CogVLM2-ìƒˆë¡œìš´-ì„¸ëŒ€ì˜-ì¸ê³µì§€ëŠ¥-ëª¨ë¸-ì¶œì‹œ"><a href="#CogVLM2-ìƒˆë¡œìš´-ì„¸ëŒ€ì˜-ì¸ê³µì§€ëŠ¥-ëª¨ë¸-ì¶œì‹œ" class="headerlink" title="CogVLM2: ìƒˆë¡œìš´ ì„¸ëŒ€ì˜ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ ì¶œì‹œ"></a>CogVLM2: ìƒˆë¡œìš´ ì„¸ëŒ€ì˜ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ ì¶œì‹œ</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/cogvlm2-llama3-chat-19B">ë§í¬</a> | 2024ë…„ 5ì›” 20ì¼</p>
<ul>
<li>ìƒˆë¡œìš´ CogVLM2 ì‹œë¦¬ì¦ˆ ëª¨ë¸ ì¶œì‹œ, Meta-Llama-3-8B-Instruct ê¸°ë°˜.</li>
<li>ì£¼ìš” ë²¤ì¹˜ë§ˆí¬(TextVQA, DocVQA ë“±)ì—ì„œ ì„±ëŠ¥ í–¥ìƒ.</li>
<li>8K ì½˜í…ì¸  ê¸¸ì´ ë° ìµœëŒ€ 1344 * 1344 ì´ë¯¸ì§€ í•´ìƒë„ ì§€ì›.</li>
<li>ì¤‘êµ­ì–´ì™€ ì˜ì–´ë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸ ë²„ì „ ì œê³µ.</li>
<li>CogVLM2 ëª¨ë¸ì€ ë¹„ê³µê°œ ëª¨ë¸ê³¼ ê²½ìŸí•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ.</li>
<li>ëª¨ë¸ í¬ê¸°ëŠ” 19Bë¡œ, ì´ë¯¸ì§€ ì´í•´ ë° ëŒ€í™” ëª¨ë¸ë¡œ ì‚¬ìš© ê°€ëŠ¥.</li>
<li>TextVQAì—ì„œ 85.0, DocVQAì—ì„œ 92.3 ë“±ì˜ ì„±ê³¼ë¥¼ ê¸°ë¡.</li>
</ul>
<h2 id="ë§ˆì´í¬ë¡œì†Œí”„íŠ¸-ìƒˆë¡œìš´-AI-ì¹©-íƒ‘ì¬-PC-ë°œí‘œ"><a href="#ë§ˆì´í¬ë¡œì†Œí”„íŠ¸-ìƒˆë¡œìš´-AI-ì¹©-íƒ‘ì¬-PC-ë°œí‘œ" class="headerlink" title="ë§ˆì´í¬ë¡œì†Œí”„íŠ¸: ìƒˆë¡œìš´ AI ì¹© íƒ‘ì¬ PC ë°œí‘œ"></a>ë§ˆì´í¬ë¡œì†Œí”„íŠ¸: ìƒˆë¡œìš´ AI ì¹© íƒ‘ì¬ PC ë°œí‘œ</h2><p><a target="_blank" rel="noopener" href="https://www.cnbc.com/amp/2024/05/20/microsoft-qualcomm-ai-pcs-snapdragon-arm-processors.html">ë§í¬</a> | 2024ë…„ 5ì›” 20ì¼</p>
<ul>
<li>ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ëŠ” Qualcomm ì¹©ì„ íƒ‘ì¬í•œ ìƒˆë¡œìš´ Surface Laptop ë° Surface Pro íƒœë¸”ë¦¿ì„ ë°œí‘œ.</li>
<li>ì´ëŸ¬í•œ PCëŠ” ì¸í„°ë„· ì—°ê²° ì—†ì´ë„ ì¼ë¶€ AI ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒ.</li>
<li>Lenovo, Dell, HP, Asus, Acer, Samsungê³¼ ê°™ì€ ë‹¤ë¥¸ ì œì¡°ì‚¬ë“¤ë„ AI ì¤€ë¹„ëœ PCë¥¼ ì¶œì‹œ.</li>
<li>Snapdragon X Elite ë° X Plus í”„ë¡œì„¸ì„œ ì‚¬ìš©ìœ¼ë¡œ ë°°í„°ë¦¬ ìˆ˜ëª… ì—°ì¥.</li>
<li>Satya Nadella, ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ CEOê°€ ìƒˆë¡œìš´ AI ê¸°ëŠ¥ ì†Œê°œ.</li>
</ul>
<h2 id="GitHub-Copilot-ì—”í„°í”„ë¼ì´ì¦ˆì—ì„œì˜-ì˜í–¥-ì—°êµ¬"><a href="#GitHub-Copilot-ì—”í„°í”„ë¼ì´ì¦ˆì—ì„œì˜-ì˜í–¥-ì—°êµ¬" class="headerlink" title="GitHub Copilot: ì—”í„°í”„ë¼ì´ì¦ˆì—ì„œì˜ ì˜í–¥ ì—°êµ¬"></a>GitHub Copilot: ì—”í„°í”„ë¼ì´ì¦ˆì—ì„œì˜ ì˜í–¥ ì—°êµ¬</h2><p><a target="_blank" rel="noopener" href="https://github.blog/2024-05-13-research-quantifying-github-copilots-impact-in-the-enterprise-with-accenture/">ë§í¬</a> | 2024ë…„ 5ì›” 13ì¼</p>
<ul>
<li>Accentureì™€ í•¨ê»˜ GitHub Copilotì˜ ì‹¤ì œ ê¸°ì—… í™˜ê²½ì—ì„œì˜ ì˜í–¥ ì—°êµ¬.</li>
<li>ê°œë°œì íš¨ìœ¨ì„± ìµœëŒ€ 55% í–¥ìƒ, 85%ê°€ ì½”ë“œ í’ˆì§ˆì— ìì‹ ê°ì„ ê°€ì§.</li>
<li>90%ì˜ ê°œë°œìê°€ Copilot ì‚¬ìš© í›„ ì§ë¬´ ë§Œì¡±ë„ ì¦ê°€.</li>
<li>80% ì´ìƒì˜ Accenture ê°œë°œìê°€ GitHub Copilotì„ ë¹ ë¥´ê²Œ ì±„íƒ.</li>
<li>Pull request ìˆ˜ 8.69% ì¦ê°€, merge rate 15% ì¦ê°€.</li>
<li>84%ì˜ ë¹Œë“œ ì„±ê³µë¥  ì¦ê°€ë¡œ ì½”ë“œ í’ˆì§ˆ í–¥ìƒ.</li>
<li>Copilot ì‚¬ìš© í›„ ê°œë°œìë“¤ì´ ì¼ê´€ëœ flow ìƒíƒœë¥¼ ìœ ì§€, ë²ˆì•„ì›ƒ ê°ì†Œ.</li>
</ul>
<h2 id="Yi-1-5-ëª¨ë¸-ì—…ê·¸ë ˆì´ë“œ-ë°-ê³µê°œ"><a href="#Yi-1-5-ëª¨ë¸-ì—…ê·¸ë ˆì´ë“œ-ë°-ê³µê°œ" class="headerlink" title="Yi-1.5: ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ ë° ê³µê°œ"></a>Yi-1.5: ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ ë° ê³µê°œ</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5">ë§í¬</a> | 2024ë…„ 5ì›” 13ì¼</p>
<ul>
<li>Yi-1.5 ëª¨ë¸ì€ 9B ë° 34B í¬ê¸°ì˜ ëª¨ë¸ë¡œ ì—…ê·¸ë ˆì´ë“œ, 16K ë° 32K ì»¨í…ìŠ¤íŠ¸ ì§€ì›.</li>
<li>500B í† í°ìœ¼ë¡œ ì§€ì†ì  ì‚¬ì „ í•™ìŠµ, 3Mì˜ ë‹¤ì–‘í•œ fine-tuning ìƒ˜í”Œ ì‚¬ìš©.</li>
<li>ì½”ë“œ, ìˆ˜í•™, ì¶”ë¡  ë° ì§€ì‹œ ìˆ˜í–‰ ëŠ¥ë ¥ í–¥ìƒ.</li>
<li>Apache 2.0 ë¼ì´ì„ ìŠ¤ í•˜ì— ê³µê°œ.</li>
<li>Hugging Face, ModelScope, WiseModelì—ì„œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥.</li>
<li>A800 (80G)ì—ì„œ ë¡œì»¬ ì‹¤í–‰ íŠœí† ë¦¬ì–¼ ì œê³µ.</li>
<li>vLLM ì„œë²„ë¥¼ í†µí•´ ì±„íŒ… API ì‚¬ìš© ê°€ëŠ¥.</li>
</ul>
<h2 id="ChatGPT-ìŒì„±-ê¸°ëŠ¥-ì„ íƒ-ê³¼ì •"><a href="#ChatGPT-ìŒì„±-ê¸°ëŠ¥-ì„ íƒ-ê³¼ì •" class="headerlink" title="ChatGPT ìŒì„± ê¸°ëŠ¥ ì„ íƒ ê³¼ì •"></a>ChatGPT ìŒì„± ê¸°ëŠ¥ ì„ íƒ ê³¼ì •</h2><p><a target="_blank" rel="noopener" href="https://openai.com/index/how-the-voices-for-chatgpt-were-chosen/">ë§í¬</a> | 2024ë…„ 5ì›” 19ì¼</p>
<ul>
<li>ChatGPTì˜ ìŒì„± ê¸°ëŠ¥ì„ ìœ„í•´ 5ê°œì˜ ëª©ì†Œë¦¬ ì„ íƒ ê³¼ì • ê³µê°œ.</li>
<li>400ê°œ ì´ìƒì˜ ì œì¶œë¬¼ ì¤‘ì—ì„œ ìµœì¢… 5ê°œì˜ ëª©ì†Œë¦¬ ì„ íƒ.</li>
<li>ë…ë¦½ì ì´ê³  ìœ ëª…í•œ ìºìŠ¤íŒ… ë””ë ‰í„° ë° í”„ë¡œë“€ì„œì™€ í˜‘ë ¥í•˜ì—¬ ìŒì„± ì„ ì • ê¸°ì¤€ ì„¤ì •.</li>
<li>ë‹¤ì–‘í•œ ë°°ê²½ì˜ ë°°ìš°ë“¤ ì°¸ì—¬, ë‹¤êµ­ì–´ êµ¬ì‚¬ ê°€ëŠ¥.</li>
<li>ì‹ ë¢°ê°ì„ ì£¼ëŠ” ë”°ëœ»í•˜ê³  ë§¤ë ¥ì ì¸ ëª©ì†Œë¦¬ ì„ ì •.</li>
<li>ë°°ìš°ë“¤ì€ ìƒŒí”„ë€ì‹œìŠ¤ì½”ì—ì„œ ë…¹ìŒ ì„¸ì…˜ì„ ê±°ì³ ChatGPTì— ëª©ì†Œë¦¬ë¥¼ ì œê³µ.</li>
<li>GPT-4oì˜ ìƒˆë¡œìš´ ìŒì„± ëª¨ë“œê°€ ê³§ ChatGPT Plus ì‚¬ìš©ìì—ê²Œ ì œê³µë  ì˜ˆì •.</li>
</ul>
<details>
  <summary>Sources</summary>
  This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is: # AI News for (today's date), ## Summary (overall short summary), ## Link1 Title, link, date - detailed summary1, - detailed summary2, - detailed summary..N, ## Link2 Title, link, date - detailed summary1, - detailed summary2, - detailed point..N, etc. The report should be written in Korean and use the ê°œì¡°ì‹ ë¬¸ì²´ style. give the very deep details for each link as much as possible.

<p><a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/cogvlm2-llama3-chat-19B">https://huggingface.co/THUDM/cogvlm2-llama3-chat-19B</a></p>
<h1 id="CogVLM2"><a href="#CogVLM2" class="headerlink" title="CogVLM2"></a>CogVLM2</h1><div align="center">
<img src=https://raw.githubusercontent.com/THUDM/CogVLM2/53d5d5ea1aa8d535edffc0d15e31685bac40f878/resources/logo.svg width="40%"/>
</div>
<p align="center">
    ğŸ‘‹ <a href="resources/WECHAT.md" target="_blank">Wechat</a> Â· ğŸ’¡<a href="http://36.103.203.44:7861/" target="_blank">Online Demo</a> Â· ğŸˆ<a href="https://github.com/THUDM/CogVLM2" target="_blank">Github Page</a>
</p>
<p align="center">
ğŸ“Experience the larger-scale CogVLM model on the <a target="_blank" rel="noopener" href="https://open.bigmodel.cn/dev/api#super-humanoid">ZhipuAI Open Platform</a>.
</p>

<h2 id="Model-introduction"><a href="#Model-introduction" class="headerlink" title="Model introduction"></a>Model introduction</h2><p>We launch a new generation of <strong>CogVLM2</strong> series of models and open source two models built with <a target="_blank" rel="noopener" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">Meta-Llama-3-8B-Instruct</a>. Compared with the previous generation of CogVLM open source models, the CogVLM2 series of open source models have the following improvements:</p>
<ol>
<li>Significant improvements in many benchmarks such as <code>TextVQA</code>, <code>DocVQA</code>.</li>
<li>Support <strong>8K</strong> content length.</li>
<li>Support image resolution up to <strong>1344 * 1344</strong>.</li>
<li>Provide an open source model version that supports both <strong>Chinese and English</strong>.</li>
</ol>
<p>You can see the details of the CogVLM2 family of open source models in the table below:</p>
<table>
<thead>
<tr>
<th>Model name</th>
<th>cogvlm2-llama3-chat-19B</th>
<th>cogvlm2-llama3-chinese-chat-19B</th>
</tr>
</thead>
<tbody><tr>
<td>Base Model</td>
<td>Meta-Llama-3-8B-Instruct</td>
<td>Meta-Llama-3-8B-Instruct</td>
</tr>
<tr>
<td>Language</td>
<td>English</td>
<td>Chinese, English</td>
</tr>
<tr>
<td>Model size</td>
<td>19B</td>
<td>19B</td>
</tr>
<tr>
<td>Task</td>
<td>Image understanding, dialogue model</td>
<td>Image understanding, dialogue model</td>
</tr>
<tr>
<td>Text length</td>
<td>8K</td>
<td>8K</td>
</tr>
<tr>
<td>Image resolution</td>
<td>1344 * 1344</td>
<td>1344 * 1344</td>
</tr>
</tbody></table>
<h2 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h2><p>Our open source models have achieved good results in many lists compared to the previous generation of CogVLM open source models. Its excellent performance can compete with some non-open source models, as shown in the table below:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Open Source</th>
<th>LLM Size</th>
<th>TextVQA</th>
<th>DocVQA</th>
<th>ChartQA</th>
<th>OCRbench</th>
<th>MMMU</th>
<th>MMVet</th>
<th>MMBench</th>
</tr>
</thead>
<tbody><tr>
<td>CogVLM1.1</td>
<td>âœ…</td>
<td>7B</td>
<td>69.7</td>
<td>-</td>
<td>68.3</td>
<td>590</td>
<td>37.3</td>
<td>52.0</td>
<td>65.8</td>
</tr>
<tr>
<td>LLaVA-1.5</td>
<td>âœ…</td>
<td>13B</td>
<td>61.3</td>
<td>-</td>
<td>-</td>
<td>337</td>
<td>37.0</td>
<td>35.4</td>
<td>67.7</td>
</tr>
<tr>
<td>Mini-Gemini</td>
<td>âœ…</td>
<td>34B</td>
<td>74.1</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>48.0</td>
<td>59.3</td>
<td>80.6</td>
</tr>
<tr>
<td>LLaVA-NeXT-LLaMA3</td>
<td>âœ…</td>
<td>8B</td>
<td>-</td>
<td>78.2</td>
<td>69.5</td>
<td>-</td>
<td>41.7</td>
<td>-</td>
<td>72.1</td>
</tr>
<tr>
<td>LLaVA-NeXT-110B</td>
<td>âœ…</td>
<td>110B</td>
<td>-</td>
<td>85.7</td>
<td>79.7</td>
<td>-</td>
<td>49.1</td>
<td>-</td>
<td>80.5</td>
</tr>
<tr>
<td>InternVL-1.5</td>
<td>âœ…</td>
<td>20B</td>
<td>80.6</td>
<td>90.9</td>
<td><strong>83.8</strong></td>
<td>720</td>
<td>46.8</td>
<td>55.4</td>
<td><strong>82.3</strong></td>
</tr>
<tr>
<td>QwenVL-Plus</td>
<td>âŒ</td>
<td>-</td>
<td>78.9</td>
<td>91.4</td>
<td>78.1</td>
<td>726</td>
<td>51.4</td>
<td>55.7</td>
<td>67.0</td>
</tr>
<tr>
<td>Claude3-Opus</td>
<td>âŒ</td>
<td>-</td>
<td>-</td>
<td>89.3</td>
<td>80.8</td>
<td>694</td>
<td><strong>59.4</strong></td>
<td>51.7</td>
<td>63.3</td>
</tr>
<tr>
<td>Gemini Pro 1.5</td>
<td>âŒ</td>
<td>-</td>
<td>73.5</td>
<td>86.5</td>
<td>81.3</td>
<td>-</td>
<td>58.5</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT-4V</td>
<td>âŒ</td>
<td>-</td>
<td>78.0</td>
<td>88.4</td>
<td>78.5</td>
<td>656</td>
<td>56.8</td>
<td><strong>67.7</strong></td>
<td>75.0</td>
</tr>
<tr>
<td>CogVLM1.1</td>
<td>âœ…</td>
<td>7B</td>
<td>69.7</td>
<td>-</td>
<td>68.3</td>
<td>590</td>
<td>37.3</td>
<td>52.0</td>
<td>65.8</td>
</tr>
<tr>
<td>CogVLM2-LLaMA3 (Ours)</td>
<td>âœ…</td>
<td>8B</td>
<td>84.2</td>
<td><strong>92.3</strong></td>
<td>81.0</td>
<td>756</td>
<td>44.3</td>
<td>60.4</td>
<td>80.5</td>
</tr>
<tr>
<td>CogVLM2-LLaMA3-Chinese (Ours)</td>
<td>âœ…</td>
<td>8B</td>
<td><strong>85.0</strong></td>
<td>88.4</td>
<td>74.7</td>
<td><strong>780</strong></td>
<td>42.8</td>
<td>60.5</td>
<td>78.9</td>
</tr>
</tbody></table>
<p>All reviews were obtained without using any external OCR tools (â€œpixel onlyâ€).</p>
<p><a target="_blank" rel="noopener" href="https://www-cnbc-com.cdn.ampproject.org/c/s/www.cnbc.com/amp/2024/05/20/microsoft-qualcomm-ai-pcs-snapdragon-arm-processors.html">https://www-cnbc-com.cdn.ampproject.org/c/s/www.cnbc.com/amp/2024/05/20/microsoft-qualcomm-ai-pcs-snapdragon-arm-processors.html</a><br>TECH<br>Microsoft announces new PCs with AI chips from Qualcomm<br>PUBLISHED MON, MAY 20 2024 2:01 PM EDT<br>UPDATED MON, MAY 20 2024 4:03 PM EDT<br>Jordan Novet<br>@JORDANNOVET<br>WATCH LIVE<br>KEY POINTS<br>Microsoft will bring out new Surface PCs that adhere to its Copilot+ standard for running artificial intelligence models.<br>These PCs and others use Arm-based Qualcomm chips to deliver longer battery life, and PCs with AMD and Intel chips will also become available.<br>Microsoft Chairman and Chief Executive Officer Satya Nadella speaks during the Microsoft May 20 Briefing event at Microsoft in Redmond, Washington, on May 20, 2024.<br>Microsoft Chairman and Chief Executive Officer Satya Nadella speaks during the Microsoft May 20 Briefing event at Microsoft in Redmond, Washington, on May 20, 2024.<br>Jason Redmond | AFP | Getty Images<br>Microsoft is touting new computers with advanced chips designed to run artificial intelligence features of software for Windows, without quickly using up battery life.</p>
<p>The company on Monday announced a Surface Laptop and a Surface Pro tablet with a Qualcomm chip that can run some AI tasks without an internet connection. Other computer makers like Lenovo, Dell, HP, Asus, Acer and Samsung are also launching AI-ready PCs powered by Qualcommâ€™s Snapdragon X Elite and X Plus processors, which promise longer battery life and will run Microsoftâ€™s Copilot AI chatbot.</p>
<p><a target="_blank" rel="noopener" href="https://github.blog/2024-05-13-research-quantifying-github-copilots-impact-in-the-enterprise-with-accenture/">https://github.blog/2024-05-13-research-quantifying-github-copilots-impact-in-the-enterprise-with-accenture/</a><br>Research: Quantifying GitHub Copilotâ€™s impact in the enterprise with Accenture<br>We conducted research with developers at Accenture to understand GitHub Copilotâ€™s real-world impact in enterprise organizations.</p>
<p>An image showing the Accenture and GitHub logos with text that reads â€œThe enterprise impact of GitHub Copilotâ€<br>Authors<br>Ya GaoYa Gao<br>GitHub Customer ResearchGitHub Customer Research<br>May 13, 2024<br>Since bringing GitHub Copilot to market, weâ€™ve conducted several lab studies to discover its impact on developer efficiency, developer satisfaction, and overall code quality. We found that our AI pair programmer helps developers code up to 55% faster and that it made 85% of developers feel more confident in their code quality. With the introduction of our first GitHub Copilot offering for businesses and organizations in 2023â€”and more recently GitHub Copilot Enterpriseâ€”itâ€™s become increasingly important for us to measure the impact of GitHub Copilot across real-world, large engineering organizations.</p>
<p>To learn more, we partnered with Accenture to study how developers integrated GitHub Copilot into their daily workflows, and we found significant improvements in several areas, including:</p>
<p>Improved developer satisfaction. 90% of developers found they were more fulfilled with their job when using GitHub Copilot, and 95% said they enjoyed coding more with Copilotâ€™s help.<br>Quickly adopted by developers. Over 80% of Accenture participants successfully adopted GitHub Copilot with a 96% success rate among initial users. 43% found it â€œextremely easy to use.â€ Additionally, 67% of total participants used GitHub Copilot at least 5 days per week, averaging 3.4 days of usage weekly.<br>Methodology<br>In this study, we collaborated with Accenture to conduct an extensive, randomized controlled trial (RCT). Participants included developers who engage in a variety of software development tasks daily, including engineering, design, and testing across a spectrum of software products and services. They hold various positions within their organizations from entry-level roles to team management positions, and may work collaboratively or independently depending on the project and team dynamics.</p>
<p>For the trial, developers were randomly assigned to two groups. One group of developers was given access to GitHub Copilot, while the other group was not. Our objective was to assess the influence of GitHub Copilot on developersâ€™ experience within the enterprise setting, where they collaborate on multifaceted projects. We collected DevOps telemetry on several output performance metrics that reflect insights into developersâ€™ regular coding activity.</p>
<p>Beyond the initial experiment, we conducted a company-wide adoption analysis, which explored installation rates, generated code acceptance rates, and the time it took developers to accept GitHub Copilotâ€™s first coding suggestion. Success was determined by whether they accepted a suggestion from GitHub Copilot or not.</p>
<p>In addition, we surveyed the GitHub Copilot users at Accenture to gain a better understanding of how developers perceived the impact of GitHub Copilot on their workflows. Not only did this survey uncover insights into how and when developers are using GitHub Copilot, but it also indicated an overwhelming improvement in developer satisfaction, which we know to be a key component of the developer experience (DevEx). The combination of both telemetry data and the information from the survey provides a full picture for us to understand GitHub Copilotâ€™s impact at the enterprise level.</p>
<p>Our findings<br>Developers quickly found value in GitHub Copilot and adopted it as part of their daily toolkit<br>More than 50,000 organizations have adopted GitHub Copilot so far, but we havenâ€™t yet had a clear view into what those adoption rates look like on the individual level. When we dug deeper into the usage patterns of GitHub Copilot among Accenture developers, 67% of respondents reported utilizing GitHub Copilot at least 5 days per week, with an average usage frequency of 3.4 days per week. Moreover, a substantial 70% of respondents relied on GitHub Copilot for coding tasks in a familiar programming language. This indicates a high level of integration of GitHub Copilot into developersâ€™ daily workflows, highlighting its importance as a valuable engineering tool and resource.</p>
<p>We also observed that developers were excited to use GitHub Copilot. 81.4% of developers installed the GitHub Copilot IDE extension on the same day that they received a license. And not only were they excited to use it, but getting started was simple and did not provide a barrier to entry.</p>
<p>In fact, 96% of those who installed the IDE extension started receiving and accepting suggestions on the same day. On average, developers took just one minute from seeing their first suggestion to accepting one, too. This was further validated in user surveys, with 43% finding GitHub Copilot â€œextremely easy to useâ€ and 51% rating it as â€œextremely useful.â€</p>
<p>As part of the GitHub Copilot service, we provide the measurement capabilities for our customers to determine gains from Copilot themselves. To produce many of the insights in this report, we leveraged public APIs available via GitHub and Azure DevOps. Among them, GitHub offers the GitHub Copilot Metrics API, designed to provide users with information about Copilot usage within your organization. You can also explore the Copilot Learning Pathways to learn more about what GitHub Copilot can help your business achieve. Keep reading for more information on conducting your own studies on GitHub Copilotâ€™s impact.</p>
<p>A chart showing how developers at Accenture gauged the ease of using GitHub Copilot. 42.8% of developers at Accenture deem it â€œextremely easy to use.â€</p>
<p>A chart showing how developers at Accenture gauged the usefulness of GitHub Copilot. 50.9% of developers at Accenture deem it â€œextremely useful.â€</p>
<p>Developers improved code quality using GitHub Copilot<br>By convention, pull requests represent a ready-to-deploy code change (for example, a new feature, bug fix, or code refactoring). When measured in aggregate, the number of pull requests per developer can be used to measure a teamâ€™s throughput or velocity. Ultimately, an increase in pull requests represents an increase in value delivered, and Accenture developers saw an 8.69% increase in pull requests. Because each pull request must pass through a code review, the pull request merge rate is an excellent measure of code quality as seen through the eyes of a maintainer or coworker. Accenture saw a 15% increase to the pull request merge rate, which means that as the volume of pull requests increased, so did the number of pull requests passing code review.</p>
<p>But we donâ€™t want to just shift issues downstream and overburden the system with low-quality code. Itâ€™s one thing for a teammate to assess quality and yet another for new code to successfully complete CI runs where test automation evaluates code quality against deterministic measures. At Accenture, we saw an 84% increase in successful builds suggesting not only that more pull requests were passing through the system, but they were also of higher quality as assessed by both human reviewers and test automation.</p>
<p>By enabling developers to maintain focus and stay in the flow, GitHub Copilot doesnâ€™t sacrifice quality for speed. And our findings provide evidence for exactly that.</p>
<p>In our study, developers accepted around 30% of GitHub Copilotâ€™s suggestions. And 90% of the developers reported that they committed code suggested by GitHub Copilot, while 91% of the developers reported that their teams had merged pull requests containing code suggested by GitHub Copilot. Analysis also showed high usage rates with the accepted codeâ€”for example, developers retained 88% of GitHub Copilot-generated characters in their editor.</p>
<p>A chart showing how developers at Accenture gauged GitHub Copilotâ€™s impact on production code, also outlined in the preceding paragraph.</p>
<p>By experiencing improved success rates in builds, developers can reduce the likelihood of errors.</p>
<p>GitHub Copilot improved the overall developer experience<br>Our survey among Accenture developers unveiled compelling findings indicating a significant boost in overall developer satisfaction with GitHub Copilot. An impressive 90% of developers expressed feeling more fulfilled with their jobs when utilizing GitHub Copilot, and a staggering 95% of developers reported enjoying coding more when leveraging GitHub Copilotâ€™s capabilities.</p>
<p>This enhancement in job satisfaction could allow developers to allocate their focus toward tasks most fulfilling to them, like solutions design or collaboration. Furthermore, our analysis revealed that developersâ€™ heightened fulfillment correlated directly with their engagement with GitHub Copilot. When using GitHub Copilot less than two days per week, fulfillment only increased â€œa little.â€ But when using GitHub Copilot more than 2 days per week, fulfillment increases â€œquite a bit.â€</p>
<p>70% of developers also reported quite a bit less mental effort was expended on repetitive tasks, and 54% spent less time searching for information or examples when utilizing GitHub Copilot. This reduction in cognitive load could enable developers to allocate their cognitive resources more efficiently, reducing burnout. GitHub Copilot also allowed developers to maintain uninterrupted focus, with a majority indicating that they could maintain flow state while using the tool, a hallmark of good DevEx. These impacts extend beyond mere task optimization, which offers enterprises a competitive edge by maximizing developer resources and fostering a conducive environment for innovation and growth.</p>
<p>A chart showing how developers at Accenture grew more fulfilled the more they used GitHub Copilot.</p>
<p>How to evaluate the impact of GitHub Copilot in your organization</p>
<p>Organizations seeking to conduct studies on the impact of GitHub Copilot can follow a methodological approach focusing on collecting and analyzing three types of data: quantitative, qualitative, and operational. To ensure readiness for data collection, organizations are advised to streamline their DevOps platform telemetry infrastructure in alignment with their specific goals and workflows.</p>
<p>Itâ€™s essential to note that success metrics should be tailored to reflect the unique processes and operations of each organization. By adopting this methodology and customizing metrics accordingly, organizations can effectively gauge the impact of GitHub Copilot.</p>
<p>Learn how to measure the impact of GitHub Copilot in your organization &gt;</p>
<p>From the lab to the real world<br>After conducting multiple lab studies on the impact of GitHub Copilot, we are now working to understand how GitHub Copilot affected developersâ€™ workdays in real-world environmentsâ€”and thatâ€™s been made possible by the tremendous adoption weâ€™ve seen among businesses and enterprise organizations alike.</p>
<p>With this study, we have uncovered compelling evidence that GitHub Copilot significantly enhances developer experience, satisfaction, and overall job fulfillment in real-world enterprise settings. With GitHub Copilot in their toolkits, developers can also enhance their skill sets and gain greater proficiency in their organizationâ€™s codebase, which ultimately leads to heightened contribution levels across teams, all without sacrificing the quality of code.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5">https://github.com/01-ai/Yi-1.5</a><br>Letâ€™s goo! Yi release 1.5 models 9B and 34B now with 16K &amp; 32K context! Apache 2.0 licensed! ğŸ”¥</p>
<blockquote>
<p>Continuous pre-training on 500B (total 3.6T) tokens<br>3M carefully curated instruction tuning set<br>Better at code, math, reasoning and instruction following<br>Chat checkpoints go up to 16K<br>Base checkpoints go up to 32K<br>Congrats, and thanks to the 01 AI team for open-sourcing such brilliant checkpoints!</p>
</blockquote>
<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#intro"></a></p>
<p>Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.</p>
<p>Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension.</p>
<p>Yi-1.5 comes in 3 model sizes: 34B, 9B, and 6B. For model details and benchmarks, seeÂ <a target="_blank" rel="noopener" href="https://huggingface.co/collections/01-ai/yi-15-2024-05-663f3ecab5f815a3eaca7ca8">Model Card</a>.</p>
<h2 id="News"><a href="#News" class="headerlink" title="News"></a>News</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#news"></a></p>
<ul>
<li>2024-05-13: The Yi-1.5 series models are open-sourced, further improving coding, math, reasoning, and instruction-following abilities.</li>
</ul>
<h2 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#requirements"></a></p>
<ul>
<li><p>Make sure Python 3.10 or a later version is installed.</p>
</li>
<li><p>Set up the environment and install the required packages.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>Download the Yi-1.5 model fromÂ <a target="_blank" rel="noopener" href="https://huggingface.co/01-ai">Hugging Face</a>,Â <a target="_blank" rel="noopener" href="https://www.modelscope.cn/organization/01ai/">ModelScope</a>, orÂ <a target="_blank" rel="noopener" href="https://wisemodel.cn/organization/01.AI">WiseModel</a>.</p>
</li>
</ul>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#quick-start"></a></p>
<p>This tutorial runs Yi-1.5-34B-Chat locally on an A800 (80G).</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from transformers import AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_path = &#x27;&lt;your-model-path&gt;&#x27;</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)</span><br><span class="line"></span><br><span class="line"># Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_path,</span><br><span class="line">    device_map=&quot;auto&quot;,</span><br><span class="line">    torch_dtype=&#x27;auto&#x27;</span><br><span class="line">).eval()</span><br><span class="line"></span><br><span class="line"># Prompt content: &quot;hi&quot;</span><br><span class="line">messages = [</span><br><span class="line">    &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;hi&quot;&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, return_tensors=&#x27;pt&#x27;)</span><br><span class="line">output_ids = model.generate(input_ids.to(&#x27;cuda&#x27;), eos_token_id=tokenizer.eos_token_id)</span><br><span class="line">response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)</span><br><span class="line"></span><br><span class="line"># Model response: &quot;Hello! How can I assist you today?&quot;</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure>

<h2 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#deployment"></a></p>
<p>Prerequisites: Before deploying Yi-1.5 models, make sure you meet theÂ <a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi/tree/main?tab=readme-ov-file#software-requirements">software and hardware requirements</a>.</p>
<h3 id="vLLM"><a href="#vLLM" class="headerlink" title="vLLM"></a>vLLM</h3><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#vllm"></a></p>
<p>Prerequisites: Download the latest version ofÂ <a target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/getting_started/installation.html">vLLM</a>.</p>
<ol>
<li><p>Start the server with a chat model.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m vllm.entrypoints.openai.api_server  --model 01-ai/Yi-1.5-9B-Chat  --served-model-name Yi-1.5-9B-Chat</span><br></pre></td></tr></table></figure>
</li>
<li><p>Use the chat API.</p>
</li>
</ol>
<ul>
<li><p>HTTP</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:8000/v1/chat/completions\</span><br><span class="line">    -H &quot;Content-Type: application/json&quot;\</span><br><span class="line">    -d &#x27;&#123;</span><br><span class="line">        &quot;model&quot;: &quot;Yi-1.5-9B-Chat&quot;,</span><br><span class="line">        &quot;messages&quot;: [</span><br><span class="line">            &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;,</span><br><span class="line">            &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;&#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;&#x27;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Python client</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from openai import OpenAI</span><br><span class="line"># Set OpenAI&#x27;s API key and API base to use vLLM&#x27;s API server.</span><br><span class="line">openai_api_key = &quot;EMPTY&quot;</span><br><span class="line">openai_api_base = &quot;http://localhost:8000/v1&quot;</span><br><span class="line"></span><br><span class="line">client = OpenAI(</span><br><span class="line">    api_key=openai_api_key,</span><br><span class="line">    base_url=openai_api_base,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">chat_response = client.chat.completions.create(</span><br><span class="line">    model=&quot;Yi-1.5-9B-Chat&quot;,</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;,</span><br><span class="line">        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me a joke.&quot;&#125;,</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">print(&quot;Chat response:&quot;, chat_response)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Web-Demo"><a href="#Web-Demo" class="headerlink" title="Web Demo"></a>Web Demo</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#web-demo"></a></p>
<p>We have deployed aÂ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/01-ai/Yi-1.5-34B-Chat">Yi-1.5-34B-Chat Space</a>Â on Huggingface.</p>
<p>Or you can build it locally by yourself, as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python demo/web_demo.py -c &lt;your-model-path&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a>Fine-tuning</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#fine-tuning"></a></p>
<p>You can useÂ <a target="_blank" rel="noopener" href="https://github.com/hiyouga/LLaMA-Factory">LLaMA-Factory</a>,Â <a target="_blank" rel="noopener" href="https://github.com/modelscope/swift">Swift</a>,Â <a target="_blank" rel="noopener" href="https://github.com/InternLM/xtuner">XTuner</a>, andÂ <a target="_blank" rel="noopener" href="https://github.com/yangjianxin1/Firefly">Firefly</a>Â for fine-tuning. These frameworks all support fine-tuning the Yi series models.</p>
<h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#api"></a></p>
<p>Yi APIs are OpenAI-compatible and provided atÂ <a target="_blank" rel="noopener" href="https://platform.lingyiwanwu.com/">Yi Platform</a>. Sign up to get free tokens, and you can also pay-as-you-go at a competitive price. Additionally, Yi APIs are also deployed onÂ <a target="_blank" rel="noopener" href="https://replicate.com/search?query=01+ai">Replicate</a>Â andÂ <a target="_blank" rel="noopener" href="https://openrouter.ai/models?q=01%20ai">OpenRouter</a>.</p>
<h2 id="License"><a href="#License" class="headerlink" title="License"></a>License</h2><p><a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi-1.5#license"></a></p>
<p>The code and weights of the Yi-1.5 series models are distributed under theÂ <a target="_blank" rel="noopener" href="https://github.com/01-ai/Yi/blob/main/LICENSE">Apache 2.0 license</a>.</p>
<p><a target="_blank" rel="noopener" href="https://openai.com/index/how-the-voices-for-chatgpt-were-chosen/">https://openai.com/index/how-the-voices-for-chatgpt-were-chosen/</a><br>May 19, 2024</p>
<p>How the voices for ChatGPT were chosen<br>We worked with industry-leading casting and directing professionals to narrow down over 400 submissions before selecting the 5 voices.</p>
<p>Asset &gt; How the voices for ChatGPT were chosen<br>Voice Mode is one of the most beloved features in ChatGPT. Each of the five distinct voices you hear has been carefully selected through an extensive process spanning five months involving professional voice actors, talent agencies, casting directors, and industry advisors. Weâ€™re sharing more on how the voices were chosen.</p>
<p>In September of 2023, we introduced voice capabilities to give users another way to interact with ChatGPT. Since then, we are encouraged by the way users have responded to the feature and the individual voices. Each of the voicesâ€”Breeze, Cove, Ember, Juniper and Skyâ€”are sampled from voice actors we partnered with to create them.</p>
<p>We support the creative community and collaborated with the voice acting industry<br>We support the creative community and worked closely with the voice acting industry to ensure we took the right steps to cast ChatGPTâ€™s voices. Each actor receives compensation above top-of-market rates, and this will continue for as long as their voices are used in our products.</p>
<p>We believe that AI voices should not deliberately mimic a celebrityâ€™s distinctive voiceâ€”Skyâ€™s voice is not an imitation of Scarlett Johansson but belongs to a different professional actress using her own natural speaking voice. To protect their privacy, we cannot share the names of our voice talents.</p>
<p>We partnered with award-winning casting directors and producers to create the criteria for voices<br>In early 2023, to identify our voice actors, we had the privilege of partnering with independent, well-known, award-winning casting directors and producers. We worked with them to create a set of criteria for ChatGPTâ€™s voices, carefully considering the unique personality of each voice and their appeal to global audiences.</p>
<p>Some of these characteristics included:</p>
<p>Actors from diverse backgrounds or who could speak multiple languages</p>
<p>A voice that feels timeless</p>
<p>An approachable voice that inspires trust</p>
<p>A warm, engaging, confidence-inspiring, charismatic voice with rich tone</p>
<p>Natural and easy to listen to</p>
<p>We received over 400 submissions from voice and screen actors<br>In May of 2023, the casting agency and our casting directors issued a call for talent. In under a week, they received over 400 submissions from voice and screen actors. To audition, actors were given a script of ChatGPT responses and were asked to record them. These samples ranged from answering questions about mindfulness to brainstorming travel plans, and even engaging in conversations about a userâ€™s day.</p>
<p>We selected five final voices and discussed our vision for human-AI interactions and the goals of Voice Mode with the actors<br>Through May 2023, the casting team independently reviewed and hand-selected an initial list of 14 actors. They further refined their list before presenting their top voices for the project to OpenAI.</p>
<p>We spoke with each actor about the vision for human-AI voice interactions and OpenAI, and discussed the technologyâ€™s capabilities, limitations, and the risks involved, as well as the safeguards we have implemented. It was important to us that each actor understood the scope and intentions of Voice Mode before committing to the project.</p>
<p>An internal team at OpenAI reviewed the voices from a product and research perspective, and after careful consideration, the voices for Breeze, Cove, Ember, Juniper and Sky were finally selected.</p>
<p>Each actor flew to San Francisco for recording sessions and their voices were launched into ChatGPT in September 2023<br>During June and July, we flew the actors to San Francisco for recording sessions and in-person meetings with the OpenAI product and research teams.</p>
<p>On September 25, 2023, we launched their voices into ChatGPT.</p>
<p>This entire process involved extensive coordination with the actors and the casting team, taking place over five months. We are continuing to collaborate with the actors, who have contributed additional work for audio research and new voice capabilities in GPT-4o.</p>
<p>New Voice Mode coming to GPT-4o for paid users, and adding new voices<br>We plan to give access to a new Voice Mode for GPT-4o(opens in a new window) in alpha to ChatGPT Plus users in the coming weeks. With GPT-4o, using your voice to interact with ChatGPT is much more natural. GPT-4o handles interruptions smoothly, manages group conversations effectively, filters out background noise, and adapts to tone.</p>
<p>Looking ahead, you can expect even more options as we plan to introduce additional voices in ChatGPT to better match the diverse interests and preferences of users.</p>
</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-05-20</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/05/20/2024á„‚á…§á†«-5á„‹á…¯á†¯-21á„‹á…µá†¯-AI-á„‚á…²á„‰á…³/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 5ì›” 21ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/05/22/2024-5-23-AI-NEWS/" title="2024ë…„ 5ì›” 23ì¼ AI ì†Œì‹">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/05/20/README-md/" title="README.md">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>