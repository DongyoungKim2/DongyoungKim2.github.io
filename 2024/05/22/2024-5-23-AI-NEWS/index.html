<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 5월 23일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="Summary오늘의 소식에서는 TimeGPT-1, LANISTR, Mistral-7B-Instruct 모델에 대해 다룹니다. TimeGPT-1은 시계열 예측 및 이상 탐지를 위한 혁신적인 모델이며, LANISTR는 구조화된 데이터와 비구조화된 데이터를 융합하여 다중모드"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 5월 23일 AI 소식</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>오늘의 소식에서는 TimeGPT-1, LANISTR, Mistral-7B-Instruct 모델에 대해 다룹니다. TimeGPT-1은 시계열 예측 및 이상 탐지를 위한 혁신적인 모델이며, LANISTR는 구조화된 데이터와 비구조화된 데이터를 융합하여 다중모드 학습을 가능하게 하는 프레임워크입니다. Mistral-7B-Instruct-v0.3은 개선된 어휘와 기능 호출을 지원하는 대형 언어 모델입니다.</p>
<h2 id="Nixtla-TimeGPT-1"><a href="#Nixtla-TimeGPT-1" class="headerlink" title="Nixtla: TimeGPT-1"></a>Nixtla: TimeGPT-1</h2><p><a target="_blank" rel="noopener" href="https://github.com/Nixtla/nixtla">Nixtla: TimeGPT-1</a> - May 23, 2024</p>
<ul>
<li>TimeGPT-1은 다양한 도메인에서 시계열 데이터를 예측하고 이상 탐지 기능을 제공하는 생산 준비된 생성 사전 학습 변환기입니다.</li>
<li>사용자가 단 몇 줄의 코드로 전기, 금융, IoT 등의 도메인에서 정확한 예측을 할 수 있도록 지원합니다.</li>
<li>설치 및 빠른 시작 가이드, API 사용법 등이 포함된 포괄적인 문서 제공.</li>
<li>TimeGPT는 제로샷 추론 능력을 통해 별도의 학습 없이 다양한 시계열 데이터에 즉시 적용 가능.</li>
<li>사용자 정의 손실 함수, 크로스 검증, 예측 간격 제공 등 다양한 기능 지원.</li>
</ul>
<h2 id="Google-LANISTR-Multimodal-learning-from-structured-and-unstructured-data"><a href="#Google-LANISTR-Multimodal-learning-from-structured-and-unstructured-data" class="headerlink" title="Google: LANISTR - Multimodal learning from structured and unstructured data"></a>Google: LANISTR - Multimodal learning from structured and unstructured data</h2><p><a target="_blank" rel="noopener" href="https://research.google/blog/lanistr-multimodal-learning-from-structured-and-unstructured-data/">Google: LANISTR</a> - May 22, 2024</p>
<ul>
<li>LANISTR는 언어, 이미지, 구조화된 데이터를 융합하여 다중모드 학습을 가능하게 하는 새로운 프레임워크입니다.</li>
<li>구조화된 데이터와 비구조화된 데이터의 융합을 통해 예측 및 분류 정확도를 향상시킵니다.</li>
<li>MIMIC-IV 의료 데이터 및 Amazon 리뷰 데이터셋을 사용하여 우수한 성능을 입증.</li>
<li>여러 모달리티에서 누락된 데이터에 대한 견고성을 보여주는 마스킹 기반 학습 전략 사용.</li>
<li>모달리티별 인코더와 다중모달 인코더-디코더 모듈을 포함한 혁신적인 아키텍처.</li>
</ul>
<h2 id="Hugging-Face-Mistral-7B-Instruct-v0-3"><a href="#Hugging-Face-Mistral-7B-Instruct-v0-3" class="headerlink" title="Hugging Face: Mistral-7B-Instruct-v0.3"></a>Hugging Face: Mistral-7B-Instruct-v0.3</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3">Hugging Face: Mistral-7B-Instruct-v0.3</a> - May 23, 2024</p>
<ul>
<li>Mistral-7B-Instruct-v0.3은 확장된 어휘와 기능 호출을 지원하는 Mistral-7B-v0.3의 명령어 튜닝 버전입니다.</li>
<li>모델 설치 및 다운로드, 채팅 기능, 명령어 따르기, 기능 호출 예제 등 제공.</li>
<li>Hugging Face의 <code>transformers</code> 라이브러리를 사용하여 텍스트 생성 가능.</li>
<li>모델은 빠르게 튜닝되어 높은 성능을 발휘하며, 적절한 가드레일 설정을 위한 커뮤니티와의 협업을 기대.</li>
</ul>
<details>
  <summary>Sources</summary>
  This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is: # AI News for (today's date), ## Summary (overall short summary), ## Link1 Title, link, date - detailed summary1, - detailed summary2, - detailed summary..N, ## Link2 Title, link, date - detailed summary1, - detailed summary2, - detailed point..N, etc. The report should be written in Korean and use the 개조식 문체 style. give the very deep details for each link as much as possible.

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://github.com/Nixtla/nixtla</span><br><span class="line"># Nixtla &amp;nbsp; [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Statistical%20Forecasting%20Algorithms%20by%20Nixtla%20&amp;url=https://github.com/Nixtla/neuralforecast&amp;via=nixtlainc&amp;hashtags=StatisticalModels,TimeSeries,Forecasting) &amp;nbsp;[![Slack](https://img.shields.io/badge/Slack-4A154B?&amp;logo=slack&amp;logoColor=white)](https://join.slack.com/t/nixtlacommunity/shared_invite/zt-1pmhan9j5-F54XR20edHk0UtYAPcW4KQ)</span><br><span class="line"></span><br><span class="line">&lt;div align=&quot;center&quot;&gt;</span><br><span class="line">&lt;img src=&quot;https://raw.githubusercontent.com/Nixtla/neuralforecast/main/nbs/imgs_indx/logo_new.png&quot;&gt;</span><br><span class="line">&lt;h1 align=&quot;center&quot;&gt;TimeGPT-1 &lt;/h1&gt;</span><br><span class="line">&lt;h3 align=&quot;center&quot;&gt;The first foundation model for forecasting and anomaly detection&lt;/h3&gt;</span><br><span class="line"></span><br><span class="line">[![CI](https://github.com/Nixtla/nixtla/actions/workflows/ci.yaml/badge.svg?branch=main)](https://github.com/Nixtla/nixtla/actions/workflows/ci.yaml)</span><br><span class="line">[![PyPi](https://img.shields.io/pypi/v/nixtla?color=blue)](https://pypi.org/project/nixtla/)</span><br><span class="line">[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://github.com/Nixtla/nixtla/blob/main/LICENSE)</span><br><span class="line">[![docs](https://img.shields.io/website-up-down-green-red/http/docs.nixtla.io/.svg?label=docs)](https://docs.nixtla.io)</span><br><span class="line">[![Downloads](https://pepy.tech/badge/nixtla)](https://pepy.tech/project/nixtla)</span><br><span class="line">[![Downloads](https://pepy.tech/badge/nixtla/month)](https://pepy.tech/project/nixtla)</span><br><span class="line">[![Downloads](https://pepy.tech/badge/nixtla/week)](https://pepy.tech/project/nixtla)</span><br><span class="line">[![fern shield](https://img.shields.io/badge/%F0%9F%8C%BF-SDK%20generated%20by%20Fern-brightgreen)](https://buildwithfern.com/?utm_source=nixtla/nixtla/readme)</span><br><span class="line"></span><br><span class="line">**TimeGPT** is a production ready, generative pretrained transformer for time series. It&#x27;s capable of accurately predicting various domains such as retail, electricity, finance, and IoT with just a few lines of code 🚀. &lt;/div&gt;</span><br><span class="line"></span><br><span class="line">## 📖 Table of Contents</span><br><span class="line">- [Quick Start](#-quick-start)</span><br><span class="line">- [Installation](#install-nixtlas-sdk)</span><br><span class="line">- [Forecasting with TimeGPT](#forecast-using-timegpt-in-3-easy-steps)</span><br><span class="line">- [Anomaly Detection](#anomaly-detection-using-timegpt-in-3-easy-steps)</span><br><span class="line">- [Zero-shot Results](#️-zero-shot-results)</span><br><span class="line">- [How to Cite](#-how-to-cite)</span><br><span class="line">- [Features and Mentions](#-features-and-mentions)</span><br><span class="line">- [License](#-license)</span><br><span class="line">- [Get in Touch](#-get-in-touch)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 🚀 Quick Start</span><br><span class="line"></span><br><span class="line">https://github.com/Nixtla/nixtla/assets/4086186/163ad9e6-7a16-44e1-b2e9-dab8a0b7b6b6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### Install nixtla&#x27;s SDK</span><br><span class="line">python</span><br><span class="line">pip install nixtla&gt;=0.5.1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### Import libraries and load data</span><br><span class="line"> python</span><br><span class="line">import pandas as pd</span><br><span class="line">from nixtla import NixtlaClient</span><br><span class="line"></span><br><span class="line">###  Forecast using TimeGPT in 3 easy steps</span><br><span class="line">python</span><br><span class="line"># Get your API Key at dashboard.nixtla.io</span><br><span class="line"></span><br><span class="line"># 1. Instantiate the NixtlaClient</span><br><span class="line">nixtla_client = NixtlaClient(api_key = &#x27;YOUR API KEY HERE&#x27;)</span><br><span class="line"></span><br><span class="line"># 2. Read historic electricity demand data</span><br><span class="line">df = pd.read_csv(&#x27;https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/electricity-short.csv&#x27;)</span><br><span class="line"></span><br><span class="line"># 3. Forecast the next 24 hours</span><br><span class="line">fcst_df = nixtla_client.forecast(df, h=24, level=[80, 90])</span><br><span class="line"></span><br><span class="line"># 4. Plot your results (optional)</span><br><span class="line">nixtla_client.plot(df, timegpt_fcst_df, time_col=&#x27;timestamp&#x27;, target_col=&#x27;value&#x27;, level=[80, 90])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">![Forecast Results](./nbs/img/forecast_readme.png)</span><br><span class="line"></span><br><span class="line">###  Anomaly detection using TimeGPT in 3 easy steps</span><br><span class="line">python</span><br><span class="line"># Get your API Key at dashboard.nixtla.io</span><br><span class="line"></span><br><span class="line"># 1. Instantiate the NixtlaClient</span><br><span class="line">nixtla_client = NixtlaClient(api_key = &#x27;YOUR API KEY HERE&#x27;)</span><br><span class="line"></span><br><span class="line"># 2. Read Data # Wikipedia visits of NFL Star (</span><br><span class="line">df = pd.read_csv(&#x27;https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/peyton_manning.csv&#x27;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 3. Detect Anomalies</span><br><span class="line">anomalies_df = nixtla_client.detect_anomalies(df, time_col=&#x27;timestamp&#x27;, target_col=&#x27;value&#x27;, freq=&#x27;D&#x27;)</span><br><span class="line"></span><br><span class="line"># 4. Plot your results (optional)</span><br><span class="line">nixtla_client.plot(df, anomalies_df,time_col=&#x27;timestamp&#x27;, target_col=&#x27;value&#x27;)</span><br><span class="line"></span><br><span class="line">![AnomalyDetection](nbs/img/anomaly.png)</span><br><span class="line"></span><br><span class="line">## 🤓 API support for other languages</span><br><span class="line">Explore our [API Reference](https://docs.nixtla.io) to discover how to leverage TimeGPT across various programming languages including JavaScript, Go, and more.</span><br><span class="line"></span><br><span class="line">## 🔥 Features and Capabilities</span><br><span class="line"></span><br><span class="line">- **Zero-shot Inference**: TimeGPT can generate forecasts and detect anomalies straight out of the box, requiring no prior training data. This allows for immediate deployment and quick insights from any time series data.</span><br><span class="line"></span><br><span class="line">- **Fine-tuning**: Enhance TimeGPT&#x27;s capabilities by fine-tuning the model on your specific datasets, enabling the model to adapt to the nuances of your unique time series data and improving performance on tailored tasks.</span><br><span class="line"></span><br><span class="line">- **API Access**: Integrate TimeGPT seamlessly into your applications via our robust API. Upcoming support for Azure Studio will provide even more flexible integration options. Alternatively, deploy TimeGPT on your own infrastructure to maintain full control over your data and workflows.</span><br><span class="line"></span><br><span class="line">- **Add Exogenous Variables**: Incorporate additional variables that might influence your predictions to enhance forecast accuracy. (E.g. Special Dates, events or prices)</span><br><span class="line"></span><br><span class="line">- **Multiple Series Forecasting**: Simultaneously forecast multiple time series data, optimizing workflows and resources.</span><br><span class="line"></span><br><span class="line">- **Custom Loss Function**: Tailor the fine-tuning process with a custom loss function to meet specific performance metrics.</span><br><span class="line"></span><br><span class="line">- **Cross Validation**: Implement out of the box cross-validation techniques to ensure model robustness and generalizability.</span><br><span class="line"></span><br><span class="line">- **Prediction Intervals**: Provide intervals in your predictions to quantify uncertainty effectively.</span><br><span class="line"></span><br><span class="line">- **Irregular Timestamps**: Handle data with irregular timestamps, accommodating non-uniform interval series without preprocessing.</span><br><span class="line"></span><br><span class="line">## 📚 Documentation with examples and use cases</span><br><span class="line"></span><br><span class="line">Dive into our [comprehensive documentation](https://docs.nixtla.io/docs/getting-started-timegpt_quickstart) to discover examples and practical use cases for TimeGPT. Our documentation covers a wide range of topics, including:</span><br><span class="line"></span><br><span class="line">- **Getting Started**: Begin with our user-friendly [Quickstart Guide](https://docs.nixtla.io/docs/getting-started-timegpt_quickstart) and learn how to [set up your API key](https://docs.nixtla.io/docs/getting-started-setting_up_your_api_key) effortlessly.</span><br><span class="line"></span><br><span class="line">- **Advanced Techniques**: Master advanced forecasting methods and learn how to enhance model accuracy with our tutorials on [anomaly detection](https://docs.nixtla.io/docs/tutorials-anomaly_detection), fine-tuning models using specific loss functions, and scaling computations across distributed frameworks such as [Spark, Dask, and Ray](https://docs.nixtla.io/docs/tutorials-computing_at_scale).</span><br><span class="line"></span><br><span class="line">- **Specialized Topics**: Explore specialized topics like [handling exogenous variables](https://docs.nixtla.io/docs/tutorials-holidays_and_special_dates), model validation through [cross-validation](https://docs.nixtla.io/docs/tutorials-cross_validation), and strategies for [forecasting under uncertainty](https://docs.nixtla.io/docs/tutorials-uncertainty_quantification).</span><br><span class="line"></span><br><span class="line">- **Real-World Applications**: Uncover how TimeGPT is applied in real-world scenarios through case studies on [forecasting web traffic](https://docs.nixtla.io/docs/use-cases-forecasting_web_traffic) and [predicting Bitcoin prices](https://docs.nixtla.io/docs/use-cases-bitcoin_price_prediction).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 🗞️ TimeGPT-1: Revolutionizing Forecasting and Anomaly Detection</span><br><span class="line"></span><br><span class="line">Time series data is pivotal across various sectors, including finance, healthcare, meteorology, and social sciences. Whether it&#x27;s monitoring ocean tides or tracking the Dow Jones&#x27;s daily closing values, time series data is crucial for forecasting and decision-making.</span><br><span class="line"></span><br><span class="line">Traditional analysis methods such as ARIMA, ETS, MSTL, Theta, CES, machine learning models like XGBoost and LightGBM, and deep learning approaches have been standard tools for analysts. However, TimeGPT introduces a paradigm shift with its standout performance, efficiency, and simplicity. Thanks to its zero-shot inference capability, TimeGPT streamlines the analytical process, making it accessible even to users with minimal coding experience.</span><br><span class="line"></span><br><span class="line">TimeGPT is user-friendly and low-code, enabling users to upload their time series data and either generate forecasts or detect anomalies with just a single line of code. As the only foundation model for time series analysis out of the box, TimeGPT can be integrated via our public APIs, through Azure Studio (coming soon), or deployed on your own infrastructure.</span><br><span class="line"></span><br><span class="line">## ⚙️ TimeGPT&#x27;s Architecture</span><br><span class="line">Self-attention, the revolutionary concept introduced by the paper “Attention is all you need“, is the basis of the this foundational model. The TimeGPT model is not based on any existing large language model(LLMs). It is independently trained on vast timeseries dataset as a large transformer model and is designed so as to minimize the forecasting error.</span><br><span class="line"></span><br><span class="line">The architecture consists of an encoder-decoder structure with</span><br><span class="line">multiple layers, each with residual connections and layer normalization. Finally, a linear layer maps the decoder’s output to the forecasting window dimension. The general intuition is that attentionbased mechanisms are able to capture the diversity of past events and correctly extrapolate potential</span><br><span class="line">future distributions.</span><br><span class="line"></span><br><span class="line">![Arquitecture](nbs/img/forecast.png)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TimeGPT was trained on, to our knowledge, the largest collection of publicly available time series,</span><br><span class="line">collectively encompassing over 100 billion data points. This training set incorporates time series</span><br><span class="line">from a broad array of domains, including finance, economics, demographics, healthcare, weather,</span><br><span class="line">IoT sensor data, energy, web traffic, sales, transport, and banking. Due to this diverse set of domains,</span><br><span class="line">the training dataset contains time series with a wide range of characteristics</span><br><span class="line"></span><br><span class="line">For the Zero-shot Results section of your README, you can enhance the clarity and effectiveness by focusing on emphasizing the key findings and their implications, while also making the text more concise and digestible. Here&#x27;s a refined version:</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## ⚡️ Zero-shot Results</span><br><span class="line">### Accuracy:</span><br><span class="line">TimeGPT has been tested for its zero-shot inference capabilities on more than 300K unique series, which involve using the model without additional fine-tuning on the test dataset. TimeGPT outperforms a comprehensive range of well-established statistical and cutting-edge deep learning models, consistently ranking among the top three performers across various frequencies.</span><br><span class="line"></span><br><span class="line">### Ease of use:</span><br><span class="line">TimeGPT also excels by offering simple and rapid predictions using a pre-trained model. This stands in stark contrast to other models that typically require an extensive training and prediction pipeline.</span><br><span class="line"></span><br><span class="line">![Results](nbs/img/results.jpg)</span><br><span class="line"></span><br><span class="line">### Efficiency and Speed:</span><br><span class="line">For zero-shot inference, our internal tests recorded an average GPU inference speed of 0.6 milliseconds per series for TimeGPT, which nearly mirrors that of the simple Seasonal Naive.</span><br><span class="line"></span><br><span class="line">## 📝 How to cite?</span><br><span class="line"></span><br><span class="line">If you find TimeGPT useful for your research, please consider citing the associated [paper](https://arxiv.org/abs/2310.03589):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@misc&#123;garza2023timegpt1,</span><br><span class="line">      title=&#123;TimeGPT-1&#125;,</span><br><span class="line">      author=&#123;Azul Garza and Max Mergenthaler-Canseco&#125;,</span><br><span class="line">      year=&#123;2023&#125;,</span><br><span class="line">      eprint=&#123;2310.03589&#125;,</span><br><span class="line">      archivePrefix=&#123;arXiv&#125;,</span><br><span class="line">      primaryClass=&#123;cs.LG&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 🎉 Features and Mentions</span><br><span class="line">TimeGPT has been featured in many publications and has been recognized for its innovative approach to time series forecasting. Here are some of the features and mentions:</span><br><span class="line"></span><br><span class="line">- [TimeGPT Revolutionizing Time Series Forecasting](https://www.analyticsvidhya.com/blog/2024/02/timegpt-revolutionizing-time-series-forecasting/)</span><br><span class="line">- [TimeGPT: The First Foundation Model for Time Series Forecasting](https://towardsdatascience.com/timegpt-the-first-foundation-model-for-time-series-forecasting-bf0a75e63b3a)</span><br><span class="line">- [TimeGPT: Revolutionising Time Series Forecasting with Generative Models](https://medium.com/@22meera99/timegpt-revolutionising-time-series-forecasting-with-generative-models-86be6c09fa51)</span><br><span class="line">- [TimeGPT on Turing Post](https://www.turingpost.com/p/timegpt)</span><br><span class="line">- [TimeGPT Presentation at AWS Events](https://www.youtube.com/watch?v=5pYkT0rTCfE&amp;ab_channel=AWSEvents)</span><br><span class="line">- [TimeGPT: Machine Learning for Time Series Made Accessible - Podcast](https://podcasts.apple.com/bg/podcast/timegpt-machine-learning-for-time-series-made-accessible/id1487704458?i=1000638551991)</span><br><span class="line">- [TimeGPT on The Data Exchange](https://thedataexchange.media/timegpt/)</span><br><span class="line">- [How TimeGPT Transforms Predictive Analytics with AI](https://hackernoon.com/how-timegpt-transforms-predictive-analytics-with-ai)</span><br><span class="line">- [TimeGPT: The First Foundation Model - AI Horizon Forecast](https://aihorizonforecast.substack.com/p/timegpt-the-first-foundation-model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 🔖 License</span><br><span class="line">TimeGPT is closed source. However, this SDK is open source and available under the Apache 2.0 License. Feel free to contribute.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://research.google/blog/lanistr-multimodal-learning-from-structured-and-unstructured-data/</span><br><span class="line">LANISTR: Multimodal learning from structured and unstructured data</span><br><span class="line">May 22, 2024</span><br><span class="line"></span><br><span class="line">Sayna Ebrahimi, Research Scientist, and Yihe Dong, Software Engineer, Cloud AI Team</span><br><span class="line"></span><br><span class="line">LANISTR is a new framework that enables multimodal learning by ingesting unstructured (image, text) and structured (time series, tabular) data, performing alignment and fusion, and ultimately generating class predictions.</span><br><span class="line"></span><br><span class="line">Recent multimodal learning breakthroughs have predominantly focused on unstructured data, spanning vision, language, video, and audio modalities (Flamingo, PaLI, CLIP, VATT, etc.). However, learning joint representations with structured data, including tabular or time-series formats, remains relatively underexplored, despite structured data being the prevalent data type in the real world. Real-world scenarios often demand the integration of structured and unstructured data, for example, in healthcare diagnostics or retail demand forecasting. This highlights the need to learn two seemingly disparate data types together in a multimodal fashion, using a unified architecture and unique pretraining strategies that align structured and unstructured modalities.</span><br><span class="line"></span><br><span class="line">Unlocking the potential benefits of multimodal learning with structured and unstructured data requires addressing two challenges that become increasingly prominent as the number of modalities, input size, and data heterogeneity increase. First, as the input feature dimensionality and heterogeneity increase, deep neural networks can become susceptible to overfitting and suboptimal generalization, particularly when trained on datasets of limited scale. This challenge is exacerbated when using unstructured and structured data together, such as time series data that often exhibit non-stationary behavior (fashion trends, sensory measurements, etc.), which, unlike other more independent and identically distributed (i.i.d.) modalities, makes it difficult to build well-generalisable models. Similarly, tabular data often include numerous columns (features) containing minimal information, leading to overfitting to spurious correlations. Second, problems caused by the absence of some modalities become more pronounced in multimodal data with more than two modalities (e.g., image+text+tabular+time series), where each sample may not include some modalities. To the best of our knowledge, a systematic study addressing these challenges in learning from unstructured and structured data remains absent from current literature.</span><br><span class="line"></span><br><span class="line">To address these challenges, in “LANISTR: Multimodal Learning from Structured and Unstructured Data”, we introduce a novel framework to learn from LANguage, Image, and STRuctured data. LANISTR enables multimodal learning by ingesting unstructured (image, text) and structured (time series, tabular) data, performing alignment and fusion, and ultimately generating predictions. Using two publicly available healthcare and retail datasets, LANISTR demonstrates remarkable improvements when fine-tuned with 0.1% and 0.01% of labeled data, respectively. Notably, these improvements are observed even with a very high ratio of samples (35.7% and 99.8%, respectively) that don’t contain all modalities, underlining the robustness of LANISTR to practical missing modality challenges.</span><br><span class="line"></span><br><span class="line">Model architecture</span><br><span class="line">LANISTR’s architecture is composed of modality-specific encoders and a multimodal encoder-decoder module, which acts as the fusion mechanism. First, raw inputs are encoded with a language encoder, an image encoder, and a structured data encoder. Depending on the dataset, we can have two separate structured data encoders, one for tabular data and one for time-series data. These modality-specific encoders are all chosen to be attention-based architectures.</span><br><span class="line"></span><br><span class="line">After the inputs are encoded, we project them using modality-specific encoders with a single layer projection head and concatenate their embeddings together before feeding them into the multimodal fusion module.</span><br><span class="line"></span><br><span class="line">A common bottleneck when working with multimodal data is extracting meaningful representations that reflect cross-modal interactions between individual modalities. We leverage cross-attention, which has been predominantly used to capture cross-modal relationships, when creating a fusion encoder with six Transformer layers.</span><br><span class="line"></span><br><span class="line">The figure below illustrates the LANISTR architecture using a toy example from a retail application. The goal is to predict the star rating a product will receive. In this example, the product is a can of dog food (image), accompanied by a user review (text), numerical and categorical specifications (tabular features), and the user&#x27;s purchase history (time sequence). LANISTR integrates these different modalities to produce a star rating prediction.</span><br><span class="line"></span><br><span class="line">LANISTR enables multimodal learning by ingesting unstructured (image, text) and structured (time series, tabular) data, performing alignment and fusion, and ultimately generating predictions.</span><br><span class="line"></span><br><span class="line">The core of LANISTR&#x27;s methodology is rooted in masking-based training applied across both unimodal and multimodal levels. LANISTR is pre-trained with two types of objectives:</span><br><span class="line"></span><br><span class="line">Unimodal masking objectives.</span><br><span class="line">We use masked language, image, time series, and tabular features modeling as a general self-supervised learning strategy for all the unimodal encoders in LANISTR. This allows the utilization of data with missing modalities for unimodal encoders, since masked inputs are fed to encoders, a form of reconstruction or prediction task can be used for training.</span><br><span class="line">Similarity-based multimodal masking loss.</span><br><span class="line">Prior work on multimodal learning with vision and language, such as FLAVA, focuses on reconstructing one modality (e.g., text) or both image and text modalities from the masked multimodal inputs. In this work, we propose a novel masked multimodal learning loss that maximizes the similarities between masked and unmasked multimodal data representations. This objective resembles an idea that originated from Siamese networks, where the goal is to maximize the similarity between two augmented versions of an image. However, in our framework, the goal is to maximize the similarity between the embeddings generated by a masked and a non-masked input. As shown below, this objective encourages the model to learn cross-modal relations, such that the cosine similarity between the embeddings of a masked and a non-masked data is maximized.</span><br><span class="line">LANSITR-img4</span><br><span class="line">Illustration of similarity-based multimodal masking objective in LANISTR. The goal is to maximize the similarity between the embeddings of a masked and a non-masked input.</span><br><span class="line"></span><br><span class="line">After pre-training, we use pre-trained weights to initialize both the unimodal encoders and the multimodal encoder. A multi-layer classification module is then attached to the multimodal encoder for the downstream task. The LANISTR model comprises 300M parameters. During fine-tuning, we maintain the unimodal encoders in a frozen state while concentrating on training the multimodal encoder and the classification module. This accounts for training approximately 15% of the entire architecture. It&#x27;s worth noting that LANISTR’s versatility extends to other tasks, such as regression or retrieval, by incorporating suitable heads and objective functions, provided labeled data is accessible.</span><br><span class="line"></span><br><span class="line">Results</span><br><span class="line">We compare LANISTR’s performance against various competitive baselines, including AutoGluon, ALBEF, and MedFuse, using MIMIC-IV (a widely-used medical dataset for clinical prediction tasks) and Amazon Review Data. With its novel architecture and objective functions, LANISTR achieves state-of-the art results on several challenging tasks.</span><br><span class="line"></span><br><span class="line">The plot below highlights the results for mortality prediction using the MIMIC-IV dataset. LANISTR achieves 87.37% in area under the receiver operating characteristic curve (AUROC) on average, significantly outperforming baseline models FLAVA and CoCa, which can only use image and text, and the MedFuse model, which only uses image and time series modalities. The late fusion baseline is a simple fusion mechanism that concatenates all three modality embeddings.</span><br><span class="line"></span><br><span class="line">LANSITR-img3</span><br><span class="line">AUROC for in-hospital mortality prediction using the MIMIC-IV dataset.</span><br><span class="line"></span><br><span class="line">For predicting product ratings using the Amazon Review dataset, we pre-train methods that can use unlabeled data (LANISTR and ALBEF) from the office products category and fine-tune them using the beauty products category. LANISTR outperforms competitive baselines by a significant margin, achieving an average of 76.27% accuracy. Notably, even without pre-training, LANISTR&#x27;s unique fusion mechanism surpasses both late fusion and AutoGluon, neither of which support pre-training. For ALBEF, we explored a &quot;Tab2Txt&#x27;&#x27; approach that incorporates tabular features as additional text input, while the original ALBEF baseline only utilized image and text modalities. We demonstrate that both are significantly outperformed by LANISTR. Our results confirm the importance of learning structured and unstructured data using unlabeled and labeled data together.</span><br><span class="line"></span><br><span class="line">Ablation studies and the particular challenges of these tasks illustrate LANISTR’s ability to actively ingest all modalities as they are, take advantage of large quantities of unlabeled data during unsupervised pre-training, and handle missing modalities seamlessly.</span><br><span class="line"></span><br><span class="line">LANSITR-img1</span><br><span class="line">Results using the Amazon Review dataset for star rating prediction tasks on the beauty products category.</span><br><span class="line"></span><br><span class="line">Conclusion</span><br><span class="line">LANISTR is a novel framework for language, image, and structured data (tabular and time series). With its unimodal and novel similarity-based multimodal masking strategy, LANISTR tackles challenges including missing modalities and limited labeled data, and achieves state-of-the-art performance across diverse domains.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3</span><br><span class="line">---</span><br><span class="line">license: apache-2.0</span><br><span class="line">---</span><br><span class="line"># Model Card for Mistral-7B-Instruct-v0.3</span><br><span class="line"></span><br><span class="line">The Mistral-7B-Instruct-v0.3 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.3.</span><br><span class="line"></span><br><span class="line">Mistral-7B-v0.3 has the following changes compared to [Mistral-7B-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/edit/main/README.md)</span><br><span class="line">- Extended vocabulary to 32768</span><br><span class="line">- Supports v3 Tokenizer</span><br><span class="line">- Supports function calling</span><br><span class="line"></span><br><span class="line">## Installation</span><br><span class="line"></span><br><span class="line">It is recommended to use `mistralai/Mistral-7B-Instruct-v0.3` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pip install mistral_inference</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Download</span><br><span class="line"></span><br><span class="line">py</span><br><span class="line">from huggingface_hub import snapshot_download</span><br><span class="line">from pathlib import Path</span><br><span class="line">mistral_models_path = Path.home().joinpath(&#x27;mistral_models&#x27;, &#x27;7B-Instruct-v0.3&#x27;)</span><br><span class="line">mistral_models_path.mkdir(parents=True, exist_ok=True)</span><br><span class="line">snapshot_download(repo_id=&quot;mistralai/Mistral-7B-Instruct-v0.3&quot;, allow_patterns=[&quot;params.json&quot;, &quot;consolidated.safetensors&quot;, &quot;tokenizer.model.v3&quot;], local_dir=mistral_models_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### Chat</span><br><span class="line"></span><br><span class="line">After installing `mistral_inference`, a `mistral-chat` CLI command should be available in your environment. You can chat with the model using</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mistral-chat $HOME/mistral_models/7B-Instruct-v0.3 --instruct --max_tokens 256</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### Instruct following</span><br><span class="line"></span><br><span class="line">py</span><br><span class="line">from mistral_inference.model import Transformer</span><br><span class="line">from mistral_inference.generate import generate</span><br><span class="line">from mistral_common.tokens.tokenizers.mistral import MistralTokenizer</span><br><span class="line">from mistral_common.protocol.instruct.messages import UserMessage</span><br><span class="line">from mistral_common.protocol.instruct.request import ChatCompletionRequest</span><br><span class="line">tokenizer = MistralTokenizer.from_file(f&quot;&#123;mistral_models_path&#125;/tokenizer.model.v3&quot;)</span><br><span class="line">model = Transformer.from_folder(mistral_models_path)</span><br><span class="line">completion_request = ChatCompletionRequest(messages=[UserMessage(content=&quot;Explain Machine Learning to me in a nutshell.&quot;)])</span><br><span class="line">tokens = tokenizer.encode_chat_completion(completion_request).tokens</span><br><span class="line">out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)</span><br><span class="line">result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### Function calling</span><br><span class="line"></span><br><span class="line">py</span><br><span class="line">from mistral_common.protocol.instruct.tool_calls import Function, Tool</span><br><span class="line">from mistral_inference.model import Transformer</span><br><span class="line">from mistral_inference.generate import generate</span><br><span class="line">from mistral_common.tokens.tokenizers.mistral import MistralTokenizer</span><br><span class="line">from mistral_common.protocol.instruct.messages import UserMessage</span><br><span class="line">from mistral_common.protocol.instruct.request import ChatCompletionRequest</span><br><span class="line">tokenizer = MistralTokenizer.from_file(f&quot;&#123;mistral_models_path&#125;/tokenizer.model.v3&quot;)</span><br><span class="line">model = Transformer.from_folder(mistral_models_path)</span><br><span class="line">completion_request = ChatCompletionRequest(</span><br><span class="line">    tools=[</span><br><span class="line">        Tool(</span><br><span class="line">            function=Function(</span><br><span class="line">                name=&quot;get_current_weather&quot;,</span><br><span class="line">                description=&quot;Get the current weather&quot;,</span><br><span class="line">                parameters=&#123;</span><br><span class="line">                    &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">                    &quot;properties&quot;: &#123;</span><br><span class="line">                        &quot;location&quot;: &#123;</span><br><span class="line">                            &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">                            &quot;description&quot;: &quot;The city and state, e.g. San Francisco, CA&quot;,</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &quot;format&quot;: &#123;</span><br><span class="line">                            &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">                            &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;],</span><br><span class="line">                            &quot;description&quot;: &quot;The temperature unit to use. Infer this from the users location.&quot;,</span><br><span class="line">                        &#125;,</span><br><span class="line">                    &#125;,</span><br><span class="line">                    &quot;required&quot;: [&quot;location&quot;, &quot;format&quot;],</span><br><span class="line">                &#125;,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">    ],</span><br><span class="line">    messages=[</span><br><span class="line">        UserMessage(content=&quot;What&#x27;s the weather like today in Paris?&quot;),</span><br><span class="line">        ],</span><br><span class="line">)</span><br><span class="line">tokens = tokenizer.encode_chat_completion(completion_request).tokens</span><br><span class="line">out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)</span><br><span class="line">result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Generate with `transformers`</span><br><span class="line"></span><br><span class="line">If you want to use Hugging Face `transformers` to generate text, you can do something like this.</span><br><span class="line"></span><br><span class="line">py</span><br><span class="line">from transformers import pipeline</span><br><span class="line">messages = [</span><br><span class="line">    &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a pirate chatbot who always responds in pirate speak!&quot;&#125;,</span><br><span class="line">    &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who are you?&quot;&#125;,</span><br><span class="line">]</span><br><span class="line">chatbot = pipeline(&quot;text-generation&quot;, model=&quot;mistralai/Mistral-7B-Instruct-v0.3&quot;)</span><br><span class="line">chatbot(messages)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Limitations</span><br><span class="line"></span><br><span class="line">The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.</span><br><span class="line">It does not have any moderation mechanisms. We&#x27;re looking forward to engaging with the community on ways to</span><br><span class="line">make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-05-22</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/05/22/2024-5-23-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 5월 23일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/05/26/2024-5-26-AI-NEWS/" title="2024년 5월 26일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/05/22/2024-5-22-AI-NEWS/" title="2024년 5월 22일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>