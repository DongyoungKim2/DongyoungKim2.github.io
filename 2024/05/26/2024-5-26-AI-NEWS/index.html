<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 5월 26일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="요약오늘의 소식에서는 OmniGlue의 이미지 매칭 기술, Mistral 모델의 메모리 효율적인 파인튜닝, 대형 언어 모델을 이용한 재무 분석, 트랜스포머의 선형적 특성, World Knowledge Model을 통한 에이전트 플래닝, LLM의 개선된 사실 기반 인용 "><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 5월 26일 AI 소식</a></h3></div><div class="post-content"><h2 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h2><p>오늘의 소식에서는 OmniGlue의 이미지 매칭 기술, Mistral 모델의 메모리 효율적인 파인튜닝, 대형 언어 모델을 이용한 재무 분석, 트랜스포머의 선형적 특성, World Knowledge Model을 통한 에이전트 플래닝, LLM의 개선된 사실 기반 인용 기술, 고해상도 3D 메쉬 생성 모델, 혼합 모달 초기 융합 모델 Chameleon, 그리고 훈련 없이 무한 비디오 생성이 가능한 FIFO-Diffusion 기법에 대해 다룹니다.</p>
<h2 id="OmniGlue-Generalizable-Feature-Matching-with-Foundation-Model-Guidance"><a href="#OmniGlue-Generalizable-Feature-Matching-with-Foundation-Model-Guidance" class="headerlink" title="OmniGlue: Generalizable Feature Matching with Foundation Model Guidance"></a>OmniGlue: Generalizable Feature Matching with Foundation Model Guidance</h2><p><a target="_blank" rel="noopener" href="https://hwjiang1510.github.io/OmniGlue/">OmniGlue</a>, 2024년 CVPR</p>
<ul>
<li>새로운 학습 가능한 이미지 매처 OmniGlue 소개</li>
<li>이미지 매칭 기술의 일반화 문제 해결</li>
<li>시각 기초 모델을 활용하여 이미지 매칭 프로세스 가이드</li>
<li>키포인트 위치 기반 주의 메커니즘 제안</li>
<li>6개 데이터셋에서 실험 수행, SuperGlue 대비 20.9% 성능 향상</li>
<li>LightGlue 대비 9.5% 성능 우수</li>
</ul>
<h2 id="Mistral-finetune"><a href="#Mistral-finetune" class="headerlink" title="Mistral-finetune"></a>Mistral-finetune</h2><p><a target="_blank" rel="noopener" href="https://github.com/mistralai/mistral-finetune">Mistral-finetune</a>, 공개</p>
<ul>
<li>메모리 효율적인 Mistral 모델 파인튜닝 코드베이스</li>
<li>LoRA 기반의 훈련 패러다임 사용</li>
<li>대부분의 가중치를 고정하고 저순위 행렬 변동만 훈련</li>
<li>A100 또는 H100 GPU 사용 권장</li>
<li>다중 GPU 단일 노드 훈련 최적화</li>
</ul>
<h2 id="Financial-Statement-Analysis-with-Large-Language-Models"><a href="#Financial-Statement-Analysis-with-Large-Language-Models" class="headerlink" title="Financial Statement Analysis with Large Language Models"></a>Financial Statement Analysis with Large Language Models</h2><p><a target="_blank" rel="noopener" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4835311">논문 링크</a>, 시카고 부스 연구 논문</p>
<ul>
<li>LLM을 활용한 재무제표 분석</li>
<li>GPT-4가 미래 수익 예측에서 인간 분석가보다 우수</li>
<li>이야기 형식 없이도 정확한 수익 변화 예측 가능</li>
<li>GPT 예측을 기반으로 한 거래 전략이 높은 샤프 비율과 알파 제공</li>
</ul>
<h2 id="Your-Transformer-is-Secretly-Linear"><a href="#Your-Transformer-is-Secretly-Linear" class="headerlink" title="Your Transformer is Secretly Linear"></a>Your Transformer is Secretly Linear</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2405.12250">논문 링크</a>, 2024년 5월 20일</p>
<ul>
<li>트랜스포머 디코더의 선형적 특성 발견</li>
<li>계층 간 임베딩 변환에서 높은 선형 관계</li>
<li>잔여 구성 요소 제거 시 선형성 감소</li>
<li>코사인 유사성 기반 정규화를 통해 모델 성능 향상</li>
</ul>
<h2 id="Agent-Planning-with-World-Knowledge-Model"><a href="#Agent-Planning-with-World-Knowledge-Model" class="headerlink" title="Agent Planning with World Knowledge Model"></a>Agent Planning with World Knowledge Model</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.14205">논문 링크</a>, 2024년 5월</p>
<ul>
<li>대형 언어 모델을 사용한 에이전트 플래닝</li>
<li>실험 결과 WKM이 블라인드 시행착오와 환각 행동 문제 해결</li>
<li>인스턴스 수준의 과제 지식이 미지의 과제에도 일반화 가능</li>
<li>강한 에이전트 모델 플래닝에 약한 WKM 가이드 가능</li>
</ul>
<h2 id="Effective-large-language-model-adaptation-for-improved-grounding"><a href="#Effective-large-language-model-adaptation-for-improved-grounding" class="headerlink" title="Effective large language model adaptation for improved grounding"></a>Effective large language model adaptation for improved grounding</h2><p><a target="_blank" rel="noopener" href="https://research.google/blog/effective-large-language-model-adaptation-for-improved-grounding/">연구 블로그</a>, 2024년 5월 24일</p>
<ul>
<li>LLM의 사실 기반 인용 개선을 위한 AGREE 프레임워크 소개</li>
<li>종합적인 실험에서 이전 접근법 대비 30% 이상의 향상된 성과</li>
<li>LLM을 튜닝하여 응답에 인용을 포함하고 사실 기반으로 만듦</li>
<li>테스트 시간 적응(TTA) 메커니즘 도입</li>
</ul>
<h2 id="CraftsMan-High-fidelity-Mesh-Generation"><a href="#CraftsMan-High-fidelity-Mesh-Generation" class="headerlink" title="CraftsMan: High-fidelity Mesh Generation"></a>CraftsMan: High-fidelity Mesh Generation</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/wyysf/CraftsMan">논문 링크</a>, 2024년 5월</p>
<ul>
<li>고해상도 3D 메쉬 생성 시스템 CraftsMan 소개</li>
<li>아티스트의 작업 흐름을 모방하여 거친 메쉬 생성 후 세부적으로 정교화</li>
<li>텍스트 프롬프트 또는 참조 이미지를 입력으로 사용</li>
<li>멀티뷰(MV) 확산 모델을 활용하여 3D 지오메트리 생성</li>
<li>표면 세부 사항을 자동 또는 상호작용 방식으로 정교화</li>
</ul>
<h2 id="Chameleon-Mixed-Modal-Early-Fusion-Foundation-Models"><a href="#Chameleon-Mixed-Modal-Early-Fusion-Foundation-Models" class="headerlink" title="Chameleon: Mixed-Modal Early-Fusion Foundation Models"></a>Chameleon: Mixed-Modal Early-Fusion Foundation Models</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2405.09818">논문 링크</a>, 2024년 5월 16일</p>
<ul>
<li>혼합 모달 초기 융합 모델 Chameleon 소개</li>
<li>시각적 질문 응답, 이미지 캡션 생성, 텍스트 및 이미지 생성 등 다양한 작업 수행</li>
<li>Llama-2보다 텍스트 작업에서 우수한 성능 발휘</li>
<li>Gemini-Pro 및 GPT-4V와 경쟁 가능</li>
</ul>
<h2 id="FIFO-Diffusion-Generating-Infinite-Videos-from-Text"><a href="#FIFO-Diffusion-Generating-Infinite-Videos-from-Text" class="headerlink" title="FIFO-Diffusion: Generating Infinite Videos from Text"></a>FIFO-Diffusion: Generating Infinite Videos from Text</h2><p><a target="_blank" rel="noopener" href="https://jjihwan.github.io/projects/FIFO-Diffusion">FIFO-Diffusion</a>, 2024년 5월</p>
<ul>
<li>텍스트 조건부 비디오 생성 기술 FIFO-Diffusion 소개</li>
<li>훈련 없이 무한 비디오 생성 가능</li>
<li>대각선 디노이징 기법을 사용하여 연속적인 프레임 처리</li>
<li>고해상도 비디오 생성에 유망한 결과 도출</li>
</ul>
<p>각 링크의 상세 내용과 연구 결과는 AI 기술의 최신 동향과 발전 가능성을 보여줍니다.</p>
<details>
  <summary>Sources</summary>
  This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is: # AI News for (today's date), ## Summary (overall short summary), ## Link1 Title, link, date - detailed summary1, - detailed summary2, - detailed summary..N, ## Link2 Title, link, date - detailed summary1, - detailed summary2, - detailed point..N, etc. The report should be written in Korean and use the 개조식 문체 style. give the very deep details for each link as much as possible.
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://hwjiang1510.github.io/OmniGlue/</span><br><span class="line">OmniGlue: Generalizable Feature Matching with Foundation Model Guidance</span><br><span class="line">Hanwen Jiang1, Arjun Karpur2, Bingyi Cao2, Qixing Huang1, Andre Araujo2</span><br><span class="line">1UT Austin     2Google Research</span><br><span class="line">CVPR 2024</span><br><span class="line"> </span><br><span class="line">Abstract</span><br><span class="line">The image matching field has been witnessing a continuous emergence of novel learnable feature matching techniques, with ever-improving performance on conventional benchmarks. However, our investigation shows that despite these gains, their potential for real-world applications is restricted by their limited generalization capabilities to novel image domains. In this paper, we introduce OmniGlue, the first learnable image matcher that is designed with generalization as a core principle. OmniGlue leverages broad knowledge from a vision foundation model to guide the feature matching process, boosting generalization to domains not seen at training time. Additionally, we propose a novel keypoint position-guided attention mechanism which disentangles spatial and appearance information, leading to enhanced matching descriptors. We perform comprehensive experiments on a suite of 6 datasets with varied image domains, including scene-level, object-centric and aerial images. OmniGlue&#x27;s novel components lead to relative gains on unseen domains of 20.9% with respect to a directly comparable reference model SuperGlue, while also outperforming the recent LightGlue method by 9.5% relatively.</span><br><span class="line"></span><br><span class="line">OmniGlue Framework</span><br><span class="line">OmniGlue is the first learnable image matcher that is de-signed with generalization as a core principle. OmniGlue benefits from two designs: foundation model guidance and keypoint-position attention guidance. The visual foundation model, which is trained on large-scale data, provides coarse but generalizable correspondence cues. It huides the inter-image feature propagation process. The keypoint-position attention guidance disentangles the positional informatation from the keypoint features, which avoids the model specializing too strongly in the training dis-tribution of keypoints and relative pose transformations.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line"></span><br><span class="line">https://github.com/mistralai/mistral-finetune</span><br><span class="line">Mistral-finetune</span><br><span class="line">Open In Colab</span><br><span class="line">mistral-finetune is a light-weight codebase that enables memory-efficient and performant finetuning of Mistral&#x27;s models. It is based on LoRA, a training paradigm where most weights are frozen and only 1-2% additional weights in the form of low-rank matrix perturbations are trained.</span><br><span class="line"></span><br><span class="line">For maximum efficiency it is recommended to use a A100 or H100 GPU. The codebase is optimized for multi-GPU-single-node training setups, but for smaller models, such as the 7B a single GPU suffices.</span><br><span class="line"></span><br><span class="line">Note</span><br><span class="line"></span><br><span class="line">The goal of this repository is to provide a simple, guided entrypoint to finetune Mistral models. As such, it is fairly opinionated (especially around data formatting) and does not aim at being exhaustive across multiple model architecture or hardware types. For more generic approaches, you can check out some other great projects like torchtune.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line"></span><br><span class="line">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4835311</span><br><span class="line">Financial Statement Analysis with Large Language Models</span><br><span class="line">Chicago Booth Research Paper Forthcoming</span><br><span class="line"></span><br><span class="line">Fama-Miller Working Paper</span><br><span class="line"></span><br><span class="line">54 Pages Posted: 21 May 2024</span><br><span class="line">Alex Kim</span><br><span class="line">University of Chicago Booth School of Business</span><br><span class="line"></span><br><span class="line">Maximilian Muhn</span><br><span class="line">University of Chicago - Booth School of Business</span><br><span class="line"></span><br><span class="line">Valeri V. Nikolaev</span><br><span class="line">University of Chicago Booth School of Business</span><br><span class="line"></span><br><span class="line">Date Written: May 20, 2024</span><br><span class="line"></span><br><span class="line">Abstract</span><br><span class="line">We investigate whether an LLM can successfully perform financial statement analysis in a way similar to a professional human analyst. We provide standardized and anonymous financial statements to GPT4 and instruct the model to analyze them to determine the direction of future earnings. Even without any narrative or industry-specific information, the LLM outperforms financial analysts in its ability to predict earnings changes. The LLM exhibits a relative advantage over human analysts in situations when the analysts tend to struggle. Furthermore, we find that the prediction accuracy of the LLM is on par with the performance of a narrowly trained state-of-the-art ML model. LLM prediction does not stem from its training memory. Instead, we find that the LLM generates useful narrative insights about a company&#x27;s future performance. Lastly, our trading strategies based on GPT&#x27;s predictions yield a higher Sharpe ratio and alphas than strategies based on other models. Taken together, our results suggest that LLMs may take a central role in decision-making.</span><br><span class="line"></span><br><span class="line">Keywords: GPT4, neural network, asset pricing, earnings, direction of earnings changes, analysts, chain-of-thought, financial statement analysis, large language models</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line"></span><br><span class="line">https://huggingface.co/papers/2405.12250</span><br><span class="line">Your Transformer is Secretly Linear</span><br><span class="line">Published on May 20</span><br><span class="line">·</span><br><span class="line">Featured in Daily Papers on May 22</span><br><span class="line">Authors:</span><br><span class="line"></span><br><span class="line">Anton Razzhigaev</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Matvey Mikhalchuk</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Elizaveta Goncharova</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Nikolai Gerasimenko</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Ivan Oseledets</span><br><span class="line">,</span><br><span class="line">Denis Dimitrov</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Andrey Kuznetsov</span><br><span class="line">Abstract</span><br><span class="line">This paper reveals a novel linear characteristic exclusive to transformer decoders, including models such as GPT, LLaMA, OPT, BLOOM and others. We analyze embedding transformations between sequential layers, uncovering a near-perfect linear relationship (Procrustes similarity score of 0.99). However, linearity decreases when the residual component is removed due to a consistently low output norm of the transformer layer. Our experiments show that removing or linearly approximating some of the most linear blocks of transformers does not affect significantly the loss or model performance. Moreover, in our pretraining experiments on smaller models we introduce a cosine-similarity-based regularization, aimed at reducing layer linearity. This regularization improves performance metrics on benchmarks like Tiny Stories and SuperGLUE and as well successfully decreases the linearity of the models. This study challenges the existing understanding of transformer architectures, suggesting that their operation may be more linear than previously assumed.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line"></span><br><span class="line">https://arxiv.org/abs/2405.14205</span><br><span class="line">Agent Planning with World Knowledge Model</span><br><span class="line">Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</span><br><span class="line">Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the &#x27;&#x27;real&#x27;&#x27; physical world. Imitating humans&#x27; mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three complex real-world simulated datasets with three state-of-the-art open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent&#x27;s understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development. Code will be available at this https URL.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line"></span><br><span class="line">https://research.google/blog/effective-large-language-model-adaptation-for-improved-grounding/</span><br><span class="line">Effective large language model adaptation for improved grounding</span><br><span class="line">May 24, 2024</span><br><span class="line"></span><br><span class="line">Xi Ye, Student Researcher, and Ruoxi Sun, Research Scientist, Google Cloud</span><br><span class="line"></span><br><span class="line">We introduce AGREE, a learning-based framework that enables LLMs to provide accurate citations in their responses, making them more reliable and increasing user trust.</span><br><span class="line"></span><br><span class="line">Over the last few years, large language models (LLMs) have showcased remarkable advances in various capabilities, such as multi-hop reasoning, generating plans, and using tools and APIs, all of which demonstrate promise for numerous downstream applications. However, their reliability in real-world deployment is sometimes compromised by the issue of &quot;hallucination&quot;, where such models generate plausible but nonfactual information. Hallucinations tend to occur more frequently when LLMs are prompted with open-ended queries that require drawing upon broad world knowledge. This poses risks in domains that demand high factual accuracy, such as news reporting and educational content.</span><br><span class="line"></span><br><span class="line">Grounding aims to combat the hallucination problems of LLMs by tracking back their claims to reliable sources. Such a system would not only provide coherent and helpful responses, but also supports its claims with relevant citations to external knowledge.</span><br><span class="line"></span><br><span class="line">With this in mind, in our paper “Effective large language model adaptation for improved grounding”, to be presented at NAACL 2024, we introduce a new framework for grounding of LLMs. This framework, which we call AGREE (Adaptation for GRounding EnhancEment), enables LLMs to self-ground the claims in their responses and to provide precise citations to retrieved documents, increasing user trust and expanding their potential applications. Comprehensive experiments on five datasets suggest AGREE leads to substantially better grounding than prior prompting-based or post-hoc citing approaches, often achieving relative improvements of over 30%.</span><br><span class="line"></span><br><span class="line">A holistic approach to improve grounding</span><br><span class="line">Prior research on improving grounding mostly follows two prominent paradigms. One is to add citations post-hoc using an additional natural language inference (NLI) model. This approach heavily relies on the knowledge within an LLM’s embeddings and does not extend well to facts beyond that. Another common method for grounding is to leverage the instruction-following and in-context learning capabilities of LLMs. With this second approach, LLMs are required to learn grounding just from a few demonstration prompts, which, in practice, does not lead to the best grounding quality.</span><br><span class="line"></span><br><span class="line">Our new framework, AGREE, takes a holistic approach to adapt LLMs for better grounding and citation generation, combining both learning-based adaptation and test-time adaptation (TTA). Different from prior prompting-based approaches, AGREE fine-tunes LLMs, enabling them to self-ground the claims in their responses and provide accurate citations. This tuning on top of the pre-trained LLMs requires well-grounded responses (with citations), for which we introduce a method that can automatically construct such data from unlabeled queries. The self-grounding capability of tuned LLMs further grants them a TTA capability that can iteratively improve their responses.</span><br><span class="line"></span><br><span class="line">High-level illustration of AGREE. At training time, we generate training data automatically and adapt LLMs for better grounding via fine-tuning. At test time, we introduce a test-time adaptation mechanism to iteratively improve their responses.</span><br><span class="line"></span><br><span class="line">Tuning LLMs for self-grounding</span><br><span class="line">During training, AGREE collects synthetic data from unlabeled queries, which we then use to fine-tune a base LLM into an adapted LLM that can self-ground its claims. Given an unlabeled query, we first retrieve relevant passages from reliable sources (e.g., Wikipedia) using a retriever model. We present the retrieved passages to the base LLM and sample a set of initial responses (without citations). Next, we use an NLI model (in our case, a variant of Google TrueNLI model), which can judge whether a claim is supported by a passage, to help add citations to the initial responses. For each sentence in an initial response, we use the NLI model to find the passage that can support the sentence, and add a citation to the supporting passage accordingly. We do not add citations to those sentences that do not have a passage that can back them up.</span><br><span class="line"></span><br><span class="line">LLMAdaptation-2-Process</span><br><span class="line">Illustration of the tuning process. We sample responses from the base model, use an NLI model to add citations to the sampled responses, and tune the base model with the best-grounded response.</span><br><span class="line"></span><br><span class="line">Now that the initial responses are augmented with automatically created citations, we then select the best-grounded responses to fine-tune the base LLM. We determine which are the best grounded by measuring the averaged grounding score over all the sentences in the response according to the NLI model. With these responses, we tune the base LLM to teach it to include citations to its responses. In addition, we also teach base LLM to indicate those sentences in its responses that are unsupported, which will be useful during test-time adaptation so the LLM can iteratively refine its responses.</span><br><span class="line"></span><br><span class="line">We create the tuning data using the queries from three commonly used datasets, Natural Questions, StrategyQA, and Fever, since they provide diverse text and require different types of reasoning processes.</span><br><span class="line"></span><br><span class="line">Test-time adaptation</span><br><span class="line">At test time, AGREE introduces an iterative inference strategy that empowers the LLM to actively seek additional information based on its self-generated citations. Given a query, we first use the retriever model to obtain an initial passage set. Next, we iteratively invoke the following procedure: 1) At each iteration, the adapted LLM generates a response containing citations to the passage set and finds any unsupported statements that do not have citations. 2) Then, we actively present more information to the LLM based on the citation information — if there are unsupported statements, we include additional information that is retrieved from reliable sources using those statements, otherwise, we include more unseen passages that are retrieved using the query to acquire more complete information.</span><br><span class="line"></span><br><span class="line">LLMAdaptation-3-TTA</span><br><span class="line">Illustration of the test-time adaptation (TTA) mechanism. The adapted LLM retrieves from the corpus based on self-generated citation information to refine its response in an iterative way.</span><br><span class="line"></span><br><span class="line">Experiments</span><br><span class="line">We conduct comprehensive experiments to demonstrate the effectiveness of AGREE both with and without TTA. We evaluate it across five datasets, including two in-domain datasets (NQ and StrategyQA) that have been used for adapting the base LLM and three out-of-domain datasets (ASQA, QAMPARI and an internal QA dataset, called “Enterprise” below) to test the generalization of our framework. We apply AGREE to adapt two LLMs and compare them against a competitive prompting-based baseline (ICLCite), and a post-hoc citing baseline (PostCite), both from ALCE.</span><br><span class="line"></span><br><span class="line">LLMAdaptation-4-Performance</span><br><span class="line">Performance across five datasets of AGREE compared to baselines ICLCite and PostCite. Our approach achieves substantially better grounding and citation precision compared to the baselines.</span><br><span class="line"></span><br><span class="line">There are three key takeaways from the figure above, which illustrates the effectiveness of our approach.</span><br><span class="line"></span><br><span class="line">Tuning is effective for superior grounding.</span><br><span class="line">Across five datasets, AGREE generates responses that are better grounded in the text corpus (measured by citation recall) and provides accurate citations to its responses (measured by citation precision). It outperforms each of our selected baselines by a substantial margin. Tuning with high-quality data is a much more effective way for LLMs to learn to ground their responses without needing an additional NLI model.</span><br><span class="line">The improvements can generalize.</span><br><span class="line">AGREE adapts the base LLM only using in-domain training sets (NQ, StrategyQA), and directly tests the model on out-of-domain test datasets (ASQA, QAMPARI, Enterprise). The results suggest that the improvements can effectively generalize to out-of-domain datasets that contain different question types or use different types of external knowledge. This is a fundamental advantage of the proposed approach — AGREE can generalize to a target domain in the zero-shot setting without needing demonstrations from that domain.</span><br><span class="line">TTA improves both grounding and answer correctness.</span><br><span class="line">Comparing our framework at its full capacity and a variant without test-time adaptation, we observe improvements in terms of both better grounding and accuracy. This is because TTA allows the LLMs to actively collect more relevant passages to construct better answers following the self-grounding guidance.</span><br><span class="line">Conclusion</span><br><span class="line">In conclusion, we present AGREE, a framework for improving the factuality and verifiability of LLM-generated content. AGREE presents an effective learning-based approach to adapt a base LLM to self-ground its response using automatically collected data. This integrated capability for grounding further enables the LLM to improve the responses at test time. Our evaluations across five datasets demonstrate the benefits of the holistic adaptation approach compared to approaches that solely rely on prompting or the parametric knowledge of LLMs. We encourage you to read the paper to learn about our findings and join us in building more trustworthy and reliable language models.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line"></span><br><span class="line">https://huggingface.co/spaces/wyysf/CraftsMan</span><br><span class="line">CraftsMan: High-fidelity Mesh Generation</span><br><span class="line">with 3D Native Generation and Interactive Geometry Refiner</span><br><span class="line">Weiyu Li*1,2, Jiarui Liu*1,2, Rui Chen1,2, Yixun Liang2,3, Xuelin Chen4, Ping Tan1,2, Xiaoxiao Long5</span><br><span class="line"></span><br><span class="line">1HKUST, 2LightIllusions, 3HKUST(GZ), 4Tencent AI Lab, 5HKU</span><br><span class="line"></span><br><span class="line">TL; DR: CraftsMan (aka 匠心) is a two-stage text/image to 3D mesh generation model. By mimicking the modeling workflow of artist/craftsman, we propose to generate a coarse mesh (5s) with smooth geometry using 3D diffusion model and then refine it (20s) using enhanced multi-view normal maps generated by 2D normal diffusion, which is also can be in a interactive manner like Zbrush.</span><br><span class="line">✨ Overview</span><br><span class="line">This repo contains source code (training / inference) of 3D diffusion model, pretrained weights and gradio demo code of our 3D mesh generation project, you can find more visualizations on our project page. If you have high-quality 3D data or some other ideas, we very much welcome any form of cooperation.</span><br><span class="line"></span><br><span class="line">Full abstract here</span><br><span class="line">We present a novel generative 3D modeling system, coined CraftsMan, which can generate high-fidelity 3D geometries with highly varied shapes, regular mesh topologies, and detailed surfaces, and, notably, allows for refining the geometry in an interactive manner. Despite the significant advancements in 3D generation, existing methods still struggle with lengthy optimization processes, irregular mesh topologies, noisy surfaces, and difficulties in accommodating user edits, consequently impeding their widespread adoption and implentation in 3D modeling softwares. Our work is inspired by the craftsman, who usually roughs out the holistic figure of the work first and elaborate the surface details subsequently. Specifically, we employ a 3D native diffusion model, which operates on latent space learned from latent set-based 3D representations, to generate coarse geometries with regular mesh topology in seconds. In particular, this process takes as input a text prompt or a reference image, and leverages a powerful multi-view (MV) diffusion model to generates multiple views of the coarse geometry, which are fed into our MV-conditioned 3D diffusion model for generating the 3D geometry, significantly improving robustness and generalizability. Following that, a normal-based geometry refiner is used to significantly enhance the surface details. This refinement can be performed automatically, or interactively with user-supplied edits. Extensive experiments demonstrate that our method achieves high e�cacy in producing superior quality 3D assets compared to existing methods.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line"></span><br><span class="line">https://huggingface.co/papers/2405.09818</span><br><span class="line">Chameleon: Mixed-Modal Early-Fusion Foundation Models</span><br><span class="line">Published on May 16</span><br><span class="line">·</span><br><span class="line">Featured in Daily Papers on May 17</span><br><span class="line">Authors:</span><br><span class="line">Chameleon Team</span><br><span class="line">Abstract</span><br><span class="line">We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line"></span><br><span class="line">https://jjihwan.github.io/projects/FIFO-Diffusion</span><br><span class="line">FIFO-Diffusion: Generating Infinite Videos from Text</span><br><span class="line">without Training</span><br><span class="line">Jihwan Kim*1 Junoh Kang*1 Jinyoung Choi1 Bohyung Han1, 2</span><br><span class="line"></span><br><span class="line">1ECE &amp; 2IPAI, Seoul National University</span><br><span class="line">(\* Equal Contribution)</span><br><span class="line">&#123;kjh26720, junoh.kang, jin0.choi, bhhan&#125;@snu.ac.kr</span><br><span class="line"></span><br><span class="line">[arXiv] [Code]</span><br><span class="line"></span><br><span class="line">1K-frame Long Videos (512 x 320 resolution, VideoCrafter2)</span><br><span class="line">A spectacular fireworks display over Sydney Harbour, 4K, high resolution.</span><br><span class="line"></span><br><span class="line">A colony of penguins waddling on an Antarctic ice sheet, 4K, ultra HD.</span><br><span class="line"></span><br><span class="line">An astronaut floating in space, high quality, 4K resolution.</span><br><span class="line"></span><br><span class="line">A spectacular fireworks display over Sydney Harbour, 4K, high resolution.</span><br><span class="line"></span><br><span class="line">A colony of penguins waddling on an Antarctic ice sheet, 4K, ultra HD.</span><br><span class="line"></span><br><span class="line">An astronaut floating in space, high quality, 4K resolution.</span><br><span class="line"></span><br><span class="line">A spectacular fireworks display over Sydney Harbour, 4K, high resolution.</span><br><span class="line"></span><br><span class="line">A colony of penguins waddling on an Antarctic ice sheet, 4K, ultra HD.</span><br><span class="line"></span><br><span class="line">An astronaut floating in space, high quality, 4K resolution.</span><br><span class="line"></span><br><span class="line">Abstract</span><br><span class="line">We propose a novel inference technique based on a pretrained diffusion model for text-conditional video generation. Our approach, called FIFO-Diffusion, is conceptually capable of generating infinitely long videos without training. This is achieved by iteratively performing diagonal denoising, which concurrently processes a series of consecutive frames with increasing noise levels in a queue; our method dequeues a fully denoised frame at the head while enqueuing a new random noise frame at the tail. However, diagonal denoising is a double-edged sword as the frames near the tail can take advantage of cleaner ones by forward reference but such a strategy induces the discrepancy between training and inference. Hence, we introduce latent partitioning to reduce the training-inference gap and lookahead denoising to leverage the benefit of forward referencing. We have demonstrated the promising results and effectiveness of the proposed methods on strong text-to-video generation baselines.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</details>
```
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-05-26</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/05/26/2024-5-26-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 5월 26일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/05/27/2024-5-27-AI-NEWS/" title="2024년 5월 27일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/05/22/2024-5-22-AI-NEWS/" title="2024년 5월 22일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>