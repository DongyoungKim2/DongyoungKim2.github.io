<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 5월 27일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="Summary오늘의 소식에서는 GPT-4’o’ 모델의 작동 원리와 유사한 AI를 만드는 방법에 대해 다룹니다. 또한 OpenGPT-4o 모델의 개발 과정과 Falcon 2-11B 모델에 대한 내용을 포함합니다.
Decoding GPT-4’o’: In-Depth Expl"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 5월 27일 AI 소식</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>오늘의 소식에서는 GPT-4’o’ 모델의 작동 원리와 유사한 AI를 만드는 방법에 대해 다룹니다. 또한 OpenGPT-4o 모델의 개발 과정과 Falcon 2-11B 모델에 대한 내용을 포함합니다.</p>
<h2 id="Decoding-GPT-4’o’-In-Depth-Exploration-of-Its-Mechanisms-and-Creating-Similar-AI"><a href="#Decoding-GPT-4’o’-In-Depth-Exploration-of-Its-Mechanisms-and-Creating-Similar-AI" class="headerlink" title="Decoding GPT-4’o’: In-Depth Exploration of Its Mechanisms and Creating Similar AI"></a>Decoding GPT-4’o’: In-Depth Exploration of Its Mechanisms and Creating Similar AI</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/KingNish/decoding-gpt-4o">Decoding GPT-4’o’</a></p>
<ul>
<li><strong>날짜</strong>: 2024년 5월 21일</li>
<li><strong>작성자</strong>: KingNish (Nishith Jain)</li>
<li><strong>내용 요약</strong>:<ul>
<li>GPT-4’o’는 여러 모델을 혼합한 혁신적인 AI 모델로, 비디오 채팅, 감정 표현이 가능한 음성 채팅, 텍스트 및 이미지 생성, 문서 및 비디오 QnA, 이미지에서 3D 생성 등의 기능을 하나의 모듈에 통합한 모델입니다.</li>
<li><strong>SuperChat</strong>: 텍스트 생성, 이미지 생성, 이미지 및 문서 분류, 비디오 분류 등을 결합한 모델입니다.</li>
<li><strong>Voice Chat</strong>: 실시간으로 감정을 분석하고 음성으로 응답하는 TTS와 STT를 결합한 모듈입니다.</li>
<li><strong>Video Chat</strong>: 사용자가 대화 시작 시 이미지를 캡처하고 추가 이미지를 생성하여 사용자 질의에 응답하는 제로 샷 이미지 분류를 사용합니다.</li>
<li><strong>AI 모델 제작 방법</strong>:<ul>
<li><strong>MultiModalification Method</strong>: 기능에 따라 2개 이상의 모델을 결합하여 다기능 모델을 생성하는 방법입니다.</li>
<li><strong>Duct Tape Method</strong>: 추가 훈련 없이 다양한 작업을 수행하기 위해 다양한 모델 또는 API를 사용하는 방법입니다.</li>
</ul>
</li>
<li><strong>추천 모델</strong>:<ul>
<li>텍스트 생성: Llama 3 70B</li>
<li>이미지 생성: Pixart Sigma 또는 RealVisXL</li>
<li>제로 샷 이미지 분류: Sigslip</li>
<li>비디오 분류: Xclip</li>
<li>3D 생성: Instant Mesh</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="How-OpenGPT-4o-works"><a href="#How-OpenGPT-4o-works" class="headerlink" title="How OpenGPT 4o works"></a>How OpenGPT 4o works</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/KingNish/opengpt-4o-working">How OpenGPT 4o works</a></p>
<ul>
<li><strong>날짜</strong>: 2024년 5월 21일</li>
<li><strong>작성자</strong>: KingNish (Nishith Jain)</li>
<li><strong>내용 요약</strong>:<ul>
<li>OpenGPT 4o는 GPT-4’o’의 오픈 소스 대안으로, 다양한 모델과 API를 결합하여 다기능 모델을 구축했습니다.</li>
<li><strong>Super Chat Module</strong>: 사용자의 입력을 Idefics 2로 처리하여 질문에 응답하고, 이미지 생성 요청 시 Pollination AI를 사용합니다.</li>
<li><strong>Voice Chat</strong>: JARVIS 코드 기반으로 구축된 음성 비서로, STT 모듈을 통해 사용자 질문을 텍스트로 변환하고, Mixtral 8x7B API를 통해 응답을 생성하여 TTS 모듈로 변환합니다.</li>
<li><strong>Live Chat</strong>: uform gen2 dpo 모델을 사용하여 실시간 상호작용을 지원합니다.</li>
<li><strong>통합 과정</strong>: Gradio를 통해 모든 모듈을 실행하며, GPU 없이도 운영됩니다.</li>
</ul>
</li>
</ul>
<h2 id="Falcon-2-11B"><a href="#Falcon-2-11B" class="headerlink" title="Falcon 2-11B"></a>Falcon 2-11B</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/tiiuae/falcon-11B">Falcon 2-11B</a></p>
<ul>
<li><strong>모델 설명</strong>: Falcon2-11B는 11B 파라미터를 가진 인과 디코더 전용 모델로, RefinedWeb과 선별된 말뭉치로 훈련되었습니다.</li>
<li><strong>지원 언어</strong>: 영어, 독일어, 스페인어, 프랑스어, 이탈리아어, 네덜란드어, 폴란드어, 포르투갈어, 루마니아어, 체코어 등 11개 언어를 지원합니다.</li>
<li><strong>주요 기능</strong>: 텍스트 생성 및 회화에 최적화된 모델입니다.</li>
</ul>
<h2 id="SimPO-Simple-Preference-Optimization-with-a-Reference-Free-Reward"><a href="#SimPO-Simple-Preference-Optimization-with-a-Reference-Free-Reward" class="headerlink" title="SimPO: Simple Preference Optimization with a Reference-Free Reward"></a>SimPO: Simple Preference Optimization with a Reference-Free Reward</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2405.14734">SimPO</a></p>
<ul>
<li><strong>발표일</strong>: 2024년 5월 24일</li>
<li><strong>저자</strong>: Yu Meng, Mengzhou Xia, Danqi Chen</li>
<li><strong>내용 요약</strong>:<ul>
<li>SimPO는 Direct Preference Optimization(DPO) 알고리즘을 단순화한 방법으로, 시퀀스의 평균 로그 확률을 암묵적 보상으로 사용하여 훈련 안정성을 높였습니다.</li>
<li>Bradley-Terry 목표에 타겟 보상 마진을 도입하여 성능을 향상시켰습니다.</li>
<li>Llama3-8B-Instruct 모델을 기반으로 한 SimPO는 AlpacaEval 2 및 Arena-Hard 벤치마크에서 뛰어난 성능을 보였습니다.</li>
</ul>
</li>
</ul>
<p>위 링크를 통해 각 기사에 대한 더 자세한 내용을 확인할 수 있습니다.</p>
<details>
  <summary>Sources</summary>
  This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is: # AI News for (today's date), ## Summary (overall short summary), ## Link1 Title, link, date - detailed summary1, - detailed summary2, - detailed summary..N, ## Link2 Title, link, date - detailed summary1, - detailed summary2, - detailed point..N, etc. The report should be written in Korean and use the 개조식 문체 style. give the very deep details for each link as much as possible.

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://huggingface.co/blog/KingNish/decoding-gpt-4o</span><br><span class="line">Decoding GPT-4&#x27;o&#x27;: In-Depth Exploration of Its Mechanisms and Creating Similar AI.</span><br><span class="line">Community Article</span><br><span class="line">Published May 21, 2024</span><br><span class="line">Nishith Jain&#x27;s avatar</span><br><span class="line">KingNish</span><br><span class="line">Nishith Jain</span><br><span class="line">OpenAI has launched the groundbreaking AI GPT-4&#x27;o&#x27;, a model that is a mixture of many models. In this blog post, we will discuss how GPT-4&#x27;o&#x27; works and how to create this kind of model.</span><br><span class="line">0. GPT 4&#x27;o&#x27; Capabilities</span><br><span class="line">Video Chat. (First time introduced feature)</span><br><span class="line">Faster and Human Like Voice Chat. (It even shows emotions and change tones.)</span><br><span class="line">Text Generation, Image Generation, Image QnA, Document QnA, Video QnA ,Sequential Image Generation, Image to 3d and best thing is All these things are Packed in 1 Modal.</span><br><span class="line">Supports 50+ languages.</span><br><span class="line">See Examples in OpenAI Post</span><br><span class="line"></span><br><span class="line">1. How GPT 4&#x27;o&#x27; works.</span><br><span class="line">Firstly GPT 4o working is mainly Divided into 3 parts.</span><br><span class="line"></span><br><span class="line">1. SuperChat</span><br><span class="line">As, GPT 4 already achieved Sequential image generation and image QnA. They have to just add doc QnA ,Video QnA and 3d generation. For, tech Giant like OpenAI it is just a piece of cake for them. This can be possible with methods we discuss at end.</span><br><span class="line"></span><br><span class="line">2. Voice Chat</span><br><span class="line">OpenAI has integrated TTS (Text-to-Speech) and STT (Speech-to-Text) into a single module, removing the text generation component they previously used. This means that when you speak, the AI analyzes your tone and words to create response in audio in real-time, similar to how streaming is used in text generation. In my opinion, OpenAi made this model comparatively less powerful because it is primarily designed for human interaction, and thus, the AI is trained accordingly.</span><br><span class="line"></span><br><span class="line">3. Video Chat</span><br><span class="line">Video chat is not actually a live video interaction. The AI captures an image at the start of the conversation and takes additional images as needed or instructed. It then employs Zero Shot Image Classification to respond to user queries. This module utilizes a more powerful model than voice chat because the AI can address a wider range of requests when it has visual information. For example, it can identify people, places, solve complex mathematical problems, detect coding errors, and much more which means it can do many things as compared to simple voice chat.</span><br><span class="line"></span><br><span class="line">Image depicting what people thinks of how OpenGPT-4 works vs Reality.</span><br><span class="line"></span><br><span class="line">What you thinkimage/png</span><br><span class="line"></span><br><span class="line">How it actually worksimage/png</span><br><span class="line"></span><br><span class="line">2. Creating AI Like GPT 4o</span><br><span class="line">We, also make 3 models like OpenAI but before these There are two methods for creating every model. First, it&#x27;s important to understand them.</span><br><span class="line"></span><br><span class="line">1. MultiModalification or Mixture of Modal Method</span><br><span class="line">This method combines 2 or more modals according to their functionality to create a new, powerful, multifunctional model, It aso requires further training.</span><br><span class="line"></span><br><span class="line">2. Duct Tape Method</span><br><span class="line">In this method You just need to use different types of Modals or API for doing Different task without ANY TRAINING.</span><br><span class="line"></span><br><span class="line">Making of SuperChat Model</span><br><span class="line">MultiModalification or Mixture of Modal Method To create SuperChat model we need to combine Text Generation, Image Generation, Image Classification, Document Classification, Video Classification models. Use the same process used in Idefics 2. A model that combines zero-shot image classification and text generation modal, Idefics 2 can chat with you and answer questions based on images.</span><br><span class="line"></span><br><span class="line">Duct Tape Method Method without API - It include One base Modal which PROMPTED to identify which type of task is that and then send users prompt to that specific type of modal then send output to user. Optional: Use text gen modal at end to add some words, to make answer more realistic. Method with API - One base model prompted to use API on specific type of query. This method is utilized by Copilot. For instance, when it&#x27;s requested to create images, compose songs, conduct web searches, or answer questions from images, it uses an API of that task to accomplish that task.</span><br><span class="line"></span><br><span class="line">Recommended models from which you can create SuperChat Modal as powerful as GPT 4o</span><br><span class="line"></span><br><span class="line">Base Modal - Llama 3 70B</span><br><span class="line">Image Generation: Pixart Sigma or RealVisXL</span><br><span class="line">Zero Shot Image Classification: Sigslip</span><br><span class="line">Zero Shot Video Classification: Xclip</span><br><span class="line">Sequential Image Gen - Control SDxl</span><br><span class="line">Zero Shot Doc Classification - idf</span><br><span class="line">3d gen - Instant Mesh</span><br><span class="line">Other Models - Animate Diff lightning</span><br><span class="line">Making of VoiceChat Model</span><br><span class="line">MultiModalification or Mixture of Modal Method To develop a human-like speaking AI that also exhibits emotions, high-quality training data is essential. Additionally, an emotion identification model is necessary to recognize users&#x27; emotions and Text gen model who understands users emotion.</span><br><span class="line"></span><br><span class="line">Duct Tape Method It include One stt Modal to encode users prompt with emotion to text gen modal with emotion encoded in answer and utilizing a TTS such as Parler TTS Expresso can further infuse emotion into the output.</span><br><span class="line"></span><br><span class="line">Suggested Models</span><br><span class="line"></span><br><span class="line">Speech to Text - Whisper</span><br><span class="line">ChatModal - Llama3 8b</span><br><span class="line">Text to Speech - Parler tts Expresso</span><br><span class="line">Emotion identifier - Speech Emotion Recognition</span><br><span class="line">Making of VideoChat Model</span><br><span class="line">As previously mentioned, it only captures images. Thus, a zero-shot image classification model is necessary, while the rest remains the same as the voice chat model. However, it also requires a highly intelligent model, due to the increased use case with vision.</span><br><span class="line"></span><br><span class="line">Suggested Models</span><br><span class="line"></span><br><span class="line">ZeroShot Image Classification : Sigslip</span><br><span class="line">Speech to Text - Whisper</span><br><span class="line">ChatModal - Llama3 8b</span><br><span class="line">Text to Speech - Parler tts Expresso</span><br><span class="line">Optional - Speech Emotion Recognition</span><br><span class="line">Alternatively</span><br><span class="line"></span><br><span class="line">Image QnA Model - Idefics 2</span><br><span class="line">VoiceChat Model</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/blog/KingNish/opengpt-4o-working</span><br><span class="line">How OpenGPT 4o works</span><br><span class="line"></span><br><span class="line">How OpenGPT 4o works</span><br><span class="line">Community Article</span><br><span class="line">Published May 21, 2024</span><br><span class="line">Nishith Jain&#x27;s avatar</span><br><span class="line">KingNish</span><br><span class="line">Nishith Jain</span><br><span class="line">In the previous blog, we discussed how ChatGPT 4o works. Today, we&#x27;re going to talk about how I developed OpenGPT 4o, an open-source alternative to GPT 4o.</span><br><span class="line">(Suggestion: Read previous blog post as this blog contains interconnected topics. Link - https://huggingface.co/blog/KingNish/decoding-gpt-4o )</span><br><span class="line"></span><br><span class="line">Selecting the Method</span><br><span class="line">There are 2 methods to Creating AI like GPT 4o.</span><br><span class="line"></span><br><span class="line">1. MultiModalification or Mixture of Modal Method</span><br><span class="line">This method combines 2 or more modals according to their functionality to create a new, powerful, multifunctional model, It also requires further training.</span><br><span class="line"></span><br><span class="line">2. Duct Tape Method</span><br><span class="line">In this method You just need to use different types of Modals or API for doing Different task without ANY TRAINING.</span><br><span class="line"></span><br><span class="line">Since I don&#x27;t have access to a GPU for training models. So, I&#x27;ve choosed the Duct Tape Method.</span><br><span class="line"></span><br><span class="line">Next Step is to select the model/API based on their performance, speed and easy implementation.</span><br><span class="line"></span><br><span class="line">Models and API used are:</span><br><span class="line">Work	Model/API	Reason</span><br><span class="line">Super Chat Model	Idefics 2	Already made, eliminating the need to build from scratch.</span><br><span class="line">Image Generation Model	Pollination AI (API)	Implementation is fast and straightforward.</span><br><span class="line">Speech to Text	Nemo (API)	Already utilized in another project (JARVIS).</span><br><span class="line">Voice Chat (Base Model)	Mixtral 8x7b (Inference API)	Offers superior speed and power compared to GPT 3.5 Turbo.</span><br><span class="line">Text to Speech	Edge tts (API)	Provides exceptionally fast text-to-speech conversion.</span><br><span class="line">Live Chat (base model)	uform gen2 dpo	Its small size and rapid performance.</span><br><span class="line">As, discussed in Prev Blog ChatGPT working is divide into 3 modules. So, Now discuss each module.</span><br><span class="line"></span><br><span class="line">Super Chat Module</span><br><span class="line">Let&#x27;s Understand working with Visuals:image/png</span><br><span class="line"></span><br><span class="line">Explaination: When a user provides input, it is processed by Idefics 2, which interprets user prompts and responds to questions. If a user wishes to generate an image, it creates an image link of Pollination AI. The process for creating this link is explained in detail to AI in its system prompt. Once the link is created, Pollination AI begins generating the image, which becomes visible to the user upon completion.</span><br><span class="line"></span><br><span class="line">System Prompt I used</span><br><span class="line">Voice Chat</span><br><span class="line">As, I have already created JARVIS, a voice assistant, so I simply utilize the code from it.</span><br><span class="line"></span><br><span class="line">Here is the visuals demonstrating how the voice chat functions.image/png</span><br><span class="line"></span><br><span class="line">Explanation: When a user asks the AI a question, it is directed to the STT (Speech to Text) module, which converts it into text and sends it to the Mixtral 8x7B API. This API processes the request and generates a response that is sent to the TTS (Text to Speech) module. This module then converts the response into audio and sends it back to the user.</span><br><span class="line"></span><br><span class="line">Live Chat</span><br><span class="line">For real-time interactions, the uform gen2 dpo model powers the live chat feature.</span><br><span class="line"></span><br><span class="line">Illustration depicting the working of video chat features.image/pngExplaination: Initially, the user provides input via both webcam and text simultaneously. Then, the AI answers users query from the picture using &quot;UForm Gen2&quot; and the answer is sent back in text format as the output.</span><br><span class="line"></span><br><span class="line">The Integration Process</span><br><span class="line">Well, All 3 modules are running through Gradio on ZERO GPU.</span><br><span class="line"></span><br><span class="line">Source Code: - https://github.com/KingNishHF/OpenGPT-4o</span><br><span class="line"></span><br><span class="line">Conclusion</span><br><span class="line">The creation of OpenGPT 4o using the duct tape method is a prime example of how diverse AI models can be woven together to create a comprehensive and multifaceted tool. It stands as a beacon of possibility in the realm of AI development, showcasing the power of collaboration between different AI technologies.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/tiiuae/falcon-11B</span><br><span class="line">tiiuae</span><br><span class="line">/</span><br><span class="line">falcon-11B</span><br><span class="line"></span><br><span class="line">like</span><br><span class="line">164</span><br><span class="line">Text Generation</span><br><span class="line">Transformers</span><br><span class="line">Safetensors</span><br><span class="line"></span><br><span class="line">tiiuae/falcon-refinedweb</span><br><span class="line">English</span><br><span class="line">German</span><br><span class="line">Spanish</span><br><span class="line">French</span><br><span class="line">Italian</span><br><span class="line">Dutch</span><br><span class="line">Polish</span><br><span class="line">Portuguese</span><br><span class="line">Romanian</span><br><span class="line">Czech</span><br><span class="line">falcon</span><br><span class="line">conversational</span><br><span class="line">custom_code</span><br><span class="line">text-generation-inference</span><br><span class="line">5 papers</span><br><span class="line"></span><br><span class="line">License:</span><br><span class="line">unknown</span><br><span class="line">Model card</span><br><span class="line">Files and versions</span><br><span class="line">Community</span><br><span class="line">7</span><br><span class="line">🚀 Falcon2-11B</span><br><span class="line">Falcon2-11B is an 11B parameters causal decoder-only model built by TII and trained on over 5,000B tokens of RefinedWeb enhanced with curated corpora. The model is made available under the TII Falcon License 2.0, the permissive Apache 2.0-based software license which includes an acceptable use policy that promotes the responsible use of AI.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2405.14734</span><br><span class="line">SimPO: Simple Preference Optimization with a Reference-Free Reward</span><br><span class="line">SimPO: Simple Preference Optimization with a Reference-Free Reward</span><br><span class="line">Published on May 24</span><br><span class="line">Authors:</span><br><span class="line"></span><br><span class="line">Yu Meng</span><br><span class="line">,</span><br><span class="line">Mengzhou Xia</span><br><span class="line">,</span><br><span class="line">Danqi Chen</span><br><span class="line">Abstract</span><br><span class="line">Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability. In this work, we propose SimPO, a simpler yet more effective approach. The effectiveness of SimPO is attributed to a key design: using the average log probability of a sequence as the implicit reward. This reward formulation better aligns with model generation and eliminates the need for a reference model, making it more compute and memory efficient. Additionally, we introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further enhancing the algorithm&#x27;s performance. We compare SimPO to DPO and its latest variants across various state-of-the-art training setups, including both base and instruction-tuned models like Mistral and Llama3. We evaluated on extensive instruction-following benchmarks, including AlpacaEval 2, MT-Bench, and the recent challenging Arena-Hard benchmark. Our results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length. Specifically, SimPO outperforms DPO by up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our top-performing model, built on Llama3-8B-Instruct, achieves a remarkable 44.7 length-controlled win rate on AlpacaEval 2 -- surpassing Claude 3 Opus on the leaderboard, and a 33.8 win rate on Arena-Hard -- making it the strongest 8B open-source model.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-05-27</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/05/27/2024-5-27-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 5월 27일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/05/30/2024-5-30-AI-NEWS/" title="2024년 5월 30일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/05/26/2024-5-26-AI-NEWS/" title="2024년 5월 26일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>