<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 5월 31일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="요약오늘의 AI 소식은 다양한 AI 기술과 그 적용에 관한 최신 정보를 제공합니다. OpenAI는 최근 인공지능을 이용한 비밀 영향 작전을 방해한 내용을 발표했고, Google은 Gemini 1.5 Pro와 1.5 Flash 모델의 새로운 기능을 소개했습니다. 또한, "><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 5월 31일 AI 소식</a></h3></div><div class="post-content"><h2 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h2><p>오늘의 AI 소식은 다양한 AI 기술과 그 적용에 관한 최신 정보를 제공합니다. OpenAI는 최근 인공지능을 이용한 비밀 영향 작전을 방해한 내용을 발표했고, Google은 Gemini 1.5 Pro와 1.5 Flash 모델의 새로운 기능을 소개했습니다. 또한, IEIT-Yuan은 Yuan2.0-M32 모델을 공개했으며, Google은 CodecLM을 통해 맞춤형 합성 데이터를 활용한 언어 모델 정렬을 발표했습니다. OpenAI는 교육 기관을 위한 ChatGPT Edu를 도입했고, Anthropic은 Claude 모델의 툴 사용 기능을 일반에 공개했습니다. 마지막으로, Tencent AI Lab은 V-Express 방법을 이용한 초상화 비디오 생성에 대한 연구를 발표했습니다.</p>
<h2 id="OpenAI가-발표한-비밀-영향-작전-방해"><a href="#OpenAI가-발표한-비밀-영향-작전-방해" class="headerlink" title="OpenAI가 발표한 비밀 영향 작전 방해"></a>OpenAI가 발표한 비밀 영향 작전 방해</h2><p><a target="_blank" rel="noopener" href="https://openai.com/index/disrupting-deceptive-uses-of-AI-by-covert-influence-operations/">링크</a> | 2024년 5월 30일</p>
<ul>
<li>OpenAI는 비밀 영향 작전에 이용된 AI 모델을 차단했다고 발표</li>
<li>지난 3개월 동안 다섯 개의 비밀 작전이 차단됨</li>
<li>이 작전들은 주로 러시아, 중국, 이란, 이스라엘에서 발생했으며, 다양한 언어로 작성된 콘텐츠 생성 및 소셜 미디어 활동을 포함</li>
<li>이러한 작전의 주제는 러시아의 우크라이나 침공, 가자지구 분쟁, 인도 선거 등 정치적 이슈 포함</li>
<li>OpenAI는 공격자들의 활동이 자사 서비스로 인해 실질적으로 증가하지 않았다고 보고</li>
<li>AI 모델의 안전 설계와 AI 도구의 효율성 덕분에 이러한 작전을 방해할 수 있었음</li>
</ul>
<h2 id="Google의-Gemini-1-5-Pro와-1-5-Flash-모델-발표"><a href="#Google의-Gemini-1-5-Pro와-1-5-Flash-모델-발표" class="headerlink" title="Google의 Gemini 1.5 Pro와 1.5 Flash 모델 발표"></a>Google의 Gemini 1.5 Pro와 1.5 Flash 모델 발표</h2><p><a target="_blank" rel="noopener" href="https://developers.googleblog.com/en/gemini-15-pro-and-15-flash-now-available/">링크</a> | 2024년 5월 30일</p>
<ul>
<li>Gemini 1.5 Pro와 1.5 Flash 모델의 안정적 출시 및 요금제 발표</li>
<li>1.5 Flash 모델은 빠른 속도와 비용 효율성을 강조하며, 요청 제한을 증가시켜 1분당 1000개의 요청 처리 가능</li>
<li>1.5 Flash 모델은 맞춤형 튜닝 지원, JSON 스키마 모드, 모바일 지원 및 라이트 모드 제공</li>
<li>Google AI Studio에서 무료로 사용할 수 있으며, 요금제를 활성화하면 더 높은 API 한도 이용 가능</li>
</ul>
<h2 id="IEIT-Yuan의-Yuan2-0-M32-모델-공개"><a href="#IEIT-Yuan의-Yuan2-0-M32-모델-공개" class="headerlink" title="IEIT-Yuan의 Yuan2.0-M32 모델 공개"></a>IEIT-Yuan의 Yuan2.0-M32 모델 공개</h2><p><a target="_blank" rel="noopener" href="https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main">링크</a> | 2024년 5월 28일</p>
<ul>
<li>Yuan2.0-M32 모델은 32명의 전문가 중 2명이 활성화된 Mixture-of-Experts(MoE) 언어 모델</li>
<li>새로운 Attention Router 네트워크를 도입하여 효율적인 전문가 선택 가능</li>
<li>2000억 개의 토큰으로 학습되었으며, 동일한 규모의 밀집 모델에 비해 9.25%의 계산만 필요</li>
<li>MATH 및 ARC-Challenge 벤치마크에서 Llama3-70B 모델을 능가</li>
<li>16K의 시퀀스 길이와 40억 개의 총 파라미터 보유</li>
</ul>
<h2 id="Google의-CodecLM-발표"><a href="#Google의-CodecLM-발표" class="headerlink" title="Google의 CodecLM 발표"></a>Google의 CodecLM 발표</h2><p><a target="_blank" rel="noopener" href="https://research.google/blog/codeclm-aligning-language-models-with-tailored-synthetic-data/">링크</a> | 2024년 5월 30일</p>
<ul>
<li>CodecLM은 고품질 합성 데이터를 사용하여 LLM(대규모 언어 모델)을 특정 다운스트림 작업에 맞게 정렬하는 프레임워크</li>
<li>Self-Rubrics 및 Contrastive Filtering을 통해 합성 데이터의 품질을 향상시키는 전략 도입</li>
<li>PaLM 2 LLM을 사용하여 다양한 공개 도메인 지침-따르기 벤치마크에서 성능 입증</li>
<li>맞춤형 합성 데이터를 생성하여 LLM의 성능을 크게 향상시킴</li>
</ul>
<h2 id="OpenAI의-ChatGPT-Edu-발표"><a href="#OpenAI의-ChatGPT-Edu-발표" class="headerlink" title="OpenAI의 ChatGPT Edu 발표"></a>OpenAI의 ChatGPT Edu 발표</h2><p><a target="_blank" rel="noopener" href="https://openai.com/index/introducing-chatgpt-edu/">링크</a> | 2024년 5월 30일</p>
<ul>
<li>대학을 위한 ChatGPT Edu 출시, GPT-4o 모델 기반으로 텍스트 및 비전 분석 가능</li>
<li>데이터 분석, 웹 브라우징, 문서 요약 등 고급 기능 포함</li>
<li>강력한 보안, 데이터 프라이버시 및 관리 제어 제공</li>
<li>대학에서 AI를 학생, 교직원 및 연구진에게 확산시키기 위한 경제적인 옵션</li>
</ul>
<h2 id="Anthropic의-Claude-모델-툴-사용-기능-일반-공개"><a href="#Anthropic의-Claude-모델-툴-사용-기능-일반-공개" class="headerlink" title="Anthropic의 Claude 모델 툴 사용 기능 일반 공개"></a>Anthropic의 Claude 모델 툴 사용 기능 일반 공개</h2><p><a target="_blank" rel="noopener" href="https://www.anthropic.com/news/tool-use-ga">링크</a> | 2024년 5월 31일</p>
<ul>
<li>Claude 3 모델 패밀리에서 외부 툴 및 API와 상호작용 가능</li>
<li>구조화되지 않은 텍스트에서 데이터 추출, 자연어 요청을 구조화된 API 호출로 변환, 데이터베이스 검색 등을 통해 정확한 답변 제공</li>
<li>스트리밍을 통한 실시간 응답, 이미지 입력 활용 가능</li>
<li>고객 사례: StudyFetch, Intuned, Hebbia가 Claude의 툴 사용 기능을 통해 AI 학습 플랫폼 및 데이터 추출 기능 향상</li>
</ul>
<h2 id="Tencent-AI-Lab의-V-Express-연구-발표"><a href="#Tencent-AI-Lab의-V-Express-연구-발표" class="headerlink" title="Tencent AI Lab의 V-Express 연구 발표"></a>Tencent AI Lab의 V-Express 연구 발표</h2><p><a target="_blank" rel="noopener" href="https://tenvence.github.io/p/v-express/">링크</a> | 2024년 5월 28일</p>
<ul>
<li>V-Express는 단일 이미지를 사용하여 초상화 비디오를 생성하는 방법을 제안</li>
<li>다양한 강도의 제어 신호(텍스트, 오디오, 이미지 참조, 포즈 등)를 균형 있게 처리</li>
<li>오디오 신호의 효과적인 제어를 위해 점진적 드롭 방법 사용</li>
<li>오디오 신호에 의해 제어되는 초상화 비디오를 효과적으로 생성</li>
</ul>
<h2 id="Figma의-GPT-4o를-이용한-자동화"><a href="#Figma의-GPT-4o를-이용한-자동화" class="headerlink" title="Figma의 GPT-4o를 이용한 자동화"></a>Figma의 GPT-4o를 이용한 자동화</h2><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=AzqKLiPQD6g&ab_channel=jarkkomoilanen">유튜브 링크</a></p>
<ul>
<li>GPT-4o를 이용한 Figma 디자인 자동화</li>
<li>PRD(제품 요구 사항 문서)를 기반으로 자동으로 디자인 생성</li>
</ul>
<p>오늘의 AI 소식은 인공지능의 다양한 분야에서 최신 기술과 그 적용 사례를 다룹니다. 인공지능이 어떻게 다양한 산업과 연구에 혁신을 가져오고 있는지에 대한 깊이 있는 이해를 제공합니다.</p>
<details>
  <summary>Sources</summary>
  This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is: # AI News for (today's date), ## Summary (overall short summary), ## Link1 Title, link, date - detailed summary1, - detailed summary2, - detailed summary..N, ## Link2 Title, link, date - detailed summary1, - detailed summary2, - detailed point..N, etc. The report should be written in Korean and use the 개조식 문체 style. give the very deep details for each link as much as possible.
  make summary with good details, note company name next to date if available.

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://openai.com/index/disrupting-deceptive-uses-of-AI-by-covert-influence-operations/</span><br><span class="line">May 30, 2024</span><br><span class="line"></span><br><span class="line">Disrupting deceptive uses of AI by covert influence operations</span><br><span class="line">We’ve terminated accounts linked to covert influence operations; no significant audience increase due to our services.</span><br><span class="line"></span><br><span class="line">The image is an abstract background with soft, blended hues of purple, pink, and blue. The pastel colors mix seamlessly, creating a dreamy and serene atmosphere, reminiscent of a twilight sky or an ethereal mist.</span><br><span class="line">OpenAI is committed to enforcing policies that prevent abuse and to improving transparency around AI-generated content. That is especially true with respect to detecting and disrupting covert influence operations (IO), which attempt to manipulate public opinion or influence political outcomes without revealing the true identity or intentions of the actors behind them.</span><br><span class="line"></span><br><span class="line">In the last three months, we have disrupted five covert IO that sought to use our models in support of deceptive activity across the internet. As of May 2024, these campaigns do not appear to have meaningfully increased their audience engagement or reach as a result of our services.</span><br><span class="line"></span><br><span class="line">This blog describes the threat actors we disrupted, attacker trends we identified, and important defensive trends - including how designing AI models with safety in mind in many cases prevented the threat actors from generating the content they desired, and how AI tools have made our own investigations more efficient. Alongside this blog, we are publishing a trend analysis that describes the behavior of these malicious actors in detail.</span><br><span class="line"></span><br><span class="line">Read the full report(opens in a new window)</span><br><span class="line"></span><br><span class="line">Threat actors work across the internet. So do we. By collaborating with industry, civil society, and government we tackle the creation, distribution, and impact of IO content.  Our investigations and disruptions were made possible in part because there’s been so much detailed threat reporting over the years by distribution platforms and the open-source community. OpenAI is publishing these findings, as other tech companies do, to promote information sharing and best practices amongst the broader community of stakeholders.</span><br><span class="line"></span><br><span class="line">Disruption of covert influence operations</span><br><span class="line">Over the last three months, our work against IO actors has disrupted covert influence operations that sought to use AI models for a range of tasks, such as generating short comments and longer articles in a range of languages, making up names and bios for social media accounts, conducting open-source research, debugging simple code, and translating and proofreading texts.</span><br><span class="line"></span><br><span class="line">Specifically, we disrupted:</span><br><span class="line"></span><br><span class="line">A previously unreported operation from Russia, which we dubbed Bad Grammar, operating mainly on Telegram and targeting Ukraine, Moldova, the Baltic States and the United States. The people behind Bad Grammar used our models to debug code for running a Telegram bot and to create short, political comments in Russian and English that were then posted on Telegram.</span><br><span class="line"></span><br><span class="line">An operation originating in Russia known as Doppelganger(opens in a new window). People acting on behalf of Doppelganger used our models to generate comments in English, French, German, Italian and Polish that were posted on X and 9GAG; translate and edit articles in English and French that were posted on websites linked to this operation; generate headlines; and convert news articles into Facebook posts.</span><br><span class="line"></span><br><span class="line">A Chinese network known as Spamouflage(opens in a new window), which used our models to research public social media activity, generate texts in languages including Chinese, English, Japanese and Korean that were then posted across platforms including X, Medium and Blogspot, and debug code for managing databases and websites, including a previously unreported domain, revealscum[.]com.</span><br><span class="line"></span><br><span class="line">An Iranian operation known as the International Union of Virtual Media(opens in a new window) (IUVM), which used our models to generate and translate long-form articles, headlines and website tags that were then published on a website linked to this Iranian threat actor, iuvmpress[.]co;</span><br><span class="line"></span><br><span class="line">Activity by a commercial company in Israel called STOIC, because technically we disrupted the activity, not the company. We nicknamed this operation Zero Zeno, for the founder of the stoic school of philosophy. The people behind Zero Zeno used our models to generate articles and comments that were then posted across multiple platforms, notably Instagram, Facebook, X, and websites associated with this operation.</span><br><span class="line"></span><br><span class="line">The content posted by these various operations focused on a wide range of issues, including Russia’s invasion of Ukraine, the conflict in Gaza, the Indian elections, politics in Europe and the United States, and criticisms of the Chinese government by Chinese dissidents and foreign governments.</span><br><span class="line"></span><br><span class="line">So far, these operations do not appear to have benefited from meaningfully increased audience engagement or reach as a result of our services. Using Brookings’ Breakout Scale,(opens in a new window) which assesses the impact of covert IO on a scale from 1 (lowest) to 6 (highest), none of the five operations included in our case studies scored higher than a 2 (activity on multiple platforms, but no breakout into authentic communities).</span><br><span class="line"></span><br><span class="line">Attacker trends</span><br><span class="line">Based on the investigations into influence operations detailed in our report, and the work of the open-source community, we have identified the following trends in how covert influence operations have recently used artificial intelligence models like ours.</span><br><span class="line"></span><br><span class="line">Content generation: All these threat actors used our services to generate text (and occasionally images) in greater volumes, and with fewer language errors than would have been possible for the human operators alone.</span><br><span class="line"></span><br><span class="line">Mixing old and new: All of these operations used AI to some degree, but none used it exclusively. Instead, AI-generated material was just one of many types of content they posted, alongside more traditional formats, such as manually written texts or memes copied from across the internet.</span><br><span class="line"></span><br><span class="line">Faking engagement: Some of the networks we disrupted used our services to help create the appearance of engagement across social media - for example, by generating replies to their own posts. This is distinct from attracting authentic engagement, which none of the networks we describe here managed to do to a meaningful degree.</span><br><span class="line"></span><br><span class="line">Productivity gains: Many of the threat actors that we identified and disrupted used our services in an attempt to enhance productivity, such as summarizing social media posts or debugging code.</span><br><span class="line"></span><br><span class="line">Defensive trends</span><br><span class="line">While much of the public debate so far has focused on the potential or actual use of AI by attackers, it is important to remember the advantages that AI offers to defenders. Our investigations also benefit from industry sharing and open-source research.</span><br><span class="line"></span><br><span class="line">Defensive design: We impose friction on threat actors through our safety systems, which reflect our approach to responsibly deploying AI. For example, we repeatedly observed cases where our models refused to generate the text or images that the actors asked for.</span><br><span class="line"></span><br><span class="line">AI-enhanced investigation: Similar to our approach to using GPT-4 for content moderation and cyber defense, we have built our own AI-powered tools to make our detection and analysis more effective. The investigations described in the accompanying report took days, rather than weeks or months, thanks to our tooling. As our models improve, we’ll continue leveraging their capabilities to improve our investigations too.</span><br><span class="line"></span><br><span class="line">Distribution matters: Like traditional forms of content, AI-generated material must be distributed if it is to reach an audience. The IO posted across a wide range of different platforms, including X, Telegram, Facebook, Medium, Blogspot, and smaller forums, but none managed to engage a substantial audience.</span><br><span class="line"></span><br><span class="line">Importance of industry sharing: To increase the impact of our disruptions on these actors, we have shared detailed threat indicators with industry peers. Our own investigations benefited from years of open-source analysis conducted by the wider research community.</span><br><span class="line"></span><br><span class="line">The human element: AI can change the toolkit that human operators use, but it does not change the operators themselves. Our investigations showed that these actors were as prone to human error as previous generations have been - for example, publishing refusal messages from our models on social media and their websites. While it is important to be aware of the changing tools that threat actors use, we should not lose sight of the human limitations that can affect their operations and decision making.</span><br><span class="line"></span><br><span class="line">We are committed to developing safe and responsible AI, which involves designing our models with safety in mind and proactively intervening against malicious use. Detecting and disrupting multi-platform abuses such as covert influence operations can be challenging because we do not always know how content generated by our products is distributed. But we are dedicated to finding and mitigating this abuse at scale by harnessing the power of generative AI.</span><br><span class="line"></span><br><span class="line">Announcements</span><br><span class="line">Safety &amp; Alignment</span><br><span class="line">Authors</span><br><span class="line">OpenAI</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://developers.googleblog.com/en/gemini-15-pro-and-15-flash-now-available/</span><br><span class="line">GEMINI</span><br><span class="line">Gemini 1.5 Pro and 1.5 Flash GA, 1.5 Flash tuning support, higher rate limits, and more API updates</span><br><span class="line">MAY 30, 2024</span><br><span class="line">Logan Kilpatrick</span><br><span class="line">Senior Product Manager</span><br><span class="line">Gemini API and Google AI Studio</span><br><span class="line">Shrestha Basu Mallick</span><br><span class="line">Group Product Manager</span><br><span class="line">Gemini API</span><br><span class="line"></span><br><span class="line">Share</span><br><span class="line">Gemini 1.5 Pro-Flash</span><br><span class="line">Building on the momentum from Google I/O, we&#x27;re announcing important updates to the Gemini API and Google AI Studio, including:</span><br><span class="line"></span><br><span class="line">Gemini 1.5 Flash and 1.5 Pro stable release and billing</span><br><span class="line">Higher rate limits on Gemini 1.5 Flash</span><br><span class="line">Gemini 1.5 Flash tuning</span><br><span class="line">JSON schema mode</span><br><span class="line">Mobile support and light mode in Google AI Studio</span><br><span class="line">We’re incredibly excited to see what you build with these new models and are committed to building towards a world class developer experience. You can get started with Gemini 1.5 Flash and 1.5 Pro free of charge in Google AI Studio.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Gemini 1.5 Flash updates</span><br><span class="line">Gemini 1.5 Flash was purpose-built as our fastest, most cost-efficient model yet for high volume tasks, at scale, to address developers’ feedback asking for lower latency and cost. Today, we are increasing the rate limit for 1.5 Flash to 1000 requests per minute (RPM) and removing the request per day limit. The 1.5 Pro rate limit will not be changed at this time, but if you need even higher limits to scale or have feedback, please reach out to us.</span><br><span class="line"></span><br><span class="line">Customizing models can help you reach the performance threshold needed to take AI models into production. To support that, we will also be rolling out tuning support for Gemini 1.5 Flash on June 17th. Tuning will be supported in both Google AI Studio and the Gemini API directly. Currently, tuning jobs are free of charge, and using a tuned model does not incur any additional per-token costs. You can learn more about tuning in the Gemini API docs.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Gemini API billing</span><br><span class="line">In addition to the free tier, starting today, developers can unlock higher API rate limits by turning on a billing account in Google AI Studio.</span><br><span class="line"></span><br><span class="line">Set up billing in Google AI Studio</span><br><span class="line">You can learn more about the Gemini 1.5 model pricing on ai.google.dev/pricing. If you run into any issues setting up billing, please let us know on our developer forum. For developers looking to scale with enterprise-grade features, the same models are available via Vertex AI, our enterprise-ready AI platform.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">JSON schema mode</span><br><span class="line">We launched JSON mode in the Gemini API and Google AI Studio earlier this year to give you more control over model output. Starting today, you can specify the desired JSON schema for the model to respond with, which unlocks many new use cases where you need the model to conform to certain output constraints like following a predefined structure or only outputting specific text. You can read more about JSON schema mode in the Gemini API docs.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Light mode and mobile support</span><br><span class="line">To give developers more flexibility in AI Studio, you can now choose your preferred UI mode (light vs dark) or use your system defaults in the settings pane. We also rolled out our first set of mobile improvements for Google AI Studio to allow you to quickly test multi modal prompts on-the-go.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">As we continue to improve our developer experience, please share your feedback on our Developer Forum. Happy building!</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main</span><br><span class="line">Yuan2.0-M32: Mixture of Experts with Attention Router</span><br><span class="line">=====================================================</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#--yuan20-m32-mixture-of-experts-with-attention-router-)</span><br><span class="line"></span><br><span class="line">👾 [ModelScope](https://www.modelscope.cn/profile/YuanLLM) - 🤗 [Hugging Face](https://huggingface.co/IEITYuan) - 💬 [WeChat](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/images/%E6%BA%90%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BA%8C%E7%BB%B4%E7%A0%81.png)- 📎 [Yuan2.0-M32 Paper](https://arxiv.org/abs/2405.17976)</span><br><span class="line"></span><br><span class="line">[![Code License](https://camo.githubusercontent.com/8a1af7455ed34ab5dd2d316b2518cbc6af01e63bf3bdbd9b7f211a6c349fc139/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f417061636865253230322e302532302d677265656e3f7374796c653d666c6174266c6162656c3d436f64652532304c6963656e7365266c696e6b3d68747470732533412532462532466769746875622e636f6d253246494549542d5975616e2532465975616e2d322e302d4d6f452533467461622533444170616368652d322e302d312d6f762d66696c65) ](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/code_license)[![Model License](https://camo.githubusercontent.com/01772df1697a1100a2d8ec4631d26b2c668433b311bb4adb636f2702771f80d3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5975616e322e302532304c6963656e73652d626c75653f7374796c653d666c6174266c6f676f436f6c6f723d626c7565266c6162656c3d4d6f64656c2532304c6963656e736526636f6c6f723d626c7565266c696e6b3d68747470732533412532462532466769746875622e636f6d253246494549542d5975616e2532465975616e2d322e30253246626c6f622532466d61696e2532464c4943454e53452d5975616e)](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/model_license)</span><br><span class="line"></span><br><span class="line">####</span><br><span class="line"></span><br><span class="line">English | [简体中文](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/README_CN.md)</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#------------english---------%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87----)</span><br><span class="line"></span><br><span class="line">* * * * *</span><br><span class="line"></span><br><span class="line">0\. Latest News 🎉🎉</span><br><span class="line">--------------------</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#0-latest-news-)</span><br><span class="line"></span><br><span class="line">-   [2024-05-28] Yuan2.0-M32 released</span><br><span class="line"></span><br><span class="line">1\. Introduction</span><br><span class="line">----------------</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#1-introduction)</span><br><span class="line"></span><br><span class="line">Yuan2.0-M32 is a Mixture-of-Experts (MoE) language model with 32 experts, of which 2 are active. A new router network, Attention Router, is proposed and has been adopted for more efficient expert selection, boosting accuracy by 3.8% over models using a classical router network. Yuan 2.0-M32 is trained from scratch with 2000B tokens, and its training computation is only 9.25% of that required by a dense model of the same parameter scale. Demonstrating competitive capabilities in coding, math, and various specialized fields, Yuan2.0-M32 operates with only 3.7B active parameters out of a total 40B, and a forward computation of 7.4 GFLOPS per token, which is just 1/19th of Llama3-70B&#x27;s requirement. Yuan 2.0-M32 has surpassed Llama3-70B on the MATH and ARC-Challenge benchmarks, achieving accuracies of 55.9% and 95.8%, respectively. The basic information of the Yuan2.0-M32 model is as follows:</span><br><span class="line"></span><br><span class="line">-   Total Parameters ： 40B</span><br><span class="line"></span><br><span class="line">-   Experts： 32</span><br><span class="line"></span><br><span class="line">-   Active Experts： 2</span><br><span class="line"></span><br><span class="line">-   Active Parameters： 3.7B</span><br><span class="line"></span><br><span class="line">-   Pretrained Tokens： 2000B tokens</span><br><span class="line"></span><br><span class="line">-   Sequence Length： 16K</span><br><span class="line"></span><br><span class="line">The technical report for the Yuan2.0-M32 model has been released, and you can find more detailed technical information and evaluation results by referring to the [paper](https://arxiv.org/abs/2405.17976).</span><br><span class="line"></span><br><span class="line">[![](https://github.com/IEIT-Yuan/Yuan2.0-M32/raw/main/docs/Yuan2.0-M32-Architecture.jpg)](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/Yuan2.0-M32-Architecture.jpg)</span><br><span class="line"></span><br><span class="line">Fig.1: Yuan 2.0-M32 Architecture</span><br><span class="line"></span><br><span class="line">2\. Model Downloads</span><br><span class="line">-------------------</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#2-model-downloads)</span><br><span class="line"></span><br><span class="line">| Model | Sequence Length | Type | Download |</span><br><span class="line">| :-: | :-: | :-: | :-: |</span><br><span class="line">| Yuan2.0-M32 | 16K | Megatron | [ModelScope](https://modelscope.cn/models/YuanLLM/Yuan2-M32/) | [HuggingFace](https://huggingface.co/IEITYuan/Yuan2-M32) | [Netdisk](https://pan.baidu.com/s/1K0LVU5NxeEujtYczF_T-Rg?pwd=cupw) |</span><br><span class="line">| Yuan2.0-M32-HF | 16K | HuggingFace | [ModelScope](https://modelscope.cn/models/YuanLLM/Yuan2-M32-hf) | [HuggingFace](https://huggingface.co/IEITYuan/Yuan2-M32-hf) | [Netdisk](https://pan.baidu.com/s/1FrbVKji7IrhpwABYSIsV-A?pwd=q6uh) |</span><br><span class="line">| Yuan2.0-M32-GGUF | 16K | GGUF | [ModelScope](https://modelscope.cn/models/YuanLLM/Yuan2-M32-gguf/summary) | [HuggingFace](https://huggingface.co/IEITYuan/Yuan2-M32-gguf) | [Netdisk](https://pan.baidu.com/s/1BWQaz-jeZ1Fe69CqYtjS9A?pwd=f4qc) |</span><br><span class="line">| Yuan2.0-M32-GGUF-INT4 | 16K | GGUF | [ModelScope](https://modelscope.cn/models/YuanLLM/Yuan2-M32-gguf-int4/summary) | [HuggingFace](https://huggingface.co/IEITYuan/Yuan2-M32-gguf-int4) | [Netdisk](https://pan.baidu.com/s/1FM8xPpkhOrRcAfe7-zUgWQ?pwd=e6ag) |</span><br><span class="line"></span><br><span class="line">3\. Evaluation</span><br><span class="line">--------------</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#3-evaluation)</span><br><span class="line"></span><br><span class="line">3.1 Benchmarks 🏆</span><br><span class="line"></span><br><span class="line">We conducted a thorough evaluation of the Yuan2.0-M32 model across a range of benchmarks, including HumanEval, GSM8K, MMLU, Math, and ARC-Challenge. These benchmarks are designed to test the model&#x27;s proficiency in key areas such as natural language understanding, knowledge acquisition, mathematical computation and reasoning, and code generation. The Yuan2.0-M32 has shown a consistent and significant advantage over other models like Llama3-8B and Mistral-8×7B, excelling in all evaluated tasks. Remarkably, its overall performance is on par with the more substantial Llama3-70B model.The detailed evaluation results are outlined in the subsequent table.</span><br><span class="line"></span><br><span class="line">-   We provided evaluation scripts for [HumanEval](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/eval_humaneval.md), [GSM8K](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/eval_gsm8k.md), [MMLU](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/eval_mmlu.md), [Math](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/eval_math.md) and [ARC-C](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/eval_arc.md) to support the replication of our evaluation results.</span><br><span class="line"></span><br><span class="line">| Model | HumanEval | GSM8K | MMLU | Math | ARC-C* |</span><br><span class="line">| --- | :-: | :-: | :-: | :-: | :-: |</span><br><span class="line">| Llama3-70B | 81.7% | 93% | 80.3 | 50.4% | 93.3% |</span><br><span class="line">| Llama3-8B | 62.2% | 79.6% | 68.4% | 30% | 78.6% |</span><br><span class="line">| Phi-3-medium | 62.2% | 91.0% | 78.0% | - | 91.6% |</span><br><span class="line">| Phi-3-small | 61% | 89.6% | 75.7% | - | 90.7% |</span><br><span class="line">| Phi-3-mini | 58.5% | 82.5% | 68.8% | - | 84.9% |</span><br><span class="line">| Mistral-8*22B | 45.1% | 78.6% | 77.8% | 41,8% | 91.3% |</span><br><span class="line">| Mistral-8*7B | 40.2% | 58.4% | 70.86% | 28.4% | 85.9% |</span><br><span class="line">| Yuan2.0-M32 | 74.4% | 92.7% | 72.2% | 55.9% | 95.8% |</span><br><span class="line"></span><br><span class="line">* *ARC-C*: AI2 Reasoning Challenge (ARC) benchmark contains more complex parts that need further reasoning.</span><br><span class="line"></span><br><span class="line">* * * * *</span><br><span class="line"></span><br><span class="line">3.2 Computational Utilization for Model</span><br><span class="line"></span><br><span class="line">| Model | Params (B) | Active Params (B) | GFLOPs/token (Inference) | GFLOPS/token (Fine-tune) | Mean Accuracy | Average Accuracy/GFLOPSs per token (Inference) |</span><br><span class="line">| --- | :-: | :-: | :-: | :-: | :-: | :-: |</span><br><span class="line">| Llama3-70B | 70 | 70 | 140 | 420 | 79.25 | 0.57 |</span><br><span class="line">| Llama3-8B | 8 | 8 | 16 | 48 | 64.15 | 4.00 |</span><br><span class="line">| Mistral-8*22B | 141 | 39 | 78 | 234 | 72.38 | 0.93 |</span><br><span class="line">| Mistral-8*7B | 47 | 12.9 | 25.8 | 77.3 | 60.83 | 2.36 |</span><br><span class="line">| Yuan2.0-M32 | 40 | 3.7 | 7.4 | 22.2 | 79.15 | 10.69 |</span><br><span class="line"></span><br><span class="line">4\. Quick Start</span><br><span class="line">---------------</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#4-quick-start)</span><br><span class="line"></span><br><span class="line">4.1 Environment Config</span><br><span class="line"></span><br><span class="line">We strongly recommend using the latest release of docker images of Yuan2.0-M32.You can launch an instance of the Yuan 2.0 container with the following Docker commands:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">4.2 Data Preprocess</span><br><span class="line"></span><br><span class="line">We have provided the data preprocess script. See documentation [here](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/data_process.md).</span><br><span class="line"></span><br><span class="line">4.3 Model Pretrain</span><br><span class="line"></span><br><span class="line">We&#x27;ve provided several scripts for pretraining in the [`example`](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/examples). The details can be seen from documentation [here](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/pretrain.md).</span><br><span class="line"></span><br><span class="line">4.4 Inference Service</span><br><span class="line"></span><br><span class="line">For a detailed deployment plan, please refer to [vllm](https://github.com/IEIT-Yuan/Yuan2.0-M32/edit/main/vllm/README_Yuan_vllm.md).</span><br><span class="line"></span><br><span class="line">5\. Statement of Agreement</span><br><span class="line">--------------------------</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#5-statement-of-agreement)</span><br><span class="line"></span><br><span class="line">The use of the source code in this repository requires compliance with the open source license agreement Apache 2.0. The Yuan2.0 model supports commercial use and does not require authorization. Please understand and comply with the [《Yuan2.0 Model License Agreement》](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/LICENSE-Yuan). Do not use the open source model and code, as well as derivatives generated from open source projects, for any purposes that may cause harm to the country and society, or for any services that have not undergone security assessment and filing. Although we have taken measures to ensure the compliance and accuracy of the data during training, the model has a huge number of parameters and is affected by probability and randomness factors. We cannot guarantee the accuracy of the output content, and the model is easily misled by input instructions. This project does not assume any data security, public opinion risks, or any model misleading, abusing, spreading caused by open-source models and code Risks and responsibilities arising from improper utilization You will be solely responsible for the risks and consequences arising from the use, copying, distribution, and modification of the model in this open source project</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://research.google/blog/codeclm-aligning-language-models-with-tailored-synthetic-data/</span><br><span class="line">Blog</span><br><span class="line">CodecLM: Aligning language models with tailored synthetic data</span><br><span class="line">May 30, 2024</span><br><span class="line"></span><br><span class="line">Zifeng Wang and Chen-Yu Lee, Research Scientists, Cloud AI Research Team</span><br><span class="line"></span><br><span class="line">We propose CodecLM, an end-to-end data synthesis framework that tailors high-quality data to align LLMs for different downstream tasks without human annotation.</span><br><span class="line"></span><br><span class="line">Instruction tuning is a critical step in LLM alignment, i.e., shaping the behavior of large language models (LLMs) to better align with the intended objective. It involves fine-tuning a pre-trained LLM on a varied set of instructions, each paired with a desired output. This process enables the model to generalize across various tasks and formats, ultimately improving its performance in understanding and responding to user instructions. In essence, instruction tuning empowers LLMs to follow instructions more effectively, thereby making them more useful and reliable tools for a wide range of applications. Recent progress in instruction tuning highlights the critical role of high-quality data in enhancing LLMs&#x27; instruction-following capabilities. However, acquiring such data through human annotation remains cost-prohibitive and difficult to scale, hindering further progress.</span><br><span class="line"></span><br><span class="line">Alternatively, recent work explores synthesizing instruction–response pairs for LLM alignment by prompting models with example data and iteratively refining the results. While these methods are effective at generating varied instructions for LLM alignment broadly, real-world applications often prioritize tailoring the LLM to specific downstream tasks such as individual enterprise applications or personal assistant agents, which often involve different instruction distributions. This need for task-specific alignment brings us to a core question for data synthesis: how can we tailor synthetic data to align LLMs for different instruction-following tasks?</span><br><span class="line"></span><br><span class="line">In “CodecLM: Aligning Language Models with Tailored Synthetic Data”, presented at NAACL 2024, we present a novel framework, CodecLM, that systematically generates tailored high-quality data to align LLMs for specific downstream tasks. Inspired by the principles of the encode-decode process, we leverage a strong LLM (i.e., an LLM that has strong instruction-following capability for data synthesis, such as Gemini Pro or text-unicorn) as a codec, to encode seed instructions from our target task into instruction metadata (keywords that capture the use case of the instruction, and the skills required for an LLM to respond to the instruction). We then decode the metadata into tailored synthetic instructions. In the decoding process, we propose two complementary strategies, Self-Rubrics and Contrastive Filtering, to enhance synthetic data quality. Self-Rubrics leverages the strong LLM to generate rubrics and actions to make synthetic instruction more challenging. Contrastive Filtering further selects the instructions to which the target LLM (the LLM to be aligned) fails to respond well. CodecLM achieves state-of-the-art performance on open-domain instruction-following benchmarks with various LLMs, demonstrating its effectiveness in LLM alignment for varied instruction distributions.</span><br><span class="line"></span><br><span class="line">CodecLM1-Hero</span><br><span class="line">Overview of CodecLM. We first encode seed instructions into metadata to capture the underlying distribution of instructions. This metadata is then decoded through two complementary strategies, Self-Rubrics and Contrastive Filtering, to tailor high-quality synthetic instructions that are aligned with the target instruction distribution. Intermediate instructions and responses are omitted in the figure for clarity.</span><br><span class="line"></span><br><span class="line">CodecLM</span><br><span class="line">The core idea of CodecLM is to customize synthetic data for different downstream tasks, which can then be used to fine-tune an LLM for the tasks of interest. To achieve this goal, we need to make sure 1) the synthetic data’s distribution is similar to that of the real downstream data, and 2) the quality of synthetic data is high enough to improve the target LLM to be tuned.</span><br><span class="line"></span><br><span class="line">First, the strong LLM encodes the seed instruction into instruction metadata, specifying its use case and skills required for responses. Next, the strong LLM decodes metadata into basic instructions. Meanwhile, Self-Rubrics (more below) leverages the strong LLM to generate rubrics and actions to improve the basic instruction, tailoring them for the downstream task. Finally, Contrastive Filtering (more below) uses a scoring function to compare answers from both the strong and target LLMs. The most effective pairs are selected for aligning the LLM, while less effective instructions are sent for further improvement. Animated below, the strong LLM&#x27;s (refined) answer is winning against the target LLM&#x27;s (simplistic) answer, indicating the improved synthetic instruction is challenging enough for the target LLM. Hence, we select the corresponding pair for instruction tuning the target LLM.</span><br><span class="line"></span><br><span class="line">Detailed workflow of CodecLM.</span><br><span class="line"></span><br><span class="line">Encoding instructions via metadata</span><br><span class="line">To capture the underlying instruction distribution from the downstream task, we extract a word-level abstraction of the input instruction distribution through instruction metadata. We define the metadata as encompassing two key aspects: use case and skills. The use case describes the intended task (e.g., question answering or creative writing), while the skills are the knowledge the LLM must have to successfully respond to the given instruction (e.g., algorithms or communication). With the metadata from the seed instruction, we can readily prompt the strong LLM to generate synthetic instructions based on the extracted metadata.</span><br><span class="line"></span><br><span class="line">Tailoring instructions via Self-Rubrics</span><br><span class="line">With the above method, however, the quality of the synthetic instructions generated by simply prompting the LLM with the metadata may not be high. A recent study found that tuning LLMs with more complex instructions can improve performance, indicating that complex instructions are often considered high quality. A common practice is to work with human experts to craft general guidance to complicate instructions, such as “add reasoning steps” (more below). However, this strategy falls short for tailoring guidance to different tasks, like solving calculus problems versus writing news articles. Therefore, we introduce Self-Rubrics, which leverages the strong LLM to tailor instructions by adjusting their complexity according to the extracted metadata.</span><br><span class="line"></span><br><span class="line">Self-Rubrics first guides the LLM to generate distinct rubrics for assessing the instruction complexity of each metadatum. Then, informed by these rubrics, the LLM generates a corresponding set of actions to enhance the instruction’s complexity. Such actions generated by Self-Rubrics are domain-specific and unambiguous — for example, for the use case of “business plan development” and skills of “market research and planning”, generic rules like “add reasoning steps” are vague. On the contrary, Self-Rubrics is able to generate actions like “add SWOT analysis” and “include comparison with market competitors” to complicate the instruction. With these instructions, one can iteratively prompt the strong LLM to tailor higher quality instructions.</span><br><span class="line"></span><br><span class="line">Selecting instructions via Contrastive Filtering</span><br><span class="line">While Self-Rubrics tailors complex instructions based on instruction metadata, not all instructions, regardless of their complexity, are equally effective for instruction tuning. Intuitively, identifying instructions an LLM finds challenging can expose opportunities for improvement. We therefore introduce Contrastive Filtering, a method to select the instructions that can enhance the target LLM.</span><br><span class="line"></span><br><span class="line">Given an input instruction, we obtain two responses from the strong LLM (the one used for data synthesis) and the target LLM (the one we target for tuning), respectively. We then measure the quality gap between the two responses using LLM-as-a-Judge: we prompt the strong LLM to generate numerical scores (e.g., from 1 to 10) reflecting each response’s quality, and define the absolute difference between two scores as the quality gap. Intuitively, a larger gap often means the target LLM produces a worse response than the strong LLM. In this case, we add the instruction and the higher-scoring response to our final pool of high-quality synthetic data. On the other hand, a smaller quality gap indicates that such instructions are unlikely to improve performance. We then save such instructions for the next iteration of Self-Rubrics for further improvement.</span><br><span class="line"></span><br><span class="line">Effectiveness of CodecLM</span><br><span class="line">We demonstrate the effectiveness of CodecLM with PaLM 2 LLMs. In particular, we use text-unicorn as the strong LLM for data synthesis, and text-bison as the target LLM for instruction tuning. We conduct experiments on multiple widely-used open domain instruction-following benchmarks, which contain instructions for various forms and complexities of task types to test LLMs’ instruction-following ability. Here we focus on the results on the Vicuna (Benchmark 1) and Evol-Instruct (Benchmark 2) test sets. We compare CodecLM with representative baselines, including Alpagasus and WizardLM+ (an enhanced version of WizardLM). Inspired by the LLM-as-a-Judge approach, we conduct LLM-based pairwise comparisons between the instruction-tuned target LLM and the strong LLM to measure how much capacity the target LLM recovers from the strong LLM. We name this metric capacity recovery ratio (CRR), where 100% CRR means the tuned target LLM performs as good as the strong LLM on the specific test set.</span><br><span class="line"></span><br><span class="line">Consistently better performance</span><br><span class="line">CodecLM outperforms comparable methods consistently on all benchmarks, highlighting its generalizability to different downstream instruction distributions. Note that common data synthesis approaches do not take the downstream instruction distribution into account, while CodecLM is able to tailor instructions for different downstream tasks, thanks to the synergy between instruction metadata, Self-Rubrics and Contrastive Filtering. Our paper has more results and in-depth analysis.</span><br><span class="line"></span><br><span class="line">CodecLM3-Results</span><br><span class="line">Results with PaLM 2–based target models on two open-domain instruction-following benchmarks. Each method trains a target model with synthetic data based on text-bison, and compares against the strong model, text-unicorn. Larger CRR means better performance.</span><br><span class="line"></span><br><span class="line">Conclusion</span><br><span class="line">Our proposed CodecLM is able to generate synthetic instruction-tuning data that is tailored to specific domains. We show that CodecLM effectively captures the underlying instruction distribution via instruction metadata, and further tailors the most effective instruction-response pairs through the novel strategies of Self-Rubrics and Contrastive Filtering. CodecLM provides a potent solution towards adapting LLMs for customized uses, without the necessity of human annotation. We believe CodecLM serves as a general framework for targeted LLM alignment, which opens the door to multiple promising research directions within the framework, such as richer metadata definition, better prompt design, and more reliable LLM-based scorers.</span><br><span class="line"></span><br><span class="line">Acknowledgments</span><br><span class="line">This research was conducted by Zifeng Wang, Chun-Liang Li, Vincent Perot, Long T. Le, Jin Miao, Zizhao Zhang, Chen-Yu Lee, Tomas Pfister. Thanks to Chih-Kuan Yeh and Sergey Ioffe for their valuable feedback.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/introducing-chatgpt-edu/</span><br><span class="line">May 30, 2024</span><br><span class="line"></span><br><span class="line">Introducing ChatGPT Edu</span><br><span class="line">An affordable offering for universities to responsibly bring AI to campus.</span><br><span class="line"></span><br><span class="line">An abstract expressionist painting of a desk and chair near a window in a warm color palette.</span><br><span class="line">We&#x27;re announcing ChatGPT Edu, a version of ChatGPT built for universities to responsibly deploy AI to students, faculty, researchers, and campus operations. Powered by GPT-4o, ChatGPT Edu can reason across text and vision and use advanced tools such as data analysis. This new offering includes enterprise-level security and controls and is affordable for educational institutions.</span><br><span class="line"></span><br><span class="line">We built ChatGPT Edu because we saw the success universities like the University of Oxford, Wharton School of the University of Pennsylvania(opens in a new window), University of Texas at Austin, Arizona State University(opens in a new window), and Columbia University in the City of New York were having with ChatGPT Enterprise.</span><br><span class="line"></span><br><span class="line">How campuses use ChatGPT today</span><br><span class="line">ChatGPT can help with various tasks across campus, such as providing personalized tutoring for students and reviewing their resumes, helping researchers write grant applications, and assisting faculty with grading and feedback. Our university partners have found innovative ways to make AI accessible to students, faculty, researchers, and campus operations. A few examples include:</span><br><span class="line"></span><br><span class="line">Professor Nabila El-Bassel at Columbia University is leading an initiative to integrate AI into community-based strategies to reduce overdose fatalities(opens in a new window). Her team built a GPT that analyzes and synthesizes large datasets to inform interventions, reducing weeks of research work into seconds.</span><br><span class="line"></span><br><span class="line">Undergraduates and MBA students in Professor Ethan Mollick’s courses at Wharton completed their final reflection assignments through discussions with a GPT trained on course materials, reporting that ChatGPT got them to think more deeply about what they’ve learned.</span><br><span class="line"></span><br><span class="line">Christiane Reves, an assistant professor at Arizona State University, is developing a custom Language Buddies GPT for students(opens in a new window) to engage in German conversations suited to their language level while receiving tailored feedback. The GPT will help students build communication skills and save faculty time on assessments.</span><br><span class="line"></span><br><span class="line">Bringing AI into the new school year</span><br><span class="line">To build on these applications, we designed ChatGPT Edu as an accessible option for universities to bring AI to their campuses at scale.</span><br><span class="line"></span><br><span class="line">ChatGPT Edu includes:</span><br><span class="line"></span><br><span class="line">Access to GPT-4o, our flagship model, excelling in text interpretation, coding, and mathematics</span><br><span class="line"></span><br><span class="line">Advanced capabilities such as data analytics, web browsing, and document summarization</span><br><span class="line"></span><br><span class="line">The ability to build GPTs, custom versions of ChatGPT, and share them within university workspaces</span><br><span class="line"></span><br><span class="line">Significantly higher message limits than the free version of ChatGPT</span><br><span class="line"></span><br><span class="line">Improved language capabilities across quality and speed, with over 50 languages supported</span><br><span class="line"></span><br><span class="line">Robust security, data privacy, and administrative controls such as group permissions, SSO, SCIM 1, and GPT management</span><br><span class="line"></span><br><span class="line">Conversations and data are not used to train OpenAI models</span><br><span class="line"></span><br><span class="line">“Integrating OpenAI&#x27;s technology into our educational and operational frameworks accelerates transformation at ASU. We&#x27;re collaborating across our community to harness these tools, extending our learnings as a scalable model for other institutions.”</span><br><span class="line">—Kyle Bowen, Deputy CIO at Arizona State University</span><br><span class="line">ChatGPT Edu is designed for schools that want to deploy AI more broadly to students and their campus communities. Contact our team to learn more.</span><br><span class="line"></span><br><span class="line">GPT-4o</span><br><span class="line">Announcements</span><br><span class="line">Footnotes</span><br><span class="line">1Coming soon to ChatGPT Edu and ChatGPT Enterprise</span><br><span class="line"></span><br><span class="line">Author</span><br><span class="line">OpenAI</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.anthropic.com/news/tool-use-ga</span><br><span class="line">Claude can now use tools</span><br><span class="line">2024년 5월 31일</span><br><span class="line">●</span><br><span class="line">3 min read</span><br><span class="line">Illustration of Claude using tools</span><br><span class="line">Tool use, which enables Claude to interact with external tools and APIs, is now generally available across the entire Claude 3 model family on the Anthropic Messages API, Amazon Bedrock, and Google Cloud&#x27;s Vertex AI. With tool use, Claude can perform tasks, manipulate data, and provide more dynamic—and accurate—responses.</span><br><span class="line"></span><br><span class="line">Tool use</span><br><span class="line">Define a toolset for Claude and specify your request in natural language. Claude will then select the appropriate tool to fulfill the task and, when appropriate, execute the corresponding action:</span><br><span class="line"></span><br><span class="line">Extract structured data from unstructured text: Pull names, dates, and amounts from invoices to reduce manual data entry.</span><br><span class="line">Convert natural language requests into structured API calls: Enable teams to self-serve common actions (e.g., &quot;cancel subscription&quot;) with simple commands.</span><br><span class="line">Answer questions by searching databases or using web APIs: Provide instant, accurate responses to customer inquiries in support chatbots.</span><br><span class="line">Automate simple tasks through software APIs: Save time and minimize errors in data entry or file management.</span><br><span class="line">Orchestrate multiple fast Claude subagents for granular tasks: Automatically find the optimal meeting time based on attendee availability.</span><br><span class="line"></span><br><span class="line">Improved developer experience</span><br><span class="line">To make it easier to leverage the intelligence of the Claude 3 models with tools, we’ve also built in features that help developers further customize the end-user experience.</span><br><span class="line"></span><br><span class="line">Tool use with streaming reduces wait times to create more engaging interactions: Streaming enables real-time responses in applications like customer support chatbots for smoother, more natural conversations.</span><br><span class="line">Forced tool use allows developers to instruct Claude on tool selection: Developers can specify which tools Claude should use or leave the choice with Claude, helping create more targeted and efficient applications.</span><br><span class="line">Tools also work with images: Claude can incorporate image inputs in live applications.</span><br><span class="line">During our beta many developers used Opus to build sophisticated user-facing assistants. To further enhance this experience, Opus will now include &lt;thinking&gt; tags in its outputs, clarifying Claude’s reasoning and simplifying the debugging process for developers. Our Claude 3 models are currently unable to support parallel tool calls.</span><br><span class="line"></span><br><span class="line">Customer spotlight: StudyFetch</span><br><span class="line">AI-native learning platform StudyFetch uses Claude&#x27;s tool use capabilities to power its personalized AI tutor, Spark.E. By integrating tools to track student progress, navigate course materials and lectures, and create interactive user interfaces, StudyFetch has created a more engaging educational environment for students globally.</span><br><span class="line"></span><br><span class="line">&quot;Claude with tool use is accurate and cost-effective, and now powers our live voice-enabled AI tutoring sessions. Within just a few days, we integrated tools into our platform,” said Ryan Trattner, CTO and Co-Founder at StudyFetch. “As a result, our AI tutor, Spark.E, acts agentically—displaying interactive UIs, tracking student progress in context, and navigating through lectures and materials. Since implementing Claude with tool use, we&#x27;ve observed a 42% increase in positive human feedback.&quot;</span><br><span class="line"></span><br><span class="line">Customer spotlight: Intuned</span><br><span class="line">Intuned, the browser automation platform, uses Claude to power data extraction within their cloud platform. With AI-powered data extraction, Intuned is able to drastically improve the developer experience in building and executing more reliable browser automations.</span><br><span class="line"></span><br><span class="line">&quot;Claude 3 Haiku with tool use has been a game changer for us. After accessing the model and running our benchmarks on it, we realized the quality, speed, and price combination is unmatched,” said Faisal Ilaiwi, Co-Founder at Intuned. “Haiku is helping us scale our customers&#x27; data extraction tasks to a completely new level.&quot;</span><br><span class="line"></span><br><span class="line">Customer spotlight: Hebbia</span><br><span class="line">Hebbia is building the AI knowledge worker for leading financial and legal services firms. They use Claude 3 Haiku to help power several complex, multi-step customer workflows.</span><br><span class="line"></span><br><span class="line">&quot;We leverage Claude 3 Haiku for generating live suggestions, automating prompt writing, and extracting key metadata from long documents,” shared Divya Mehta, Product Manager at Hebbia. “Claude 3 Haiku&#x27;s tool use feature has unlocked capabilities and speed for our platform to generate reliable suggestions and prompts in real-time.&quot;</span><br><span class="line"></span><br><span class="line">Get started</span><br><span class="line">You can get started with tool use today on the Anthropic Messages API, Amazon Bedrock, and Google Cloud&#x27;s Vertex AI. To learn more, explore our documentation and Anthropic Cookbooks on tool use.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://tenvence.github.io/p/v-express/</span><br><span class="line">V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation</span><br><span class="line">Cong Wang1*, Kuan Tian2*, Jun Zhang2†, Yonghang Guan2, Feng Luo2, Fei Shen2,</span><br><span class="line">Zhiwei Jiang1†, Qing Gu1, Xiao Han2, Wei Yang2</span><br><span class="line">1 Nanjing University, 2 Tencent AI Lab</span><br><span class="line">* Equal Contribution, † Corresponding Authors</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Abstract</span><br><span class="line">In the field of portrait video generation, the use of single images to generate portrait videos has become increasingly prevalent. A common approach involves leveraging generative models to enhance adapters for controlled generation. However, control signals can vary in strength, including text, audio, image reference, pose, depth map, etc. Among these, weaker conditions often struggle to be effective due to interference from stronger conditions, posing a challenge in balancing these conditions. In our work on portrait video generation, we identified audio signals as particularly weak, often overshadowed by stronger signals such as pose and original image. However, direct training with weak signals often leads to difficulties in convergence. To address this, we propose V-Express, a simple method that balances different control signals through a series of progressive drop operations. Our method gradually enables effective control by weak conditions, thereby achieving generation capabilities that simultaneously take into account pose, input image, and audio. The experimental results demonstrate that our method can effectively generate portrait videos controlled by audio. Furthermore, our method provides a potential solution for the simultaneous and effective use of conditions of varying strengths.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.youtube.com/watch?v=AzqKLiPQD6g&amp;ab_channel=jarkkomoilanen</span><br><span class="line">figma - Automation powered by GPT-4o generates Figma designs based on PRD.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-05-31</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/05/31/2024-5-31-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 5월 31일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/06/03/2024-6-3-AI-NEWS/" title="2024년 6월 3일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/05/30/2024-5-30-AI-NEWS/" title="2024년 5월 30일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>