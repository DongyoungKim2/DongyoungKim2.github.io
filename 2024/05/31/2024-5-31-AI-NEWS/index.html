<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 5ì›” 31ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="ìš”ì•½ì˜¤ëŠ˜ì˜ AI ì†Œì‹ì€ ë‹¤ì–‘í•œ AI ê¸°ìˆ ê³¼ ê·¸ ì ìš©ì— ê´€í•œ ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. OpenAIëŠ” ìµœê·¼ ì¸ê³µì§€ëŠ¥ì„ ì´ìš©í•œ ë¹„ë°€ ì˜í–¥ ì‘ì „ì„ ë°©í•´í•œ ë‚´ìš©ì„ ë°œí‘œí–ˆê³ , Googleì€ Gemini 1.5 Proì™€ 1.5 Flash ëª¨ë¸ì˜ ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì†Œê°œí–ˆìŠµë‹ˆë‹¤. ë˜í•œ, "><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 5ì›” 31ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><h2 id="ìš”ì•½"><a href="#ìš”ì•½" class="headerlink" title="ìš”ì•½"></a>ìš”ì•½</h2><p>ì˜¤ëŠ˜ì˜ AI ì†Œì‹ì€ ë‹¤ì–‘í•œ AI ê¸°ìˆ ê³¼ ê·¸ ì ìš©ì— ê´€í•œ ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. OpenAIëŠ” ìµœê·¼ ì¸ê³µì§€ëŠ¥ì„ ì´ìš©í•œ ë¹„ë°€ ì˜í–¥ ì‘ì „ì„ ë°©í•´í•œ ë‚´ìš©ì„ ë°œí‘œí–ˆê³ , Googleì€ Gemini 1.5 Proì™€ 1.5 Flash ëª¨ë¸ì˜ ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì†Œê°œí–ˆìŠµë‹ˆë‹¤. ë˜í•œ, IEIT-Yuanì€ Yuan2.0-M32 ëª¨ë¸ì„ ê³µê°œí–ˆìœ¼ë©°, Googleì€ CodecLMì„ í†µí•´ ë§ì¶¤í˜• í•©ì„± ë°ì´í„°ë¥¼ í™œìš©í•œ ì–¸ì–´ ëª¨ë¸ ì •ë ¬ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. OpenAIëŠ” êµìœ¡ ê¸°ê´€ì„ ìœ„í•œ ChatGPT Eduë¥¼ ë„ì…í–ˆê³ , Anthropicì€ Claude ëª¨ë¸ì˜ íˆ´ ì‚¬ìš© ê¸°ëŠ¥ì„ ì¼ë°˜ì— ê³µê°œí–ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, Tencent AI Labì€ V-Express ë°©ë²•ì„ ì´ìš©í•œ ì´ˆìƒí™” ë¹„ë””ì˜¤ ìƒì„±ì— ëŒ€í•œ ì—°êµ¬ë¥¼ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.</p>
<h2 id="OpenAIê°€-ë°œí‘œí•œ-ë¹„ë°€-ì˜í–¥-ì‘ì „-ë°©í•´"><a href="#OpenAIê°€-ë°œí‘œí•œ-ë¹„ë°€-ì˜í–¥-ì‘ì „-ë°©í•´" class="headerlink" title="OpenAIê°€ ë°œí‘œí•œ ë¹„ë°€ ì˜í–¥ ì‘ì „ ë°©í•´"></a>OpenAIê°€ ë°œí‘œí•œ ë¹„ë°€ ì˜í–¥ ì‘ì „ ë°©í•´</h2><p><a target="_blank" rel="noopener" href="https://openai.com/index/disrupting-deceptive-uses-of-AI-by-covert-influence-operations/">ë§í¬</a> | 2024ë…„ 5ì›” 30ì¼</p>
<ul>
<li>OpenAIëŠ” ë¹„ë°€ ì˜í–¥ ì‘ì „ì— ì´ìš©ëœ AI ëª¨ë¸ì„ ì°¨ë‹¨í–ˆë‹¤ê³  ë°œí‘œ</li>
<li>ì§€ë‚œ 3ê°œì›” ë™ì•ˆ ë‹¤ì„¯ ê°œì˜ ë¹„ë°€ ì‘ì „ì´ ì°¨ë‹¨ë¨</li>
<li>ì´ ì‘ì „ë“¤ì€ ì£¼ë¡œ ëŸ¬ì‹œì•„, ì¤‘êµ­, ì´ë€, ì´ìŠ¤ë¼ì—˜ì—ì„œ ë°œìƒí–ˆìœ¼ë©°, ë‹¤ì–‘í•œ ì–¸ì–´ë¡œ ì‘ì„±ëœ ì½˜í…ì¸  ìƒì„± ë° ì†Œì…œ ë¯¸ë””ì–´ í™œë™ì„ í¬í•¨</li>
<li>ì´ëŸ¬í•œ ì‘ì „ì˜ ì£¼ì œëŠ” ëŸ¬ì‹œì•„ì˜ ìš°í¬ë¼ì´ë‚˜ ì¹¨ê³µ, ê°€ìì§€êµ¬ ë¶„ìŸ, ì¸ë„ ì„ ê±° ë“± ì •ì¹˜ì  ì´ìŠˆ í¬í•¨</li>
<li>OpenAIëŠ” ê³µê²©ìë“¤ì˜ í™œë™ì´ ìì‚¬ ì„œë¹„ìŠ¤ë¡œ ì¸í•´ ì‹¤ì§ˆì ìœ¼ë¡œ ì¦ê°€í•˜ì§€ ì•Šì•˜ë‹¤ê³  ë³´ê³ </li>
<li>AI ëª¨ë¸ì˜ ì•ˆì „ ì„¤ê³„ì™€ AI ë„êµ¬ì˜ íš¨ìœ¨ì„± ë•ë¶„ì— ì´ëŸ¬í•œ ì‘ì „ì„ ë°©í•´í•  ìˆ˜ ìˆì—ˆìŒ</li>
</ul>
<h2 id="Googleì˜-Gemini-1-5-Proì™€-1-5-Flash-ëª¨ë¸-ë°œí‘œ"><a href="#Googleì˜-Gemini-1-5-Proì™€-1-5-Flash-ëª¨ë¸-ë°œí‘œ" class="headerlink" title="Googleì˜ Gemini 1.5 Proì™€ 1.5 Flash ëª¨ë¸ ë°œí‘œ"></a>Googleì˜ Gemini 1.5 Proì™€ 1.5 Flash ëª¨ë¸ ë°œí‘œ</h2><p><a target="_blank" rel="noopener" href="https://developers.googleblog.com/en/gemini-15-pro-and-15-flash-now-available/">ë§í¬</a> | 2024ë…„ 5ì›” 30ì¼</p>
<ul>
<li>Gemini 1.5 Proì™€ 1.5 Flash ëª¨ë¸ì˜ ì•ˆì •ì  ì¶œì‹œ ë° ìš”ê¸ˆì œ ë°œí‘œ</li>
<li>1.5 Flash ëª¨ë¸ì€ ë¹ ë¥¸ ì†ë„ì™€ ë¹„ìš© íš¨ìœ¨ì„±ì„ ê°•ì¡°í•˜ë©°, ìš”ì²­ ì œí•œì„ ì¦ê°€ì‹œì¼œ 1ë¶„ë‹¹ 1000ê°œì˜ ìš”ì²­ ì²˜ë¦¬ ê°€ëŠ¥</li>
<li>1.5 Flash ëª¨ë¸ì€ ë§ì¶¤í˜• íŠœë‹ ì§€ì›, JSON ìŠ¤í‚¤ë§ˆ ëª¨ë“œ, ëª¨ë°”ì¼ ì§€ì› ë° ë¼ì´íŠ¸ ëª¨ë“œ ì œê³µ</li>
<li>Google AI Studioì—ì„œ ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ìš”ê¸ˆì œë¥¼ í™œì„±í™”í•˜ë©´ ë” ë†’ì€ API í•œë„ ì´ìš© ê°€ëŠ¥</li>
</ul>
<h2 id="IEIT-Yuanì˜-Yuan2-0-M32-ëª¨ë¸-ê³µê°œ"><a href="#IEIT-Yuanì˜-Yuan2-0-M32-ëª¨ë¸-ê³µê°œ" class="headerlink" title="IEIT-Yuanì˜ Yuan2.0-M32 ëª¨ë¸ ê³µê°œ"></a>IEIT-Yuanì˜ Yuan2.0-M32 ëª¨ë¸ ê³µê°œ</h2><p><a target="_blank" rel="noopener" href="https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main">ë§í¬</a> | 2024ë…„ 5ì›” 28ì¼</p>
<ul>
<li>Yuan2.0-M32 ëª¨ë¸ì€ 32ëª…ì˜ ì „ë¬¸ê°€ ì¤‘ 2ëª…ì´ í™œì„±í™”ëœ Mixture-of-Experts(MoE) ì–¸ì–´ ëª¨ë¸</li>
<li>ìƒˆë¡œìš´ Attention Router ë„¤íŠ¸ì›Œí¬ë¥¼ ë„ì…í•˜ì—¬ íš¨ìœ¨ì ì¸ ì „ë¬¸ê°€ ì„ íƒ ê°€ëŠ¥</li>
<li>2000ì–µ ê°œì˜ í† í°ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìœ¼ë©°, ë™ì¼í•œ ê·œëª¨ì˜ ë°€ì§‘ ëª¨ë¸ì— ë¹„í•´ 9.25%ì˜ ê³„ì‚°ë§Œ í•„ìš”</li>
<li>MATH ë° ARC-Challenge ë²¤ì¹˜ë§ˆí¬ì—ì„œ Llama3-70B ëª¨ë¸ì„ ëŠ¥ê°€</li>
<li>16Kì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ì™€ 40ì–µ ê°œì˜ ì´ íŒŒë¼ë¯¸í„° ë³´ìœ </li>
</ul>
<h2 id="Googleì˜-CodecLM-ë°œí‘œ"><a href="#Googleì˜-CodecLM-ë°œí‘œ" class="headerlink" title="Googleì˜ CodecLM ë°œí‘œ"></a>Googleì˜ CodecLM ë°œí‘œ</h2><p><a target="_blank" rel="noopener" href="https://research.google/blog/codeclm-aligning-language-models-with-tailored-synthetic-data/">ë§í¬</a> | 2024ë…„ 5ì›” 30ì¼</p>
<ul>
<li>CodecLMì€ ê³ í’ˆì§ˆ í•©ì„± ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ LLM(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)ì„ íŠ¹ì • ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ë§ê²Œ ì •ë ¬í•˜ëŠ” í”„ë ˆì„ì›Œí¬</li>
<li>Self-Rubrics ë° Contrastive Filteringì„ í†µí•´ í•©ì„± ë°ì´í„°ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ì „ëµ ë„ì…</li>
<li>PaLM 2 LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ê³µê°œ ë„ë©”ì¸ ì§€ì¹¨-ë”°ë¥´ê¸° ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì„±ëŠ¥ ì…ì¦</li>
<li>ë§ì¶¤í˜• í•©ì„± ë°ì´í„°ë¥¼ ìƒì„±í•˜ì—¬ LLMì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚´</li>
</ul>
<h2 id="OpenAIì˜-ChatGPT-Edu-ë°œí‘œ"><a href="#OpenAIì˜-ChatGPT-Edu-ë°œí‘œ" class="headerlink" title="OpenAIì˜ ChatGPT Edu ë°œí‘œ"></a>OpenAIì˜ ChatGPT Edu ë°œí‘œ</h2><p><a target="_blank" rel="noopener" href="https://openai.com/index/introducing-chatgpt-edu/">ë§í¬</a> | 2024ë…„ 5ì›” 30ì¼</p>
<ul>
<li>ëŒ€í•™ì„ ìœ„í•œ ChatGPT Edu ì¶œì‹œ, GPT-4o ëª¨ë¸ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë° ë¹„ì „ ë¶„ì„ ê°€ëŠ¥</li>
<li>ë°ì´í„° ë¶„ì„, ì›¹ ë¸Œë¼ìš°ì§•, ë¬¸ì„œ ìš”ì•½ ë“± ê³ ê¸‰ ê¸°ëŠ¥ í¬í•¨</li>
<li>ê°•ë ¥í•œ ë³´ì•ˆ, ë°ì´í„° í”„ë¼ì´ë²„ì‹œ ë° ê´€ë¦¬ ì œì–´ ì œê³µ</li>
<li>ëŒ€í•™ì—ì„œ AIë¥¼ í•™ìƒ, êµì§ì› ë° ì—°êµ¬ì§„ì—ê²Œ í™•ì‚°ì‹œí‚¤ê¸° ìœ„í•œ ê²½ì œì ì¸ ì˜µì…˜</li>
</ul>
<h2 id="Anthropicì˜-Claude-ëª¨ë¸-íˆ´-ì‚¬ìš©-ê¸°ëŠ¥-ì¼ë°˜-ê³µê°œ"><a href="#Anthropicì˜-Claude-ëª¨ë¸-íˆ´-ì‚¬ìš©-ê¸°ëŠ¥-ì¼ë°˜-ê³µê°œ" class="headerlink" title="Anthropicì˜ Claude ëª¨ë¸ íˆ´ ì‚¬ìš© ê¸°ëŠ¥ ì¼ë°˜ ê³µê°œ"></a>Anthropicì˜ Claude ëª¨ë¸ íˆ´ ì‚¬ìš© ê¸°ëŠ¥ ì¼ë°˜ ê³µê°œ</h2><p><a target="_blank" rel="noopener" href="https://www.anthropic.com/news/tool-use-ga">ë§í¬</a> | 2024ë…„ 5ì›” 31ì¼</p>
<ul>
<li>Claude 3 ëª¨ë¸ íŒ¨ë°€ë¦¬ì—ì„œ ì™¸ë¶€ íˆ´ ë° APIì™€ ìƒí˜¸ì‘ìš© ê°€ëŠ¥</li>
<li>êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ, ìì—°ì–´ ìš”ì²­ì„ êµ¬ì¡°í™”ëœ API í˜¸ì¶œë¡œ ë³€í™˜, ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ë“±ì„ í†µí•´ ì •í™•í•œ ë‹µë³€ ì œê³µ</li>
<li>ìŠ¤íŠ¸ë¦¬ë°ì„ í†µí•œ ì‹¤ì‹œê°„ ì‘ë‹µ, ì´ë¯¸ì§€ ì…ë ¥ í™œìš© ê°€ëŠ¥</li>
<li>ê³ ê° ì‚¬ë¡€: StudyFetch, Intuned, Hebbiaê°€ Claudeì˜ íˆ´ ì‚¬ìš© ê¸°ëŠ¥ì„ í†µí•´ AI í•™ìŠµ í”Œë«í¼ ë° ë°ì´í„° ì¶”ì¶œ ê¸°ëŠ¥ í–¥ìƒ</li>
</ul>
<h2 id="Tencent-AI-Labì˜-V-Express-ì—°êµ¬-ë°œí‘œ"><a href="#Tencent-AI-Labì˜-V-Express-ì—°êµ¬-ë°œí‘œ" class="headerlink" title="Tencent AI Labì˜ V-Express ì—°êµ¬ ë°œí‘œ"></a>Tencent AI Labì˜ V-Express ì—°êµ¬ ë°œí‘œ</h2><p><a target="_blank" rel="noopener" href="https://tenvence.github.io/p/v-express/">ë§í¬</a> | 2024ë…„ 5ì›” 28ì¼</p>
<ul>
<li>V-ExpressëŠ” ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ˆìƒí™” ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆ</li>
<li>ë‹¤ì–‘í•œ ê°•ë„ì˜ ì œì–´ ì‹ í˜¸(í…ìŠ¤íŠ¸, ì˜¤ë””ì˜¤, ì´ë¯¸ì§€ ì°¸ì¡°, í¬ì¦ˆ ë“±)ë¥¼ ê· í˜• ìˆê²Œ ì²˜ë¦¬</li>
<li>ì˜¤ë””ì˜¤ ì‹ í˜¸ì˜ íš¨ê³¼ì ì¸ ì œì–´ë¥¼ ìœ„í•´ ì ì§„ì  ë“œë¡­ ë°©ë²• ì‚¬ìš©</li>
<li>ì˜¤ë””ì˜¤ ì‹ í˜¸ì— ì˜í•´ ì œì–´ë˜ëŠ” ì´ˆìƒí™” ë¹„ë””ì˜¤ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ìƒì„±</li>
</ul>
<h2 id="Figmaì˜-GPT-4oë¥¼-ì´ìš©í•œ-ìë™í™”"><a href="#Figmaì˜-GPT-4oë¥¼-ì´ìš©í•œ-ìë™í™”" class="headerlink" title="Figmaì˜ GPT-4oë¥¼ ì´ìš©í•œ ìë™í™”"></a>Figmaì˜ GPT-4oë¥¼ ì´ìš©í•œ ìë™í™”</h2><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=AzqKLiPQD6g&ab_channel=jarkkomoilanen">ìœ íŠœë¸Œ ë§í¬</a></p>
<ul>
<li>GPT-4oë¥¼ ì´ìš©í•œ Figma ë””ìì¸ ìë™í™”</li>
<li>PRD(ì œí’ˆ ìš”êµ¬ ì‚¬í•­ ë¬¸ì„œ)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ìœ¼ë¡œ ë””ìì¸ ìƒì„±</li>
</ul>
<p>ì˜¤ëŠ˜ì˜ AI ì†Œì‹ì€ ì¸ê³µì§€ëŠ¥ì˜ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ìµœì‹  ê¸°ìˆ ê³¼ ê·¸ ì ìš© ì‚¬ë¡€ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì¸ê³µì§€ëŠ¥ì´ ì–´ë–»ê²Œ ë‹¤ì–‘í•œ ì‚°ì—…ê³¼ ì—°êµ¬ì— í˜ì‹ ì„ ê°€ì ¸ì˜¤ê³  ìˆëŠ”ì§€ì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ì´í•´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.</p>
<details>
  <summary>Sources</summary>
  This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is: # AI News for (today's date), ## Summary (overall short summary), ## Link1 Title, link, date - detailed summary1, - detailed summary2, - detailed summary..N, ## Link2 Title, link, date - detailed summary1, - detailed summary2, - detailed point..N, etc. The report should be written in Korean and use the ê°œì¡°ì‹ ë¬¸ì²´ style. give the very deep details for each link as much as possible.
  make summary with good details, note company name next to date if available.

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://openai.com/index/disrupting-deceptive-uses-of-AI-by-covert-influence-operations/</span><br><span class="line">May 30, 2024</span><br><span class="line"></span><br><span class="line">Disrupting deceptive uses of AI by covert influence operations</span><br><span class="line">Weâ€™ve terminated accounts linked to covert influence operations; no significant audience increase due to our services.</span><br><span class="line"></span><br><span class="line">The image is an abstract background with soft, blended hues of purple, pink, and blue. The pastel colors mix seamlessly, creating a dreamy and serene atmosphere, reminiscent of a twilight sky or an ethereal mist.</span><br><span class="line">OpenAI is committed to enforcing policies that prevent abuse and to improving transparency around AI-generated content. That is especially true with respect to detecting and disrupting covert influence operations (IO), which attempt to manipulate public opinion or influence political outcomes without revealing the true identity or intentions of the actors behind them.</span><br><span class="line"></span><br><span class="line">In the last three months, we have disrupted five covert IO that sought to use our models in support of deceptive activity across the internet. As of May 2024, these campaigns do not appear to have meaningfully increased their audience engagement or reach as a result of our services.</span><br><span class="line"></span><br><span class="line">This blog describes the threat actors we disrupted, attacker trends we identified, and important defensive trends - including how designing AI models with safety in mind in many cases prevented the threat actors from generating the content they desired, and how AI tools have made our own investigations more efficient. Alongside this blog, we are publishing a trend analysis that describes the behavior of these malicious actors in detail.</span><br><span class="line"></span><br><span class="line">Read the full report(opens in a new window)</span><br><span class="line"></span><br><span class="line">Threat actors work across the internet. So do we. By collaborating with industry, civil society, and government we tackle the creation, distribution, and impact of IO content.  Our investigations and disruptions were made possible in part because thereâ€™s been so much detailed threat reporting over the years by distribution platforms and the open-source community. OpenAI is publishing these findings, as other tech companies do, to promote information sharing and best practices amongst the broader community of stakeholders.</span><br><span class="line"></span><br><span class="line">Disruption of covert influence operations</span><br><span class="line">Over the last three months, our work against IO actors has disrupted covert influence operations that sought to use AI models for a range of tasks, such as generating short comments and longer articles in a range of languages, making up names and bios for social media accounts, conducting open-source research, debugging simple code, and translating and proofreading texts.</span><br><span class="line"></span><br><span class="line">Specifically, we disrupted:</span><br><span class="line"></span><br><span class="line">A previously unreported operation from Russia, which we dubbed Bad Grammar, operating mainly on Telegram and targeting Ukraine, Moldova, the Baltic States and the United States. The people behind Bad Grammar used our models to debug code for running a Telegram bot and to create short, political comments in Russian and English that were then posted on Telegram.</span><br><span class="line"></span><br><span class="line">An operation originating in Russia known as Doppelganger(opens in a new window). People acting on behalf of Doppelganger used our models to generate comments in English, French, German, Italian and Polish that were posted on X and 9GAG; translate and edit articles in English and French that were posted on websites linked to this operation; generate headlines; and convert news articles into Facebook posts.</span><br><span class="line"></span><br><span class="line">A Chinese network known as Spamouflage(opens in a new window), which used our models to research public social media activity, generate texts in languages including Chinese, English, Japanese and Korean that were then posted across platforms including X, Medium and Blogspot, and debug code for managing databases and websites, including a previously unreported domain, revealscum[.]com.</span><br><span class="line"></span><br><span class="line">An Iranian operation known as the International Union of Virtual Media(opens in a new window) (IUVM), which used our models to generate and translate long-form articles, headlines and website tags that were then published on a website linked to this Iranian threat actor, iuvmpress[.]co;</span><br><span class="line"></span><br><span class="line">Activity by a commercial company in Israel called STOIC, because technically we disrupted the activity, not the company. We nicknamed this operation Zero Zeno, for the founder of the stoic school of philosophy. The people behind Zero Zeno used our models to generate articles and comments that were then posted across multiple platforms, notably Instagram, Facebook, X, and websites associated with this operation.</span><br><span class="line"></span><br><span class="line">The content posted by these various operations focused on a wide range of issues, including Russiaâ€™s invasion of Ukraine, the conflict in Gaza, the Indian elections, politics in Europe and the United States, and criticisms of the Chinese government by Chinese dissidents and foreign governments.</span><br><span class="line"></span><br><span class="line">So far, these operations do not appear to have benefited from meaningfully increased audience engagement or reach as a result of our services. Using Brookingsâ€™ Breakout Scale,(opens in a new window) which assesses the impact of covert IO on a scale from 1 (lowest) to 6 (highest), none of the five operations included in our case studies scored higher than a 2 (activity on multiple platforms, but no breakout into authentic communities).</span><br><span class="line"></span><br><span class="line">Attacker trends</span><br><span class="line">Based on the investigations into influence operations detailed in our report, and the work of the open-source community, we have identified the following trends in how covert influence operations have recently used artificial intelligence models like ours.</span><br><span class="line"></span><br><span class="line">Content generation: All these threat actors used our services to generate text (and occasionally images) in greater volumes, and with fewer language errors than would have been possible for the human operators alone.</span><br><span class="line"></span><br><span class="line">Mixing old and new: All of these operations used AI to some degree, but none used it exclusively. Instead, AI-generated material was just one of many types of content they posted, alongside more traditional formats, such as manually written texts or memes copied from across the internet.</span><br><span class="line"></span><br><span class="line">Faking engagement: Some of the networks we disrupted used our services to help create the appearance of engagement across social media - for example, by generating replies to their own posts. This is distinct from attracting authentic engagement, which none of the networks we describe here managed to do to a meaningful degree.</span><br><span class="line"></span><br><span class="line">Productivity gains: Many of the threat actors that we identified and disrupted used our services in an attempt to enhance productivity, such as summarizing social media posts or debugging code.</span><br><span class="line"></span><br><span class="line">Defensive trends</span><br><span class="line">While much of the public debate so far has focused on the potential or actual use of AI by attackers, it is important to remember the advantages that AI offers to defenders. Our investigations also benefit from industry sharing and open-source research.</span><br><span class="line"></span><br><span class="line">Defensive design: We impose friction on threat actors through our safety systems, which reflect our approach to responsibly deploying AI. For example, we repeatedly observed cases where our models refused to generate the text or images that the actors asked for.</span><br><span class="line"></span><br><span class="line">AI-enhanced investigation: Similar to our approach to using GPT-4 for content moderation and cyber defense, we have built our own AI-powered tools to make our detection and analysis more effective. The investigations described in the accompanying report took days, rather than weeks or months, thanks to our tooling. As our models improve, weâ€™ll continue leveraging their capabilities to improve our investigations too.</span><br><span class="line"></span><br><span class="line">Distribution matters: Like traditional forms of content, AI-generated material must be distributed if it is to reach an audience. The IO posted across a wide range of different platforms, including X, Telegram, Facebook, Medium, Blogspot, and smaller forums, but none managed to engage a substantial audience.</span><br><span class="line"></span><br><span class="line">Importance of industry sharing: To increase the impact of our disruptions on these actors, we have shared detailed threat indicators with industry peers. Our own investigations benefited from years of open-source analysis conducted by the wider research community.</span><br><span class="line"></span><br><span class="line">The human element: AI can change the toolkit that human operators use, but it does not change the operators themselves. Our investigations showed that these actors were as prone to human error as previous generations have been - for example, publishing refusal messages from our models on social media and their websites. While it is important to be aware of the changing tools that threat actors use, we should not lose sight of the human limitations that can affect their operations and decision making.</span><br><span class="line"></span><br><span class="line">We are committed to developing safe and responsible AI, which involves designing our models with safety in mind and proactively intervening against malicious use. Detecting and disrupting multi-platform abuses such as covert influence operations can be challenging because we do not always know how content generated by our products is distributed. But we are dedicated to finding and mitigating this abuse at scale by harnessing the power of generative AI.</span><br><span class="line"></span><br><span class="line">Announcements</span><br><span class="line">Safety &amp; Alignment</span><br><span class="line">Authors</span><br><span class="line">OpenAI</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://developers.googleblog.com/en/gemini-15-pro-and-15-flash-now-available/</span><br><span class="line">GEMINI</span><br><span class="line">Gemini 1.5 Pro and 1.5 Flash GA, 1.5 Flash tuning support, higher rate limits, and more API updates</span><br><span class="line">MAY 30, 2024</span><br><span class="line">Logan Kilpatrick</span><br><span class="line">Senior Product Manager</span><br><span class="line">Gemini API and Google AI Studio</span><br><span class="line">Shrestha Basu Mallick</span><br><span class="line">Group Product Manager</span><br><span class="line">Gemini API</span><br><span class="line"></span><br><span class="line">Share</span><br><span class="line">Gemini 1.5 Pro-Flash</span><br><span class="line">Building on the momentum from Google I/O, we&#x27;re announcing important updates to the Gemini API and Google AI Studio, including:</span><br><span class="line"></span><br><span class="line">Gemini 1.5 Flash and 1.5 Pro stable release and billing</span><br><span class="line">Higher rate limits on Gemini 1.5 Flash</span><br><span class="line">Gemini 1.5 Flash tuning</span><br><span class="line">JSON schema mode</span><br><span class="line">Mobile support and light mode in Google AI Studio</span><br><span class="line">Weâ€™re incredibly excited to see what you build with these new models and are committed to building towards a world class developer experience. You can get started with Gemini 1.5 Flash and 1.5 Pro free of charge in Google AI Studio.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Gemini 1.5 Flash updates</span><br><span class="line">Gemini 1.5 Flash was purpose-built as our fastest, most cost-efficient model yet for high volume tasks, at scale, to address developersâ€™ feedback asking for lower latency and cost. Today, we are increasing the rate limit for 1.5 Flash to 1000 requests per minute (RPM) and removing the request per day limit. The 1.5 Pro rate limit will not be changed at this time, but if you need even higher limits to scale or have feedback, please reach out to us.</span><br><span class="line"></span><br><span class="line">Customizing models can help you reach the performance threshold needed to take AI models into production. To support that, we will also be rolling out tuning support for Gemini 1.5 Flash on June 17th. Tuning will be supported in both Google AI Studio and the Gemini API directly. Currently, tuning jobs are free of charge, and using a tuned model does not incur any additional per-token costs. You can learn more about tuning in the Gemini API docs.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Gemini API billing</span><br><span class="line">In addition to the free tier, starting today, developers can unlock higher API rate limits by turning on a billing account in Google AI Studio.</span><br><span class="line"></span><br><span class="line">Set up billing in Google AI Studio</span><br><span class="line">You can learn more about the Gemini 1.5 model pricing on ai.google.dev/pricing. If you run into any issues setting up billing, please let us know on our developer forum. For developers looking to scale with enterprise-grade features, the same models are available via Vertex AI, our enterprise-ready AI platform.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">JSON schema mode</span><br><span class="line">We launched JSON mode in the Gemini API and Google AI Studio earlier this year to give you more control over model output. Starting today, you can specify the desired JSON schema for the model to respond with, which unlocks many new use cases where you need the model to conform to certain output constraints like following a predefined structure or only outputting specific text. You can read more about JSON schema mode in the Gemini API docs.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Light mode and mobile support</span><br><span class="line">To give developers more flexibility in AI Studio, you can now choose your preferred UI mode (light vs dark) or use your system defaults in the settings pane. We also rolled out our first set of mobile improvements for Google AI Studio to allow you to quickly test multi modal prompts on-the-go.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">As we continue to improve our developer experience, please share your feedback on our Developer Forum. Happy building!</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main</span><br><span class="line">Yuan2.0-M32: Mixture of Experts with Attention Router</span><br><span class="line">=====================================================</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#--yuan20-m32-mixture-of-experts-with-attention-router-)</span><br><span class="line"></span><br><span class="line">ğŸ‘¾Â [ModelScope](https://www.modelscope.cn/profile/YuanLLM)Â - ğŸ¤—Â [Hugging Face](https://huggingface.co/IEITYuan)Â - ğŸ’¬Â [WeChat](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/images/%E6%BA%90%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BA%8C%E7%BB%B4%E7%A0%81.png)- ğŸ“Â [Yuan2.0-M32 Paper](https://arxiv.org/abs/2405.17976)</span><br><span class="line"></span><br><span class="line">[![Code License](https://camo.githubusercontent.com/8a1af7455ed34ab5dd2d316b2518cbc6af01e63bf3bdbd9b7f211a6c349fc139/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f417061636865253230322e302532302d677265656e3f7374796c653d666c6174266c6162656c3d436f64652532304c6963656e7365266c696e6b3d68747470732533412532462532466769746875622e636f6d253246494549542d5975616e2532465975616e2d322e302d4d6f452533467461622533444170616368652d322e302d312d6f762d66696c65)Â ](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/code_license)[![Model License](https://camo.githubusercontent.com/01772df1697a1100a2d8ec4631d26b2c668433b311bb4adb636f2702771f80d3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5975616e322e302532304c6963656e73652d626c75653f7374796c653d666c6174266c6f676f436f6c6f723d626c7565266c6162656c3d4d6f64656c2532304c6963656e736526636f6c6f723d626c7565266c696e6b3d68747470732533412532462532466769746875622e636f6d253246494549542d5975616e2532465975616e2d322e30253246626c6f622532466d61696e2532464c4943454e53452d5975616e)](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/model_license)</span><br><span class="line"></span><br><span class="line">####</span><br><span class="line"></span><br><span class="line">EnglishÂ |Â [ç®€ä½“ä¸­æ–‡](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/README_CN.md)</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#------------english---------%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87----)</span><br><span class="line"></span><br><span class="line">* * * * *</span><br><span class="line"></span><br><span class="line">0\. Latest News ğŸ‰ğŸ‰</span><br><span class="line">--------------------</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#0-latest-news-)</span><br><span class="line"></span><br><span class="line">-   [2024-05-28]Â Yuan2.0-M32 released</span><br><span class="line"></span><br><span class="line">1\. Introduction</span><br><span class="line">----------------</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#1-introduction)</span><br><span class="line"></span><br><span class="line">Yuan2.0-M32Â is a Mixture-of-Experts (MoE) language model with 32 experts, of which 2 are active. A new router network, Attention Router, is proposed and has been adopted for more efficient expert selection, boosting accuracy by 3.8% over models using a classical router network. Yuan 2.0-M32 is trained from scratch with 2000B tokens, and its training computation is only 9.25% of that required by a dense model of the same parameter scale. Demonstrating competitive capabilities in coding, math, and various specialized fields, Yuan2.0-M32 operates with only 3.7B active parameters out of a total 40B, and a forward computation of 7.4 GFLOPS per token, which is just 1/19th of Llama3-70B&#x27;s requirement. Yuan 2.0-M32 has surpassed Llama3-70B on the MATH and ARC-Challenge benchmarks, achieving accuracies of 55.9% and 95.8%, respectively. The basic information of theÂ Yuan2.0-M32Â model is as follows:</span><br><span class="line"></span><br><span class="line">-   Total Parameters ï¼šÂ 40B</span><br><span class="line"></span><br><span class="line">-   Expertsï¼šÂ 32</span><br><span class="line"></span><br><span class="line">-   Active Expertsï¼šÂ 2</span><br><span class="line"></span><br><span class="line">-   Active Parametersï¼šÂ 3.7B</span><br><span class="line"></span><br><span class="line">-   Pretrained Tokensï¼šÂ 2000B tokens</span><br><span class="line"></span><br><span class="line">-   Sequence Lengthï¼šÂ 16K</span><br><span class="line"></span><br><span class="line">The technical report for the Yuan2.0-M32 model has been released, and you can find more detailed technical information and evaluation results by referring to theÂ [paper](https://arxiv.org/abs/2405.17976).</span><br><span class="line"></span><br><span class="line">[![](https://github.com/IEIT-Yuan/Yuan2.0-M32/raw/main/docs/Yuan2.0-M32-Architecture.jpg)](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/Yuan2.0-M32-Architecture.jpg)</span><br><span class="line"></span><br><span class="line">Fig.1: Yuan 2.0-M32 Architecture</span><br><span class="line"></span><br><span class="line">2\. Model Downloads</span><br><span class="line">-------------------</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#2-model-downloads)</span><br><span class="line"></span><br><span class="line">| Model | Sequence Length | Type | Download |</span><br><span class="line">| :-: | :-: | :-: | :-: |</span><br><span class="line">| Yuan2.0-M32 | 16K | Megatron | [ModelScope](https://modelscope.cn/models/YuanLLM/Yuan2-M32/)Â |Â [HuggingFace](https://huggingface.co/IEITYuan/Yuan2-M32)Â |Â [Netdisk](https://pan.baidu.com/s/1K0LVU5NxeEujtYczF_T-Rg?pwd=cupw) |</span><br><span class="line">| Yuan2.0-M32-HF | 16K | HuggingFace | [ModelScope](https://modelscope.cn/models/YuanLLM/Yuan2-M32-hf)Â |Â [HuggingFace](https://huggingface.co/IEITYuan/Yuan2-M32-hf)Â |Â [Netdisk](https://pan.baidu.com/s/1FrbVKji7IrhpwABYSIsV-A?pwd=q6uh) |</span><br><span class="line">| Yuan2.0-M32-GGUF | 16K | GGUF | [ModelScope](https://modelscope.cn/models/YuanLLM/Yuan2-M32-gguf/summary)Â |Â [HuggingFace](https://huggingface.co/IEITYuan/Yuan2-M32-gguf)Â |Â [Netdisk](https://pan.baidu.com/s/1BWQaz-jeZ1Fe69CqYtjS9A?pwd=f4qc) |</span><br><span class="line">| Yuan2.0-M32-GGUF-INT4 | 16K | GGUF | [ModelScope](https://modelscope.cn/models/YuanLLM/Yuan2-M32-gguf-int4/summary)Â |Â [HuggingFace](https://huggingface.co/IEITYuan/Yuan2-M32-gguf-int4)Â |Â [Netdisk](https://pan.baidu.com/s/1FM8xPpkhOrRcAfe7-zUgWQ?pwd=e6ag) |</span><br><span class="line"></span><br><span class="line">3\. Evaluation</span><br><span class="line">--------------</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#3-evaluation)</span><br><span class="line"></span><br><span class="line">3.1 BenchmarksÂ ğŸ†</span><br><span class="line"></span><br><span class="line">We conducted a thorough evaluation of the Yuan2.0-M32 model across a range of benchmarks, including HumanEval, GSM8K, MMLU, Math, and ARC-Challenge. These benchmarks are designed to test the model&#x27;s proficiency in key areas such as natural language understanding, knowledge acquisition, mathematical computation and reasoning, and code generation. The Yuan2.0-M32 has shown a consistent and significant advantage over other models like Llama3-8B and Mistral-8Ã—7B, excelling in all evaluated tasks. Remarkably, its overall performance is on par with the more substantial Llama3-70B model.The detailed evaluation results are outlined in the subsequent table.</span><br><span class="line"></span><br><span class="line">-   We provided evaluation scripts forÂ [HumanEval](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/eval_humaneval.md),Â [GSM8K](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/eval_gsm8k.md),Â [MMLU](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/eval_mmlu.md),Â [Math](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/eval_math.md)Â andÂ [ARC-C](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/eval_arc.md)Â to support the replication of our evaluation results.</span><br><span class="line"></span><br><span class="line">| Model | HumanEval | GSM8K | MMLU | Math | ARC-C* |</span><br><span class="line">| --- | :-: | :-: | :-: | :-: | :-: |</span><br><span class="line">| Llama3-70B | 81.7% | 93% | 80.3 | 50.4% | 93.3% |</span><br><span class="line">| Llama3-8B | 62.2% | 79.6% | 68.4% | 30% | 78.6% |</span><br><span class="line">| Phi-3-medium | 62.2% | 91.0% | 78.0% | - | 91.6% |</span><br><span class="line">| Phi-3-small | 61% | 89.6% | 75.7% | - | 90.7% |</span><br><span class="line">| Phi-3-mini | 58.5% | 82.5% | 68.8% | - | 84.9% |</span><br><span class="line">| Mistral-8*22B | 45.1% | 78.6% | 77.8% | 41,8% | 91.3% |</span><br><span class="line">| Mistral-8*7B | 40.2% | 58.4% | 70.86% | 28.4% | 85.9% |</span><br><span class="line">| Yuan2.0-M32 | 74.4% | 92.7% | 72.2% | 55.9% | 95.8% |</span><br><span class="line"></span><br><span class="line">*Â *ARC-C*: AI2 Reasoning Challenge (ARC) benchmark contains more complex parts that need further reasoning.</span><br><span class="line"></span><br><span class="line">* * * * *</span><br><span class="line"></span><br><span class="line">3.2 Computational Utilization for Model</span><br><span class="line"></span><br><span class="line">| Model | Params (B) | Active Params (B) | GFLOPs/token (Inference) | GFLOPS/token (Fine-tune) | Mean Accuracy | Average Accuracy/GFLOPSs per token (Inference) |</span><br><span class="line">| --- | :-: | :-: | :-: | :-: | :-: | :-: |</span><br><span class="line">| Llama3-70B | 70 | 70 | 140 | 420 | 79.25 | 0.57 |</span><br><span class="line">| Llama3-8B | 8 | 8 | 16 | 48 | 64.15 | 4.00 |</span><br><span class="line">| Mistral-8*22B | 141 | 39 | 78 | 234 | 72.38 | 0.93 |</span><br><span class="line">| Mistral-8*7B | 47 | 12.9 | 25.8 | 77.3 | 60.83 | 2.36 |</span><br><span class="line">| Yuan2.0-M32 | 40 | 3.7 | 7.4 | 22.2 | 79.15 | 10.69 |</span><br><span class="line"></span><br><span class="line">4\. Quick Start</span><br><span class="line">---------------</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#4-quick-start)</span><br><span class="line"></span><br><span class="line">4.1 Environment Config</span><br><span class="line"></span><br><span class="line">We strongly recommend using the latest release of docker images of Yuan2.0-M32.You can launch an instance of the Yuan 2.0 container with the following Docker commands:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">4.2 Data Preprocess</span><br><span class="line"></span><br><span class="line">We have provided the data preprocess script. See documentationÂ [here](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/data_process.md).</span><br><span class="line"></span><br><span class="line">4.3 Model Pretrain</span><br><span class="line"></span><br><span class="line">We&#x27;ve provided several scripts for pretraining in theÂ [`example`](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/examples). The details can be seen from documentationÂ [here](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/docs/pretrain.md).</span><br><span class="line"></span><br><span class="line">4.4 Inference Service</span><br><span class="line"></span><br><span class="line">For a detailed deployment plan, please refer toÂ [vllm](https://github.com/IEIT-Yuan/Yuan2.0-M32/edit/main/vllm/README_Yuan_vllm.md).</span><br><span class="line"></span><br><span class="line">5\. Statement of Agreement</span><br><span class="line">--------------------------</span><br><span class="line"></span><br><span class="line">[](https://github.com/IEIT-Yuan/Yuan2.0-M32/tree/main#5-statement-of-agreement)</span><br><span class="line"></span><br><span class="line">The use of the source code in this repository requires compliance with the open source license agreement Apache 2.0. The Yuan2.0 model supports commercial use and does not require authorization. Please understand and comply with theÂ [ã€ŠYuan2.0 Model License Agreementã€‹](https://github.com/IEIT-Yuan/Yuan2.0-M32/blob/main/LICENSE-Yuan). Do not use the open source model and code, as well as derivatives generated from open source projects, for any purposes that may cause harm to the country and society, or for any services that have not undergone security assessment and filing. Although we have taken measures to ensure the compliance and accuracy of the data during training, the model has a huge number of parameters and is affected by probability and randomness factors. We cannot guarantee the accuracy of the output content, and the model is easily misled by input instructions. This project does not assume any data security, public opinion risks, or any model misleading, abusing, spreading caused by open-source models and code Risks and responsibilities arising from improper utilization You will be solely responsible for the risks and consequences arising from the use, copying, distribution, and modification of the model in this open source project</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://research.google/blog/codeclm-aligning-language-models-with-tailored-synthetic-data/</span><br><span class="line">Blog</span><br><span class="line">CodecLM: Aligning language models with tailored synthetic data</span><br><span class="line">May 30, 2024</span><br><span class="line"></span><br><span class="line">Zifeng Wang and Chen-Yu Lee, Research Scientists, Cloud AI Research Team</span><br><span class="line"></span><br><span class="line">We propose CodecLM, an end-to-end data synthesis framework that tailors high-quality data to align LLMs for different downstream tasks without human annotation.</span><br><span class="line"></span><br><span class="line">Instruction tuning is a critical step in LLM alignment, i.e., shaping the behavior of large language models (LLMs) to better align with the intended objective. It involves fine-tuning a pre-trained LLM on a varied set of instructions, each paired with a desired output. This process enables the model to generalize across various tasks and formats, ultimately improving its performance in understanding and responding to user instructions. In essence, instruction tuning empowers LLMs to follow instructions more effectively, thereby making them more useful and reliable tools for a wide range of applications. Recent progress in instruction tuning highlights the critical role of high-quality data in enhancing LLMs&#x27; instruction-following capabilities. However, acquiring such data through human annotation remains cost-prohibitive and difficult to scale, hindering further progress.</span><br><span class="line"></span><br><span class="line">Alternatively, recent work explores synthesizing instructionâ€“response pairs for LLM alignment by prompting models with example data and iteratively refining the results. While these methods are effective at generating varied instructions for LLM alignment broadly, real-world applications often prioritize tailoring the LLM to specific downstream tasks such as individual enterprise applications or personal assistant agents, which often involve different instruction distributions. This need for task-specific alignment brings us to a core question for data synthesis: how can we tailor synthetic data to align LLMs for different instruction-following tasks?</span><br><span class="line"></span><br><span class="line">In â€œCodecLM: Aligning Language Models with Tailored Synthetic Dataâ€, presented at NAACL 2024, we present a novel framework, CodecLM, that systematically generates tailored high-quality data to align LLMs for specific downstream tasks. Inspired by the principles of the encode-decode process, we leverage a strong LLM (i.e., an LLM that has strong instruction-following capability for data synthesis, such as Gemini Pro or text-unicorn) as a codec, to encode seed instructions from our target task into instruction metadata (keywords that capture the use case of the instruction, and the skills required for an LLM to respond to the instruction). We then decode the metadata into tailored synthetic instructions. In the decoding process, we propose two complementary strategies, Self-Rubrics and Contrastive Filtering, to enhance synthetic data quality. Self-Rubrics leverages the strong LLM to generate rubrics and actions to make synthetic instruction more challenging. Contrastive Filtering further selects the instructions to which the target LLM (the LLM to be aligned) fails to respond well. CodecLM achieves state-of-the-art performance on open-domain instruction-following benchmarks with various LLMs, demonstrating its effectiveness in LLM alignment for varied instruction distributions.</span><br><span class="line"></span><br><span class="line">CodecLM1-Hero</span><br><span class="line">Overview of CodecLM. We first encode seed instructions into metadata to capture the underlying distribution of instructions. This metadata is then decoded through two complementary strategies, Self-Rubrics and Contrastive Filtering, to tailor high-quality synthetic instructions that are aligned with the target instruction distribution. Intermediate instructions and responses are omitted in the figure for clarity.</span><br><span class="line"></span><br><span class="line">CodecLM</span><br><span class="line">The core idea of CodecLM is to customize synthetic data for different downstream tasks, which can then be used to fine-tune an LLM for the tasks of interest. To achieve this goal, we need to make sure 1) the synthetic dataâ€™s distribution is similar to that of the real downstream data, and 2) the quality of synthetic data is high enough to improve the target LLM to be tuned.</span><br><span class="line"></span><br><span class="line">First, the strong LLM encodes the seed instruction into instruction metadata, specifying its use case and skills required for responses. Next, the strong LLM decodes metadata into basic instructions. Meanwhile, Self-Rubrics (more below) leverages the strong LLM to generate rubrics and actions to improve the basic instruction, tailoring them for the downstream task. Finally, Contrastive Filtering (more below) uses a scoring function to compare answers from both the strong and target LLMs. The most effective pairs are selected for aligning the LLM, while less effective instructions are sent for further improvement. Animated below, the strong LLM&#x27;s (refined) answer is winning against the target LLM&#x27;s (simplistic) answer, indicating the improved synthetic instruction is challenging enough for the target LLM. Hence, we select the corresponding pair for instruction tuning the target LLM.</span><br><span class="line"></span><br><span class="line">Detailed workflow of CodecLM.</span><br><span class="line"></span><br><span class="line">Encoding instructions via metadata</span><br><span class="line">To capture the underlying instruction distribution from the downstream task, we extract a word-level abstraction of the input instruction distribution through instruction metadata. We define the metadata as encompassing two key aspects: use case and skills. The use case describes the intended task (e.g., question answering or creative writing), while the skills are the knowledge the LLM must have to successfully respond to the given instruction (e.g., algorithms or communication). With the metadata from the seed instruction, we can readily prompt the strong LLM to generate synthetic instructions based on the extracted metadata.</span><br><span class="line"></span><br><span class="line">Tailoring instructions via Self-Rubrics</span><br><span class="line">With the above method, however, the quality of the synthetic instructions generated by simply prompting the LLM with the metadata may not be high. A recent study found that tuning LLMs with more complex instructions can improve performance, indicating that complex instructions are often considered high quality. A common practice is to work with human experts to craft general guidance to complicate instructions, such as â€œadd reasoning stepsâ€ (more below). However, this strategy falls short for tailoring guidance to different tasks, like solving calculus problems versus writing news articles. Therefore, we introduce Self-Rubrics, which leverages the strong LLM to tailor instructions by adjusting their complexity according to the extracted metadata.</span><br><span class="line"></span><br><span class="line">Self-Rubrics first guides the LLM to generate distinct rubrics for assessing the instruction complexity of each metadatum. Then, informed by these rubrics, the LLM generates a corresponding set of actions to enhance the instructionâ€™s complexity. Such actions generated by Self-Rubrics are domain-specific and unambiguous â€” for example, for the use case of â€œbusiness plan developmentâ€ and skills of â€œmarket research and planningâ€, generic rules like â€œadd reasoning stepsâ€ are vague. On the contrary, Self-Rubrics is able to generate actions like â€œadd SWOT analysisâ€ and â€œinclude comparison with market competitorsâ€ to complicate the instruction. With these instructions, one can iteratively prompt the strong LLM to tailor higher quality instructions.</span><br><span class="line"></span><br><span class="line">Selecting instructions via Contrastive Filtering</span><br><span class="line">While Self-Rubrics tailors complex instructions based on instruction metadata, not all instructions, regardless of their complexity, are equally effective for instruction tuning. Intuitively, identifying instructions an LLM finds challenging can expose opportunities for improvement. We therefore introduce Contrastive Filtering, a method to select the instructions that can enhance the target LLM.</span><br><span class="line"></span><br><span class="line">Given an input instruction, we obtain two responses from the strong LLM (the one used for data synthesis) and the target LLM (the one we target for tuning), respectively. We then measure the quality gap between the two responses using LLM-as-a-Judge: we prompt the strong LLM to generate numerical scores (e.g., from 1 to 10) reflecting each responseâ€™s quality, and define the absolute difference between two scores as the quality gap. Intuitively, a larger gap often means the target LLM produces a worse response than the strong LLM. In this case, we add the instruction and the higher-scoring response to our final pool of high-quality synthetic data. On the other hand, a smaller quality gap indicates that such instructions are unlikely to improve performance. We then save such instructions for the next iteration of Self-Rubrics for further improvement.</span><br><span class="line"></span><br><span class="line">Effectiveness of CodecLM</span><br><span class="line">We demonstrate the effectiveness of CodecLM with PaLM 2 LLMs. In particular, we use text-unicorn as the strong LLM for data synthesis, and text-bison as the target LLM for instruction tuning. We conduct experiments on multiple widely-used open domain instruction-following benchmarks, which contain instructions for various forms and complexities of task types to test LLMsâ€™ instruction-following ability. Here we focus on the results on the Vicuna (Benchmark 1) and Evol-Instruct (Benchmark 2) test sets. We compare CodecLM with representative baselines, including Alpagasus and WizardLM+ (an enhanced version of WizardLM). Inspired by the LLM-as-a-Judge approach, we conduct LLM-based pairwise comparisons between the instruction-tuned target LLM and the strong LLM to measure how much capacity the target LLM recovers from the strong LLM. We name this metric capacity recovery ratio (CRR), where 100% CRR means the tuned target LLM performs as good as the strong LLM on the specific test set.</span><br><span class="line"></span><br><span class="line">Consistently better performance</span><br><span class="line">CodecLM outperforms comparable methods consistently on all benchmarks, highlighting its generalizability to different downstream instruction distributions. Note that common data synthesis approaches do not take the downstream instruction distribution into account, while CodecLM is able to tailor instructions for different downstream tasks, thanks to the synergy between instruction metadata, Self-Rubrics and Contrastive Filtering. Our paper has more results and in-depth analysis.</span><br><span class="line"></span><br><span class="line">CodecLM3-Results</span><br><span class="line">Results with PaLM 2â€“based target models on two open-domain instruction-following benchmarks. Each method trains a target model with synthetic data based on text-bison, and compares against the strong model, text-unicorn. Larger CRR means better performance.</span><br><span class="line"></span><br><span class="line">Conclusion</span><br><span class="line">Our proposed CodecLM is able to generate synthetic instruction-tuning data that is tailored to specific domains. We show that CodecLM effectively captures the underlying instruction distribution via instruction metadata, and further tailors the most effective instruction-response pairs through the novel strategies of Self-Rubrics and Contrastive Filtering. CodecLM provides a potent solution towards adapting LLMs for customized uses, without the necessity of human annotation. We believe CodecLM serves as a general framework for targeted LLM alignment, which opens the door to multiple promising research directions within the framework, such as richer metadata definition, better prompt design, and more reliable LLM-based scorers.</span><br><span class="line"></span><br><span class="line">Acknowledgments</span><br><span class="line">This research was conducted by Zifeng Wang, Chun-Liang Li, Vincent Perot, Long T. Le, Jin Miao, Zizhao Zhang, Chen-Yu Lee, Tomas Pfister. Thanks to Chih-Kuan Yeh and Sergey Ioffe for their valuable feedback.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/introducing-chatgpt-edu/</span><br><span class="line">May 30, 2024</span><br><span class="line"></span><br><span class="line">Introducing ChatGPT Edu</span><br><span class="line">An affordable offering for universities to responsibly bring AI to campus.</span><br><span class="line"></span><br><span class="line">An abstract expressionist painting of a desk and chair near a window in a warm color palette.</span><br><span class="line">We&#x27;re announcing ChatGPT Edu, a version of ChatGPT built for universities to responsibly deploy AI to students, faculty, researchers, and campus operations. Powered by GPT-4o, ChatGPT Edu can reason across text and vision and use advanced tools such as data analysis. This new offering includes enterprise-level security and controls and is affordable for educational institutions.</span><br><span class="line"></span><br><span class="line">We built ChatGPT Edu because we saw the success universities like the University of Oxford, Wharton School of the University of Pennsylvania(opens in a new window), University of Texas at Austin, Arizona State University(opens in a new window), and Columbia University in the City of New York were having with ChatGPT Enterprise.</span><br><span class="line"></span><br><span class="line">How campuses use ChatGPT today</span><br><span class="line">ChatGPT can help with various tasks across campus, such as providing personalized tutoring for students and reviewing their resumes, helping researchers write grant applications, and assisting faculty with grading and feedback. Our university partners have found innovative ways to make AI accessible to students, faculty, researchers, and campus operations. A few examples include:</span><br><span class="line"></span><br><span class="line">Professor Nabila El-Bassel at Columbia University is leading an initiative to integrate AI into community-based strategies to reduce overdose fatalities(opens in a new window). Her team built a GPT that analyzes and synthesizes large datasets to inform interventions, reducing weeks of research work into seconds.</span><br><span class="line"></span><br><span class="line">Undergraduates and MBA students in Professor Ethan Mollickâ€™s courses at Wharton completed their final reflection assignments through discussions with a GPT trained on course materials, reporting that ChatGPT got them to think more deeply about what theyâ€™ve learned.</span><br><span class="line"></span><br><span class="line">Christiane Reves, an assistant professor at Arizona State University, is developing a custom Language Buddies GPT for students(opens in a new window) to engage in German conversations suited to their language level while receiving tailored feedback. The GPT will help students build communication skills and save faculty time on assessments.</span><br><span class="line"></span><br><span class="line">Bringing AI into the new school year</span><br><span class="line">To build on these applications, we designed ChatGPT Edu as an accessible option for universities to bring AI to their campuses at scale.</span><br><span class="line"></span><br><span class="line">ChatGPT Edu includes:</span><br><span class="line"></span><br><span class="line">Access to GPT-4o, our flagship model, excelling in text interpretation, coding, and mathematics</span><br><span class="line"></span><br><span class="line">Advanced capabilities such as data analytics, web browsing, and document summarization</span><br><span class="line"></span><br><span class="line">The ability to build GPTs, custom versions of ChatGPT, and share them within university workspaces</span><br><span class="line"></span><br><span class="line">Significantly higher message limits than the free version of ChatGPT</span><br><span class="line"></span><br><span class="line">Improved language capabilities across quality and speed, with over 50 languages supported</span><br><span class="line"></span><br><span class="line">Robust security, data privacy, and administrative controls such as group permissions, SSO, SCIM 1, and GPT management</span><br><span class="line"></span><br><span class="line">Conversations and data are not used to train OpenAI models</span><br><span class="line"></span><br><span class="line">â€œIntegrating OpenAI&#x27;s technology into our educational and operational frameworks accelerates transformation at ASU. We&#x27;re collaborating across our community to harness these tools, extending our learnings as a scalable model for other institutions.â€</span><br><span class="line">â€”Kyle Bowen, Deputy CIO at Arizona State University</span><br><span class="line">ChatGPT Edu is designed for schools that want to deploy AI more broadly to students and their campus communities. Contact our team to learn more.</span><br><span class="line"></span><br><span class="line">GPT-4o</span><br><span class="line">Announcements</span><br><span class="line">Footnotes</span><br><span class="line">1Coming soon to ChatGPT Edu and ChatGPT Enterprise</span><br><span class="line"></span><br><span class="line">Author</span><br><span class="line">OpenAI</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.anthropic.com/news/tool-use-ga</span><br><span class="line">Claude can now use tools</span><br><span class="line">2024ë…„ 5ì›” 31ì¼</span><br><span class="line">â—</span><br><span class="line">3 min read</span><br><span class="line">Illustration of Claude using tools</span><br><span class="line">Tool use, which enables Claude to interact with external tools and APIs, is now generally available across the entire Claude 3 model family on the Anthropic Messages API, Amazon Bedrock, and Google Cloud&#x27;s Vertex AI. With tool use, Claude can perform tasks, manipulate data, and provide more dynamicâ€”and accurateâ€”responses.</span><br><span class="line"></span><br><span class="line">Tool use</span><br><span class="line">Define a toolset for Claude and specify your request in natural language. Claude will then select the appropriate tool to fulfill the task and, when appropriate, execute the corresponding action:</span><br><span class="line"></span><br><span class="line">Extract structured data from unstructured text: Pull names, dates, and amounts from invoices to reduce manual data entry.</span><br><span class="line">Convert natural language requests into structured API calls: Enable teams to self-serve common actions (e.g., &quot;cancel subscription&quot;) with simple commands.</span><br><span class="line">Answer questions by searching databases or using web APIs: Provide instant, accurate responses to customer inquiries in support chatbots.</span><br><span class="line">Automate simple tasks through software APIs: Save time and minimize errors in data entry or file management.</span><br><span class="line">Orchestrate multiple fast Claude subagents for granular tasks: Automatically find the optimal meeting time based on attendee availability.</span><br><span class="line"></span><br><span class="line">Improved developer experience</span><br><span class="line">To make it easier to leverage the intelligence of the Claude 3 models with tools, weâ€™ve also built in features that help developers further customize the end-user experience.</span><br><span class="line"></span><br><span class="line">Tool use with streaming reduces wait times to create more engaging interactions: Streaming enables real-time responses in applications like customer support chatbots for smoother, more natural conversations.</span><br><span class="line">Forced tool use allows developers to instruct Claude on tool selection: Developers can specify which tools Claude should use or leave the choice with Claude, helping create more targeted and efficient applications.</span><br><span class="line">Tools also work with images: Claude can incorporate image inputs in live applications.</span><br><span class="line">During our beta many developers used Opus to build sophisticated user-facing assistants. To further enhance this experience, Opus will now include &lt;thinking&gt; tags in its outputs, clarifying Claudeâ€™s reasoning and simplifying the debugging process for developers. Our Claude 3 models are currently unable to support parallel tool calls.</span><br><span class="line"></span><br><span class="line">Customer spotlight: StudyFetch</span><br><span class="line">AI-native learning platform StudyFetch uses Claude&#x27;s tool use capabilities to power its personalized AI tutor, Spark.E. By integrating tools to track student progress, navigate course materials and lectures, and create interactive user interfaces, StudyFetch has created a more engaging educational environment for students globally.</span><br><span class="line"></span><br><span class="line">&quot;Claude with tool use is accurate and cost-effective, and now powers our live voice-enabled AI tutoring sessions. Within just a few days, we integrated tools into our platform,â€ said Ryan Trattner, CTO and Co-Founder at StudyFetch. â€œAs a result, our AI tutor, Spark.E, acts agenticallyâ€”displaying interactive UIs, tracking student progress in context, and navigating through lectures and materials. Since implementing Claude with tool use, we&#x27;ve observed a 42% increase in positive human feedback.&quot;</span><br><span class="line"></span><br><span class="line">Customer spotlight: Intuned</span><br><span class="line">Intuned, the browser automation platform, uses Claude to power data extraction within their cloud platform. With AI-powered data extraction, Intuned is able to drastically improve the developer experience in building and executing more reliable browser automations.</span><br><span class="line"></span><br><span class="line">&quot;Claude 3 Haiku with tool use has been a game changer for us. After accessing the model and running our benchmarks on it, we realized the quality, speed, and price combination is unmatched,â€ said Faisal Ilaiwi, Co-Founder at Intuned. â€œHaiku is helping us scale our customers&#x27; data extraction tasks to a completely new level.&quot;</span><br><span class="line"></span><br><span class="line">Customer spotlight: Hebbia</span><br><span class="line">Hebbia is building the AI knowledge worker for leading financial and legal services firms. They use Claude 3 Haiku to help power several complex, multi-step customer workflows.</span><br><span class="line"></span><br><span class="line">&quot;We leverage Claude 3 Haiku for generating live suggestions, automating prompt writing, and extracting key metadata from long documents,â€ shared Divya Mehta, Product Manager at Hebbia. â€œClaude 3 Haiku&#x27;s tool use feature has unlocked capabilities and speed for our platform to generate reliable suggestions and prompts in real-time.&quot;</span><br><span class="line"></span><br><span class="line">Get started</span><br><span class="line">You can get started with tool use today on the Anthropic Messages API, Amazon Bedrock, and Google Cloud&#x27;s Vertex AI. To learn more, explore our documentation and Anthropic Cookbooks on tool use.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://tenvence.github.io/p/v-express/</span><br><span class="line">V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation</span><br><span class="line">Cong Wang1*, Kuan Tian2*, Jun Zhang2â€ , Yonghang Guan2, Feng Luo2, Fei Shen2,</span><br><span class="line">Zhiwei Jiang1â€ , Qing Gu1, Xiao Han2, Wei Yang2</span><br><span class="line">1 Nanjing University, 2 Tencent AI Lab</span><br><span class="line">* Equal Contribution, â€  Corresponding Authors</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Abstract</span><br><span class="line">In the field of portrait video generation, the use of single images to generate portrait videos has become increasingly prevalent. A common approach involves leveraging generative models to enhance adapters for controlled generation. However, control signals can vary in strength, including text, audio, image reference, pose, depth map, etc. Among these, weaker conditions often struggle to be effective due to interference from stronger conditions, posing a challenge in balancing these conditions. In our work on portrait video generation, we identified audio signals as particularly weak, often overshadowed by stronger signals such as pose and original image. However, direct training with weak signals often leads to difficulties in convergence. To address this, we propose V-Express, a simple method that balances different control signals through a series of progressive drop operations. Our method gradually enables effective control by weak conditions, thereby achieving generation capabilities that simultaneously take into account pose, input image, and audio. The experimental results demonstrate that our method can effectively generate portrait videos controlled by audio. Furthermore, our method provides a potential solution for the simultaneous and effective use of conditions of varying strengths.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.youtube.com/watch?v=AzqKLiPQD6g&amp;ab_channel=jarkkomoilanen</span><br><span class="line">figma - Automation powered by GPT-4o generates Figma designs based on PRD.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-05-31</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/05/31/2024-5-31-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 5ì›” 31ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/06/03/2024-6-3-AI-NEWS/" title="2024ë…„ 6ì›” 3ì¼ AI ì†Œì‹">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/05/30/2024-5-30-AI-NEWS/" title="2024ë…„ 5ì›” 30ì¼ AI ì†Œì‹">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>