<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 6ì›” 3ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="Summaryì˜¤ëŠ˜ì˜ AI ì†Œì‹ì—ì„œëŠ” Hugging Faceì˜ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ FineWebê³¼ FineWeb-Eduì˜ ì¶œì‹œ, OpenAIì˜ ì°¨ì„¸ëŒ€ AI ëª¨ë¸ ê°œë°œ, ê·¸ë¦¬ê³  ê¸°ì—…ì—ì„œ ìƒì„±í˜• AIì˜ ROI ê·¹ëŒ€í™” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.
FineWeb ê¸°ìˆ  ë³´ê³ ì„œ ë° FineWeb"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 6ì›” 3ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>ì˜¤ëŠ˜ì˜ AI ì†Œì‹ì—ì„œëŠ” Hugging Faceì˜ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ FineWebê³¼ FineWeb-Eduì˜ ì¶œì‹œ, OpenAIì˜ ì°¨ì„¸ëŒ€ AI ëª¨ë¸ ê°œë°œ, ê·¸ë¦¬ê³  ê¸°ì—…ì—ì„œ ìƒì„±í˜• AIì˜ ROI ê·¹ëŒ€í™” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.</p>
<h2 id="FineWeb-ê¸°ìˆ -ë³´ê³ ì„œ-ë°-FineWeb-Edu-ì¶œì‹œ"><a href="#FineWeb-ê¸°ìˆ -ë³´ê³ ì„œ-ë°-FineWeb-Edu-ì¶œì‹œ" class="headerlink" title="FineWeb ê¸°ìˆ  ë³´ê³ ì„œ ë° FineWeb Edu ì¶œì‹œ"></a>FineWeb ê¸°ìˆ  ë³´ê³ ì„œ ë° FineWeb Edu ì¶œì‹œ</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">Hugging Face ë¸”ë¡œê·¸</a>, 2024ë…„ 5ì›” 31ì¼</p>
<ul>
<li>FineWeb: 15ì¡° í† í° ê·œëª¨ì˜ ëŒ€ê·œëª¨ ì˜ì–´ ì›¹ ë°ì´í„°ì…‹, CommonCrawlì—ì„œ íŒŒìƒ</li>
<li>FineWeb-Edu: 1.3ì¡° ë° 5.4ì¡° ê³ í’ˆì§ˆ êµìœ¡ìš© ë°ì´í„°ì…‹</li>
<li>êµìœ¡ ì½˜í…ì¸  í•„í„°ë§ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ê¸° ì‚¬ìš©, Llama-3-70B-Instructë¡œ í’ˆì§ˆ í‰ê°€</li>
<li>ë…ë¦½ì ì¸ MinHash ì¤‘ë³µ ì œê±° ë°©ë²• ì‚¬ìš©</li>
<li>FineWeb-EduëŠ” MMLU, ARC, OpenBookQAì—ì„œ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì„ ëŠ¥ê°€</li>
<li>ODC-By 1.0 ë¼ì´ì„¼ìŠ¤ë¡œ ì œê³µ, ì™„ì „ ì¬í˜„ ê°€ëŠ¥</li>
</ul>
<h2 id="OpenAIì˜-ìƒˆë¡œìš´-í”Œë˜ê·¸ì‹­-AI-ëª¨ë¸-í›ˆë ¨-ì‹œì‘"><a href="#OpenAIì˜-ìƒˆë¡œìš´-í”Œë˜ê·¸ì‹­-AI-ëª¨ë¸-í›ˆë ¨-ì‹œì‘" class="headerlink" title="OpenAIì˜ ìƒˆë¡œìš´ í”Œë˜ê·¸ì‹­ AI ëª¨ë¸ í›ˆë ¨ ì‹œì‘"></a>OpenAIì˜ ìƒˆë¡œìš´ í”Œë˜ê·¸ì‹­ AI ëª¨ë¸ í›ˆë ¨ ì‹œì‘</h2><p><a target="_blank" rel="noopener" href="https://www.nytimes.com/2024/05/28/technology/openai-gpt4-new-model.html">ë‰´ìš• íƒ€ì„ì¦ˆ</a>, 2024ë…„ 5ì›” 28ì¼</p>
<ul>
<li>OpenAI, GPT-4 í›„ì† ëª¨ë¸ ê°œë°œ ì°©ìˆ˜</li>
<li>ìƒˆë¡œìš´ ëª¨ë¸ì€ ChatGPTë¥¼ í¬í•¨í•œ ì—¬ëŸ¬ AI ì œí’ˆì˜ ì—”ì§„ìœ¼ë¡œ ì‚¬ìš© ì˜ˆì •</li>
<li>ìƒˆë¡­ê²Œ êµ¬ì„±ëœ ì•ˆì „ ë° ë³´ì•ˆ ìœ„ì›íšŒê°€ ê¸°ìˆ ì˜ ìœ„í—˜ì„± ê´€ë¦¬ ë°©ì•ˆ ë…¼ì˜</li>
<li>Scarlett Johanssonì˜ ëª©ì†Œë¦¬ì™€ ìœ ì‚¬í•œ ìŒì„±ì„ ì‚¬ìš©í•œ GPT-4o ëª¨ë¸ ë…¼ë€</li>
<li>ì°¨ì„¸ëŒ€ ëª¨ë¸ì€ í–¥í›„ 9ê°œì›”ì—ì„œ 1ë…„ ì´ìƒ í›„ì— ì¶œì‹œ ì˜ˆìƒ</li>
</ul>
<h2 id="ROI-ê·¹ëŒ€í™”ë¥¼-ìœ„í•œ-ì „ì‚¬ì -ìƒì„±í˜•-AI-êµ¬ì¶•-ëª¨ë²”ì‚¬ë¡€"><a href="#ROI-ê·¹ëŒ€í™”ë¥¼-ìœ„í•œ-ì „ì‚¬ì -ìƒì„±í˜•-AI-êµ¬ì¶•-ëª¨ë²”ì‚¬ë¡€" class="headerlink" title="ROI ê·¹ëŒ€í™”ë¥¼ ìœ„í•œ ì „ì‚¬ì  ìƒì„±í˜• AI êµ¬ì¶• ëª¨ë²”ì‚¬ë¡€"></a>ROI ê·¹ëŒ€í™”ë¥¼ ìœ„í•œ ì „ì‚¬ì  ìƒì„±í˜• AI êµ¬ì¶• ëª¨ë²”ì‚¬ë¡€</h2><p><a target="_blank" rel="noopener" href="https://cxotoday.com/specials/maximizing-roi-best-practices-for-scaling-generative-ai-across-the-enterprise/">Gartner ë³´ê³ ì„œ</a>, 2024ë…„ 4ì›”</p>
<ul>
<li>í™œìš© ì‚¬ë¡€ ìš°ì„ ìˆœìœ„ ì„¤ì • í”„ë¡œì„¸ìŠ¤ êµ¬ì¶•</li>
<li>êµ¬ì¶• í˜¹ì€ êµ¬ë§¤ë¥¼ ìœ„í•œ ì˜ì‚¬ ê²°ì • í”„ë ˆì„ì›Œí¬ ê°œë°œ</li>
<li>í™•ì¥ì„±ì„ ìœ„í•œ ì‹œë²” ìš´ì˜</li>
<li>ìœ ì—°í•œ ìƒì„±í˜• AI í”Œë«í¼ ì•„í‚¤í…ì²˜ ì„¤ê³„</li>
<li>â€˜ì±…ì„ê° ìˆëŠ” AIâ€™ ë„ì…</li>
<li>ë°ì´í„° ë° AI ë¦¬í„°ëŸ¬ì‹œì— ëŒ€í•œ íˆ¬ì í•„ìš”</li>
</ul>
<h2 id="â€œì •ê·œì§-40-ëŠ”-AI-ì‚¬ì—…-ì¸ë ¥â€â€¦AI-ì»´í¼ë‹ˆë¡œ-ê±°ë“­ë‚œ-SKT"><a href="#â€œì •ê·œì§-40-ëŠ”-AI-ì‚¬ì—…-ì¸ë ¥â€â€¦AI-ì»´í¼ë‹ˆë¡œ-ê±°ë“­ë‚œ-SKT" class="headerlink" title="â€œì •ê·œì§ 40%ëŠ” AI ì‚¬ì—… ì¸ë ¥â€â€¦AI ì»´í¼ë‹ˆë¡œ ê±°ë“­ë‚œ SKT"></a>â€œì •ê·œì§ 40%ëŠ” AI ì‚¬ì—… ì¸ë ¥â€â€¦AI ì»´í¼ë‹ˆë¡œ ê±°ë“­ë‚œ SKT</h2><p><a target="_blank" rel="noopener" href="https://v.daum.net/v/20240509060115802">ë‹¤ìŒ ì†Œì‹</a>, 2024ë…„ 5ì›” 9ì¼</p>
<ul>
<li>SKí…”ë ˆì½¤, ì „ì²´ ì •ê·œì§ì˜ 40%ê°€ AI ê´€ë ¨ ì¸ë ¥ìœ¼ë¡œ êµ¬ì„±</li>
<li>1ë¶„ê¸° ë§¤ì¶œ 4ì¡°4746ì–µì›, ì˜ì—…ì´ìµ 4985ì–µì› ê¸°ë¡</li>
<li>ë°ì´í„°ì„¼í„°ì™€ í´ë¼ìš°ë“œ ì‚¬ì—… ë§¤ì¶œ ê°ê° 25.6%, 38.3% ì¦ê°€</li>
<li>AI ì„œë¹„ìŠ¤ ì•± â€˜ì—ì´ë‹·â€™ ëˆ„ì  ê°€ì…ì ìˆ˜ 400ë§Œëª… ë‹¬ì„±</li>
<li>ê¸€ë¡œë²Œ í…”ì½” AI ì–¼ë¼ì´ì–¸ìŠ¤ì™€ í˜‘ë ¥, AI ê°œì¸ë¹„ì„œ ì„œë¹„ìŠ¤ í˜„ì§€í™” ê³„íš</li>
</ul>
<p>ì´ìƒìœ¼ë¡œ ì˜¤ëŠ˜ì˜ AI ì†Œì‹ë¥¼ ë§ˆì¹©ë‹ˆë‹¤. ë” ìì„¸í•œ ë‚´ìš©ì€ ê° ë§í¬ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.</p>
<details>
  <summary>Sources</summary>
  This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is: # AI News for (today's date), ## Summary (overall short summary), ## Link1 Title, link, date - detailed summary1, - detailed summary2, - detailed summary..N, ## Link2 Title, link, date - detailed summary1, - detailed summary2, - detailed point..N, etc. The report should be written in Korean and use the ê°œì¡°ì‹ ë¬¸ì²´ style. give the very deep details for each link as much as possible.
  make summary with good details, note company name next to date if available.

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1</span><br><span class="line">FineWeb Technical Report and FineWeb Edu released! ğŸ· FineWeb is a 15T token open-source English web dataset derived from CommonCrawl! ğŸ“š FineWeb-Edu is a 1.3T &amp; 5.4T high-quality subset. ğŸ˜</span><br><span class="line">TL;DR:</span><br><span class="line">ğŸ· 15T tokens in FineWeb outperforming other open datasets</span><br><span class="line">ğŸ“š 1.3T highest-quality educational dataset FineWeb-Edu</span><br><span class="line">ğŸ§  5.4T high-quality educational tokens in FineWeb-Edu-2</span><br><span class="line">âœ… Text Classifier for educational content filtering trained on synthetic data</span><br><span class="line">ğŸ¤– Used Llama-3-70B-Instruct for educational quality annotations</span><br><span class="line">ğŸ§¹ Independent MinHash deduplication per dump</span><br><span class="line">ğŸ“ FineWeb Edu outperforms other datasets on MMLU, ARC, OpenBookQA</span><br><span class="line">ğŸ†“ Available under ODC-By 1.0 license</span><br><span class="line">ğŸ› ï¸ Full reproducibility with datatrove and nanotron</span><br><span class="line">FineWeb 15T:</span><br><span class="line">https://lnkd.in/ehEPRCam</span><br><span class="line">Technical Report:</span><br><span class="line">https://lnkd.in/eQNrb58w</span><br><span class="line">FineWeb Edu 5T:</span><br><span class="line">https://lnkd.in/eQtHZ3qA</span><br><span class="line">FineWeb Edu 1.3T:</span><br><span class="line">https://lnkd.in/e22vD8_D</span><br><span class="line"></span><br><span class="line">Kudos to the Guilherme Penedo Hynek KydlÃ­Äek Anton Lozhkov Colin Raffel Leandro von Werra Thomas Wolf Loubna Ben Allal for their relentless push for open science and transparency! ğŸ¤—</span><br><span class="line"></span><br><span class="line"> FineWeb: decanting the web for the finest text data at scale</span><br><span class="line">AUTHORS</span><br><span class="line">Guilherme Penedo, Hynek KydlÃ­Äek, Loubna Ben Allal, Anton Lozhkov, Colin Raffel, Leandro Werra, Thomas Wolf</span><br><span class="line">AFFILIATION</span><br><span class="line">HuggingFace</span><br><span class="line">PUBLISHED</span><br><span class="line">May 31, 2024</span><br><span class="line">The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-of-the-art open LLMs like Llama 3</span><br><span class="line">[1]</span><br><span class="line"> and Mixtral</span><br><span class="line">[2]</span><br><span class="line"> are not publicly available and very little is known about how they were created.</span><br><span class="line"></span><br><span class="line">Reading time: 45 min. For the best reading experience, we recommend not using a mobile phone.</span><br><span class="line">Recently, we released ğŸ· FineWeb, a new, large-scale (15-trillion tokens, 44TB disk space) dataset for LLM pretraining. FineWeb is derived from 96 CommonCrawl snapshots and produces better-performing LLMs than other open pretraining datasets. To bring more clarity in machine learning and advance the open understanding of how to train good quality large language models, we carefully documented and ablated all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. The present long form report is a deep dive in how to create a large and high-quality web-scale dataset for LLM pretraining. The dataset itself, ğŸ· FineWeb, is available here.</span><br><span class="line"></span><br><span class="line">We are extremely thankful to the whole distill.pub team (Christopher Olah, Shan Carter, Ludwig Schubert in particular) for creating the template on which we based this blog post. Thanks also for inspiring us with exquisitely crafted articles and blog posts.</span><br><span class="line">In this report we also introduce ğŸ“š FineWeb-Edu, a subset of FineWeb constructed using scalable automated high-quality annotations for educational value, and which outperforms all openly accessible web-datasets on a number of educational benchmarks such as MMLU, ARC, and OpenBookQA. ğŸ“š FineWeb-Edu is available in two sizes/filtering-level: 1.3 trillion (very high educational content) and 5.4 trillion (high educational content) tokens (all tokens are measured with GPT2 tokenizer</span><br><span class="line">[3]</span><br><span class="line">). You can download it here.</span><br><span class="line"></span><br><span class="line">Both datasets are released under the permissive ODC-By 1.0 license</span><br><span class="line"></span><br><span class="line">TLDR: This blog covers a discussion on processing and evaluating data quality at scale, the ğŸ· FineWeb recipe (listing and explaining all of our design choices), and the process followed to create its ğŸ“š FineWeb-Edu subset.</span><br><span class="line"></span><br><span class="line">Web data</span><br><span class="line">Finding the raw data</span><br><span class="line">A common question often asked regarding web datasets used to train LLMs is â€œwhere do they even get all that data?â€. There are generally two options:</span><br><span class="line"></span><br><span class="line">you either crawl it yourself, like companies such as OpenAI or Anthropic (among others) do (see here and here)</span><br><span class="line">you use a public repository of crawled webpages, like the one maintained by the non-profit CommonCrawl</span><br><span class="line">To build ğŸ· FineWeb, following what has been done in the past by a number of LLM training teams, we used CommonCrawl (CC) as a starting point. The Common Crawl nonâ€“profit organization has been crawling the web since 2007 and releases a new crawl containing 200 to 400 TiB of textual content obtained via automatic web crawling usually every 1 or 2 months.</span><br><span class="line"></span><br><span class="line">As an example, the latest CC crawl (April 2024) contains 2.7 billion web pages, totaling 386 TiB of uncompressed HTML text content 1 . Ninety-six crawls have been released since 2013 and 3 crawls from 2008 to 2012, which are in a different (older) format. 2</span><br><span class="line"></span><br><span class="line">Processing at scale</span><br><span class="line">Given the sheer size of the data involved, one of the main challenges we had to overcome was having a modular, scalable codebase that would allow us to quickly iterate on our processing decisions and easily try out new ideas, while appropriately parallelizing our workloads and providing clear insights into the data.</span><br><span class="line"></span><br><span class="line">For this purpose, we developed datatrove</span><br><span class="line">[4]</span><br><span class="line">, an open-source data processing library that allowed us to seamlessly scale our filtering and deduplication setup to thousands of CPU cores. All the data processing steps involved in the creation of ğŸ· FineWeb used this library. You will find the exact scripts we used in the datatrove repository.</span><br><span class="line"></span><br><span class="line">What is good data?</span><br><span class="line">This is probably the main question to keep in mind when creating a dataset. In most contexts and, in particular, in the context of large language model pretraining 3 , &quot;high quality&quot; is not a very well defined term</span><br><span class="line">[5]</span><br><span class="line">, and not even a property of documents that can always be clearly perceived through direct human observation alone.</span><br><span class="line">[6]</span><br><span class="line"></span><br><span class="line">It is still common to train a model on a given corpus considered &quot;clean&quot; (typically wikipedia 4 ) and use it to check the perplexity on the dataset that we were trying to curate</span><br><span class="line">[7]</span><br><span class="line">. Unfortunately this does not always correlate with improved performance on a set of downstream tasks of interest</span><br><span class="line">[8]</span><br><span class="line">, and as a result another often used approach is to train small models 5 on a representative subset of our dataset and evaluate them on a set of evaluation tasks. Small models are used because training costs and time are a function of model size. In this second approach, it is important to choose a diverse and representative set of dataset-evaluation tasks and try not to overfit to any one individual benchmark as it would risk hurting the generality of the obtained LLM after pretraining.</span><br><span class="line"></span><br><span class="line">Yet another way to compare different datasets would be to train a model on each dataset and have humans rate and compare the generations of the models (like on the LMSYS Chatbot Arena)</span><br><span class="line">[9]</span><br><span class="line">. This would arguably provide the most reliable results in terms of representing real model usage, but getting ablation results this way is unfortunately expensive and slow. It also often requires for the models to have undergone an instruction finetuning stage to acquire conversational capabilities, as pretrained models are not directly designed to follow instructions and are thus much more sensitive to prompt details.</span><br><span class="line">[10]</span><br><span class="line"></span><br><span class="line">In this work, we went with the approach of training small models and evaluating them on a set of &quot;early-signal&quot; benchmark tasks. We believe this is a reasonable proxy for the quality of the data used to train these models, when keeping in mind the above-mentioned caveat around overfitting on the evaluation benchmarks.</span><br><span class="line"></span><br><span class="line">Ablations and evaluation setup</span><br><span class="line">To compare the impact of a given processing step, we trained two models on two versions of the dataset, one version processed with the extra step (the one we wish to evaluate) and another version with this step ablated (cut/removed). Apart from the data, these two models would be otherwise identical: the same number of parameters, architecture hyper-parameters, and trained on an equal number of randomly sampled tokens from each version of the data, for a single epoch â€” the only difference being thus the training data. We then evaluated each model on the same set of tasks and compared average scores.</span><br><span class="line"></span><br><span class="line">Our ablation models were trained using nanotron. Our &quot;ablation models&quot; have 1.82B parameters (including embeddings), used the Llama architecture with a 2048 sequence length, a global batch size of ~2 million tokens, and the GPT2 tokenizer. For most ablations we trained on ~28B tokens (roughly the Chinchilla</span><br><span class="line">[11]</span><br><span class="line"> optimal training size for this model size). To confirm relative performance improvements after each step of filtering we conducted longer training runs on 350 billion tokens as mentioned further below.</span><br><span class="line"></span><br><span class="line">We&#x27;ll make the configuration to reproduce these ablation models available soon in Nanotron.</span><br><span class="line">We evaluated the models using lighteval. We carefully selected a set of benchmark for ablations by selecting benchmarks that would provide good signal at a relatively small scale (&quot;small&quot; models trained on only &quot;a few billion&quot; tokens). We generally used the following criteria to select these benchmarks among all the benchmarks available in lighteval:</span><br><span class="line"></span><br><span class="line">small variance between runs trained on different samplings of the same dataset: we want our runs on a subset of the data to be representative of the whole dataset, and the resulting scores to be, in the limit of what is possible, less sensitive to exact data point choices than to our filter&#x27;s effect</span><br><span class="line">performance increasing monotonically (or close) over a training run: ideally, as the number of seen tokens increases, the performance on a high-signal benchmark should not decrease (which would be indicative of unreliable results at a small scale)</span><br><span class="line">performance above random baseline for this task by at least a few standard deviations: given our small ablation models and trainings we usually don&#x27;t reach extremely high scores on any benchmark, but we want to make sure that the scores we get are above random noise.</span><br><span class="line">After consideration, we selected the following list of benchmarks:</span><br><span class="line"></span><br><span class="line">CommonSense QA</span><br><span class="line">[12]</span><br><span class="line">HellaSwag</span><br><span class="line">[13]</span><br><span class="line">OpenBook QA</span><br><span class="line">[14]</span><br><span class="line">PIQA</span><br><span class="line">[15]</span><br><span class="line">SIQA</span><br><span class="line">[16]</span><br><span class="line">WinoGrande</span><br><span class="line">[17]</span><br><span class="line">ARC</span><br><span class="line">[18]</span><br><span class="line">MMLU</span><br><span class="line">[19]</span><br><span class="line">To ensure our checkpoint evaluation stayed within a limited timeframe, we capped the longer benchmarks at 1000 samples (wall-clock evaluation taking less than 5 min on a single node of 8 GPUs - done in parallel to the training).</span><br><span class="line"></span><br><span class="line">You can find the full list of tasks and prompts we used here.</span><br><span class="line">The ğŸ· FineWeb recipe</span><br><span class="line">In the next subsections we will explain each of the steps taken to produce the FineWeb dataset.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">You can find a fully reproducible datatrove config here.</span><br><span class="line">Starting point: text extraction</span><br><span class="line">CommonCrawl data is available in two main formats: WARC and WET. WARC (Web ARChive format) files contain the raw data from the crawl, including the full page HTML and request metadata. WET (WARC Encapsulated Text) files provide a text only version of those websites.</span><br><span class="line"></span><br><span class="line">A large number of datasets take the WET files as their starting point. In our experience the default text extraction used by Common Crawl to create these WET files is suboptimal for the goals of LLM pretraining 6 and there are a variety of open-source libraries that provide better text extraction. We extracted the text content from the WARC files using the trafilatura library</span><br><span class="line">[20]</span><br><span class="line">, which from visual inspection of the results provided good quality extraction when compared to other libraries.</span><br><span class="line"></span><br><span class="line">You can find a benchmark comparing several text extraction libraries here.</span><br><span class="line">To validate this decision, we processed the 2019-18 dump directly using the WET files and with text extracted from WARC files using trafilatura 7 . We applied the same processing to each one (our base filtering+minhash, detailed below) and trained two models. While the resulting dataset is about 25% larger for the WET data (around 254 billion tokens), it proves to be of much worse quality than the one that used trafilatura to extract text from WARC files (which is around 200 billion tokens). Visual inspection of some samples confirms that many of these additional tokens on the WET files are unnecessary page boilerplate.</span><br><span class="line"></span><br><span class="line">It is important to note, however, that text extraction is one of the most costly steps of our processing, so we believe that using the readily available WET data could be a reasonable trade-off for lower budget teams.</span><br><span class="line"></span><br><span class="line">Metric:</span><br><span class="line"></span><br><span class="line">Aggregate Score</span><br><span class="line">Rolling window:</span><br><span class="line"></span><br><span class="line">0</span><br><span class="line">Base filtering</span><br><span class="line">Filtering is an important part of the curation process. It consists in removing part of the data (be it words, lines, or even full documents) that lowers the performance of the model and is thus deemed to be â€œlower qualityâ€ in our eval-driven process of dataset crafting.</span><br><span class="line"></span><br><span class="line">As a basis for our filtering we used part of the setup from RefinedWeb</span><br><span class="line">[21]</span><br><span class="line">. Namely, we:</span><br><span class="line"></span><br><span class="line">Applied URL filtering using a blocklist to remove adult content</span><br><span class="line">Applied a fastText language classifier</span><br><span class="line">[22]</span><br><span class="line">[23]</span><br><span class="line"> to keep only English text with a score â‰¥ 0.65</span><br><span class="line">Applied quality and repetition filters from MassiveText</span><br><span class="line">[24]</span><br><span class="line"> (using the default thresholds)</span><br><span class="line">After applying this filtering to each of the text extracted dumps (there are currently 96 dumps) we obtained roughly 36 trillion tokens of data 8 .</span><br><span class="line"></span><br><span class="line">Deduplicating the data</span><br><span class="line">Deduplication is one of the most important steps when creating large web datasets for LLM pretraining. Methods to deduplicate datasets attempt to identify and remove redundant/repeated data from the dataset.</span><br><span class="line"></span><br><span class="line">WHY DEDUPLICATE?</span><br><span class="line">The web has many aggregators, mirrors, templated pages or just otherwise repeated content spread over different domains and webpages. Sometimes, these duplicated pages can even be introduced by the crawler itself, when different links point to the same page.</span><br><span class="line"></span><br><span class="line">Removing these duplicates (deduplicating) has been correlated with improvements in model performance</span><br><span class="line">[25]</span><br><span class="line"> and a reduction in memorization of pretraining data</span><br><span class="line">[26]</span><br><span class="line">, which might allow for better generalization. Additionally, the performance uplift obtained through deduplication can be equated to increased training efficiency: by removing duplicated content, a model can reach the same performance level with fewer training iterations â€“ or equivalently, for a given number of training tokens, a model will have seen more diverse data.</span><br><span class="line">[27]</span><br><span class="line">[28]</span><br><span class="line"></span><br><span class="line">There are different ways to identify and even define duplicated data. Common approaches rely on hashing techniques to speed up the process, or on building efficient data structures to index the data (like suffix arrays). Methods can also be â€œfuzzyâ€, by using some similarity metric to mark documents as duplicates, or â€œexactâ€ by checking for exact matches between two documents (or lines, paragraphs, or whatever other granularity level being used) 9 .</span><br><span class="line"></span><br><span class="line">OUR DEDUPLICATION PARAMETERS</span><br><span class="line">Following RefinedWeb</span><br><span class="line">[21]</span><br><span class="line">, we decided to apply MinHash, a fuzzy hash based deduplication technique that scales efficiently to many CPU-nodes and allows us to tune similarity thresholds (by controlling the number and size of buckets) as well as the length of the subsequences considered (by controlling the n-gram size). We chose to collect each document&#x27;s 5-grams 10 and compute minhashes using 112 hash functions in total, split into 14 buckets of 8 hashes each â€” targeting documents that are at least 75% similar. Documents with the same 8 minhashes in any bucket are considered a duplicate of each other.</span><br><span class="line"></span><br><span class="line">This would mean that for two documents with a similarity (s) of 0.7, 0.75, 0.8 and 0.85, the probability that they would be identified as duplicates would be 56%, 77%, 92% and 98.8% respectively (1-(1-s^8)^&#123;14&#125;). See the plot below for a match probability comparison between our setup with 112 hashes and the one from RefinedWeb, with 9000 hashes, divided into 450 buckets of 20 hashes (that requires a substantially larger amount of compute resources, as each individual hash must be computed, stored and then compared with hashes from other documents):</span><br><span class="line"></span><br><span class="line">While the high number of hash functions in RefinedWeb allows for a steeper, more well defined cut off (documents with real similarity near the threshold are more likely to be correctly identified), we believe the compute and storage savings are a reasonable trade off.</span><br><span class="line"></span><br><span class="line">It should also be noted that intra-document deduplication is already handled by our repetition filter, which removes documents with many repeated lines and paragraphs.</span><br><span class="line"></span><br><span class="line">MORE DEDUPLICATION IS ALWAYS BETTER, RIGHT?</span><br><span class="line">Initially, we were operating under the assumption that more deduplication is always better, so our first approach was to take the entire dataset (all 90+ dumps) and deduplicate them together as one big dataset using MinHash.</span><br><span class="line"></span><br><span class="line">We did this in an iterative manner: starting with the most recent dump (which at the time was 2023-50) and proceeding chronologically until we reached the oldest crawl. We deduplicated each dump not only within itself, but removing any document matching any other documents in the previously processed dumps.</span><br><span class="line"></span><br><span class="line">For instance, for the second most recent dump (2023-40 at the time), we deduplicated it against the most recent one in addition to within itself. As a result, the older the dumps, the larger the number of dumps it was deduplicated against and the more data we removed from it (indeed, in the oldest dumps, the deduplication step removed more than 90% of the base filtered data).</span><br><span class="line"></span><br><span class="line">Deduplicating the dataset in this manner resulted in 4 trillion tokens of data, but, quite surprisingly to us, when training on a randomly sampled 350 billion tokens subset, our ablation models showed next to no improvement over a model trained on the non deduplicated data, scoring far below its predecessor RefinedWeb on our aggregate of tasks (see graph below).</span><br><span class="line"></span><br><span class="line">Metric:</span><br><span class="line"></span><br><span class="line">Aggregate Score</span><br><span class="line">Rolling window:</span><br><span class="line"></span><br><span class="line">5</span><br><span class="line">This challenged our assumption that more deduplication would inevitably result in higher benchmark scores, so we decided to take a closer look at one of the oldest dumps, dump 2013-48:</span><br><span class="line"></span><br><span class="line">pre deduplication, this dump had ~490 billion tokens</span><br><span class="line">after our iterative MinHash, ~31 billion tokens remained (94% of data had been removed)</span><br><span class="line">As an experiment, we tried training two models on 28 billion tokens sampled from the following data from 2013-48:</span><br><span class="line"></span><br><span class="line">the fully deduplicated remaining ~31 billion tokens (originally kept data)</span><br><span class="line">171 billion tokens obtained by individually deduplicating (without considering the other dumps) the ~460 billion tokens that had been removed from this dump in the iterative dedup process (originally removed data) 11</span><br><span class="line">Metric:</span><br><span class="line"></span><br><span class="line">Aggregate Score</span><br><span class="line">Rolling window:</span><br><span class="line"></span><br><span class="line">0</span><br><span class="line">These results show that, for this older dump taken in isolation, the data that was kept (10% of the original data) was actually worse than the 90% of data we removed 12 . This is also confirmed by visual inspection: originally kept data contains far more ads, lists of keywords and generally badly formatted text than originally removed data.</span><br><span class="line"></span><br><span class="line">TAKING A STEP BACK: INDIVIDUAL DUMP DEDUP</span><br><span class="line">We decided to experiment with an alternative approach: we deduplicated each dump with MinHash individually (independently of the other dumps). This resulted in 20 trillion tokens of data.</span><br><span class="line"></span><br><span class="line">When training on a random sample from this dataset we see that it now matches RefinedWebâ€™s performance (see curves below):</span><br><span class="line"></span><br><span class="line">Metric:</span><br><span class="line"></span><br><span class="line">Aggregate Score</span><br><span class="line">Rolling window:</span><br><span class="line"></span><br><span class="line">5</span><br><span class="line">We hypothesize that the main improvement gained from deduplication is the removal of very large clusters that are present in every single dump (you will find some examples of these clusters in the RefinedWeb paper, each containing hundreds of thousands of documents) and that further deduplication for clusters with a low number of duplicates (less than ~100 i.e. the number of dumps) actually harms performance: data that does not find a duplicate match in any other dump might actually be worse quality/more out of distribution (as evidenced by the results on the 2013-48 data).</span><br><span class="line"></span><br><span class="line">While you might see some performance improvement when deduplicating a few dumps together, at the scale of the entire dataset (all the dumps), the effect from this upsampling of lower quality data side effect seems to be more impactful.</span><br><span class="line"></span><br><span class="line">One possibility to consider is that as filtering quality improves, this effect may not be as prevalent, since the filtering might be able to remove some of this lower quality data. We also experimented with applying different, and often â€œlighterâ€, deduplication approaches on top of the individually deduplicated dumps. You can read about them further below.</span><br><span class="line"></span><br><span class="line">A NOTE ON MEASURING THE EFFECT OF DEDUPLICATION</span><br><span class="line">Given the nature of deduplication, its effect is not always very visible in a smaller slice of the dataset (such as 28B tokens, the size we used for our filtering ablations). Furthermore, one must consider the fact that there are specific effects at play when deduplicating across all CommonCrawl dumps, as some URLs/pages are recrawled from one dump to the next.</span><br><span class="line"></span><br><span class="line">To visualize the effect of scaling the number of training tokens on measuring deduplication impact, we considered the following (very extreme and unrealistic regarding the degree of duplication observed) theoretical scenario:</span><br><span class="line"></span><br><span class="line">there are 100 CommonCrawl dumps (roughly accurate)</span><br><span class="line">each dump has been perfectly individually deduplicated (every single document is unique in this dump)</span><br><span class="line">each dump is a perfect copy of each other (maximum possible duplication across dumps, effectively the worst case scenario)</span><br><span class="line">each dump has 200 billion tokens (for a total of 20 trillion, the resulting size of our individual dedup above)</span><br><span class="line">each dump is made up of documents of 1k tokens (200M documents per dump)</span><br><span class="line">We then simulated uniformly sampling documents from this entire dataset of 20 trillion tokens, to obtain subsets of 1B, 10B, 100B, 350B and 1T tokens. In the image below you can see how often each document would be repeated.</span><br><span class="line"></span><br><span class="line">For 1B almost all documents would be unique (#duplicates=1), despite the fact that in the entire dataset each document is repeated 100 times (once per dump). We start seeing some changes at the 100B scale (0.5% of the total dataset), with a large number of documents being repeated twice, and a few even 4-8 times. At the larger scale of 1T (5% of the total dataset), the majority of the documents are repeated up to 8 times, with some being repeated up to 16 times.</span><br><span class="line"></span><br><span class="line">We ran our performance evaluations for the deduplicated data at the 350B scale, which would, under this theoretical scenario, be made up of a significant portion of documents duplicated up to 8 times. This simulation illustrates the inherent difficulties associated with measuring deduplication impact on the training of LLMs, once the biggest duplicate clusters have been removed.</span><br><span class="line"></span><br><span class="line">OTHER (FAILED) GLOBAL APPROACHES</span><br><span class="line">To build on top of our newly found method (independently deduplicating each dump). We attempted to improve the performance by further deduplicating the independently minhash deduped 20 trillion tokens of data with alternative global (over all dumps) deduplication methods. We explored the following approaches:</span><br><span class="line"></span><br><span class="line">URL deduplication, where we only kept one document per normalized (lowercased) URL (71.5% of tokens removed, 5.6T left) â€” FineWeb URL dedup</span><br><span class="line">Line deduplication:</span><br><span class="line">remove all but 1 (randomly chosen) occurrence of each duplicated line (77.8% of tokens dropped, 4.4T left) â€” FineWeb line dedup</span><br><span class="line">same as above, but only removing duplicate lines with at least 10 words and dropping documents with fewer than 3 sentences after deduplication (85% of tokens dropped, 2.9T left) â€” FineWeb line dedup w/ min words</span><br><span class="line">remove all but 1 occurrence of each span of 3 duplicated lines with each number treated as 0 when finding duplicates, (80.9% of tokens removed, 3.7T left) â€” FineWeb 3-line dedup</span><br><span class="line">The performance of the models trained on each of these was consistently worse (even if to different degrees) than that of the original independently deduplicated data:</span><br><span class="line"></span><br><span class="line">Metric:</span><br><span class="line"></span><br><span class="line">Aggregate Score</span><br><span class="line">Rolling window:</span><br><span class="line"></span><br><span class="line">5</span><br><span class="line">Additional quality filtering</span><br><span class="line">By this point we had reached the same performance of the previous work we attempted to reproduce and extend: RefinedWeb, using our base filtering and independent MinHash. Still, on our aggregate of tasks, another heavily filtered dataset, the C4 dataset</span><br><span class="line">[29]</span><br><span class="line">, still showed stronger performances on some benchmarks of our evaluation suite.</span><br><span class="line"></span><br><span class="line">We therefore set out to find new filtering steps that would, at first, allow us to match the performance of C4 and, at a second stage, surpass it. A natural starting point was to look into the processing of C4 itself.</span><br><span class="line"></span><br><span class="line">C4: A DATASET THAT HAS STOOD THE TEST OF TIME</span><br><span class="line">The C4 dataset was first released in 2019. It was obtained from the 2019-18 CommonCrawl dump by removing non english data, applying some heuristic filters on both the line and document level, deduplicating on the line level, and removing documents containing words from a word blocklist.</span><br><span class="line"></span><br><span class="line">Despite its age and limited size for current standards (around 175B gpt2 tokens), this dataset is, to this day, a common sub-set of typical LLM training, being used in models such as the relatively recent Llama1</span><br><span class="line">[30]</span><br><span class="line">. This success is due to the strong performance that models trained on this dataset exhibit, excelling in particular on the Hellaswag benchmark</span><br><span class="line">[13]</span><br><span class="line">, one of the benchmarks in our â€œearly signalâ€ group with the highest signal-to-noise ratio. We experimented applying each of the different filters used in C4 to a baseline of the independently deduped FineWeb 2019-18 dump:</span><br><span class="line"></span><br><span class="line">Metric:</span><br><span class="line"></span><br><span class="line">HellaSwag</span><br><span class="line">Rolling window:</span><br><span class="line"></span><br><span class="line">3</span><br><span class="line">applying â€œAll filtersâ€ (drop lines not ending on punctuation marks, mentioning javascript and cookie notices + drop documents outside length thresholds, containing â€œlorem ipsumâ€ or a curly bracket, &#123;) allows us to match C4â€™s HellaSwag performance (&quot;All filters&quot; vs &quot;C4&quot; curves, respectively).</span><br><span class="line">The curly bracket filter, and the word lengths filter only give a small boost, removing 2.8% and 4.3% of tokens, respectively</span><br><span class="line">The terminal punctuation filter, by itself, gives the biggest individual boost, but removes around 30% of all tokens (!)</span><br><span class="line">The lorem_ipsum, javascript and policy rules each remove &lt;0.5% of training tokens, so we did not train on them individually</span><br><span class="line">&quot;All filters except the (very destructive) terminal_punct&quot; performs better than terminal_punct by itself, while removing less in total (~7%)</span><br><span class="line">We decided to apply all C4 filters mentioned above except the terminal punctuation one. We validated these results with a longer run, which you will find in a plot in the next section.</span><br><span class="line"></span><br><span class="line">A STATISTICAL APPROACH TO DEVELOP HEURISTIC FILTERS</span><br><span class="line">To develop new heuristic filters and select their thresholds we devised a systematic process:</span><br><span class="line"></span><br><span class="line">we started by collecting a very large list of high level statistics of our datasets (over fifty different metrics) ranging from common document-level metrics (e.g. number of lines, avg. line/word length, etc) to inter-document repetition metrics (inspired by MassiveText), on both a high quality and a lower quality web dataset;</span><br><span class="line">we selected the metrics for which the Wasserstein distance between the two distributions (of the metric computed on each dataset) was larger;</span><br><span class="line">we inspected the histograms of the two distributions and empirically chose a threshold that would make the lower quality dataset more closely resemble the higher quality one on this metric;</span><br><span class="line">we validated the resulting filter (metric-threshold pair) by using it on a reference dataset and running small ablations.</span><br><span class="line">Due to our (new) assumption that global MinHash greatly upsamples lower quality data in the oldest dumps, we computed metrics on both the independently MinHashed and the (worse quality) global MinHashed versions of the 2013-48 and 2015-22 crawls (two older crawls). We then compared the statistics at a macro level, by looking at the distribution of these metrics for each one.</span><br><span class="line"></span><br><span class="line">Perhaps not too surprisingly given our findings for deduplication, we found significant disparities in most of the metrics for the two deduplication methods. For instance, the line-char-duplicates metric (nb. of characters in duplicated lines / nb. characters), roughly doubled from the independent dedup (0.0053 for 2015-22 and 0.0058 for 2013-48), to the global dedup (0.011 for 2015-22 and 0.01 for 2013-48), indicating that the latter had higher inter-document repetition.</span><br><span class="line"></span><br><span class="line">Following the process listed above for these datasets yielded seventeen candidate metric-threshold pairs. In the image below, you can see three of these histograms:</span><br><span class="line"></span><br><span class="line">Metric:</span><br><span class="line"></span><br><span class="line">Lines Ended With Punctuation</span><br><span class="line">As an example, we inspected the histograms of &quot;fraction of lines ending with punctuation&quot; (see the image above) and observed an increased document density of global MinHash at around 0.12. We then filtered with this threshold and found that the removed data had a higher amount of short lists or consisted of only document layout text (&quot;Home&quot;, &quot;Sign up&quot;, etc).</span><br><span class="line"></span><br><span class="line">We then assessed the effectiveness of these seventeen newly created filters, by conducting several of our 28 billion tokens ablation runs on the 2019-18 crawl. Out of all those runs, we identified three filters (the ones based on the histograms above) that demonstrated the most significant improvements on the aggregate score:</span><br><span class="line"></span><br><span class="line">Remove documents where the fraction of lines ending with punctuation â‰¤ 0.12 (10.14% of tokens removed) â€” vs the 30% from the original C4 terminal punct filter</span><br><span class="line">Remove documents where the fraction of characters in duplicated lines â‰¥ 0.1 (12.47% of tokens removed) â€” the original MassiveText threshold for this ratio is â‰¥ 0.2</span><br><span class="line">Remove documents where the fraction of lines shorter than 30 characters â‰¥ 0.67 (3.73% of tokens removed)</span><br><span class="line">When applying the three together, ~22% of tokens were removed.</span><br><span class="line">Metric:</span><br><span class="line"></span><br><span class="line">Aggregate Score</span><br><span class="line">Rolling window:</span><br><span class="line"></span><br><span class="line">3</span><br><span class="line">These filters allowed us to further improve performance and to, notably, surpass the C4 dataset performance while providing a much larger dataset at the same time.</span><br><span class="line"></span><br><span class="line">The final ğŸ· FineWeb dataset</span><br><span class="line">The final ğŸ· FineWeb dataset comprises 15T tokens and includes the following previously mentioned steps, in order, each providing a performance boost on our group of benchmark tasks:</span><br><span class="line"></span><br><span class="line">base filtering</span><br><span class="line">independent MinHash deduplication per dump</span><br><span class="line">a selection of C4 filters</span><br><span class="line">our custom filters (mentioned in the previous section)</span><br><span class="line">Metric:</span><br><span class="line"></span><br><span class="line">Aggregate Score</span><br><span class="line">Rolling window:</span><br><span class="line"></span><br><span class="line">5</span><br><span class="line">COMPARISONS WITH OTHER WEB-SCALE DATASETS</span><br><span class="line">We compared ğŸ· FineWeb with the following datasets that are usually considered the highest quality openly accessible web-scale datasets (we also indicate for each the approximate number of tokens in the public version of the dataset):</span><br><span class="line"></span><br><span class="line">RefinedWeb (500B tokens)</span><br><span class="line">[21]</span><br><span class="line">C4 (172B tokens)</span><br><span class="line">[29]</span><br><span class="line">Dolma v1.6 (3T tokens) (the CommonCrawl part)</span><br><span class="line">[31]</span><br><span class="line"> 13</span><br><span class="line">The Pile (340B tokens)</span><br><span class="line">[32]</span><br><span class="line">SlimPajama (627B tokens)</span><br><span class="line">[33]</span><br><span class="line">RedPajama2 (20T tokens)</span><br><span class="line">[34]</span><br><span class="line"> (deduplicated)</span><br><span class="line">and our new ğŸ· FineWeb (15T tokens) (this report)</span><br><span class="line">You will find the 350B-tokens-trained ablation models openly accessible and gathered in this collection. We have uploaded checkpoints at every 1000 training steps. You will also find our full evaluation results here.</span><br><span class="line"></span><br><span class="line">Metric:</span><br><span class="line"></span><br><span class="line">Aggregate Score</span><br><span class="line">Rolling window:</span><br><span class="line"></span><br><span class="line">5</span><br><span class="line">ğŸ· FineWeb is thus â€“ to the best of our knowledge â€“ the open dataset leading to the current highest model performances while allowing to train on several trillion tokens.</span><br><span class="line"></span><br><span class="line">ğŸ“š FineWeb-Edu</span><br><span class="line"></span><br><span class="line">ğŸ“š FineWeb-Edu outperforms ğŸ· FineWeb and all other open web datasets on our group of evaluation tasks.</span><br><span class="line">ğŸ“š FineWeb-Edu is an additional development of FineWeb that we are excited to introduce in this tech report and openly release. ğŸ“š FineWeb-Edu is based on a new approach that has recently emerged for filtering LLM training datasets: using synthetic data to develop classifiers for identifying educational content. This technique was notably used in the trainings of Llama 3</span><br><span class="line">[1]</span><br><span class="line"> and Phi3</span><br><span class="line">[35]</span><br><span class="line">, but its large-scale impact on web data filtering has, in our opinion, thur far not been publicly explored to its full potential.</span><br><span class="line"></span><br><span class="line">The popular Phi3 models were trained on 3.3 and 4.8 trillion tokens, with the paper</span><br><span class="line">[35]</span><br><span class="line"> stating:</span><br><span class="line"></span><br><span class="line">Our training data consists of heavily filtered publicly available web data (according to the &#x27;educational level&#x27;) from various open internet sources, as well as synthetic LLM-generated data.</span><br><span class="line">Similarly, Llama 3 blog post</span><br><span class="line">[36]</span><br><span class="line"> notes:</span><br><span class="line"></span><br><span class="line">We found that previous generations of Llama are good at identifying high-quality data, so we used Llama 2 to help build the text-quality classifiers that are powering Llama 3.</span><br><span class="line">However, these classifiers and filtered datasets are not publicly available. To further enhance ğŸ· FineWeb&#x27;s quality, we developed an educational quality classifier using annotations generated by Llama-3-70B-Instruct to create ğŸ“š FineWeb-Edu.</span><br><span class="line"></span><br><span class="line">Annotating for educational quality at scale</span><br><span class="line">We used Llama-3-70B-Instruct to annotate 500k samples from ğŸ· FineWeb, scoring each for their educational quality on a scale from 0 to 5.</span><br><span class="line"></span><br><span class="line">We explored various prompt formats to automatically extract an educational score using an LLM and found that the additive scale by Yuan et al.</span><br><span class="line">[37]</span><br><span class="line"> worked best. This scale allows the LLM to reason about each additional point awarded, unlike the single-rating Likert scale which fits samples into predefined boxes. Then, to avoid the LLM favoring highly technical pages like arXiv abstracts and submissions, we focused on grade-school and middle-school level knowledge. By setting a threshold of 3 (on a scale of 0 to 5) during the filtering process, we were able to also retain some high-level educational pages.</span><br><span class="line"></span><br><span class="line">Prompt for LLM annotation</span><br><span class="line">Prompt used for Llama3 annotations of the educational score, also available here.</span><br><span class="line">In terms of open-weight models to use for annotating the data, we experimented with several models including Mixtral-8x7B-Instruct and Mixtral-8x22B-Instruct, Llama-3-70B-Instruct as well as a jury gathering the scores from these three models</span><br><span class="line">[38]</span><br><span class="line">. In our experiments we found that using Llama3 alone gave the most reliable results.</span><br><span class="line"></span><br><span class="line">Training a classifier</span><br><span class="line">To scale our annotations to the trillions of tokens in FineWeb, we used the Llama3-70B annotations to train a small classifier. The model we used was a Snowflake-arctic-embed embedding model with a classification head with a single regression output on top of it. We trained this model on the 450,000 Llama 3 annotations for 20 epochs with a learning rate of 3e-4, freezing the embedding and encoder layers. We saved the checkpoint with the highest F1 score on our held-out validation set of 45k samples, treating Llama 3 annotations as ground-truth. After training, we rounded the scores to integers from 0 to 5.</span><br><span class="line"></span><br><span class="line">We then converted the problem to a binary classification task by using a fixed threshold to determine if a file is educational. With a threshold of 3, the model achieved an F1 score of 82% on the validation set, indicating strong performance in distinguishing high-quality educational content.</span><br><span class="line"></span><br><span class="line">The classifier is available at: HuggingFaceFW/fineweb-edu-classifier. The training and inference code is available on GitHub.</span><br><span class="line"></span><br><span class="line">Filtering and results</span><br><span class="line">We applied the classifier to the 15T tokens of ğŸ· FineWeb, a process that required 6,000 H100 GPU hours. We investigated the impact of using different thresholds for the filtering and found that using a threshold of 3 gave the best overall results. Although using a threshold higher than 3 improves performance on knowledge and reasoning intensive benchmarks, it significantly degrades performance on HellaSwag and PIQA. The plot below shows the performance of each threshold compared to FineWeb on six different benchmarks; it uses a 1.82B model trained on 8B tokens.</span><br><span class="line"></span><br><span class="line">Metric:</span><br><span class="line"></span><br><span class="line">MMLU</span><br><span class="line">Note: this ablation was conducted on 8B tokens from the 2024-10 dump for both the FineWeb and FineWeb-Edu subsets, which might not be representative of the entire dataset. The next ablation shows that the findings for threshold 3 hold on a longer run of 350B tokens from all FineWeb dumps, except for HellaSwag, where we noticed a slight performance degradation.</span><br><span class="line"></span><br><span class="line">We built ğŸ“š FineWeb-Edu by filtering out samples with scores lower than 3. This removed 92% of the dataset, leaving us with 1.3 trillion educational tokens. To evaluate the effectiveness of this filtering at a larger scale, we conducted an ablation using a 1.82B model trained on 350 billion tokens, similar to the FineWeb filtering ablation mentioned above:</span><br><span class="line"></span><br><span class="line">Metric:</span><br><span class="line"></span><br><span class="line">MMLU</span><br><span class="line">Here are the key highlights of the ablation results above:</span><br><span class="line"></span><br><span class="line">ğŸ“š FineWeb-Edu surpasses ğŸ· FineWeb and all other open web datasets, with remarkable improvements on educational benchmarks such as MMLU, ARC, and OpenBookQA.</span><br><span class="line">It achieves the same performance with significantly less data, requiring 10x fewer tokens compared to C4 and Dolma to match MMLU results.</span><br><span class="line">This demonstrates the effectiveness of using classifiers trained on LLM annotations for large-scale data filtering.</span><br><span class="line">Given that a threshold of 2 also demonstrated strong performance while retaining more data, we are releasing an additional dataset filtered with this threshold, containing 5.4 trillion tokens under HuggingFaceFW/fineweb-edu-score-2.</span><br><span class="line"></span><br><span class="line">You can find the two datasets along with the classifier used for the filtering in this collection.</span><br><span class="line"></span><br><span class="line">Bonus: CommonCrawl over time</span><br><span class="line">Just like fine wine, not all crawls are created equal.</span><br><span class="line"></span><br><span class="line">While ablating filtering steps, we noticed that certain crawls outperformed others by a significant margin. We decided to investigate this phenomenon.</span><br><span class="line"></span><br><span class="line">Benchmark performance by crawl</span><br><span class="line">For each crawl, we trained two 1.8B models on 27 billion tokens randomly sampled from that crawl&#x27;s data (after the base filtering and MinHash deduplication steps), where each run had a different random 27BT sampling of this data. We trained 192 such models, totaling over 60 thousand H100 GPU-hours. We subsequently took the last 3 checkpoints for both runs and plotted the average of these 6 data points per crawl.</span><br><span class="line"></span><br><span class="line">The plot below clearly shows that some dumps perform far worse than others. Each year has a different color, and the number of crawls per year also varies.</span><br><span class="line"></span><br><span class="line">Metric:</span><br><span class="line"></span><br><span class="line">Aggregate Score</span><br><span class="line">We investigated possible causes for this behaviour such as changes in the most common URLs of each dump, as well as potential benchmark contamination, but could not find any conclusive explanation. We leave further investigation for future work.</span><br><span class="line"></span><br><span class="line">Synthetic data</span><br><span class="line">We wondered if the strong performance of the last few crawls could be, in part, attributed to the presence of a larger quantity of synthetic data (data generated by LLMs). Such a change would not be surprising due to the recent increase in popularity of LLMs, notably of ChatGPT.</span><br><span class="line"></span><br><span class="line">Since, to the best of our knowledge, there is no foolproof method to detect synthetic data, we opted to use a proxy metric: we measured the frequency of the following words in each crawl: &quot;delve&quot;, &quot;as a large language model&quot;, &quot;it&#x27;s important to note&quot;, &quot;rich tapestry&quot;, &quot;intertwined&quot;, &quot;certainly!&quot;, &quot;dive into&quot;, all of which are commonly used by ChatGPT.</span><br><span class="line"></span><br><span class="line">It is important to note that not all samples containing one of these phrases were necessarily generated by ChatGPT (and also that many ChatGPT generated samples do not contain any of these phrases), but assuming that the amount of synthetic data were to not change across crawls, one would expect these frequencies to remain approximately constant over time.</span><br><span class="line"></span><br><span class="line">The results are shown in the following plot:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">While the frequency remained approximately constant until 2023-14 (ChatGPT was released at the end of 2022), we find a steep increase of our proxy metric in recent crawls. While this simple test is not enough to conclude that ChatGPT completions and other synthetic data is improving the quality of the most recent crawl, it at the very least does not seem to drastically harm it.</span><br><span class="line"></span><br><span class="line">We expect to continue seeing increasing quantities of synthetic data on new CC crawls. However, while for relatively small trainings this data does not seem to harm performance (and might actually improve it), it is not clear that this holds for much larger trainings.</span><br><span class="line"></span><br><span class="line">Conclusion and looking forward</span><br><span class="line">Through our open science efforts we hope to keep shining a light on the black box that is the training of high performance large language models as well as to give every model trainer the ability to create state-of-the-art LLMs. We are excited to continue iterating on FineWeb and to release increasingly better filtered subsets of web data, in a fully open and reproducible manner.</span><br><span class="line"></span><br><span class="line">In the short term, we are looking forward to applying the learnings from (English) FineWeb to other languages. While English currently dominates the LLM landscape, we believe that making high quality web data in other languages as accessible as possible would be incredibly impactful.</span><br><span class="line"></span><br><span class="line">In a nutshell: the future is bright and exciting for studying the science of creating datasets at scale and in the open ğŸ¤—.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.nytimes.com/2024/05/28/technology/openai-gpt4-new-model.html?smid=nytcore-ios-share&amp;referringSource=articleShare&amp;sgrp=c-cb</span><br><span class="line">OpenAI Says It Has Begun Training a New Flagship A.I. Model</span><br><span class="line">The advanced A.I. system would succeed GPT-4, which powers ChatGPT. The company has also created a new safety committee to address A.I.â€™s risks.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Listen to this article Â· 4:48 min Learn more</span><br><span class="line">Share full article</span><br><span class="line"></span><br><span class="line">As Sam Altmanâ€™s OpenAI trains its new model, its new Safety and Security committee will work to hone policies and processes for safeguarding the technology, the company said.Credit...Jason Redmond/Agence France-Presse â€” Getty Images</span><br><span class="line">Cade Metz</span><br><span class="line">By Cade Metz</span><br><span class="line">Reporting from San Francisco</span><br><span class="line"></span><br><span class="line">May 28, 2024</span><br><span class="line">OpenAI said on Tuesday that it had begun training a new flagship artificial intelligence model that would succeed the GPT-4 technology that drives its popular online chatbot, ChatGPT.</span><br><span class="line"></span><br><span class="line">The San Francisco start-up, which is one of the worldâ€™s leading A.I. companies, said in a blog post that it expected the new model to bring â€œthe next level of capabilitiesâ€ as it strove to build â€œartificial general intelligence,â€ or A.G.I., a machine that can do anything the human brain can do. The new model would be an engine for A.I. products including chatbots, digital assistants akin to Appleâ€™s Siri, search engines and image generators.</span><br><span class="line"></span><br><span class="line">OpenAI also said it was creating a new Safety and Security Committee to explore how it should handle the risks posed by the new model and future technologies.</span><br><span class="line"></span><br><span class="line">â€œWhile we are proud to build and release models that are industry-leading on both capabilities and safety, we welcome a robust debate at this important moment,â€ the company said.</span><br><span class="line"></span><br><span class="line">OpenAI is aiming to move A.I. technology forward faster than its rivals, while also appeasing critics who say the technology is becoming increasingly dangerous, helping to spread disinformation, replace jobs and even threaten humanity. Experts disagree on when tech companies will reach artificial general intelligence, but companies including OpenAI, Google, Meta and Microsoft have steadily increased the power of A.I. technologies for more than a decade, demonstrating a noticeable leap roughly every two to three years.</span><br><span class="line"></span><br><span class="line">OpenAIâ€™s GPT-4, which was released in March 2023, enables chatbots and other software apps to answer questions, write emails, generate term papers and analyze data. An updated version of the technology, which was unveiled this month and is not yet widely available, can also generate images and respond to questions and commands in a highly conversational voice.</span><br><span class="line"></span><br><span class="line">Days after OpenAI showed the updated version â€” called GPT-4o â€” the actress Scarlett Johansson said it used a voice that sounded â€œeerily similar to mine.â€ She said that she had declined efforts by OpenAIâ€™s chief executive, Sam Altman, to license her voice for the product and that she had hired a lawyer and asked OpenAI to stop using the voice. The company said the voice was not Ms. Johanssonâ€™s.</span><br><span class="line"></span><br><span class="line">Technologies like GPT-4o learn their skills by analyzing vast amounts of digital data, including sounds, photos, videos, Wikipedia articles, books and news articles. The New York Times sued OpenAI and Microsoft in December, claiming copyright infringement of news content related to A.I. systems.</span><br><span class="line"></span><br><span class="line">Digital â€œtrainingâ€ of A.I. models can take months or even years. Once the training is completed, A.I. companies typically spend several more months testing the technology and fine-tuning it for public use.</span><br><span class="line"></span><br><span class="line">Editorsâ€™ Picks</span><br><span class="line">Is Heat Actually Good for Sore Muscles?</span><br><span class="line">Is This Season of â€˜Hacksâ€™ Trolling Jerry Seinfeld?</span><br><span class="line">Bill Waltonâ€™s Long, Special Relationship With the Grateful Dead</span><br><span class="line">That could mean that OpenAIâ€™s next model will not arrive for another nine months to a year or more.</span><br><span class="line"></span><br><span class="line">As OpenAI trains its new model, its new Safety and Security committee will work to hone policies and processes for safeguarding the technology, the company said. The committee includes Mr. Altman, as well as the OpenAI board members Bret Taylor, Adam Dâ€™Angelo and Nicole Seligman. The company said the new policies could be in place in the late summer or fall.</span><br><span class="line"></span><br><span class="line">This month, OpenAI said Ilya Sutskever, a co-founder and one of the leaders of its safety efforts, was leaving the company. This caused concern that OpenAI was not grappling enough with the dangers posed by A.I.</span><br><span class="line"></span><br><span class="line">Dr. Sutskever had joined three other board members in November to remove Mr. Altman from OpenAI, saying Mr. Altman could no longer be trusted with the companyâ€™s plan to create artificial general intelligence for the good of humanity. After a lobbying campaign by Mr. Altmanâ€™s allies, he was reinstated five days later and has since reasserted control over the company.</span><br><span class="line"></span><br><span class="line">Dr. Sutskever led what OpenAI called its Superalignment team, which explored ways of ensuring that future A.I. models would not do harm. Like others in the field, he had grown increasingly concerned that A.I. posed a threat to humanity.</span><br><span class="line"></span><br><span class="line">Jan Leike, who ran the Superalignment team with Dr. Sutskever, resigned from the company this month, leaving the teamâ€™s future in doubt.</span><br><span class="line"></span><br><span class="line">OpenAI has folded its long-term safety research into its larger efforts to ensure that its technologies are safe. That work will be led by John Schulman, another co-founder, who previously headed the team that created ChatGPT. The new safety committee will oversee Dr. Schulmanâ€™s research and provide guidance for how the company will address technological risks.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://cxotoday.com/specials/maximizing-roi-best-practices-for-scaling-generative-ai-across-the-enterprise/</span><br><span class="line">Gartner- ROI ê·¹ëŒ€í™”ë¥¼ ìœ„í•œ ì „ì‚¬ì  ìƒì„±í˜• AI êµ¬ì¶• ëª¨ë²”ì‚¬ë¡€</span><br><span class="line">ìƒì„±í˜• AIëŠ” ë‹¤ì–‘í•œ ì‚°ì—… ë¶„ì•¼ì— ë¹„ì¦ˆë‹ˆìŠ¤ í˜ì‹ ì„ ì¼ìœ¼í‚¬ ìˆ˜ ìˆëŠ” ì ì¬ë ¥ì„ ê°–ê³  ìˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ ë° ê¸°ìˆ  ë¦¬ë”ë“¤ì€ ìƒì„±í˜• AIê°€ ê°–ê³  ìˆëŠ” ì¥ì ì´ ì ì¬ì ì¸ ìœ„í—˜ë³´ë‹¤ í¬ë‹¤ê³  í™•ì‹ í•œë‹¤. ê·¸ëŸ¬ë‚˜ ìƒì„±í˜• AIì˜ ëª¨ë²” ì‚¬ë¡€ì— ëŒ€í•œ ì´í•´ë„ ë¶€ì¡±ì€ ê¸°ì—…ë“¤ì´ ìƒì„±í˜• AIë„ì…ì„ ê°€ë¡œë§‰ëŠ” í•œ ì›ì¸ì´ ë˜ê³  ìˆë‹¤.</span><br><span class="line">ê°€íŠ¸ë„ˆëŠ” 2025ë…„ê¹Œì§€ ìƒì„±í˜• AI í”„ë¡œì íŠ¸ ì¤‘ ìµœì†Œ 30%ê°€ ë°ì´í„° í’ˆì§ˆ ì €í•˜, ë¶€ì ì ˆí•œ ë¦¬ìŠ¤í¬ ê´€ë¦¬, ë¹„ìš© ì¦ê°€ ë“±ìœ¼ë¡œ ì¸í•´ ì‹¤ì¦ë‹¨ê³„(POC) ì´í›„ ì¤‘ë‹¨ë  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡í–ˆë‹¤. ìµœê³ ì •ë³´ì±…ì„ì(CIO)ê°€ ìƒì„±í˜• AI í™•ì¥ì„ ìœ„í•´ ë‹¤ì–‘í•œ ëª¨ë²” ì‚¬ë¡€ë¥¼ ì°¸ê³ í•´ì•¼ í•˜ëŠ” ì´ìœ ì´ë‹¤.</span><br><span class="line">í™œìš© ì‚¬ë¡€ ìš°ì„ ìˆœìœ„ ì„¤ì •ì„ ìœ„í•œ í”„ë¡œì„¸ìŠ¤ êµ¬ì¶•</span><br><span class="line">ìƒì„±í˜• AIë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” AI êµ¬ì¶• ëª©í‘œë¥¼ ì„¤ì •í•˜ê³  ë‹¬ì„± ê°€ëŠ¥í•œ ëª©í‘œì— ëŒ€í•œ ì‚¬ì „ ë…¼ì˜ë¥¼ ì§„í–‰í•˜ëŠ” ê²ƒì´ë‹¤. ì´í›„ì—ëŠ” ìƒì„±í˜• AI ê¸°ìˆ ë¡œ ì‹œë²” ìš´ì˜í•  ìˆ˜ ìˆëŠ” ì ì¬ì  í™œìš© ì‚¬ë¡€ë¥¼ ìˆ˜ì§‘í•´ì•¼ í•œë‹¤. í™œìš© ì‚¬ë¡€ ìš°ì„ ìˆœìœ„ ì„¤ì •ì€ ì¡°ì§ì˜ í•„ìˆ˜ì ì¸ ì „ëµ ìš”ì†Œë‹¤. ìš°ì„ ìˆœìœ„ ì„¤ì •ì€ ê¸°ìˆ ì˜ ë§¤ë ¥ë„ë‚˜ â€˜í™”ë ¤í•œ ë°ëª¨â€™ì— ì˜í•´ì„œ ê²°ì •ë¼ì„œëŠ” ì•ˆ ë˜ê³ , ì¡°ì§ ê°€ì¹˜ ì œì•ˆì— ëŒ€í•œ ì´ì²´ì ì¸ í‰ê°€ì— ë”°ë¼ ê²°ì •ë¼ì•¼ í•œë‹¤. ê³µê¸‰ì—…ì²´ê°€ ê·¸ë“¤ì˜ ì—­ëŸ‰ì„ ë°˜ì˜í•´ í• ì¸ëœ ì‹¤ì¦ ê³¼ì •ì„ ì œì•ˆí•˜ê¸°ë„ í•œë‹¤.</span><br><span class="line">í•˜ì§€ë§Œ í•µì‹¬ì€ â–³ì‹¤ì§ˆì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ ì œê³µ â–³ì‹¤í˜„ ê°€ëŠ¥ì„±ì´ ë†’ì€ í™œìš© ì‚¬ë¡€ ì‹ë³„ â–³ê·œëª¨ í™•ì¥ ì‹œ ìœ„í—˜ê³¼ ë¹„ìš© ì¦ê°€ íšŒí”¼ì— ìˆë‹¤. ë”°ë¼ì„œ ìš°ì„ ìˆœìœ„ë¥¼ ì •í•˜ëŠ” ì‘ì—…ì—ëŠ” ê¸°ìˆ íŒ€ê³¼ ë”ë¶ˆì–´ ìƒì„±í˜• AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ í™œìš©í•  ë¹„ì¦ˆë‹ˆìŠ¤ ë¶€ì„œ, ë³´ì•ˆ, ë¦¬ìŠ¤í¬íŒ€ê¹Œì§€ ëª¨ë‘ ì°¸ì—¬í•´ì•¼ í•œë‹¤.</span><br><span class="line">êµ¬ì¶• í˜¹ì€ êµ¬ë§¤ë¥¼ ìœ„í•œ ì˜ì‚¬ ê²°ì • í”„ë ˆì„ì›Œí¬ ê°œë°œ</span><br><span class="line">ìƒì„±í˜• AIë¥¼ í™•ì¥í•˜ë ¤ë©´ ì¡°ì§ ë‚´ ì ì¬ì  í™œìš© ì‚¬ë¡€ì— ëŒ€í•´ êµ¬ì¶• í˜¹ì€ êµ¬ë§¤ ì˜ì‚¬ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆëŠ” ì²´ê³„ì ì¸ ì ‘ê·¼ ë°©ì‹ì´ í•„ìš”í•˜ë‹¤. ê²½ìŸ ìš°ìœ„ë¥¼ í™•ë³´í•  ìˆ˜ ìˆê³ , í”„ë¡œì„¸ìŠ¤ì— í•„ìš”í•œ ê¸°ìˆ ê³¼ ì§€ì‹ì„ ê°–ì¶”ê³  ìˆë‹¤ê³  íŒë‹¨ë˜ë©´ AIë¥¼ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. CIOëŠ” ìƒì„±í˜• AIë¥¼ êµ¬ì¶•í• ì§€ êµ¬ë§¤í• ì§€ì— ëŒ€í•œ ê²°ì •ì„ ë‚´ë¦¬ê¸° ì „ì— ì ‘ê·¼ ë°©ì‹ì˜ ëª¨ë“  ì¥ë‹¨ì ì„ í‰ê°€í•´ì•¼ í•œë‹¤.</span><br><span class="line">í™•ì¥ì„±ì„ ìœ„í•œ ì‹œë²”ìš´ì˜</span><br><span class="line">ê¸°ì—…ì€ ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ì‹œë²”ì ìœ¼ë¡œ ìš´ì˜í•´ ì¡°ì§ ë‚´ì—ì„œ í™œìš© ê°€ëŠ¥í•œ ê¸°ìˆ ì„ ì²´ë“í•˜ê³ , ì‹¤í—˜ì„ í†µí•´ í•™ìŠµí•´ì•¼ í•œë‹¤. ì‹œë²”ì ìœ¼ë¡œ ìš´ì˜í•  ë•Œ ë°ì´í„°, ê°œì¸ì •ë³´ ë³´í˜¸, ë³´ì•ˆ, ì‚¬ìš©ì„± ë“±ì„ ê¼¼ê¼¼í•˜ê²Œ ì‚´í´ì•¼ í•œë‹¤. ë‹¤ìŒìœ¼ë¡œ í™•ì¥, ê°œì„ , ì¤‘ë‹¨ ë“±ì— ëŒ€í•œ ê²°ì •ì„ ë‚´ë¦¬ê¸° ìœ„í•´ì„œëŠ” í™œìš© ì‚¬ë¡€ë¥¼ ë“¤ì—¬ë‹¤ë³´ê³  í…ŒìŠ¤íŠ¸ ì „ì— ë°˜ë“œì‹œ ì• ìì¼ ì‚¬ê³ ë°©ì‹(Agile Mindset)ì„ í™•ë³´í•´ì•¼ í•œë‹¤.</span><br><span class="line">ì¡°ì§ ì „ë°˜ì— ì•ˆì „í•œ ì‹¤í—˜ì„ í•  ìˆ˜ ìˆëŠ” ìƒŒë“œë°•ìŠ¤ í™˜ê²½ì´ êµ¬ì¶•ë¼ì•¼ í•˜ëŠ” ê²ƒë„ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤. ì ì ˆí•œ ë³´ì•ˆ ë° ê°œì¸ì •ë³´ ë³´í˜¸ ì¡°ì¹˜ëŠ” ë¬¼ë¡ , ìƒŒë“œë°•ìŠ¤ ë‚´ì—ì„œ ì‹¤í—˜ì„ ë°˜ë³µì„ ìœ„í•œ ì—¬ëŸ¬ ìƒì„±í˜• AI ëª¨ë¸ì— ëŒ€í•œ ê°€ìš©ì„±ì„ ê°–ì¶°ì•¼ í•œë‹¤. ì´ë¥¼ í†µí•´ ê°œë°œìëŠ” íŠ¹ì • í™œìš© ì‚¬ë¡€ì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ìœ ì—°í•˜ê²Œ ì„ íƒí•  ìˆ˜ ìˆë‹¤.</span><br><span class="line">ìœ ì—°í•œ ìƒì„±í˜• AI í”Œë«í¼ ì•„í‚¤í…ì²˜ ì„¤ê³„</span><br><span class="line">ìƒì„±í˜• AI í™˜ê²½ì€ ì¸í”„ë¼, ëª¨ë¸, AI ì—”ì§€ë‹ˆì–´ë§ ë„êµ¬, ì• í”Œë¦¬ì¼€ì´ì…˜ì´ë¼ëŠ” ë„¤ ê°œì˜ ì¤‘ìš”í•œ ë ˆì´ì–´ë¡œ êµ¬ì„±ëœë‹¤. ê¸°ì—…ì€ ìì‚¬ í”Œë«í¼ ì•„í‚¤í…ì²˜ê°€ ë†’ì€ ìœ ì—°ì„±ê³¼ í™•ì¥ì„±ì„ ê°–ê³  ìˆìœ¼ë©° ê±°ë²„ë„ŒìŠ¤ê°€ í¬í•¨ë¼ ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤. ìƒì„±í˜• AI ëª¨ë¸ í™˜ê²½ì€ ë¹ ë¥´ê²Œ ë³€í™”í•˜ê³  ìˆìœ¼ë©°, ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ê³¼ ë„ë©”ì¸ ëª¨ë¸ì´ ê¸‰ë¶€ìƒí•˜ëŠ” ê²ƒì²˜ëŸ¼ í˜„ì¬ë¡œì„œëŠ” ìƒìƒí•  ìˆ˜ ì—†ëŠ” ë°©ì‹ìœ¼ë¡œ ëŠì„ì—†ì´ ì§„í™”í•  ê²ƒì´ë‹¤. ì´ ë•Œë¬¸ì— ì¡°ì§ì€ ì¶”í›„ ëª¨ë¸ êµì²´ê°€ ê°€ëŠ¥í•˜ë„ë¡ ë†’ì€ ìœ ì—°ì„±ì„ ê°€ì§„ ì•„í‚¤í…ì²˜ë¥¼ í™•ë³´í•´ì•¼ í•œë‹¤.</span><br><span class="line">ìƒì„±í˜• AIì˜ ìµœì „ì„ ì— ìˆëŠ” â€˜ì±…ì„ê° ìˆëŠ” AIâ€™</span><br><span class="line">ìƒì„±í˜• AIëŠ” ê¸°ì—…ë“¤ì—ê²Œ í° ê¸°íšŒë¥¼ ì œê³µí•œë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°íšŒë¥¼ ì œê³µí•˜ëŠ” ë§Œí¼ ìœ„í—˜ë¶€ë‹´ë„ ë†’ë‹¤. â€˜ì±…ì„ê° ìˆëŠ” AIâ€™ë¼ëŠ” ë§ì´ ë‚˜ì˜¨ ê²ƒë„ ì´ëŸ° ì´ìœ  ë•Œë¬¸ì´ë‹¤. â€˜ì±…ì„ê° ìˆëŠ” AIâ€™ëŠ” AI ë„ì… ì‹œ ì ì ˆí•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë° ìœ¤ë¦¬ì  ì„ íƒì„ ë‚´ë¦¬ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ì¸¡ë©´ì„ í¬ê´„í•˜ëŠ” ìš©ì–´ë‹¤.</span><br><span class="line">ì´ëŸ¬í•œ ëª…í™•í•œ í”„ë ˆì„ì›Œí¬ê°€ ì—†ë‹¤ë©´ ì¡°ì§ì€ í•´ë‹¹ ê¸°ìˆ ì˜ ì´ì ê³¼ ë¦¬ìŠ¤í¬ ê°„ ê· í˜•ì„ ë§ì¶”ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê²Œ ëœë‹¤. ì¡°ì§ì€ ê³µì •ì„±, ìœ í•´ì„± ì™„í™”, ìœ¤ë¦¬, ìœ„í—˜ ê´€ë¦¬, ê°œì¸ì •ë³´ ë³´í˜¸, ì§€ì† ê°€ëŠ¥ì„±, ê·œì • ì¤€ìˆ˜ ë“± ì£¼ìš” ì˜ì—­ì— ê±¸ì³ ëª…í™•í•œ ì›ì¹™ê³¼ ì •ì±…ì„ ìˆ˜ë¦½í•´ ì±…ì„ê° ìˆëŠ” AIì— ëŒ€í•œ ë¹„ì „ì„ ì •ì˜í•˜ê³  ê³µí‘œí•´ì•¼ í•œë‹¤.</span><br><span class="line">ë°ì´í„° ë° AI ë¦¬í„°ëŸ¬ì‹œì— ëŒ€í•œ íˆ¬ì</span><br><span class="line">ì „í†µì ì¸ AIì™€ ë‹¬ë¦¬ ìƒì„±í˜• AIëŠ” ë‹¤ìˆ˜ì˜ ì§ì›ë“¤ì´ ì ê·¹ì ì´ê³  ì§ì ‘ì ìœ¼ë¡œ í™œìš©í•œë‹¤. ìƒì„±í˜• AIì˜ ê´‘ë²”ìœ„í•œ ë°°í¬ë¥¼ ìœ„í•´ì„œëŠ” ê´€ë ¨ í™œìš© ì‚¬ë¡€ë¥¼ ì‹ë³„í•˜ê³  í•´ë‹¹ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬í˜„í•˜ê³  ìš´ì˜í•  ìˆ˜ ìˆëŠ” ì—­ëŸ‰ì´ ìˆì–´ì•¼ í•œë‹¤. ë˜í•œ ë§¥ë½ ë‚´ì—ì„œ AIë¥¼ í™œìš©í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì¸ AI ë¦¬í„°ëŸ¬ì‹œì—ë„ ì¤‘ì ì„ ë‘¬ì•¼ í•œë‹¤.</span><br><span class="line">ê¸°ì—…ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶€ì„œë¥¼ ëŒ€ìƒìœ¼ë¡œ ë§ì¶¤í˜• êµìœ¡ì„ ì‹¤ì‹œí•˜ê³ , ê³ ìœ„ ê²½ì˜ì§„ì„ ëŒ€ìƒìœ¼ë¡œ ë°ì´í„° ë° AI ë¦¬í„°ëŸ¬ì‹œ ê¸°ìˆ ì„ êµìœ¡í•´ì•¼ í•œë‹¤. ë˜í•œ ì‹ ì†í•œ ì—”ì§€ë‹ˆì–´ë§, ëª¨ë¸ ê²€ì¦ ë° íŠœë‹, ì¸í”„ë¼ ê´€ë¦¬, ì±…ì„ê° ìˆëŠ” AIì™€ ê°™ì€ ë¶„ì•¼ì—ì„œ ìƒì„±í˜• AIì— íŠ¹í™”ëœ ê¸°ìˆ ì„ ê°–ì¶˜ ê¸°ìˆ íŒ€ì˜ ì—­ëŸ‰ì„ ê°•í™”í•˜ëŠ” ê³¼ì •ì´ ë°˜ë“œì‹œ í•„ìš”í•˜ë‹¤.</span><br><span class="line"></span><br><span class="line">Maximizing ROI: Best Practices for Scaling Generative AI Across the Enterprise CXOtoday News Desk2 months ago By Arun Chandrasekaran Generative artificial intelligence (GenAI) has the potential to revolutionize businesses in various industries. Most business and technology leaders are convinced that the advantages of GenAI outweigh any potential risks. However, lack of understanding about emerging industry best practices is constraining organization wide pilots and scalable production deployments. Through 2025, Gartner predicts that at least 30% of GenAI projects will be abandoned after proof of concept (POC) due to poor data quality, inadequate risk controls, escalating costs or unclear business value. To avoid obstacles to scaling GenAI, chief information officers (CIOs) must embrace the following emerging industry best practices. Establish a Continuous Process to Prioritize Use Cases The initial step in the GenAI journey is to establish the organizationâ€™s AI goals and engage in a preliminary discussion about what is achievable. The subsequent step involves gathering potential use cases that can be piloted with GenAI technologies. Prioritizing GenAI use cases is a strategic imperative for organizations. Such prioritization should not be driven solely by the appeal of technology, or the â€œflashiest demo,â€ but by a holistic assessment of its value proposition to the organization. While vendors may suggest discounted POCs reflecting their capabilities, the key is to identify use cases that deliver tangible business value and are the most technically feasible and avoid those that could lead to growing risks and costs when scaled in production. The task of prioritizing should be a collective decision, involving not only the technology teams but also the business lines that will utilize the GenAI application as well as security and risk teams. Create a Decision Framework for Build Versus Buy Scaling GenAI requires a systematic approach to build versus buy decisions for the many potential use cases in the organization. Ideally, businesses should consider building an AI product when it can provide a competitive advantage in their industry and when they have the necessary skills and knowledge for the process. In the context of GenAI, use cases where enterprises want to minimize risks for regulatory or brand equity reasons may also warrant a build approach. CIOs must evaluate all pros and cons of the approach before determining their build-versus-buy decisions for GenAI. Pilot Use Cases for Scalability Businesses must run pilots to try new ideas, build muscle memory within the organization on the art of the possible and learn by experimentation. They must ensure that pilots are built with scalability in mind by envisioning future data, privacy, security and usability needs. An agile mindset must be adopted before experimenting and testing the use cases to determine the next step â€” scale, refine or stop. A sandbox environment must be established to allow for safe experimentation throughout the organization. This should include appropriate security and privacy measures, as well as the availability of multiple GenAI models for experimentation and iteration within the sandbox. This allows developers to have the flexibility to select the most suitable models for each specific use case. Design a Composable Generative AI Platform Architecture The GenAI landscape consists of four critical layers â€” infrastructure, models, AI engineering tools and applications. Enterprises must ensure that their platform architecture is composable, scalable and embedded with governance upfront. The GenAI model landscape is fast-paced and will constantly evolve, often in ways we cannot envision today (such as the rise of open-source models and domain models). Organizations must ensure there is enough flexibility in their architecture to swap models through composability. Responsible AI Is at the Forefront of All Generative AI Efforts GenAI creates not only new opportunities, but also new risks. Responsible AI is an umbrella term for all the different aspects of making appropriate business and ethical choices when adopting AI. Without a clear responsible AI framework, organizations will struggle to balance the benefits and risks of this technology. Organizations need to define and publicize a vision for responsible AI with clear principles and policies across focus areas like fairness, toxicity mitigation, ethics, risk management, privacy, sustainability and regulatory compliance. Invest in Data and AI Literacy Unlike traditional AI, GenAI is poised for active and direct use by a large segment of employees. This broad deployment requires a strong emphasis on AI literacy: the ability to utilize AI in context with competency to identify relevant use cases, as well as implement and operate corresponding AI applications. Enterprises must create and conduct personalized training programs targeting various business functions and training senior management on the data and AI literacy skills. Upskilling the technology teams with GenAI-specific skills in areas such as prompt engineering, model validation and tuning, infrastructure management and responsible AI is crucial. Additional analysis on GenAI for enterprises will be presented during the Gartner Data &amp; Analytics Summit, taking place April 24-25 in Mumbai, India.       (The author is Arun Chandrasekaran, Distinguished VP Analyst at Gartner, and the views expressed in this article are his personal)</span><br><span class="line"></span><br><span class="line">Read more at: https://cxotoday.com/specials/maximizing-roi-best-practices-for-scaling-generative-ai-across-the-enterprise/</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://v.daum.net/v/20240509060115802</span><br><span class="line">&quot;ì •ê·œì§ 40%ëŠ” AI ì‚¬ì—… ì¸ë ¥&quot;â€¦ì§„ì§œ AIì»´í¼ë‹ˆë¡œ ê±°ë“­ë‚œ SKT</span><br><span class="line">ìœ¤ì •ë¯¼ ê¸°ì2024. 5. 9. 06:01</span><br><span class="line">ìŒì„±ìœ¼ë¡œ ë“£ê¸°ë²ˆì—­ ì„¤ì •ê¸€ì”¨í¬ê¸° ì¡°ì ˆí•˜ê¸°ì¸ì‡„í•˜ê¸°</span><br><span class="line">SKT, ì´í†µì‚¬ ì‹¤ì ì—ì„œ ë³´ê¸° ë“œë¬¸ AI ì¸ë ¥ í˜„í™© ê³µê°œ</span><br><span class="line">AI ë§¤ì¶œ ì„±ê³¼ ê°€ì‹œí™”â€¦&quot;ëª¨ë“  ë°©ì•ˆ ë™ì›í•´ AI íˆ¬ì ì¬ì› í™•ë³´&quot;</span><br><span class="line">[ì„œìš¸=ë‰´ì‹œìŠ¤]ìœ¤ì •ë¯¼ ê¸°ì =</span><br><span class="line"></span><br><span class="line">[ì„œìš¸=ë‰´ì‹œìŠ¤] SKí…”ë ˆì½¤ì€ ì—°ê²° ê¸°ì¤€ 1ë¶„ê¸° ë§¤ì¶œ 4ì¡°4746ì–µì›, ì˜ì—…ì´ìµ 4985ì–µì›, ë‹¹ê¸°ìˆœì´ìµ 3619ì–µì›ì„ ê¸°ë¡í–ˆë‹¤ê³  8ì¼ ë°í˜”ë‹¤. ì‚¬ì§„ì€ AI ì‚¬ì—… ì„±ê³¼ (ì‚¬ì§„=SKí…”ë ˆì½¤ ì œê³µ) *ì¬íŒë§¤ ë° DB ê¸ˆì§€</span><br><span class="line"></span><br><span class="line">[ì„œìš¸=ë‰´ì‹œìŠ¤] SKí…”ë ˆì½¤ì€ ì—°ê²° ê¸°ì¤€ 1ë¶„ê¸° ë§¤ì¶œ 4ì¡°4746ì–µì›, ì˜ì—…ì´ìµ 4985ì–µì›, ë‹¹ê¸°ìˆœì´ìµ 3619ì–µì›ì„ ê¸°ë¡í–ˆë‹¤ê³  8ì¼ ë°í˜”ë‹¤. ì‚¬ì§„ì€ AI ì‚¬ì—… ì„±ê³¼ (ì‚¬ì§„=SKí…”ë ˆì½¤ ì œê³µ) *ì¬íŒë§¤ ë° DB ê¸ˆì§€</span><br><span class="line">&quot;ì •ê·œì§ 5286ëª… ì¤‘ ì¸ê³µì§€ëŠ¥(AI) ì‚¬ì—… ê´€ë ¨ ì¸ë ¥ ë¹„ì¤‘ì€ 40%ë‹¤.&quot;</span><br><span class="line"></span><br><span class="line">SKí…”ë ˆì½¤ì€ ê·¸ë™ì•ˆ ì´ë™í†µì‹ ì‚¬ì— ë³¼ ìˆ˜ ì—†ì—ˆë˜ ìƒˆë¡œìš´ ì‹¤ì ì„ ê³µê°œí–ˆë‹¤. ë°”ë¡œ AI ì¸ë ¥ ìˆ˜ë‹¤. ê¸€ë¡œë²Œ AI ì»´í¼ë‹ˆë¡œì˜ ë„ì•½ì„ ì„ ì–¸í•œ SKí…”ë ˆì½¤ì´ ì „í†µì ì¸ ì´ë™í†µì‹ ì‚¬ì˜ ëª¨ìŠµì— ë²—ì–´ë‚˜ê¸° ìœ„í•´ ë‚´ë†“ì€ ì§€í‘œë‹¤.</span><br><span class="line"></span><br><span class="line">5G(5ì„¸ëŒ€ ì´ë™í†µì‹ ) ì‹œì¥ì´ ì„±ìˆ™ê¸°ì— ì ‘ì–´ë“¤ì—ˆê³  ì•Œëœ°í° ì‹œì¥ë„ í™•ëŒ€ë˜ë©´ì„œ í†µì‹ ì„œë¹„ìŠ¤ ì‚°ì—… ì„±ì¥ì´ ë‘”í™”ì„¸ë¥¼ ë³´ì´ê³  ìˆë‹¤. ì´ì— SKí…”ë ˆì½¤ì€ ì¼ì°ì´ AIì™€ ê´€ë ¨í•œ ë¹„í†µì‹  ì‚¬ì—…ì— ì£¼ë ¥í–ˆê³  AI ë¶„ì•¼ ìš°ìˆ˜ ì¸ë ¥ë„ í™•ë³´í•˜ë©´ì„œ ê´€ë ¨ ì‚¬ì—… ë§¤ì¶œë„ ì„±ì¥ì„¸ë¥¼ ë³´ì´ê³  ìˆë‹¤.</span><br><span class="line"></span><br><span class="line">ìƒì„±í˜• AI ìˆ˜ìš” ì¦ê°€ì— ë°ì´í„°ì„¼í„°Â·í´ë¼ìš°ë“œ í˜¸í™©</span><br><span class="line">ì—ì´ë‹· ê°€ì…ì 400ë§Œëª… ë‹¬ì„± ë“± ì„œë¹„ìŠ¤ ì„±ê³¼ ê°€ì‹œí™”</span><br><span class="line">[ì„œìš¸=ë‰´ì‹œìŠ¤] SKí…”ë ˆì½¤ì´ ê¸€ë¡œë²Œ ì„œë²„ ì œì¡° ìŠ¤íƒ€íŠ¸ì—… ê¸°ì—…ì¸ ìŠˆí¼ë§ˆì´í¬ë¡œì™€ ê¸€ë¡œë²Œ ê·¸ë˜í”½ì²˜ë¦¬ì¥ì¹˜(GPU) í´ë¼ìš°ë“œ íšŒì‚¬ì¸ ëŒë‹¤ì™€ í˜‘ë ¥í•´ ì¸ê³µì§€ëŠ¥ ë°ì´í„°ì„¼í„°(AI DC) ì‹œì¥ ê³µëµì— ë‚˜ì„ ë‹¤ê³  29ì¼ ë°í˜”ë‹¤. 28ì¼(í˜„ì§€ì‹œê°„) MWC24 ì „ì‹œì¥ì—ì„œ ìœ ì˜ìƒ SKí…”ë ˆì½¤ ì‚¬ì¥(ì™¼ìª½ì—ì„œ 10ë²ˆì§¸)ê³¼ ì„¼ë¦¬ ì²¸ ìŠˆí¼ë§ˆì´í¬ë¡œ ìµœê³ ì„±ì¥ì±…ì„ì(CGO, ì™¼ìª½ì—ì„œ 11ë²ˆì§¸)ê°€ AIë°ì´í„°ì„¼í„°(AIDC) ë¶„ì•¼ í˜‘ë ¥ì„ ìœ„í•œ MOU ì²´ê²° í›„ ê¸°ë… ì´¬ì˜í•˜ëŠ” ëª¨ìŠµ (ì‚¬ì§„=SKí…”ë ˆì½¤ ì œê³µ) *ì¬íŒë§¤ ë° DB ê¸ˆì§€</span><br><span class="line"></span><br><span class="line">[ì„œìš¸=ë‰´ì‹œìŠ¤] SKí…”ë ˆì½¤ì´ ê¸€ë¡œë²Œ ì„œë²„ ì œì¡° ìŠ¤íƒ€íŠ¸ì—… ê¸°ì—…ì¸ ìŠˆí¼ë§ˆì´í¬ë¡œì™€ ê¸€ë¡œë²Œ ê·¸ë˜í”½ì²˜ë¦¬ì¥ì¹˜(GPU) í´ë¼ìš°ë“œ íšŒì‚¬ì¸ ëŒë‹¤ì™€ í˜‘ë ¥í•´ ì¸ê³µì§€ëŠ¥ ë°ì´í„°ì„¼í„°(AI DC) ì‹œì¥ ê³µëµì— ë‚˜ì„ ë‹¤ê³  29ì¼ ë°í˜”ë‹¤. 28ì¼(í˜„ì§€ì‹œê°„) MWC24 ì „ì‹œì¥ì—ì„œ ìœ ì˜ìƒ SKí…”ë ˆì½¤ ì‚¬ì¥(ì™¼ìª½ì—ì„œ 10ë²ˆì§¸)ê³¼ ì„¼ë¦¬ ì²¸ ìŠˆí¼ë§ˆì´í¬ë¡œ ìµœê³ ì„±ì¥ì±…ì„ì(CGO, ì™¼ìª½ì—ì„œ 11ë²ˆì§¸)ê°€ AIë°ì´í„°ì„¼í„°(AIDC) ë¶„ì•¼ í˜‘ë ¥ì„ ìœ„í•œ MOU ì²´ê²° í›„ ê¸°ë… ì´¬ì˜í•˜ëŠ” ëª¨ìŠµ (ì‚¬ì§„=SKí…”ë ˆì½¤ ì œê³µ) *ì¬íŒë§¤ ë° DB ê¸ˆì§€</span><br><span class="line">[ì„œìš¸=ë‰´ì‹œìŠ¤]SKí…”ë ˆì½¤ì€ ê¸°ì—… í˜„ì¥ì—ì„œ ì‹¤ì œ ì§„í–‰ ì¤‘ì¸ ì—°êµ¬ê³¼ì œ ìˆ˜í–‰ì„ í†µí•´ ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì˜ ë¯¸ë˜ ì¸ì¬ë¥¼ ìœ¡ì„±í•˜ëŠ” &#x27;SKT AI í ë¡œìš°ì‹­&#x27; 5ê¸° ê³¼ì •ì„ ì„±ê³µì ìœ¼ë¡œ ë§ˆë¬´ë¦¬í–ˆë‹¤ê³  20ì¼ ë°í˜”ë‹¤. ì‚¬ì§„ì€ SKT AI í ë¡œìš°ì‹­ 5ê¸° í•™ìƒë“¤ì´ ìˆ˜ë£Œì‹ì„ ë§ˆì¹˜ê³  ê¸°ë…ì´¬ì˜ì„ í•˜ëŠ” ëª¨ìŠµ. (ì‚¬ì§„=SKí…”ë ˆì½¤ ì œê³µ)</span><br><span class="line"></span><br><span class="line">[ì„œìš¸=ë‰´ì‹œìŠ¤]SKí…”ë ˆì½¤ì€ ê¸°ì—… í˜„ì¥ì—ì„œ ì‹¤ì œ ì§„í–‰ ì¤‘ì¸ ì—°êµ¬ê³¼ì œ ìˆ˜í–‰ì„ í†µí•´ ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì˜ ë¯¸ë˜ ì¸ì¬ë¥¼ ìœ¡ì„±í•˜ëŠ” &#x27;SKT AI í ë¡œìš°ì‹­&#x27; 5ê¸° ê³¼ì •ì„ ì„±ê³µì ìœ¼ë¡œ ë§ˆë¬´ë¦¬í–ˆë‹¤ê³  20ì¼ ë°í˜”ë‹¤. ì‚¬ì§„ì€ SKT AI í ë¡œìš°ì‹­ 5ê¸° í•™ìƒë“¤ì´ ìˆ˜ë£Œì‹ì„ ë§ˆì¹˜ê³  ê¸°ë…ì´¬ì˜ì„ í•˜ëŠ” ëª¨ìŠµ. (ì‚¬ì§„=SKí…”ë ˆì½¤ ì œê³µ)</span><br><span class="line"></span><br><span class="line">9ì¼ SKí…”ë ˆì½¤ì— ë”°ë¥´ë©´ ì§€ë‚œ 1ë¶„ê¸° ì—”í„°í”„ë¼ì´ì¦ˆ ë¶€ë¬¸ ë§¤ì¶œì•¡ì€ 4154ì–µì›ìœ¼ë¡œ ì „ë…„ ë™ê¸° ëŒ€ë¹„ 8.7% ëŠ˜ì—ˆë‹¤.</span><br><span class="line"></span><br><span class="line">SKí…”ë ˆì½¤ì€ AI ì¸í”„ë¼ ì˜ì—­ì¸ ë°ì´í„°ì„¼í„°ì™€ í´ë¼ìš°ë“œ ê´€ë ¨ ì‚¬ì—…ì´ ì—”í„°í”„ë¼ì´ì¦ˆ ë§¤ì¶œ ì„±ì¥ì— ê²¬ì¸í–ˆë‹¤ê³  ì „í–ˆë‹¤. ë°ì´í„°ì„¼í„°ì™€ í´ë¼ìš°ë“œ ë§¤ì¶œì€ ê°ê° 583ì–µì›, 350ì–µì›ìœ¼ë¡œ ì „ë…„ ëŒ€ë¹„ 25.6%, 38.3% ëŠ˜ì—ˆë‹¤.</span><br><span class="line"></span><br><span class="line">ë°ì´í„°ì„¼í„°ëŠ” ë°ì´í„° ì²˜ë¦¬ ìš©ëŸ‰ì„ í™•ë³´í•˜ê¸° ìœ„í•œ ì‹œì„¤ë¡œ ìµœê·¼ ìƒì„±í˜• AI ìˆ˜ìš” ì¦ê°€ì— ë©ë‹¬ì•„ ê³ ì„±ëŠ¥ ë°ì´í„°ì„¼í„° ìˆ˜ìš”ë„ ì»¤ì§€ê³  ìˆë‹¤. SKí…”ë ˆì½¤ë„ ì§€ì†ì ì¸ ê°€ë™ë¥  ì¦ê°€ì— í˜ì…ì–´ ë§¤ì¶œ ì„±ì¥ì„ ê±°ë’€ìœ¼ë©° AI ë°ì´í„°ì„¼í„° ì‚¬ì—…ìœ¼ë¡œ ë°œì „ì‹œí‚¬ ê³„íšì´ë‹¤.</span><br><span class="line"></span><br><span class="line">ì¼ë¡€ë¡œ SKí…”ë ˆì½¤ì€ SKí•˜ì´ë‹‰ìŠ¤, SKë¸Œë¡œë“œë°´ë“œ, SKì—”ë¬´ë¸Œ, ì‚¬í”¼ì˜¨ ë“± ê·¸ë£¹ì‚¬ ì—­ëŸ‰ì„ ê²°ì§‘í•œ AI ë°ì´í„°ì„¼í„° ì†”ë£¨ì…˜ íŒ¨í‚¤ì§€ë¥¼ ì¤€ë¹„ ì¤‘ì´ë©° ë¯¸êµ­ ì„œë²„ ì œì¡° ê¸°ì—…ì¸ ìŠˆí¼ë§ˆì´í¬ë¡œì™€ ê·¸ë˜í”½ì²˜ë¦¬ì¥ì¹˜(GPU) í´ë¼ìš°ë“œ ê¸°ì—… ëŒë‹¤ ë“± ê¸€ë¡œë²Œ ì‚¬ì—… í˜‘ë ¥ë„ ì¶”ì§„í•˜ê³  ìˆë‹¤.</span><br><span class="line"></span><br><span class="line">ë˜ í˜„ì¬ ì—­ëŸ‰ì˜ 2ë°°ì¸ 200ë©”ê°€ì™€íŠ¸(ã¿) ì´ìƒìœ¼ë¡œ í™•ì¥í•´ êµ­ë‚´ 1ìœ„ ì‚¬ì—…ìë¥¼ ëª©í‘œë¡œ ìˆ˜ë„ê¶Œì— ì‹ ê·œ ë°ì´í„°ì„¼í„° ì„¤ë¦½ë„ ì¶”ì§„ ì¤‘ì´ë¼ê³  ë°í˜”ë‹¤.</span><br><span class="line"></span><br><span class="line">í´ë¼ìš°ë“œ ì‚¬ì—…ë„ AI ìˆ˜ìš” ì¦ëŒ€ì— ë”°ë¼ ë©€í‹° í´ë¼ìš°ë“œ ìœ„ì£¼ë¡œ ì‚¬ì—…ì„ í™•ì¥í•˜ê² ë‹¤ë©° ë¹„ìš© ìµœì í™” ê¸°ìˆ ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë³¸ê²©ì ì¸ ìŠ¤ì¼€ì¼ì—…ì— ë‚˜ì„œê² ë‹¤ëŠ” ì…ì¥ì´ë‹¤.</span><br><span class="line"></span><br><span class="line">í•˜ì§€ë§Œ AI ì‚¬ì—… ê´€ë ¨í•´ ì„±ê³¼ë¥¼ ë‚¼ ê²ƒì´ë¼ê³  ê°•ì¡°í•˜ë ¤ë©´ ê·¸ë§Œí¼ì˜ ë§ì€ ìš°ìˆ˜ ì¸ë ¥ì´ í•„ìš”í•˜ë‹¤. ì´ëŸ¬í•œ ì ì´ SKí…”ë ˆì½¤ì´ ì‹¤ì ì— AI ì¸ë ¥ ìˆ˜ë¥¼ ê³µê°œí•œ ì´ìœ ë¡œ í’€ì´ëœë‹¤. SKí…”ë ˆì½¤ì€ ì§€ë‚œë‹¬ 1ì¼ ê¸°ì¤€ ìì‚¬ ì •ê·œì§ ì„ì§ì› 5286ëª… ê°€ìš´ë° AI ì‚¬ì—…, ê°œë°œ ë“± ê´€ë ¨ ì—…ë¬´ì— ì§ê°„ì ‘ì ìœ¼ë¡œ ê¸°ì—¬í•œ ì¸ë ¥ ë¹„ì¤‘ì´ 40%(2118ëª…)ì— ë‹¬í–ˆê³  ì§€ë‚œí•´ 1ì›”1ì¼ ëŒ€ë¹„ 573ëª… ëŠ˜ì—ˆë‹¤ê³  ë°í˜”ë‹¤.</span><br><span class="line"></span><br><span class="line">[ì„œìš¸=ë‰´ì‹œìŠ¤] ì¡°ì„±ë´‰ ê¸°ì = SKí…”ë ˆì½¤ì´ ê°ê° í˜¸ì£¼, ì‹±ê°€í¬ë¥´ì˜ ìµœëŒ€ ì˜ë£Œê¸°ê¸° ìœ í†µì‚¬ì¸ ì—ì´í‹°ì—‘ìŠ¤(ATX)ì™€ ìŠ¤ë¯¸í…Œí¬(Smitech)ì™€ íŒŒíŠ¸ë„ˆì‹­ì„ ë§ºê³ , ì§„ë‹¨ë²”ìœ„ë„ ê¸°ì¡´ ê°œì—ì„œ ê³ ì–‘ì´ë¡œ í™•ëŒ€í•˜ëŠ” ë“± êµ­ë‚´ì™¸ì—ì„œ ë°˜ë ¤ë™ë¬¼ AIí—¬ìŠ¤ì¼€ì–´ ì‚¬ì—…ì˜ ì˜ì—­ì„ ë„“í˜€ê°€ê³  ìˆë‹¤ê³  19ì¼ ë°í˜”ë‹¤. ì‚¬ì§„ì€ ì§€ë‚œ 17ì¼ ë™ë¬¼ë³‘ì›ì—ì„œ ìˆ˜ì˜ì‚¬ê°€ ì—‘ìŠ¤ì¹¼ë¦¬ë²„ë¥¼ í™œìš©í•´ ê³ ì–‘ì´ì˜ ì—‘ìŠ¤ë ˆì´ ì‚¬ì§„ì„ íŒë…í•˜ê³  ì§„ë£Œí•˜ëŠ” ëª¨ìŠµ. (ì‚¬ì§„=SKí…”ë ˆì½¤ ì œê³µ) 2023.11.19.photo@newsis.com *ì¬íŒë§¤ ë° DB ê¸ˆì§€</span><br><span class="line"></span><br><span class="line">[ì„œìš¸=ë‰´ì‹œìŠ¤] ì¡°ì„±ë´‰ ê¸°ì = SKí…”ë ˆì½¤ì´ ê°ê° í˜¸ì£¼, ì‹±ê°€í¬ë¥´ì˜ ìµœëŒ€ ì˜ë£Œê¸°ê¸° ìœ í†µì‚¬ì¸ ì—ì´í‹°ì—‘ìŠ¤(ATX)ì™€ ìŠ¤ë¯¸í…Œí¬(Smitech)ì™€ íŒŒíŠ¸ë„ˆì‹­ì„ ë§ºê³ , ì§„ë‹¨ë²”ìœ„ë„ ê¸°ì¡´ ê°œì—ì„œ ê³ ì–‘ì´ë¡œ í™•ëŒ€í•˜ëŠ” ë“± êµ­ë‚´ì™¸ì—ì„œ ë°˜ë ¤ë™ë¬¼ AIí—¬ìŠ¤ì¼€ì–´ ì‚¬ì—…ì˜ ì˜ì—­ì„ ë„“í˜€ê°€ê³  ìˆë‹¤ê³  19ì¼ ë°í˜”ë‹¤. ì‚¬ì§„ì€ ì§€ë‚œ 17ì¼ ë™ë¬¼ë³‘ì›ì—ì„œ ìˆ˜ì˜ì‚¬ê°€ ì—‘ìŠ¤ì¹¼ë¦¬ë²„ë¥¼ í™œìš©í•´ ê³ ì–‘ì´ì˜ ì—‘ìŠ¤ë ˆì´ ì‚¬ì§„ì„ íŒë…í•˜ê³  ì§„ë£Œí•˜ëŠ” ëª¨ìŠµ. (ì‚¬ì§„=SKí…”ë ˆì½¤ ì œê³µ) 2023.11.19.photo@newsis.com *ì¬íŒë§¤ ë° DB ê¸ˆì§€</span><br><span class="line"></span><br><span class="line">AI ì¸ë ¥ í™•ë³´ ì˜í–¥ì¸ì§€ SKí…”ë ˆì½¤ì€ AI ì„œë¹„ìŠ¤ë¥¼ ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•˜ê³  ìˆë‹¤. ê·¸ ê²°ê³¼ AI ì„œë¹„ìŠ¤ ì•±ì¸ &#x27;ì—ì´ë‹·&#x27; ëˆ„ì  ê°€ì…ì ìˆ˜ëŠ” 400ë§Œëª…(ì§€ë‚œ 3ì›” ë§ ê¸°ì¤€)ì— ë‹¬ì„±í–ˆë‹¤. ì§€ë‚œí•´ 9ì›” ê³µì‹ ì¶œì‹œ í›„ 120% ì„±ì¥í•œ ìˆ˜ì¹˜ë‹¤. í†µí™”ë…¹ìŒÂ·ìš”ì•½, ì‹¤ì‹œê°„ í†µí™”í†µì—­ ì„œë¹„ìŠ¤ê°€ ì œê³µëœ ì˜í–¥ìœ¼ë¡œ í’€ì´ëœë‹¤.</span><br><span class="line">SKí…”ë ˆì½¤ì€ ë…ì¼ ë„ì´ì¹˜í…”ë ˆì½¤, ì•„ëì—ë¯¸ë¦¬íŠ¸ ì´ì•¤, ì‹±ê°€í¬ë¥´ ì‹±í…”, ì¼ë³¸ ì†Œí”„íŠ¸ë±…í¬ ë“± ê¸€ë¡œë²Œ í…”ì½” AI ì–¼ë¼ì´ì–¸ìŠ¤(GTAA) ì°½ë¦½ì‚¬ë“¤ê³¼ í˜‘ë ¥í•´ ì—ì´ë‹·ì„ AI ê°œì¸ë¹„ì„œ ì„œë¹„ìŠ¤(PAA)ë¡œì¨ í˜„ì§€í™”í•´ ìœ ì¹˜í•  ê³„íšì´ë‹¤.</span><br><span class="line"></span><br><span class="line">ë°˜ë ¤ë™ë¬¼ ì—‘ìŠ¤ë ˆì´ ì‚¬ì§„ì„ AIë¡œ ë¶„ì„í•´ ìˆ˜ì˜ì‚¬ì˜ ì§ˆë³‘ ì§„ë‹¨ì„ ë•ëŠ” ì§„ë‹¨ ë³´ì¡° ì„œë¹„ìŠ¤ &#x27;ì—‘ìŠ¤ì¹¼ë¦¬ë²„&#x27; ì´ìš© ë³‘ì› ìˆ˜ë„ ì „ë…„ ëŒ€ë¹„ ì•½ 5ë°° ì¦ê°€í•œ 570ê³³ì— ë‹¬í–ˆë‹¤. ì—‘ìŠ¤ì¹¼ë¦¬ë²„ëŠ” í˜„ì¬ í˜¸ì£¼, ì‹±ê°€í¬ë¥´ ë“±ì— ì§„ì¶œí–ˆìœ¼ë©° ì—°ë‚´ ë¯¸êµ­, ìœ ëŸ½, ë™ë‚¨ì•„ ì§€ì—­ì—ë„ ìƒìš©í™”ë¥¼ ì¶”ì§„í•œë‹¤.</span><br><span class="line"></span><br><span class="line">SKí…”ë ˆì½¤ì€ AI ë“± ë¯¸ë˜ ì„±ì¥ íˆ¬ì ì—¬ë ¥ì„ í™•ë³´í•˜ê² ë‹¤ëŠ” ì…ì¥ì´ë‹¤. ê¹€ì–‘ì„­ SKí…”ë ˆì½¤ ìµœê³ ì¬ë¬´ì±…ì„ì(CFO)ëŠ” ì§€ë‚œ 8ì¼ SKí…”ë ˆì½¤ 1ë¶„ê¸° ì‹¤ì  ì»¨í¼ëŸ°ìŠ¤ì½œì—ì„œ AI íˆ¬ì ê´€ë ¨í•œ ìë³¸ í• ë‹¹ ê³„íšì— ëŒ€í•´ &quot;(í†µìƒì ìœ¼ë¡œ) ì—°ê°„ ëŒ€ëµ 1ì¡°ì› ì •ë„ì˜ ìºì‹œí”Œë¡œìš°(í˜„ê¸ˆ íë¦„)ê°€ ë‚¨ëŠ”ë° 7000ì–µì› ì´ìƒ í˜„ê¸ˆë°°ë‹¹ì„ ê¾¸ì¤€íˆ í•˜ë‹¤ ë³´ë‹ˆ íˆ¬ìë‚˜ ì°¨ì…ê¸ˆ ê´€ë¦¬ ì°¨ì›ì—ì„œ ìƒê°í•˜ë©´ ìš´ì‹ ì˜ í­ì´ ë„“ì§€ ì•Šì€ ê²ƒì´ ì‚¬ì‹¤&quot;ì´ë¼ê³  ë§í–ˆë‹¤.</span><br><span class="line"></span><br><span class="line">í•˜ì§€ë§Œ ê·¸ëŠ” &quot;ì½”ìŠ¤íŠ¸ ì½˜íŠ¸ë¡¤ì„ í†µí•œ ìˆ˜ìµì„± ê°œì„ , ìì‚° ìœ ë™í™”, íˆ¬ì íš¨ìœ¨í™” ë“± íšŒì‚¬ê°€ ìƒê°í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ë°©ì•ˆì„ í†µí•´ì„œ ì¶”ê°€ ë¦¬ì†ŒìŠ¤ ì°½ì¶œì„ ì¶”ì§„í•´ ë‚˜ê°ˆ ê³„íš&quot;ì´ë¼ê³  ë°í˜”ë‹¤.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-06-03</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/06/03/2024-6-3-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 6ì›” 3ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/06/04/computex-2024/" title="NVIDIA CEO Jensen Huang Keynote at COMPUTEX 2024">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/05/31/2024-5-31-AI-NEWS/" title="2024ë…„ 5ì›” 31ì¼ AI ì†Œì‹">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>