<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 6월 5일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="Summary오늘의 소식에서는 인공지능 모델 GLM-4-9B의 성능과 다언어 지원, 트랜스포머의 알고리즘적 추론 능력, 한국어 RAG 평가 데이터셋, 인텔의 제온6 프로세서 출시, 엔비디아의 차세대 AI 전용칩, AMD의 새로운 라이젠 AI 300 칩, LLM의 신뢰도"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 6월 5일 AI 소식</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>오늘의 소식에서는 인공지능 모델 GLM-4-9B의 성능과 다언어 지원, 트랜스포머의 알고리즘적 추론 능력, 한국어 RAG 평가 데이터셋, 인텔의 제온6 프로세서 출시, 엔비디아의 차세대 AI 전용칩, AMD의 새로운 라이젠 AI 300 칩, LLM의 신뢰도 표현 개선 연구, 그리고 Skywork-MoE 모델의 최신 업데이트에 대해 다룹니다.</p>
<h2 id="GLM-4-9B-모델-소개"><a href="#GLM-4-9B-모델-소개" class="headerlink" title="GLM-4-9B 모델 소개"></a>GLM-4-9B 모델 소개</h2><p><a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4">링크</a><br>2024-06-04, Zhipu AI</p>
<ul>
<li>GLM-4-9B는 Zhipu AI에서 출시한 최신 프리트레인 모델 시리즈의 오픈소스 버전.</li>
<li>의미, 수학, 추론, 코드 및 지식 데이터셋 평가에서 Llama-3-8B보다 우수한 성능을 보임.</li>
<li>GLM-4-9B-Chat 버전은 웹 브라우징, 코드 실행, 맞춤형 도구 호출, 긴 텍스트 추론 등의 고급 기능을 포함.</li>
<li>26개 언어를 지원하며, GLM-4V-9B는 다이얼로그 능력을 갖춘 멀티모달 모델.</li>
<li>GLM-4V-9B는 GPT-4-turbo-2024-04-09, Gemini 1.0 Pro, Qwen-VL-Max, Claude 3 Opus보다 뛰어난 성능을 입증.</li>
</ul>
<h2 id="Understanding-Transformer-Reasoning-Capabilities-via-Graph-Algorithms"><a href="#Understanding-Transformer-Reasoning-Capabilities-via-Graph-Algorithms" class="headerlink" title="Understanding Transformer Reasoning Capabilities via Graph Algorithms"></a>Understanding Transformer Reasoning Capabilities via Graph Algorithms</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.18512">링크</a><br>2024-05-28, Google Research</p>
<ul>
<li>트랜스포머 기반 신경망의 알고리즘적 추론 능력에 대한 이론적 이해를 조사.</li>
<li>네트워크 깊이, 폭, 추가 토큰 수에 따른 알고리즘 문제 해결 능력을 분석.</li>
<li>그래프 연결성 같은 과제에 대해 로그 깊이가 필요하며, 작은 임베딩 차원의 단일 레이어 트랜스포머가 컨텍스트 검색 작업을 해결 가능.</li>
<li>GraphQA 벤치마크를 사용한 실증적 증거 제시.</li>
</ul>
<h2 id="Allganize-RAG-리더보드"><a href="#Allganize-RAG-리더보드" class="headerlink" title="Allganize RAG 리더보드"></a>Allganize RAG 리더보드</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-KO">링크</a><br>2024-06-04, Allganize</p>
<ul>
<li>5개 도메인(금융, 공공, 의료, 법률, 커머스)에 대한 한국어 RAG 성능 평가.</li>
<li>기존 RAG는 테이블과 이미지에 대한 질문 답변에 취약.</li>
<li>Allganize는 RAG 평가 데이터를 공개하여 도메인 맞춤형 성능 평가 가능.</li>
<li>문서 업로드 후 자체 질문 사용해 성능 측정.</li>
</ul>
<h2 id="Fine-tune-Embedding-models-for-RAG"><a href="#Fine-tune-Embedding-models-for-RAG" class="headerlink" title="Fine-tune Embedding models for RAG"></a>Fine-tune Embedding models for RAG</h2><p><a target="_blank" rel="noopener" href="https://www.philschmid.de/fine-tune-embedding-model-for-rag">링크</a><br>2024-06-04, Phil Schmid</p>
<ul>
<li>RAG 애플리케이션을 위한 임베딩 모델 커스터마이징 방법 소개.</li>
<li>Matryoshka Representation Learning을 활용하여 효율성 증대.</li>
<li>금융 RAG 애플리케이션을 위한 임베딩 모델 파인튜닝 과정 설명.</li>
<li>새로운 Sentence Transformers 3 릴리스로 인해 파인튜닝이 더욱 간편해짐.</li>
</ul>
<h2 id="인텔-제온6-‘시에라-포레스트’-출시"><a href="#인텔-제온6-‘시에라-포레스트’-출시" class="headerlink" title="인텔, 제온6 ‘시에라 포레스트’ 출시"></a>인텔, 제온6 ‘시에라 포레스트’ 출시</h2><p><a target="_blank" rel="noopener" href="https://m.ddaily.co.kr/page/view/2024060408520160213">링크</a><br>2024-06-04, 디지털데일리</p>
<ul>
<li>인텔, 타이베이에서 데이터센터 및 AI 생태계 혁신 기술 공개.</li>
<li>제온 6 프로세서, E-코어 및 P-코어 모델 설계로 고밀도 스케일아웃 워크로드 처리 가능.</li>
<li>인텔 제온 6 E-코어, 전력 비용 절감과 효율적 컴퓨팅 제공.</li>
<li>DDR5, PCIe 5.0, UPI 및 CXL 기술 지원.</li>
</ul>
<h2 id="엔비디아-차세대-AI-전용칩-공개"><a href="#엔비디아-차세대-AI-전용칩-공개" class="headerlink" title="엔비디아 차세대 AI 전용칩 공개"></a>엔비디아 차세대 AI 전용칩 공개</h2><p><a target="_blank" rel="noopener" href="https://n.news.naver.com/article/050/0000075863?cds=news_edit">링크</a><br>2024-06-04, 김정우 기자</p>
<ul>
<li>뱅크오브아메리카, 엔비디아 목표가 1500달러로 상향.</li>
<li>엔비디아의 차차세대 AI 전용칩 발표로 시장 지배력 강화 예상.</li>
<li>엔비디아 주가 1154달러로 최고치 경신.</li>
</ul>
<h2 id="NVIDIA-Collaborates-with-Hugging-Face-to-Simplify-Generative-AI-Model-Deployments"><a href="#NVIDIA-Collaborates-with-Hugging-Face-to-Simplify-Generative-AI-Model-Deployments" class="headerlink" title="NVIDIA Collaborates with Hugging Face to Simplify Generative AI Model Deployments"></a>NVIDIA Collaborates with Hugging Face to Simplify Generative AI Model Deployments</h2><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/nvidia-collaborates-with-hugging-face-to-simplify-generative-ai-model-deployments/?ncid=so-link-334086&=&linkId=100000264631409/">링크</a><br>2024-06-03, NVIDIA</p>
<ul>
<li>NVIDIA, Hugging Face와 협력하여 생성 AI 모델 배포 간소화.</li>
<li>NVIDIA NIM, 저지연, 고처리량 AI 추론 제공.</li>
<li>Llama 3 8B 및 Llama 3 70B 모델 Hugging Face에서 몇 번의 클릭으로 배포 가능.</li>
</ul>
<h2 id="xAI-시리즈-B-펀딩에서-60억-달러-조달"><a href="#xAI-시리즈-B-펀딩에서-60억-달러-조달" class="headerlink" title="xAI, 시리즈 B 펀딩에서 60억 달러 조달"></a>xAI, 시리즈 B 펀딩에서 60억 달러 조달</h2><p>2024-06-04</p>
<ul>
<li>xAI, 시리즈 B 펀딩 라운드에서 60억 달러 조달, 기업가치 180억 달러로 평가.</li>
<li>펀딩 자금은 첫 제품 출시, 고급 인프라 구축, 연구 개발 가속화에 사용될 예정.</li>
<li>OpenAI, Anthropic, ScaleAI와 경쟁.</li>
</ul>
<h2 id="AMD-새로운-라이젠-AI-300-칩-공개"><a href="#AMD-새로운-라이젠-AI-300-칩-공개" class="headerlink" title="AMD, 새로운 라이젠 AI 300 칩 공개"></a>AMD, 새로운 라이젠 AI 300 칩 공개</h2><p><a target="_blank" rel="noopener" href="https://www.windowscentral.com/hardware/laptops/amd-ryzen-ai-300-announce">링크</a><br>2024-06-03, Windows Central</p>
<ul>
<li>AMD, 컴퓨텍스 2024에서 라이젠 AI 300 모바일 프로세서 공개.</li>
<li>새로운 Zen 5 아키텍처 기반, Copilot+ 호환.</li>
<li>라이젠 AI 9 HX 370 및 라이젠 AI 9 365, 각각 50 TOPS 성능 제공.</li>
<li>Acer, ASUS, HP, Lenovo, MSI의 다양한 노트북 모델에 채택될 예정.</li>
</ul>
<h2 id="SaySelf-Teaching-LLMs-to-Express-Confidence-with-Self-Reflective-Rationales"><a href="#SaySelf-Teaching-LLMs-to-Express-Confidence-with-Self-Reflective-Rationales" class="headerlink" title="SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales"></a>SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.20974v1">링크</a><br>2024-05-31, Tianyang Xu 외</p>
<ul>
<li>SaySelf는 LLM이 정확한 신뢰도 추정치를 표현하도록 교육하는 프레임워크.</li>
<li>강화 학습을 통해 신뢰도 추정치를 보정, 과도한 신뢰도 패널티 부여.</li>
<li>실험 결과, 신뢰도 보정 오류 감소 및 작업 성능 유지.</li>
</ul>
<h2 id="Skywork-MoE-모델-업데이트"><a href="#Skywork-MoE-모델-업데이트" class="headerlink" title="Skywork-MoE 모델 업데이트"></a>Skywork-MoE 모델 업데이트</h2><p><a target="_blank" rel="noopener" href="https://github.com/SkyworkAI/Skywork-MoE/tree/main">링크</a><br>2024-06-03, SkyworkAI</p>
<ul>
<li>Skywork-MoE는 1460억 개의 파라미터와 22억 개의 활성화된 파라미터를 가진 모델.</li>
<li>전문가 다변화를 촉진하는 Gating Logit Normalization과 보조 손실 계수 조정을 위한 Adaptive Auxiliary Loss Coefficients 도입.</li>
<li>Grok-1, DBRX, Mistral 8*22, Deepseek-V2보다 우수한 성능을 발휘.</li>
</ul>
<details>
  <summary>Sources</summary>
  This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is: # AI News for (today's date), ## Summary (overall short summary), ## Link1 Title, link, date - detailed summary1, - detailed summary2, - detailed summary..N, ## Link2 Title, link, date - detailed summary1, - detailed summary2, - detailed point..N, etc. The report should be written in Korean and use the 개조식 문체 style. give the very deep details for each link as much as possible.
  make summary with good details, note company name next to date if available.

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://github.com/THUDM/GLM-4</span><br><span class="line">GLM-4</span><br><span class="line">🤗 HF Repo • 🤖 ModelScope • 🐦 Twitter • 👋 Join Slack and WeChat</span><br><span class="line"></span><br><span class="line">📍Experience and use a larger-scale GLM business model on the Zhipu AI Open Platform</span><br><span class="line"></span><br><span class="line">Model Introduction</span><br><span class="line">GLM-4-9B is the open-source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI. In the evaluation of data sets in semantics, mathematics, reasoning, code, and knowledge, GLM-4-9B and its human preference-aligned version GLM-4-9B-Chat have shown superior performance beyond Llama-3-8B. In addition to multi-round conversations, GLM-4-9B-Chat also has advanced features such as web browsing, code execution, custom tool calls (Function Call), and long text reasoning (supporting up to 128K context). This generation of models has added multi-language support, supporting 26 languages including Japanese, Korean, and German. We have also launched the GLM-4-9B-Chat-1M model that supports 1M context length (about 2 million Chinese characters) and the multimodal model GLM-4V-9B based on GLM-4-9B. GLM-4V-9B possesses dialogue capabilities in both Chinese and English at a high resolution of 1120*1120. In various multimodal evaluations, including comprehensive abilities in Chinese and English, perception &amp; reasoning, text recognition, and chart understanding, GLM-4V-9B demonstrates superior performance compared to GPT-4-turbo-2024-04-09, Gemini 1.0 Pro, Qwen-VL-Max, and Claude 3 Opus.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2405.18512</span><br><span class="line">google research</span><br><span class="line">[Submitted on 28 May 2024]</span><br><span class="line">Understanding Transformer Reasoning Capabilities via Graph Algorithms</span><br><span class="line">Clayton Sanford, Bahare Fatemi, Ethan Hall, Anton Tsitsulin, Mehran Kazemi, Jonathan Halcrow, Bryan Perozzi, Vahab Mirrokni</span><br><span class="line">Which transformer scaling regimes are able to perfectly solve different classes of algorithmic problems? While tremendous empirical advances have been attained by transformer-based neural networks, a theoretical understanding of their algorithmic reasoning capabilities in realistic parameter regimes is lacking. We investigate this question in terms of the network&#x27;s depth, width, and number of extra tokens for algorithm execution. Our novel representational hierarchy separates 9 algorithmic reasoning problems into classes solvable by transformers in different realistic parameter scaling regimes. We prove that logarithmic depth is necessary and sufficient for tasks like graph connectivity, while single-layer transformers with small embedding dimensions can solve contextual retrieval tasks. We also support our theoretical analysis with ample empirical evidence using the GraphQA benchmark. These results show that transformers excel at many graph reasoning tasks, even outperforming specialized graph neural networks.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-KO</span><br><span class="line">Allganize RAG Leaderboard</span><br><span class="line">Allganize RAG 리더보드는 5개 도메인(금융, 공공, 의료, 법률, 커머스)에 대해서 한국어 RAG의 성능을 평가합니다.</span><br><span class="line">일반적인 RAG는 간단한 질문에 대해서는 답변을 잘 하지만, 문서의 테이블과 이미지에 대한 질문은 답변을 잘 못합니다.</span><br><span class="line"></span><br><span class="line">RAG 도입을 원하는 수많은 기업들은 자사에 맞는 도메인, 문서 타입, 질문 형태를 반영한 한국어 RAG 성능표를 원하고 있습니다.</span><br><span class="line">평가를 위해서는 공개된 문서와 질문, 답변 같은 데이터 셋이 필요하지만, 자체 구축은 시간과 비용이 많이 드는 일입니다.</span><br><span class="line">이제 올거나이즈는 RAG 평가 데이터를 모두 공개합니다.</span><br><span class="line"></span><br><span class="line">RAG는 Parser, Retrieval, Generation 크게 3가지 파트로 구성되어 있습니다.</span><br><span class="line">현재, 공개되어 있는 RAG 리더보드 중, 3가지 파트를 전체적으로 평가하는 한국어로 구성된 리더보드는 없습니다.</span><br><span class="line"></span><br><span class="line">Allganize RAG 리더보드에서는 문서를 업로드하고, 자체적으로 만든 질문을 사용해 답변을 얻었습니다.</span><br><span class="line">생성한 답변과 정답 답변을 자동 성능 평가 방법을 적용해 각 RAG 방법별 성능 측정을 했습니다.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.philschmid.de/fine-tune-embedding-model-for-rag</span><br><span class="line">Fine-tune Embedding models for Retrieval Augmented Generation (RAG)</span><br><span class="line">June 4, 2024</span><br><span class="line">11 minute read</span><br><span class="line">View Code</span><br><span class="line">Embedding models are crucial for successful RAG applications, but they&#x27;re often trained on general knowledge, which limits their effectiveness for company or domain specific adoption. Customizing embedding for your domain specific data can significantly boost the retrieval performance of your RAG Application. With the new release of Sentence Transformers 3, it&#x27;s easier than ever to fine-tune embedding models.</span><br><span class="line"></span><br><span class="line">In this blog, we&#x27;ll show you how to fine-tune an embedding model for a financial RAG applications using a synthetic dataset from the 2023_10 NVIDIA SEC Filing. We&#x27;ll also leverage Matryoshka Representation Learning to boost efficiency. In the blog, we are going to:</span><br><span class="line"></span><br><span class="line">Create &amp; Prepare embedding dataset</span><br><span class="line">Create baseline and evaluate pretrained model</span><br><span class="line">Define loss function with Matryoshka Representation</span><br><span class="line">Fine-tune embedding model with SentenceTransformersTrainer</span><br><span class="line">Evaluate fine-tuned model against baseline</span><br><span class="line">🪆 Matryoshka Embeddings</span><br><span class="line"></span><br><span class="line">Matryoshka Representation Learning (MRL) is a technique designed to create embeddings that can be truncated to various dimensions without significant loss of performance. This approach frontloads important information into earlier dimensions of the embedding, allowing for efficient storage and processing while maintaining high accuracy in downstream tasks such as retrieval, classification, and clustering.</span><br><span class="line"></span><br><span class="line">For example, a Matryoshka model can preserve ~99.9% of its performance while needing 3x less storage. This is particularly useful for applications where storage and processing resources are limited, such as on-device applications or large-scale retrieval systems.</span><br><span class="line"></span><br><span class="line">Note: This blog was created to run on consumer size GPUs (24GB), e.g. NVIDIA A10G or RTX 4090/3090, but can be easily adapted to run on bigger GPUs.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://m.ddaily.co.kr/page/view/2024060408520160213</span><br><span class="line">PC/프린팅/디바이스</span><br><span class="line">인텔, 제온6 ‘시에라 포레스트’ 전격 출시…전력효율 최대 [컴퓨텍스 2024]</span><br><span class="line">디지털데일리 발행일 2024-06-04 12:00:00</span><br><span class="line">타이베이(대만)=김문기 기자</span><br><span class="line">팻 겔싱어 인텔 CEO가 기존 대비 코어수가 2배 증가한 인텔 제온 시에라 포레스트 실물을 공개한 모습</span><br><span class="line">팻 겔싱어 인텔 CEO가 기존 대비 코어수가 2배 증가한 인텔 제온 시에라 포레스트 실물을 공개한 모습</span><br><span class="line">[디지털데일리 김문기 기자] 인텔(대표 팻 겔싱어)은 4일(현지시간) 대만 타이베이에서 진행된 컴퓨텍스 2024에서 데이터센터, 클라우드와 네트워크에서 에지 및 PC에 이르기까지 AI 생태계를 획기적으로 가속화할 최첨단 기술 및 아키텍처를 공개했다.</span><br><span class="line"></span><br><span class="line">겔싱어 CEO와 업계 리더들은 인텔이 AI 혁신을 이끌고 차세대 기술을 예정보다 앞서 제공하고 있다는 점을 분명히 했다. 인텔은 불과 6개월 만에 5세대 인텔 제온(5th Gen Intel Xeon) 프로세서를 출시한데 이어 제온 6 첫 제품을 선보였으며, 가우디 AI 가속기를 선공개하고 기업 고객에게 비용 효율적인 고성능 생성형 AI 훈련 및 추론 시스템을 제공했다.</span><br><span class="line"></span><br><span class="line">이러한 발전을 통해 인텔은 실행 속도를 가속화하는 동시에 혁신과 생산 속도의 한계를 넘어 AI를 대중화하고 업계를 활성화하고 있다 인텔 제온 6 프로세서를 통해 고밀도 스케일아웃 워크로드를 위한 성능 및 전력 효율성을 향상시켰다.</span><br><span class="line"></span><br><span class="line">디지털 혁신이 가속화됨에 따라 기업들은 노후화된 데이터센터 시스템을 교체해 비용 절감, 지속 가능성 목표 달성, 물리적 공간 및 랙 공간 활용 극대화하고 기업 전반에 걸쳐 새로운 디지털 역량을 창출해야 한다는 압박에 직면해 있다.</span><br><span class="line"></span><br><span class="line">이에 따라 모든 제온 6 플랫폼 및 프로세서 제품군은 이러한 과제를 해결할 목적으로 E-코어(Efficient -core) 및 P-코어(Performance-core) 모델이 설계됐다. AI 및 기타 고성능 컴퓨팅 요구사항부터 확장 가능한 클라우드 네이티브 애플리케이션에 이르기까지 광범위한 워크로드 및 사용 사례를 처리할 수 있다. E-코어와 P-코어는 모두 공통의 소프트웨어 스택과 하드웨어 및 소프트웨어 공급업체의 개방형 생태계와 호환 가능한 아키텍처를 기반으로 구축됐다.</span><br><span class="line"></span><br><span class="line">가장 먼저 출시되는 제온 6 프로세서는 인텔 제온 6 E-코어 기반 코드명 ‘시에라 포레스트’다. 당장 사용이 가능하다.</span><br><span class="line"></span><br><span class="line">고집적도 코어 및 뛰어난 와트당 성능을 갖춘 인텔 제온 6 E-코어는 전력 비용을 크게 낮추면서 효율적인 컴퓨팅을 제공한다. 향상된 성능 및 전력 효율성은 클라우드 네이티브 애플리케이션 및 콘텐츠 전송 네트워크, 네트워크 마이크로서비스, 소비자 디지털 서비스 등 가장 까다로운 고밀도 스케일아웃 워크로드에 적합하다. 와트당 최대 2.7배 높은 5G 사용자 평면 기능(5G-User Plane Function) 성능 및 와트당 최대 3.5배 높은 차세대 방화벽 성능3을 제공한다. 이는 인텔 이더넷 800 시리즈(Intel Ethernet 800 Series)로 테스트한 결과다.</span><br><span class="line"></span><br><span class="line">또한, 제온 6 E-코어는 집적도가 매우 뛰어나 랙 수준을 3대 1로 통합할 수 있어 미디어 트랜스코딩 워크로드에서 2세대 인텔 제온 프로세서 대비 최대 4.2배의 랙 레벨 성능 향상과 최대 2.6배의 와트당 성능 향상을 고객에게 제공할 수 있다. 더 적은 전력과 랙 공간을 사용하는 제온 6 프로세서는 혁신적인 새로운 AI 프로젝트를 위한 컴퓨팅 용량과 인프라를 확보한다.</span><br><span class="line"></span><br><span class="line">인텔 제온 6 프로세서는 동일한 하드웨어 플랫폼과 소프트웨어 스택을 공유하는 6700 시리즈 및 6900 시리즈 플랫폼으로 제공된다. 이와 더불어 DDR5, PCIe 5.0, UPI 및 CXL 등 관련 기술의 세대별 성능 향상이 포함된다</span><br><span class="line"></span><br><span class="line">6700 시리즈는 P-코어의 MCR DIMM과 함께 최대 1.4배 더 커진 메모리 대역폭을 제공해 한 번에 더 많은 데이터를 처리할 수 있으며, 5세대 인텔 제온 프로세서 대비 최대 1.1배 증가한 입출력(I/O) 대역폭을 제공해 데이터 입력 및 출력 시스템의 속도와 효율을 높인다.</span><br><span class="line"></span><br><span class="line">6900 시리즈는 5세대 인텔 제온 프로세서 대비 최대 1.8배 늘어난 소켓 간 대역폭을 제공한다. 이는 시스템 상 다양한 부분 간 더 빠르고 효율적인 통신을 가능하게 해, 특히 높은 성능을 필요로 하는 까다로운 작업에서 효과적이라는 설명이다.</span><br><span class="line"></span><br><span class="line">6700 및 6900 시리즈 모두 CXL 2.0를 지원한다. 두 시리즈가 세운 새 기준은 가속기, 메모리 확장기 및 기타 장치와 같은 추가 구성 요소와 컴퓨터 간 연결과 통신을 지원한다.</span><br><span class="line"></span><br><span class="line">한편, 제온 6 P-코어 기반 코드명 그래나이트 래피즈는 4분기 출시될 예정이다.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://n.news.naver.com/article/050/0000075863?cds=news_edit</span><br><span class="line">천비디아’도 옛말...엔비디아 목표가 1500달러로 상향</span><br><span class="line">입력2024.06.04. 오전 9:19 기사원문</span><br><span class="line">김정우 기자</span><br><span class="line">김정우 기자</span><br><span class="line">  3</span><br><span class="line">8</span><br><span class="line">텍스트 음성 변환 서비스 사용하기</span><br><span class="line">글자 크기 변경하기</span><br><span class="line">SNS 보내기</span><br><span class="line">인쇄하기</span><br><span class="line">엔비디아, 차세대 인공지능(AI) 전용칩 공개</span><br><span class="line">“신제품 효과로 시장 지배력 더욱 강화”</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">미국의 투자은행 뱅크오브아메리카(BoA)가 엔비디아의 목표가를 1500달러로 상향했다. 엔비디아가 차세대 인공지능(AI) 전용칩을 발표한 데 따른 것이다.</span><br><span class="line"></span><br><span class="line">BoA는 3일(현지시간) 보고서를 내고 엔비디아의 목표가를 기존의 1320달러에서 1500달러로 조정했다. 월가의 투자은행 중 가장 높은 목표가다.</span><br><span class="line"></span><br><span class="line">현재 엔비디아 주가가 약 1150달러선인 것을 감안하면 향후 30% 더 상승한다는 의미다.</span><br><span class="line"></span><br><span class="line">BoA는 “엔비디아가 차차세대 AI 전용칩 계획을 발표, 시장 지배력이 더욱 강화될 것”이라며 목표가 상향 이유를 설명했다.</span><br><span class="line"></span><br><span class="line">한편 엔비디아 주가는 지난달 30일 1154달러까지 치솟아 사상 최고치를 경신한 바 있다.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://developer.nvidia.com/blog/nvidia-collaborates-with-hugging-face-to-simplify-generative-ai-model-deployments/?ncid=so-link-334086&amp;=&amp;linkId=100000264631409/</span><br><span class="line">Generative AI</span><br><span class="line">NVIDIA Collaborates with Hugging Face to Simplify Generative AI Model Deployments</span><br><span class="line">Jun 03, 2024</span><br><span class="line">By Jig Bhadaliya, Rohit Taneja and Chintan Patel</span><br><span class="line"></span><br><span class="line">+4</span><br><span class="line">Like</span><br><span class="line"> Discuss (0)</span><br><span class="line"></span><br><span class="line">LTFRE</span><br><span class="line">As generative AI experiences rapid growth, the community has stepped up to foster this expansion in two significant ways: swiftly publishing state-of-the-art foundational models, and streamlining their integration into application development and production.</span><br><span class="line"></span><br><span class="line">NVIDIA is aiding this effort by optimizing foundation models to enhance performance, allowing enterprises to generate tokens faster, reduce the costs of running the models, and improve end user experience with NVIDIA NIM.</span><br><span class="line"></span><br><span class="line">NVIDIA NIM</span><br><span class="line">NVIDIA NIM inference microservices are designed to streamline and accelerate the deployment of generative AI models across NVIDIA accelerated infrastructure anywhere, including cloud, data center, and workstations.</span><br><span class="line"></span><br><span class="line">NIM leverages TensorRT-LLM inference optimization engine, industry-standard APIs, and prebuilt containers to provide low-latency, high-throughput AI inference that scales with demand. It supports a wide range of LLMs including Llama 3, Mixtral 8x22B, Phi-3, and Gemma, as well as optimizations for domain-specific applications in speech, image, video, healthcare, and more.</span><br><span class="line"></span><br><span class="line">NIM delivers superior throughput, enabling enterprises to generate tokens up to 5x faster. For generative AI applications, token processing is the key performance metric, and increased token throughput directly translates to higher revenue for enterprises.</span><br><span class="line"></span><br><span class="line">By simplifying the integration and deployment process, NIM enables enterprises to rapidly move from AI model development to production, enhancing efficiency, reducing operational costs, and allowing businesses to focus on innovation and growth.</span><br><span class="line"></span><br><span class="line">And now, we’re going a step further with Hugging Face to help developers run models in a matter of minutes.</span><br><span class="line"></span><br><span class="line">Deploy NIM on Hugging Face with a few clicks</span><br><span class="line">Hugging Face is a leading platform for AI models and has become the go-to destination for AI developers as it enhances the accessibility of AI models.</span><br><span class="line"></span><br><span class="line">Leverage the power of seamless deployment with NVIDIA NIM, starting with Llama 3 8B and Llama 3 70B, on your preferred cloud service provider, all directly accessible from Hugging Face.</span><br><span class="line"></span><br><span class="line">NIM delivers superior throughput and achieves near-100% utilization with multiple concurrent requests, enabling enterprises to generate text 3x faster. For generative AI applications, token processing is the key performance metric, and increased token throughput directly translates to higher revenue for enterprises.</span><br><span class="line"></span><br><span class="line">The Llama 3 NIM is performance optimized to deliver higher throughput, which translates to higher revenue and lower TCO. The Llama 3 8B NIM processes ~9300 tokens per second compared to the non-NIM version which processes ~2700 tokens per second on HF Endpoints.</span><br><span class="line">Figure 1. Llama 3 8B NIM on Hugging Face achieves 3x throughput</span><br><span class="line">The dedicated NIM endpoint on Hugging Face spins up instances on your preferred cloud, automatically fetches and deploys the NVIDIA optimized model, and enables you to start inference with just a few clicks, all in a matter of minutes.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">BREAKING: Elon Musk’s</span><br><span class="line">OpenAI</span><br><span class="line">Rival,</span><br><span class="line">xAI</span><br><span class="line">, Raises $6 Billion At $18 Billion Valuation — Funding Secured 🤯</span><br><span class="line"></span><br><span class="line">xAI has announced their Series B funding round of $6 billion at a $18 billion pre-money valuation.</span><br><span class="line"></span><br><span class="line">The round includes investors like:</span><br><span class="line">Valor Equity Partners</span><br><span class="line">,</span><br><span class="line">Andreessen Horowitz</span><br><span class="line">&amp;</span><br><span class="line">Sequoia Capital</span><br><span class="line">amongst others.</span><br><span class="line"></span><br><span class="line">What are they going to do with the money?</span><br><span class="line"></span><br><span class="line">“The funds from the round will be used to take xAI’s first products to market, build advanced infrastructure, and accelerate the research and development of future technologies.”</span><br><span class="line"></span><br><span class="line">For comparison, OpenAI is valued at $86 billion and has 100m active users.</span><br><span class="line"></span><br><span class="line">Other AI competitors</span><br><span class="line">Anthropic</span><br><span class="line">and ScaleAI are valued at $18.4 billion and $13.8 billion respectively.</span><br><span class="line"></span><br><span class="line">Funding secured.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.windowscentral.com/hardware/laptops/amd-ryzen-ai-300-announce</span><br><span class="line">AMD just toppled Snapdragon X NPU dominance with its Ryzen AI 300 chips ready for Copilot+</span><br><span class="line">News</span><br><span class="line">By Cale Hunt published 2 days ago</span><br><span class="line">Zen 5 is here.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> Comments (1)</span><br><span class="line">AMD Ryzen AI 300 press image</span><br><span class="line">A render of AMD&#x27;s Ryzen AI 300 chip (Image credit: AMD)</span><br><span class="line">What you need to know</span><br><span class="line">AMD unveiled new Ryzen AI 300 mobile processors for laptops at Computex 2024.</span><br><span class="line">The new chips are built on AMD&#x27;s new &quot;Zen 5&quot; architecture and are compatible with Copilot+.</span><br><span class="line">The Ryzen AI 9 HX 370 and Ryzen AI 9 365 each have an NPU with 50 TOPS performance for local AI acceleration.</span><br><span class="line">Acer, ASUS, HP, Lenovo, and MSI have stated that the new Ryzen AI chips are coming to AI laptops.</span><br><span class="line">Computex 2024 is underway in Taipei, Taiwan, and AMD was one of the first to unveil a bunch of new hardware at its keynote address. Alongside the new Zen 5 Ryzen 9000 desktop processors (CPU), AMD took the wrapping off of its Ryzen AI 300 chips. These are the long-rumored &quot;Strix Point&quot; APUs complete with Zen 5 CPU cores, RDNA 3.5 graphics, and XDNA 2 Neural Processing Unit (NPU) for localized AI acceleration.</span><br><span class="line"></span><br><span class="line">The big news here if you&#x27;re following the emerging world of AI PCs is AMD&#x27;s offering of 50 TOPS (Trillion Operations Per Second) of power from the NPU, making it more than capable enough to handle the new Copilot+ AI features coming to Windows 11. That also makes it more powerful than the Hexagon NPU in Qualcomm&#x27;s Snapdragon X Elite and Snapdragon X Plus chips, which comes in at 45 TOPS.</span><br><span class="line"></span><br><span class="line">For anyone interested in Copilot+ without Windows on ARM, this is our first official look at what AMD has cooked up.</span><br><span class="line"></span><br><span class="line">Strix Point has arrived with a rebrand</span><br><span class="line">AMD Ryzen AI 300 breakdown</span><br><span class="line"></span><br><span class="line">A slide from AMD showing a breakdown of the Ryzen AI 300 chip. (Image credit: AMD)</span><br><span class="line">AMD unveiled two new chips from its Ryzen AI 300 series, which has been rebranded to hopefully help avoid some confusion when shopping for a new laptop.</span><br><span class="line"></span><br><span class="line">The Ryzen AI 9 HX 370 is the more powerful chip, offering a total of 12 cores and 24 threads. The cores are split into four standard Zen 5 and eight Zen 5c, which are essentially smaller Zen cores that are more efficient at the cost of overall performance, freeing up space for the GPU and NPU. The Ryzen AI 9 HX 370 has a base TDP of 28W, but the configurable TDP (cTDP) ranges from 15W to 54W.</span><br><span class="line"></span><br><span class="line">Header Cell - Column 0	Cores/Threads	Base/Boost Freq.	NPU TOPS	TDP/cTDP	Graphics</span><br><span class="line">AMD Ryzen AI 9 HX 370	12 / 24	2.0GHz / 5.1GHz	50	28W / 15-54W	AMD Radeon 890M</span><br><span class="line">AMD Ryzen AI 9 365	10 / 20	2.0GHz / 5.0GHz	50	28W / 15-54W	AMD Radeon 880M</span><br><span class="line">The Ryzen AI 9 365 is a tier below the flagship HX 370 model, offering 10 cores (four Zen 5 and six Zen 5c), 20 threads, and a boost clock up to 5.0GHz. It has the same 28W base TDP and wide configurable TDP range.</span><br><span class="line"></span><br><span class="line">AMD Ryzen AI 300 GPU performance</span><br><span class="line"></span><br><span class="line">A slide from AMD showing Ryzen AI 9 HX 370 integrated GPU performance compared to Intel&#x27;s Arc graphics. (Image credit: AMD)</span><br><span class="line">The Ryzen AI 9 HX 370 has the new RDNA 3.5 Radeon 890M integrated GPU with 16 Compute Units (CU), while the Ryzen AI 9 365 has a Raden 880M with 12 CUs. AMD claims up to an average of 36% better gaming performance compared to Intel&#x27;s integrated Arc graphics in its Core Ultra 185H CPU. That, of course, is comparing the top-tier Radeon 890M GPU. The Ryzen AI 9 365&#x27;s integrated GPU with fewer CUs will come in with lower performance.</span><br><span class="line"></span><br><span class="line">Get the Windows Central Newsletter</span><br><span class="line">All the latest news, reviews, and guides for Windows and Xbox diehards.</span><br><span class="line"></span><br><span class="line">Your Email Address</span><br><span class="line">Contact me with news and offers from other Future brands</span><br><span class="line">Receive email from us on behalf of our trusted partners or sponsors</span><br><span class="line">By submitting your information you agree to the Terms &amp; Conditions and Privacy Policy and are aged 16 or over.</span><br><span class="line">AMD Ryzen AI 300 chips have the fastest NPU so far</span><br><span class="line">AMD Ryzen AI 300 NPU performance slide</span><br><span class="line"></span><br><span class="line">An AMD slide showing Ryzen AI 300 NPU performance compared to Qualcomm, Intel, and Apple. (Image credit: AMD)</span><br><span class="line">What I&#x27;m most excited about is the Ryzen AI NPU that AMD says can hit up to 50 TOPS. May 20, 2024, was a huge day in the world of Windows laptops thanks to Qualcomm, Microsoft, and major laptop brands teaming up to deliver a long list of new Copilot+ PCs.</span><br><span class="line"></span><br><span class="line">Windows Central Editor-in-Chief Daniel Rubino called the combination of ARM64 and AI a &quot;Great Reset&quot; for Windows PCs, and we can&#x27;t wait to get our hands on new laptops with Snapdragon X chips to test their power and efficiency.</span><br><span class="line"></span><br><span class="line">A big part of Qualcomm&#x27;s magic is its NPU with 45 TOPS of power for local AI acceleration. Until today this was the most powerful NPU available in a laptop chip, and it was the only entry into the world of Copilot+. AMD has now pulled ahead in the TOPS race, and it has opened up new laptop options for those who don&#x27;t want a system running Windows on ARM.</span><br><span class="line"></span><br><span class="line">Copilot+ requires an AI PC with Windows 11 and an NPU with at least 40 TOPS of power. That leaves, at this time, Qualcomm and AMD as your only announced options. It&#x27;s said that Intel&#x27;s next-gen &quot;Lunar Lake&quot; mobile chips will have an NPU with 45 TOPS, but that still leaves AMD in the lead.</span><br><span class="line"></span><br><span class="line">Copilot+ features include Windows Recall, Live Caption, Windows Studio Effects improvements, Co-Creator local image and text creation, and more.</span><br><span class="line"></span><br><span class="line">AMD Ryzen AI 300 performance chart</span><br><span class="line"></span><br><span class="line">An AMD slide comparing Snapdragon X Elite and Ryzen AI 9 HX 370 performance. (Image credit: AMD)</span><br><span class="line">How much of a difference the extra 5 TOPS will make in local AI work remains to be seen, and AMD isn&#x27;t talking much about efficiency compared to Qualcomm&#x27;s ARM64 chips. AMD did, however, show off some graphs comparing the Snapdragon X Elite (no mention of SKU used to compare) and the Ryzen AI 9 HX 370 in a number of benchmarks.</span><br><span class="line"></span><br><span class="line">Part of AMD&#x27;s Ryzen AI 300 announcement includes quotes from major laptop brands like Acer, ASUS, HP, Lenovo, and MSI. We know that Acer&#x27;s Swift series will see the new Ryzen AI 300 chips, as will a wide range of ASUS laptops from the ROG Zephyrus, ProArt, Vivobook, Zenbook, and TUF Gaming brands.</span><br><span class="line"></span><br><span class="line">Furthermore, HP says an OmniBook AI PC is getting Ryzen AI 300. Lenovo also plans on adding Ryzen AI 300 chips to its Yoga, ThinkPad, and ThinkBook stables. Finally, MSI says its Stealth, Summit, Prestige, and Creator laptops will get the chips later this year.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2405.20974v1</span><br><span class="line">[Submitted on 31 May 2024]</span><br><span class="line">SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales</span><br><span class="line">Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, Jing Gao</span><br><span class="line">Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work elicits confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results in both in-distribution and out-of-distribution datasets demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. We show that the generated self-reflective rationales are reasonable and can further contribute to the calibration. The code is made public at \url&#123;this https URL&#125;.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/SkyworkAI/Skywork-MoE/tree/main</span><br><span class="line">English | 简体中文</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">🤗 Hugging Face • 🤖 ModelScope • 👾 Wisemodel • 💬 WeChat• 📜Tech Report</span><br><span class="line"></span><br><span class="line">GitHub Stars GitHub Forks</span><br><span class="line"></span><br><span class="line">Project Introduction</span><br><span class="line">Skywork-MoE is a high-performance mixture-of-experts (MoE) model with 146 billion parameters, 16 experts, and 22 billion activated parameters. This model is initialized from the pre-existing dense checkpoints of our Skywork-13B model.</span><br><span class="line"></span><br><span class="line">We introduce two innovative techniques: Gating Logit Normalization, which enhances expert diversification, and Adaptive Auxiliary Loss Coefficients, which allow for layer-specific adjustment of auxiliary loss coefficients.</span><br><span class="line"></span><br><span class="line">Skywork-MoE demonstrates comparable or superior performance to models with more parameters or more activated parameters, such as Grok-1, DBRX, Mistral 8*22, and Deepseek-V2.</span><br><span class="line"></span><br><span class="line">News and Updates</span><br><span class="line">2024.6.3 We release the Skywork-MoE-Base model.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-06-05</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/06/05/2024-6-5-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 6월 5일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/06/07/2024-6-7-AI-NEWS/" title="2024년 6월 7일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/06/04/computex-2024/" title="NVIDIA CEO Jensen Huang Keynote at COMPUTEX 2024">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>