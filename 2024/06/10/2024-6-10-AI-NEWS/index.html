<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 6월 10일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="SummaryMeta의 Yann LeCun은 AI 연구실 관리의 중요성에 대해 논하며 특히 AI 프로젝트는 상부 관리의 감시에서 독립적으로 수행되어야만 한다고 강조했습니다. Gartner는 생성 AI가 가장 널리 사용되는 AI 솔루션이라고 발표했습니다. NVIDIA는 "><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 6월 10일 AI 소식</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Meta의 Yann LeCun은 AI 연구실 관리의 중요성에 대해 논하며 특히 AI 프로젝트는 상부 관리의 감시에서 독립적으로 수행되어야만 한다고 강조했습니다. Gartner는 생성 AI가 가장 널리 사용되는 AI 솔루션이라고 발표했습니다. NVIDIA는 LoRA 어댑터를 활용한 효율적인 모델 배포 방법을 설명했고, OpenAI는 새로운 model spec을 공개했습니다. DeepMind는 AlphaFold 3을 발표하며, 생물학적 분자 구조 예측에서의 혁신을 제시했습니다.</p>
<h2 id="연구실-관리의-중요성"><a href="#연구실-관리의-중요성" class="headerlink" title="연구실 관리의 중요성"></a>연구실 관리의 중요성</h2><p><a target="_blank" rel="noopener" href="https://www.linkedin.com/posts/yann-lecun_it-is-of-paramount-importance-that-the-management-activity-7205642101001203714-4nug?utm_source=share&utm_medium=member_ios">링크</a>, 2024년 6월 9일,<br>Meta</p>
<p>Yann LeCun<br>VP &amp; Chief AI Scientist at MetaVP &amp; Chief AI Scientist at Meta</p>
<p>연구소의 관리를 위해서는 다음의 것들이 매우 중요합니다.</p>
<ol>
<li>뛰어나고 창의적인 인재를 발굴하고, 채용하며, 유지하는 것.</li>
<li>이들이 최고의 연구를 할 수 있도록 환경, 자원, 자유를 제공하는 것.</li>
<li>유망한 연구 방향을 찾아내고(주로 연구자들이 제안하는 방향), 그 방향에 자원을 투자하는 것. 과학자들에게 책임을 맡기고 간섭하지 않는 것.</li>
<li>헛된 주장이나 비현실적인 아이디어를 잘 구별하는 것. 이는 과학자들이 부정직해서가 아니라 종종 자신을 속이기 쉬워서입니다. 자신이 대단한 발명을 했다고 생각하기 쉽습니다. 출판을 장려하고 오픈 소싱을 통해 연구 커뮤니티가 좋은 연구와 그렇지 않은 연구를 구별하도록 하는 방법이 있습니다.</li>
<li>연구자들이 야심찬 목표를 가진 연구 프로젝트에 참여하도록 동기를 부여하는 것. 단순한 개선 작업은 너무 쉽고 덜 위험할 수 있습니다.</li>
<li>단기적 성과와 단순한 지표(예: 논문 수)에 지나치게 집중하지 않는 방식으로 연구자들을 평가하는 것. 당신의 판단력을 사용하십시오. 그것이 당신이 높은 보수를 받는 이유입니다.</li>
<li>프로젝트를 상부 경영진의 감시에서 보호하는 것. 감시하는 냄비는 절대 끓지 않습니다. 계획된 혁신과 6개월 단위의 마일스톤으로는 결코 돌파구를 마련할 수 없습니다.</li>
</ol>
<h2 id="Gartner-설문-조사-생성-AI가-가장-널리-사용되는-AI-솔루션"><a href="#Gartner-설문-조사-생성-AI가-가장-널리-사용되는-AI-솔루션" class="headerlink" title="Gartner 설문 조사: 생성 AI가 가장 널리 사용되는 AI 솔루션"></a>Gartner 설문 조사: 생성 AI가 가장 널리 사용되는 AI 솔루션</h2><p><a target="_blank" rel="noopener" href="https://www.gartner.com/en/newsroom/press-releases/2024-05-07-gartner-survey-finds-generative-ai-is-now-the-most-frequently-deployed-ai-solution-in-organizations">링크</a>, 2024년 5월 7일,<br>Gartner</p>
<ul>
<li>설문 조사에 따르면, 29%의 응답자가 생성 AI를 사용 중.</li>
<li>생성 AI는 그래프 기술, 최적화 알고리즘, 규칙 기반 시스템 등을 제치고 가장 많이 사용됨.</li>
<li>Microsoft Copilot for 365와 Adobe Firefly와 같은 기존 응용 프로그램에 포함된 생성 AI 활용이 가장 일반적임.</li>
<li>AI 도입의 주요 장애물은 AI 프로젝트의 가치 추정 및 입증의 어려움.</li>
<li>성숙한 AI 조직은 AI 운영 모델, AI 엔지니어링, 업스킬링, 신뢰 및 보안 관리에 중점.</li>
</ul>
<h2 id="LoRA-어댑터를-활용한-효율적-모델-배포"><a href="#LoRA-어댑터를-활용한-효율적-모델-배포" class="headerlink" title="LoRA 어댑터를 활용한 효율적 모델 배포"></a>LoRA 어댑터를 활용한 효율적 모델 배포</h2><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/seamlessly-deploying-a-swarm-of-lora-adapters-with-nvidia-nim/?ncid=so-link-634884&=&linkId=100000265563449">링크</a>, 2024년 6월 7일,<br>NVIDIA</p>
<ul>
<li>LoRA는 전체 모델을 업데이트하지 않고도 작은 수의 추가 매개변수만 튜닝.</li>
<li>두 가지 LoRA 배포 방법: LoRA 어댑터 병합 및 동적 로드.</li>
<li>NIM을 통해 다양한 LoRA 어댑터를 한꺼번에 배치하여 여러 작업을 동시에 처리 가능.</li>
<li>NVIDIA NIM은 GPU 메모리와 호스트 메모리에서 어댑터를 동적으로 로드하여 성능 향상.</li>
</ul>
<h2 id="OpenAI의-새로운-모델-사양-공개"><a href="#OpenAI의-새로운-모델-사양-공개" class="headerlink" title="OpenAI의 새로운 모델 사양 공개"></a>OpenAI의 새로운 모델 사양 공개</h2><p><a target="_blank" rel="noopener" href="https://cdn.openai.com/spec/model-spec-2024-05-08.html">링크</a>, 2024년 5월 8일,<br>OpenAI</p>
<ul>
<li>모델 사양(Model Spec)은 모델 행동을 안내하는 고급 지침.</li>
<li>공개 피드백을 통해 모델 사양을 조정.</li>
<li>헌법 AI(Constitutional AI)와 달리 인간 피드백을 활용하여 모델을 강화.</li>
<li>모델 사양은 플랫폼 규칙, 법률 준수, 지적 재산권 존중 등 여섯 가지 행동 원칙 포함.</li>
</ul>
<h2 id="AlphaFold-3-모든-생화학을-아우르는-혁신"><a href="#AlphaFold-3-모든-생화학을-아우르는-혁신" class="headerlink" title="AlphaFold 3: 모든 생화학을 아우르는 혁신"></a>AlphaFold 3: 모든 생화학을 아우르는 혁신</h2><p><a target="_blank" rel="noopener" href="https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/">링크</a>, 2024년 5월 8일,<br>DeepMind</p>
<ul>
<li>AlphaFold 3는 단백질뿐만 아니라 DNA, RNA, 리간드 등 모든 생물학적 활성 분자의 구조를 예측.</li>
<li>기존 아미노산 구조 지식을 바탕으로 분자의 3D 구조 생성.</li>
<li>PoseBusters 데이터베이스에서 77%의 예측 성공률 기록.</li>
<li>단백질-단백질 상호작용 예측에서 77% 성공률 달성.</li>
</ul>
<h2 id="Hugging-Face의-DITTO-시연-피드백을-통한-모델-정렬"><a href="#Hugging-Face의-DITTO-시연-피드백을-통한-모델-정렬" class="headerlink" title="Hugging Face의 DITTO: 시연 피드백을 통한 모델 정렬"></a>Hugging Face의 DITTO: 시연 피드백을 통한 모델 정렬</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2406.00888">링크</a>, 2024년 6월 3일,<br>Hugging Face</p>
<ul>
<li>DITTO는 10개 미만의 시연을 통해 LLM 출력을 사용자 행동에 맞추는 방법 제안.</li>
<li>비교 데이터 생성 및 반복 학습을 통해 성능 향상.</li>
<li>소수의 시연으로도 모델을 효과적으로 사용자 정의 가능.</li>
</ul>
<h2 id="Intel의-Lunar-Lake-AI-PC를-위한-새로운-코어와-GPU"><a href="#Intel의-Lunar-Lake-AI-PC를-위한-새로운-코어와-GPU" class="headerlink" title="Intel의 Lunar Lake: AI PC를 위한 새로운 코어와 GPU"></a>Intel의 Lunar Lake: AI PC를 위한 새로운 코어와 GPU</h2><p><a target="_blank" rel="noopener" href="https://www.linkedin.com/pulse/intel-unwraps-lunar-lake-ai-pcs-new-cores-gpu-npu-ryan-shrout-jtx4c/?utm_source=share&utm_medium=member_ios&utm_campaign=share_via">링크</a>, 2024년 6월 4일,<br>Intel</p>
<ul>
<li>Lunar Lake는 새로운 코어 IP, GPU, NPU, 메모리 시스템을 갖춘 혁신적인 아키텍처.</li>
<li>P-코어와 E-코어의 성능 개선으로 IPC와 단일 스레드 부동 소수점 성능 향상.</li>
<li>새로운 Xe2 Battlemage 아키텍처의 GPU는 50% 더 높은 그래픽 성능 제공.</li>
</ul>
<h2 id="Microsoft의-Copilot-사용-경험"><a href="#Microsoft의-Copilot-사용-경험" class="headerlink" title="Microsoft의 Copilot 사용 경험"></a>Microsoft의 Copilot 사용 경험</h2><p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/worklab/our-year-with-copilot-what-microsoft-has-learned-about-ai-at-work">링크</a>, 2024년 6월 9일,<br>Microsoft</p>
<ul>
<li>Copilot 도입 첫 해, AI가 업무에 미치는 영향 평가.</li>
<li>초기 도입 부서: 판매, 고객 서비스, 인사.</li>
<li>Copilot 사용으로 생산성, 작업 즐거움, 워크라이프 밸런스 개선.</li>
</ul>
<h2 id="LLM-구축-경험에서-얻은-교훈"><a href="#LLM-구축-경험에서-얻은-교훈" class="headerlink" title="LLM 구축 경험에서 얻은 교훈"></a>LLM 구축 경험에서 얻은 교훈</h2><p><a target="_blank" rel="noopener" href="https://applied-llms.org/">링크</a>, 2024년 6월 8일,<br>Applied LLMs</p>
<ul>
<li>LLM 제품 구축의 전술적, 운영적, 전략적 측면을 다룸.</li>
<li>전술적: 프롬프트 작성, RAG, 흐름 엔지니어링, 평가 및 모니터링.</li>
<li>운영적: 제품 배송의 일상적 문제와 효과적인 팀 구축.</li>
<li>전략적: 장기적 관점과 시스템 중심 접근 방법 강조.</li>
</ul>
<h2 id="로컬-파일을-위한-생성-검색-엔진-구축"><a href="#로컬-파일을-위한-생성-검색-엔진-구축" class="headerlink" title="로컬 파일을 위한 생성 검색 엔진 구축"></a>로컬 파일을 위한 생성 검색 엔진 구축</h2><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/how-to-build-a-generative-search-engine-for-your-local-files-using-llama-3-399551786965">링크</a>, 2024년 6월 8일,<br>Towards Data Science</p>
<ul>
<li>로컬 파일과 상호작용하는 오픈 소스 생성 검색 엔진 구현.</li>
<li>Qdrant와 Streamlit을 사용하여 Llama 3 모델을 로컬에서 실행.</li>
<li>파일 인덱싱 및 쿼리 응답을 위한 구조와 사용자 인터페이스 설계.</li>
<li>성능과 유연성을 높이기 위해 문서 청크화 및 벡터 유사성 메트릭 사용.</li>
</ul>
<h2 id="실행-가능한-코드-작업으로-LLM-에이전트-개선"><a href="#실행-가능한-코드-작업으로-LLM-에이전트-개선" class="headerlink" title="실행 가능한 코드 작업으로 LLM 에이전트 개선"></a>실행 가능한 코드 작업으로 LLM 에이전트 개선</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2402.01030">링크</a>, 2024년 2월 2일,<br>Hugging Face</p>
<ul>
<li>LLM 에이전트의 행동 공간을 통합하기 위해 실행 가능한 Python 코드를 사용.</li>
<li>CodeAct를 통해 JSON이나 텍스트 대신 실행 가능한 코드로 작업을 수행.</li>
<li>API-Bank와 새로운 벤치마크에서 최대 20% 더 높은 성공률 달성.</li>
<li>Llama2와 Mistral에서 파인튜닝된 CodeActAgent를 통해 복잡한 작업 수행 가능.</li>
</ul>
<details>
  <summary>Sources</summary>
This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is: 
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># (today&#x27;s date in 년 월 일) AI 소식,</span><br><span class="line">## Summary </span><br><span class="line">(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.) </span><br><span class="line">## Title, </span><br><span class="line">한글제목 (title 이 영문이라면)</span><br><span class="line">[링크](link), date, </span><br><span class="line">company name</span><br><span class="line">- detailed summary1, (개조식 문체 사용)</span><br><span class="line">- detailed summary2,  (개조식 문체 사용)</span><br><span class="line">...</span><br><span class="line">- detailed summary N,  (개조식 문체 사용)</span><br><span class="line">##  Title, </span><br><span class="line">한글제목 (title 이 영문이라면)</span><br><span class="line">[링크](link), date, </span><br><span class="line">company name</span><br><span class="line">- detailed summary1, (개조식 문체 사용)</span><br><span class="line">- detailed summary2,  (개조식 문체 사용)</span><br><span class="line">... </span><br><span class="line">- detailed summary N,  (개조식 문체 사용)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
The report should be written in Korean and use the 개조식 문체 style. give the very deep details for each link as much as possible.

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://www.linkedin.com/posts/yann-lecun_it-is-of-paramount-importance-that-the-management-activity-7205642101001203714-4nug?utm_source=share&amp;utm_medium=member_ios</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Yann  LeCun</span><br><span class="line">VP &amp; Chief AI Scientist at MetaVP &amp; Chief AI Scientist at Meta</span><br><span class="line">16 hours ago</span><br><span class="line">Follow</span><br><span class="line"></span><br><span class="line">It is of paramount importance that the management of a research lab be composed of reputable scientists.</span><br><span class="line"></span><br><span class="line">Their main jobs are to:</span><br><span class="line">1. Identify, recruit, and retain brilliant and creative people.</span><br><span class="line">2. Give them the environment, resources, and freedom to do their best work.</span><br><span class="line">3. Identify promising research directions (often coming from the researchers themselves) and invest resources in them. Put the scientists in charge and get out of the way.</span><br><span class="line">4. Be really good at detecting BS, not necessarily because scientists are dishonest, but often because they are self-deluded. It&#x27;s easy to think you&#x27;ve invented the best thing since sliced bread. Encouraging publications and open sourcing is a way to use the research community to help distinguish good work from not-so-good work.</span><br><span class="line">5. Inspire researchers to work on research projects that have ambitious goals. It&#x27;s too easy and less risky to work on valuable improvements that are incremental.</span><br><span class="line">6. Evaluate people in ways that don&#x27;t overly focus on short-term impact and simple metrics (e.g. number of publications). Use your judgment. That&#x27;s why you get paid the big bucks.</span><br><span class="line">7. Insulate rogue-but-promising projects from the scrutiny of upper management. A watched pot never boils. Planned innovation and 6-months milestones never bring breakthroughs.</span><br><span class="line"></span><br><span class="line">You can&#x27;t do any of this cat herding jobs unless you are an experienced, talented, and reputable scientist with a research record that buys you at least some legitimacy in the eyes of the scientists in your organization.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.gartner.com/en/newsroom/press-releases/2024-05-07-gartner-survey-finds-generative-ai-is-now-the-most-frequently-deployed-ai-solution-in-organizations</span><br><span class="line">Gartner Survey Finds Generative AI Is Now the Most Frequently Deployed AI Solution in Organizations</span><br><span class="line">STAMFORD, Conn., May 7, 2024</span><br><span class="line"></span><br><span class="line">Estimating and Demonstrating Business Value Is No. 1 AI Adoption Barrier</span><br><span class="line">Generative artificial intelligence (GenAI) is the No. 1 type of AI solution deployed in organizations, according to a new survey by Gartner, Inc.</span><br><span class="line"></span><br><span class="line">According to the survey conducted in the fourth quarter of 2023, 29% of the 644 respondents from organizations in the U.S., Germany and the U.K. said that they have deployed and are using GenAI, making GenAI the most frequently deployed AI solution. GenAI was found to be more common than other solutions like graph techniques, optimization algorithms, rule-based systems, natural language processing and other types of machine learning.</span><br><span class="line"></span><br><span class="line">The survey also found that utilizing GenAI embedded in existing applications (such as Microsoft’s Copilot for 365 or Adobe Firefly) is the top way to fulfill GenAI use cases, with 34% of respondents saying this is their primary method of using GenAI. This was found to be more common than other options such as customizing GenAI models with prompt engineering (25%), training or fine-tuning bespoke GenAI models (21%), or using standalone GenAI tools, like ChatGPT or Gemini (19%).</span><br><span class="line"></span><br><span class="line">“GenAI is acting as a catalyst for the expansion of AI in the enterprise,” said Leinar Ramos, Sr Director Analyst at Gartner. “This creates a window of opportunity for AI leaders, but also a test on whether they will be able to capitalize on this moment and deliver value at scale.”</span><br><span class="line"></span><br><span class="line">Demonstrating AI Value Is Top Barrier to Adoption</span><br><span class="line">The primary obstacle to AI adoption, as reported by 49% of survey participants, is the difficulty in estimating and demonstrating the value of AI projects. This issue surpasses other barriers such as talent shortages, technical difficulties, data-related problems, lack of business alignment and trust in AI (see Figure 1).</span><br><span class="line"></span><br><span class="line">“Business value continues to be a challenge for organizations when it comes to AI,” said Ramos. “As organizations scale AI, they need to consider the total cost of ownership of their projects, as well as the wide spectrum of benefits beyond productivity improvement.”</span><br><span class="line"></span><br><span class="line">Figure 1: Top Barriers to Implement AI Techniques (Sum of Top 3 Ranks)</span><br><span class="line">[Image Alt Text for SEO]</span><br><span class="line">Source: Gartner (May 2024)</span><br><span class="line"></span><br><span class="line">&quot;GenAI has increased the degree of AI adoption throughout the business and made topics like AI upskilling and AI governance much more important,” said Ramos. “GenAI is forcing organizations to mature their AI capabilities.”</span><br><span class="line"></span><br><span class="line">Learnings from AI-Mature Organizations</span><br><span class="line">“Organizations who are struggling to derive business value from AI can learn from mature AI organizations,” said Ramos. “These are organizations that are applying AI more widely across different business units and processes, deploying many more use cases that stay longer in production.”</span><br><span class="line"></span><br><span class="line">The survey found 9% of organizations are currently AI-mature and found that what makes these organizations different is that they focus on four foundational capabilities:</span><br><span class="line"></span><br><span class="line">A scalable AI operating model, balancing centralized and distributed capabilities.</span><br><span class="line">A focus on AI engineering, designing a systematic way of building and deploying AI projects into production.</span><br><span class="line">An investment on upskilling and change management across the wider organization.</span><br><span class="line">A focus on trust, risk and security management (TRiSM) capabilities to mitigate the risks that come from AI implementations and drive better business outcomes.</span><br><span class="line">“AI-mature organizations invest in foundational capabilities that will remain relevant regardless of what happens tomorrow in the world of AI, and that allows them to scale their AI deployments efficiently and safely,” said Ramos.</span><br><span class="line"></span><br><span class="line">Focusing on these foundational capabilities can help organizations mature and alleviate the current challenge of bringing AI projects to production. The survey found that, on average, only 48% of AI projects make it into production, and it takes 8 months to go from AI prototype to production.</span><br><span class="line"></span><br><span class="line">Gartner clients can read more in “Survey Shows How GenAI Puts Organizational AI Maturity to the Test.” Learn more in the complimentary Gartner webinar “What Mature Organizations Do Differently for AI Success.”</span><br><span class="line"></span><br><span class="line">Gartner IT Symposium/Xpo</span><br><span class="line">CIOs and IT executives will explore AI adoption and implementation at Gartner IT Symposium/Xpo. Follow news and updates from the conferences on Twitter using #GartnerSYM.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://developer.nvidia.com/blog/seamlessly-deploying-a-swarm-of-lora-adapters-with-nvidia-nim/?ncid=so-link-634884&amp;=&amp;linkId=100000265563449</span><br><span class="line">Technical Blog</span><br><span class="line">NVIDIA</span><br><span class="line"></span><br><span class="line">Filter</span><br><span class="line">Subscribe</span><br><span class="line">Generative AI</span><br><span class="line">Seamlessly Deploying a Swarm of LoRA Adapters with NVIDIA NIM</span><br><span class="line">Jun 07, 2024</span><br><span class="line">By Shashank Verma, Neal Vaidya, Vinh Nguyen, Wei Du, Scot Junkin and BoYang Hsueh</span><br><span class="line"></span><br><span class="line">+11</span><br><span class="line">Like</span><br><span class="line"> Discuss (0)</span><br><span class="line"></span><br><span class="line">The latest state-of-the-art foundation large language models (LLMs) have billions of parameters and are pretrained on trillions of tokens of input text. They often achieve striking results on a wide variety of use cases without any need for customization. Despite this, studies have shown that the best accuracy on downstream tasks can be achieved by adapting LLMs with high-quality, domain-specific datasets.</span><br><span class="line"></span><br><span class="line">In many cases, smaller customized models can match or even outperform larger generic LLMs while offering significantly lower deployment costs. However, customizing models for specific downstream tasks can bring significant challenges, during both creation and deployment.</span><br><span class="line"></span><br><span class="line">Full fine-tuning (that is, updating all parameters of the model) for the largest LLMs can be difficult due to the amount of computational infrastructure required to learn across the whole model. Infrastructure costs are also increased at deployment time, where users are required to either host multiple large models in memory or tolerate increased latency as entire models are swapped in and out. Low-rank adaptation (LoRA) is a technique for mitigating both of these issues.</span><br><span class="line"></span><br><span class="line">This post provides a brief overview of LoRA, and explains the two ways to deploy LoRA fine-tuned models. We will also discuss our approach for enabling a heterogeneous LoRA deployment of a swarm of LoRA adapters, enabling mixed-batch inference requests.</span><br><span class="line"></span><br><span class="line">Low-rank adaptation</span><br><span class="line">In the past few years, LoRA has emerged as a popular technique that tunes a very small number of additional parameters, as compared to full fine-tuning. These additional parameters, called the LoRA adapter, represent the low-rank decomposition of the changes in the dense layers of the network. LoRA operates on the observation that LLMs are overparameterized, and that newly learned information during fine-tuning has a low “intrinsic rank.” In other words, the effective changes in the model parameters are confined to a lower-dimensional subspace of the entire, very high-dimensional parameter space. With LoRA, it’s possible to reduce the number of trainable parameters by 10,000x.</span><br><span class="line"></span><br><span class="line">Figure 1 illustrates the parameters introduced in the form of trainable low-rank matrices A and B. The pretrained weights are frozen while A and B are trained during LoRA customization to represent the newly added information.</span><br><span class="line">Figure 1. Parameters in A and B represent the newly added information. Image credit: LoRA: Low-Rank Adaptation of Large Language Models</span><br><span class="line">Figure 1 depicts the core idea behind LoRA:</span><br><span class="line"></span><br><span class="line">The weights of the pretrained model (W) are frozen during customization</span><br><span class="line">Instead of updating W, two smaller trainable matrices A and B are injected, which learn task-specific information. The matrix multiplication B*A forms a matrix with the same dimensions as W, thus it can be added to W (= W + BA).</span><br><span class="line">The ranks of A and B matrices are small values like 8, 16, and so on. Cumulatively, they have far fewer trainable parameters than W, which makes customization computationally and memory efficient. This rank (r) parameter is typically customizable at training time.</span><br><span class="line"></span><br><span class="line">There exists a tradeoff between rank size and computational efficiency. A larger rank value enables better expressivity, so the model can capture more patterns relevant to the downstream task. Very high rank values (like 64) approach the capacity of learning information close to full supervised fine-tuning. That is, updating all the parameters in the model. On the downside, larger ranks are also more expensive to train and inference, both in terms of memory and compute requirements. In practice, LoRA fine-tuning with a rank value as small as 8 is already very effective, and is a good starting point for a variety of downstream tasks.</span><br><span class="line"></span><br><span class="line">Deploying a LoRA-tuned model</span><br><span class="line">LoRA fine-tunes can be deployed in the following ways.</span><br><span class="line"></span><br><span class="line">Option 1: Merging the LoRA adapter</span><br><span class="line">The additional LoRA weights can be merged with the pretrained model to create a purpose-built variant that is structurally equivalent to its predecessor. This avoids incurring any additional inference latency of managing the adapter separately. Merging weights is a simpler approach, but less flexible. The disadvantage of this approach is that the whole model becomes “bespoke” and can only serve one task at a time—that is, the one it is fine-tuned for. This makes it difficult to batch together inputs for different tasks for efficiency in deployment. It is only recommended if you plan to serve a single task per deployment.</span><br><span class="line"></span><br><span class="line">Option 2: Dynamically loading the LoRA adapter</span><br><span class="line">LoRA adapters (A and B in Figure 1) are kept separate from the base model (W). At inference, the runtime dynamically loads the adapter weights corresponding to incoming requests to serve it. It enables flexibility in serving and batching inputs from various tasks concurrently to make the best use of the available compute, without having to maintain separate custom models.</span><br><span class="line"></span><br><span class="line">Some use cases require several, and even hundreds or thousands of LoRAs over the same base model. For these, ‌dynamic LoRA adapter selection is a better path. Examples include:</span><br><span class="line"></span><br><span class="line">Enterprises serving personalized models for their customers, for serving recommendations, or adapting to their specific personas or preferences.</span><br><span class="line">A/B testing to compare between various LoRA fine-tunes of the same use case.</span><br><span class="line">Enterprises serving multiple downstream use cases based on the same base foundation model. For example, IT service teams deploying a multi-LoRA setup for bug summarization, ticket routing and classification, implementing chatbots and knowledge retrieval over specific document corpuses, root cause analysis, and more.</span><br><span class="line">NVIDIA NIM  offers optimized inference microservices that support such dynamic loading of LoRA adapters and allow sending mixed-batch requests. The following sections take a deeper look at our approach.</span><br><span class="line"></span><br><span class="line">Heterogenous, multiple LoRA deployment with NVIDIA NIM</span><br><span class="line">With NIM, each inference microservice is associated with a single foundation model. This model can have any number of “customizations” in the form of low-rank adapters associated with it.</span><br><span class="line"></span><br><span class="line">Adapters, trained using either the NVIDIA NeMo framework or Hugging Face PEFT library are placed into an adapter store and given a unique name.</span><br><span class="line">When making a request to the NIM, clients can specify that they want a particular customization by including the LoRA model name.</span><br><span class="line">When NIM receives a request for some customized model, it will pull the associated adapter from the adapter store into a multi-tier cache. Some adapters are resident in GPU memory and some in host memory, depending on how recently they were used.</span><br><span class="line">During execution, NIM will run specialized GPU kernels that let data flow through both the foundation model and multiple different low-rank adapters simultaneously. This enables it to respond to requests for multiple different custom models at the same time.</span><br><span class="line">This image illustrates an architecture diagram for a mixed batch input neural network model. The key components are: Mixed Batch Input, GPU Memory, Adapter Store, Adapter Cache, and Output Batch.</span><br><span class="line">Figure 2. NVIDIA NIM dynamic LoRA architecture, which enables sending a mixed batch of input over the same foundation model</span><br><span class="line">Handling a mixed batch of requests</span><br><span class="line">The requests in one batch might use different LoRA adapters to support different tasks. Therefore, one traditional General Matrix Multiplication (GEMM) can’t be used to compute all the requests together. Computing them one-by-one sequentially would lead to significant additional overhead. To solve this problem, we used NVIDIA CUTLASS to implement a batched GEMM to fuse batched, heterogeneous request processing into a single kernel. This improves ‌GPU utilization and performance.</span><br><span class="line"></span><br><span class="line">Furthermore, we found that the GPU utilization of the batched GEMM is not sufficiently high for the first matrix component of each adapter, because this first matrix has a very large input dimension and small output dimension. Each adapter has two matrix components, A (shaped d-by-r) and B (shaped r-by-d), as seen in Figure 1. Since d is typically much larger than the LoRA rank r, we applied the splitK method to split the GEMM into several tiles on more streaming multiprocessors (SMs), improving the GPU utilization, and use an additional reduction kernel to reduce the partial results after the splitK-batched-GEMM.</span><br><span class="line"></span><br><span class="line">Best practices for performance benchmarking</span><br><span class="line">Evaluating the latency and throughput performance of such a multi-LoRA deployment is nontrivial. In this section, we discuss several major considerations generally worth looking at when benchmarking the performance of an LLM LoRA inference framework.</span><br><span class="line"></span><br><span class="line">Base model: Both small and large models can be used as base models for LoRA fine-tuning and inference, such as Llama 3 8B and Llama 3 70B. Smaller models excel at many tasks, especially traditional non-generative NLP tasks, such as text classification, while larger models excel at complex reasoning tasks. One of the advantages of LoRA is that even a large 70B model can be tuned on a single NVIDIA DGX H100 or A100 node with FP16, or even a single NVIDIA H100 or NVIDIA A100 GPU with 4-bit quantization.</span><br><span class="line">Adapters:  In practice, from the end user’s point of view, it’s desirable to have the flexibility to experiment and select the size that yields the best accuracy. System operators, on the other hand, may want to enforce a certain fixed size uniformly, for uniform LoRAs enable better batching and hence performance. Popular choices for LoRA ranks are 8/16/32/64.</span><br><span class="line">Test parameters: Several other test parameters to be considered for benchmarking include:</span><br><span class="line">Output length control: The ignore_eos parameter tells the inference framework to continue generating text until it reaches the max_token_length limit. This ensures the use case OSL (output sequence length) specification is met. This parameter is increasingly supported by LLM inference frameworks and significantly simplifies benchmarking setup. Notably, with ignore_eos you don’t have to train on “real” tasks for performance profiling purposes.</span><br><span class="line">System load: Concurrency (number of concurrent users) is commonly used to drive load into the system. This should reflect real use cases, while also taking into account the max “batch size” that the system can effectively serve concurrently. For an 8B model on one GPU, consider up to 250 concurrent users for a realistic server load.</span><br><span class="line">Task type: Both generative and non-generative tasks should be considered. These differ in the ISL (input sequence length) and OSL. ISL in the [200, 2000] token range, and OSL in the [1, 2000] token range reflect a wide range of LLM applications from text classification and summary, to translation and code generation.</span><br><span class="line">Tooling: The benchmarking tool should support calling the LoRA models. GenAI-Perf is an LLM benchmarking tool designed with LoRA support. Adapters are called either uniformly at random or in a round-robin fashion, or following a distribution to reflect real usage patterns. For example, 20% of adapters account for 80% of requests.</span><br><span class="line">Metrics: In the LLM domain, the main metrics are latency. TTFT (time to first token), ITL (inter-token latency) and throughput, TPS (total system tokens per second).</span><br><span class="line">Other supplementary metrics include total requests per second and end-to-end request latency.</span><br><span class="line"></span><br><span class="line">Compared to serving a base model (or merged LoRA model), the addition of dynamic LoRAs—a single LoRA, multiple LoRAs of the same rank, or multiple LoRAs of different ranks—all induce increasing cost, both in latency and throughput. Ideally, this cost should be reasonable in exchange for the improved accuracy and flexibility that dynamic LoRAs provide.</span><br><span class="line"></span><br><span class="line">In the coming weeks and months, we’ll have more to share on the performance characteristics of NIM when serving LoRA.</span><br><span class="line"></span><br><span class="line">What’s next</span><br><span class="line">There are exciting new enhancements to LoRA in research that aim to improve the efficiency or accuracy of fine-tuned models. Our future direction includes incorporating these into NIM.</span><br><span class="line"></span><br><span class="line">Tied-LoRA</span><br><span class="line">Tied-LoRA is a novel technique from NVIDIA Research that increases the parameter efficiency of LoRA. In LoRA, task-specific low-rank matrices are added that approximate the weight updates for each layer of the LLM. In Tied-LoRA, these low-rank matrices are shared (“tied”) between the various layers, further reducing the number of trainable parameters. Additionally, this technique allows selectively training or freezing of different components of LoRA (low-rank matrices, and scaling vectors) enabling the user to experiment with performance and parameter efficiency trade-offs.</span><br><span class="line"></span><br><span class="line">Support for this method with NVIDIA NIM is planned for future releases.</span><br><span class="line"></span><br><span class="line">DoRA</span><br><span class="line">DoRA, another technique developed by NVIDIA Research, bridges the performance gap between fully fine-tuned models and LoRA tuning. It achieves this by decomposing pretrained weights into two components: magnitude and direction. For fine-tuning, DoRA specifically uses LoRA for directional updates, thereby minimizing the number of trainable parameters efficiently. This approach enhances the learning capacity and training stability of LoRA without incurring additional inference overhead. DoRA consistently outperforms LoRA in fine-tuning models like LLaMA, LLaVA, and VL-BART across various downstream tasks, including commonsense reasoning, visual instruction tuning, and image and video-text understanding.</span><br><span class="line"></span><br><span class="line">Conclusion</span><br><span class="line">NVIDIA NIM enables you to seamlessly deploy and scale multiple LoRA adapters. NIM is generally available now, starting with support for Meta Llama 3 8B and Llama 3 70B, and LoRA adapters in both NVIDIA NeMo and Hugging Face model formats. We’re committed to adding support for additional state-of-the-art community models in future releases.</span><br><span class="line"></span><br><span class="line">To get started with multi-LoRA in NIM, check out the Jupyter Notebook tutorial on LoRA tuning a Llama 3 model using NVIDIA NeMo, deploying fine-tuned adapter(s) with NIM, and sending mixed inference requests. For more information about NIM, see the documentation.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.deeplearning.ai/the-batch/issue-249/</span><br><span class="line">Published</span><br><span class="line">May 16, 2024</span><br><span class="line">Reading time</span><br><span class="line">14 min read</span><br><span class="line">Share</span><br><span class="line">Dear friends,</span><br><span class="line"></span><br><span class="line">In the last couple of days, Google announced a doubling of Gemini Pro 1.5&#x27;s input context window from 1 million to 2 million tokens, and OpenAI released GPT-4o, which generates tokens 2x faster and 50% cheaper than GPT-4 Turbo and natively accepts and generates multimodal tokens. I view these developments as the latest in an 18-month trend. Given the improvements we&#x27;ve seen, best practices for developers have changed as well.</span><br><span class="line"></span><br><span class="line">Since the launch of ChatGPT in November 2022, with key milestones that include the releases of GPT-4, Gemini 1.5 Pro, Claude 3 Opus, and Llama 3-70B, many model providers have improved their capabilities in two important ways: (i) reasoning, which allows LLMs to think through complex concepts and and follow complex instructions; and (ii) longer input context windows.</span><br><span class="line"></span><br><span class="line">The reasoning capability of GPT-4 and other advanced models makes them quite good at interpreting complex prompts with detailed instructions. Many people are used to dashing off a quick, 1- to 2-sentence query to an LLM. In contrast, when building applications, I see sophisticated teams frequently writing prompts that might be 1 to 2 pages long (my teams call them “mega-prompts”) that provide complex instructions to specify in detail how we’d like an LLM to perform a task. I still see teams not going far enough in terms of writing detailed instructions. For an example of a moderately lengthy prompt, check out Claude 3’s system prompt. It’s detailed and gives clear guidance on how Claude should behave.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">This is a very different style of prompting than we typically use with LLMs’ web user interfaces, where we might dash off a quick query and, if the response is unsatisfactory, clarify what we want through repeated conversational turns with the chatbot.</span><br><span class="line"></span><br><span class="line">Further, the increasing length of input context windows has added another technique to the developer’s toolkit. GPT-3 kicked off a lot of research on few-shot in-context learning. For example, if you’re using an LLM for text classification, you might give a handful — say 1 to 5 examples — of text snippets and their class labels, so that it can use those examples to generalize to additional texts. However, with longer input context windows — GPT-4o accepts 128,000 input tokens, Claude 3 Opus 200,000 tokens, and Gemini 1.5 Pro 1 million tokens (2 million just announced in a limited preview) — LLMs aren’t limited to a handful of examples. With many-shot learning, developers can give dozens, even hundreds of examples in the prompt, and this works better than few-shot learning.</span><br><span class="line"></span><br><span class="line">When building complex workflows, I see developers getting good results with this process:</span><br><span class="line"></span><br><span class="line">Write quick, simple prompts and see how it does.</span><br><span class="line">Based on where the output falls short, flesh out the prompt iteratively. This often leads to a longer, more detailed, prompt, perhaps even a mega-prompt.</span><br><span class="line">If that’s still insufficient, consider few-shot or many-shot learning (if applicable) or, less frequently, fine-tuning.</span><br><span class="line">If that still doesn’t yield the results you need, break down the task into subtasks and apply an agentic workflow.</span><br><span class="line">I hope a process like this will help you build applications more easily. If you’re interested in taking a deeper dive into prompting strategies, I recommend the Medprompt paper, which lays out a complex set of prompting strategies that can lead to very good results.</span><br><span class="line"></span><br><span class="line">Keep learning!</span><br><span class="line"></span><br><span class="line">Andrew</span><br><span class="line"></span><br><span class="line">P.S. Two new short courses:</span><br><span class="line"></span><br><span class="line">“Multi AI Agent Systems with crewAI” taught by crewAI Founder and CEO João Moura: Learn to take a complex task and break it into subtasks for a team of specialized agents. You’ll learn how to design agent roles, goals, and tool sets, and decide how the agents collaborate (such as which agents can delegate to other agents). You&#x27;ll see how a multi-agent system can carry out research, write an article, perform financial analysis, or plan an event. Architecting multi-agent systems requires a new mode of thinking that&#x27;s more like managing a team than chatting with LLMs. Sign up here!</span><br><span class="line">“Building Multimodal Search and RAG” taught by Weaviate&#x27;s Sebastian Witalec: In this course, you&#x27;ll create RAG systems that reason over contextual information across text, images and video. You will learn how to train multimodal embedding models to map similar data to nearby vectors, so as to carry out semantic search across multiple modalities, and learn about visual instruction tuning to add image capabilities to large language models. Sign up here!</span><br><span class="line">News</span><br><span class="line"></span><br><span class="line">Why ChatGPT Acts That Way</span><br><span class="line">OpenAI pulled back the curtain on revised rules that will guide its models.</span><br><span class="line"></span><br><span class="line">What’s new: OpenAI published its Model Spec, high-level guidelines for use by human labelers to steer model behavior. The company is inviting public comments on the spec until May 22. It has not stated whether or how it will incorporate comments.</span><br><span class="line"></span><br><span class="line">How it works: During training, human labelers rate a model’s responses so it can be fine-tuned to conform with human preferences in the process known as reinforcement from human feedback (RLHF). The Model Spec outlines the principles — some new, some previously in use — that will drive those ratings. The principles are arranged hierarchically, and each category will override those below it.</span><br><span class="line"></span><br><span class="line">Three top-level objectives describe basic principles for model behavior: (i) “Assist the developer and end user” defines the relationship between humans and the model. (ii) “Benefit humanity” guides the model to consider both benefits and harms that may result from its behavior. (iii) “Reflect well on OpenAI” reinforces the company’s brand identity as well as social norms and laws.</span><br><span class="line">Six rules govern behavior. In order, models are to prioritize platform rules above requests from developers, users, and tools; follow laws; withhold hazardous information; respect intellectual property; protect privacy; and keep their output “safe for work.” (These rules can lead to contradictions. For instance, the model will comply if a user asks ChatGPT to translate a request for drug-related information because the directive to follow requests from users precedes the one to withhold hazardous information.)</span><br><span class="line">What OpenAI calls defaults govern the model’s interaction style. These include “ask clarifying questions when necessary,” “express uncertainty,” “assume an objective point of view,” and “don&#x27;t try to change anyone&#x27;s mind.” For example, if a user insists the Earth is flat, the model may respond, “Everyone&#x27;s entitled to their own beliefs, and I&#x27;m not here to persuade you!”</span><br><span class="line">The spec will evolve in response to the AI community’s needs. In the future, developers may be able to customize it. For instance, the company is considering allowing developers to lift prohibitions on “not safe for work” output such as erotica, gore, and some profanity.</span><br><span class="line">Behind the news: OpenAI’s use of the Model Spec and RLHF contrasts with Anthropic’s Constitutional AI. To steer the behavior of Anthropic models, that company’s engineers define a constitution, or list of principles, such as “Please choose the response that is the most helpful, honest, and harmless” and “Do NOT choose responses that are toxic, racist, or sexist, or that encourage or support illegal, violent, or unethical behavior.” Rather than human feedback, Anthropic relies on AI feedback to interpret behavioral principles and guide reinforcement learning.</span><br><span class="line"></span><br><span class="line">Why it matters: AI developers require a degree of confidence that the models they use will behave as they expect and in their users’ best interests. OpenAI’s decision to subject its guidelines to public scrutiny could help to instill such confidence, and its solicitation of public comments might make its models more responsive to social and market forces.</span><br><span class="line"></span><br><span class="line">We’re thinking: OpenAI’s openness with respect to its Model Spec is a welcome step toward improving its models’ safety and performance.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">AlphaFold 3 Embraces All Biochemistry</span><br><span class="line">The latest update of DeepMind’s AlphaFold model is designed to find the structures of not just proteins but all biologically active molecules as well as interactions between them.</span><br><span class="line"></span><br><span class="line">What’s new: Google announced AlphaFold 3, which models the 3D shapes of biomolecules including proteins, DNA, RNA, and ligands (molecules that bind to proteins or DNA, which includes antibodies and many drugs) in any combination. AlphaFold Server provides access for noncommercial uses (with some limitations). Unlike earlier versions, AlphaFold 3 is not open source.</span><br><span class="line"></span><br><span class="line">Key insight: Given a sequence of amino acids (the building blocks of proteins), the previous version of AlphaFold drew on an existing knowledge of amino acid structures, computed their locations and angles, and assembled them like Lego blocks. To adapt the system for molecules that aren’t made of amino acids, AlphaFold 3 represents them as collections of individual atoms and uses a generative model to find their positions in space.</span><br><span class="line"></span><br><span class="line">How it works: Given a list of molecules, AlphaFold 3 generates their joint 3D structure, revealing how they fit together. Several transformers hone embeddings of proteins and amino acids, while a diffusion model (also a transformer) processes embeddings of atoms. The team trained the system on five datasets including ground truth protein, DNA, and RNA structures interactions in the Protein Data Bank. They also trained it on protein shapes computed by AlphaFold 2; that model’s explicit knowledge of amino acid structures helped overcome AlphaFold 3’s tendency to hallucinate in some instances. Among the key processes:</span><br><span class="line"></span><br><span class="line">Given a protein’s amino acid sequence, a molecule’s set of atoms, or any combination thereof, AlphaFold 3 first represents each common amino acid, nucleotide, and individual atom (that isn’t a part of a common amino acid or nucleotide) with a single token.</span><br><span class="line">For each token, the system draws on existing databases to compute a variety of features, which fall into five categories: (i) per-token features like position, (ii) features of proteins in the Protein Data Bank, (iii) features of a given molecule, (iv) features derived from a genetic search (for example, whether two amino acid sequences appear to be related evolutionarily) and (v) features that describe chemical bonds between two tokens.</span><br><span class="line">Given these features, a transformer produces a single embedding that represents all tokens and pairwise embeddings that represent relationships between each pair of tokens. A second transformer refines the pairwise embeddings based on known molecules that share subsequences of amino acids or nucleotides with the input. A third transformer further refines the embeddings.</span><br><span class="line">Given the features, embeddings, and a noisy point cloud of atoms, the diffusion model removes the noise. (That is, it learned to modify the atoms’ positions to match those in their dataset.)</span><br><span class="line">AlphaFold 3 learned to optimize seven additional loss terms, including one that minimized the difference between the predicted and actual length of bonds between molecules and another that minimized the difference between predicted and actual distances between pairs of atoms.</span><br><span class="line">Results: On PoseBusters, a database of protein and protein-molecule shapes, AlphaFold 3 successfully found the shapes of about 77 percent of examples, while AutoDock Vina (a non-learning program that models molecular interactions) achieved about 53 percent. On a Protein Data Bank evaluation set, AlphaFold 3 successfully found about 84 percent of protein shapes, while AlphaFold Multimer 2.3 (an update of AlphaFold 2) found 83 percent. Modeling protein-protein interactions, AlphaFold 3 achieved 77 percent, while AlphaFold Multimer 2.3 achieved 67 percent, according to DockQ (a metric for the quality of such interactions).</span><br><span class="line"></span><br><span class="line">Behind the news: The original AlphaFold solved one of the most challenging problems in molecular biology by figuring out how long chains of amino acids would fold, giving scientists clear targets for designing new bioactive molecules. Google spun off Isomorphic Labs to apply AlphaFold 2 to drug discovery. That company will use AlphaFold 3 and control commercial access to it.</span><br><span class="line"></span><br><span class="line">Why it matters: AlphaFold 3 is a triumph of machine learning. It extends the utility of the previous version beyond proteins, and it computes with unprecedented accuracy how biological molecules will combine, allowing for a more comprehensive understanding of how drugs interact with the body. Its ability to predict how antibodies will bind to proteins could help stave off future pandemics and other illnesses.</span><br><span class="line"></span><br><span class="line">We’re thinking: Although Isomorphic Labs retains control of AlphaFold 3, biologists said the information in the paper is enough for other researchers to develop similar systems. We look forward to open versions!</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2402.01030</span><br><span class="line">Executable Code Actions Elicit Better LLM Agents</span><br><span class="line">Published on Feb 2</span><br><span class="line">Authors:</span><br><span class="line"></span><br><span class="line">Xingyao Wang</span><br><span class="line">,</span><br><span class="line">Yangyi Chen</span><br><span class="line">,</span><br><span class="line">Lifan Yuan</span><br><span class="line">,</span><br><span class="line">Yizhe Zhang</span><br><span class="line">,</span><br><span class="line">Yunzhu Li</span><br><span class="line">,</span><br><span class="line">Hao Peng</span><br><span class="line">,</span><br><span class="line">Heng Ji</span><br><span class="line">Abstract</span><br><span class="line">Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents&#x27; actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2406.00888</span><br><span class="line">Show, Don&#x27;t Tell: Aligning Language Models with Demonstrated Feedback</span><br><span class="line">Published on Jun 3</span><br><span class="line">·</span><br><span class="line">Featured in Daily Papers on Jun 4</span><br><span class="line">Authors:</span><br><span class="line"></span><br><span class="line">Omar Shaikh</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Michelle Lam</span><br><span class="line">,</span><br><span class="line">Joey Hejna</span><br><span class="line">,</span><br><span class="line">Yijia Shao</span><br><span class="line">,</span><br><span class="line">Michael Bernstein</span><br><span class="line">,</span><br><span class="line">Diyi Yang</span><br><span class="line">Abstract</span><br><span class="line">Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. We argue that it is instead possible to align an LLM to a specific setting by leveraging a very small number (&lt;10) of demonstrations as feedback. Our method, Demonstration ITerated Task Optimization (DITTO), directly aligns language model outputs to a user&#x27;s demonstrated behaviors. Derived using ideas from online imitation learning, DITTO cheaply generates online comparison data by treating users&#x27; demonstrations as preferred over output from the LLM and its intermediate checkpoints. We evaluate DITTO&#x27;s ability to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, we conduct a user study soliciting a range of demonstrations from participants (N=16). Across our benchmarks and user study, we find that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an average of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs.</span><br><span class="line"></span><br><span class="line">Humans learn faster by being shown rather than told. Well, LLMs also learn faster if you show them! 👀 DITTO from Stanford University proposes that LLMs can be tuned with less than 10 samples! 🤯</span><br><span class="line">Implementation:</span><br><span class="line">1️⃣ Collect a small number (&lt;10) of User/Expert demonstrations (input &amp; output)</span><br><span class="line">2️⃣ Select the SFT Model you want to tune</span><br><span class="line">3️⃣ Generate new negative samples for the demonstrations</span><br><span class="line">4️⃣ Create Pairwise comparison data where (expert &gt; generation)</span><br><span class="line">5️⃣ SFT until defined breakpoint (loss), then apply DPO using the pairwise comparison data</span><br><span class="line">🔄 Repeat 3-5, but in every new iteration, add 20% of “replay” data, with Current Iteration &gt; previous iteration outputs pairs</span><br><span class="line">Insights:</span><br><span class="line">📈 DITTO outperforms few-shot prompting</span><br><span class="line">🔄 Generating 10 negative samples per demonstration improves performance.</span><br><span class="line">📊 DITTO 22.34% relative Improvement</span><br><span class="line">🚀 31.5% performance improvement from the first to the fourth iteration.</span><br><span class="line">🏆 Outperforms SPIN &gt; 10% on using ~10 seed demonstrations</span><br><span class="line">🤗 Built with the @huggingface alignment-handbook</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://towardsdatascience.com/how-to-build-a-generative-search-engine-for-your-local-files-using-llama-3-399551786965</span><br><span class="line"></span><br><span class="line">How to Build a Generative Search Engine for Your Local Files Using Llama 3</span><br><span class="line">Use Qdrant, NVidia NIM API, or Llama 3 8B locally for your local GenAI assistant</span><br><span class="line">Nikola Milosevic (Data Warrior)</span><br><span class="line">Towards Data Science</span><br><span class="line">Nikola Milosevic (Data Warrior)</span><br><span class="line"></span><br><span class="line">·</span><br><span class="line">Follow</span><br><span class="line"></span><br><span class="line">Published in</span><br><span class="line">Towards Data Science</span><br><span class="line"></span><br><span class="line">·</span><br><span class="line">12 min read</span><br><span class="line">·</span><br><span class="line">2 days ago</span><br><span class="line">303</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">On the 23rd of May, I received an email from a person at Nvidia inviting me to the Generative AI Agents Developer Contest by NVIDIA and LangChain. My first thought was that it is quite a little time, and given we had a baby recently and my parents were supposed to come, I would not have time to participate. But then second thoughts came, and I decided that I could code something and submit it. I thought about what I could make for a few days, and one idea stuck with me — an Open-Source Generative Search Engine that lets you interact with local files. Microsoft Copilot already provides something like this, but I thought I could make an open-source version, for fun, and share a bit of learnings that I gathered during the quick coding of the system.</span><br><span class="line"></span><br><span class="line">System Design</span><br><span class="line">In order to build a local generative search engine or assistant, we would need several components:</span><br><span class="line"></span><br><span class="line">An index with the content of the local files, with an information retrieval engine to retrieve the most relevant documents for a given query/question.</span><br><span class="line">A language model to use selected content from local documents and generate a summarized answer</span><br><span class="line">A user interface</span><br><span class="line">How the components interact is presented in a diagram below.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">System design and architecture. Qdrant is used for vector store, while Streamlit is for the user interface. Llama 3 is either used via Nvidia NIM API (70B version) or is downloaded via HuggingFace (8B version). Document chunking is done using Langchain. Image by author</span><br><span class="line">First, we need to index our local files into the index that can be queried for the content of the local files. Then, when the user asks a question, we would use the created index, with some of the asymmetric paragraph or document embeddings to retrieve the most relevant documents that may contain the answer. The content of these documents and the question are passed to the deployed large language model, which would use the content of given documents to generate answers. In the instruction prompt, we would ask a large language model to also return references to the used document. Ultimately, everything will be visualized to the user on the user interface.</span><br><span class="line"></span><br><span class="line">Now, let’s have a look in more detail at each of the components.</span><br><span class="line"></span><br><span class="line">Semantic Index</span><br><span class="line">We are building a semantic index that will provide us with the most relevant documents based on the similarity of the file&#x27;s content and a given query. To create such an index we will use Qdrant as a vector store. Interestingly, a Qdrant client library does not require a full installation of Qdrant server and can do a similarity of documents that fit in working memory (RAM). Therefore, all we need to do is to pip install Qdrant client.</span><br><span class="line"></span><br><span class="line">We can initialize Qdrant in the following way (note that the hf parameter is later defined due to the story flow, but with Qdrant client you already need to define which vectorization method and metric is being used):</span><br><span class="line"></span><br><span class="line">from qdrant_client import QdrantClient</span><br><span class="line">from qdrant_client.models import Distance, VectorParams</span><br><span class="line">client = QdrantClient(path=&quot;qdrant/&quot;)</span><br><span class="line">collection_name = &quot;MyCollection&quot;</span><br><span class="line">if client.collection_exists(collection_name):</span><br><span class="line">    client.delete_collection(collection_name)</span><br><span class="line"></span><br><span class="line">client.create_collection(collection_name,vectors_config=VectorParams(size=768, distance=Distance.DOT))</span><br><span class="line">qdrant = Qdrant(client, collection_name, hf)</span><br><span class="line">In order to create a vector index, we will have to embed the documents on the hard drive. For embeddings, we will have to select the right embedding method and the right vector comparison metric. Several paragraph, sentence, or word embedding methods can be used, with varied results. The main issue with creating vector search, based on the documents, is the problem of asymmetric search. Asymmetric search problems are common to information retrieval and happen when one has short queries and long documents. Word or sentence embeddings are often fine-tuned to provide similarity scores based on documents of similar size (sentences, or paragraphs). Once that is not the case, the proper information retrieval may fail.</span><br><span class="line"></span><br><span class="line">However, we can find an embedding methodology that would work well on asymmetric search problems. For example, models fine-tuned on the MSMARCO dataset usually work well. MSMARCO dataset is based on Bing Search queries and documents and has been released by Microsoft. Therefore, it is ideal for the problem we are dealing with.</span><br><span class="line"></span><br><span class="line">For this particular implementation, I have selected an already fine-tuned model, called:</span><br><span class="line"></span><br><span class="line">sentence-transformers/msmarco-bert-base-dot-v5</span><br><span class="line">This model is based on BERT and it was fine-tuned using dot product as a similarity metric. We have already initialized qdrant client to use dot product as a similarity metric in line (note this model has dimension of 768):</span><br><span class="line"></span><br><span class="line">client.create_collection(collection_name,vectors_config=VectorParams(size=768, distance=Distance.DOT))</span><br><span class="line">We could use other metrics, such as cosine similarity, however, given this model is fine-tuned using dot product, we will get the best performance using this metric. On top of that, thinking geometrically: Cosine similarity focuses solely on the difference in angles, whereas the dot product takes into account both angle and magnitude. By normalizing data to have uniform magnitudes, the two measures become equivalent. In situations where ignoring magnitude is beneficial, cosine similarity is useful. However, the dot product is a more suitable similarity measure if the magnitude is significant.</span><br><span class="line"></span><br><span class="line">The code for initializing the MSMarco model is (in case you have available GPU, use it. by all means):</span><br><span class="line"></span><br><span class="line">    model_name = &quot;sentence-transformers/msmarco-bert-base-dot-v5&quot;</span><br><span class="line">    model_kwargs = &#123;&#x27;device&#x27;: &#x27;cpu&#x27;&#125;</span><br><span class="line">    encode_kwargs = &#123;&#x27;normalize_embeddings&#x27;: True&#125;</span><br><span class="line">    hf = HuggingFaceEmbeddings(</span><br><span class="line">        model_name=model_name,</span><br><span class="line">        model_kwargs=model_kwargs,</span><br><span class="line">        encode_kwargs=encode_kwargs</span><br><span class="line">    )</span><br><span class="line">The next problem: we need to deal with is that BERT-like models have limited context size, due to the quadratic memory requirements of transformer models. In the case of many BERT-like models, this context size is set to 512 tokens. There are two options: (1) we can base our answer only on the first 512 tokens and ignore the rest of the document, or (2) create an index, where one document will be split into multiple chunks and stored in the index as chunks. In the first case, we would lose a lot of important information, and therefore, we picked the second variant. To chunk documents, we can use a prebuilt chunker from LangChain:</span><br><span class="line"></span><br><span class="line">from langchain_text_splitters import TokenTextSplitter</span><br><span class="line">text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=50)</span><br><span class="line">texts = text_splitter.split_text(file_content)</span><br><span class="line">metadata = []</span><br><span class="line">for i in range(0,len(texts)):</span><br><span class="line">    metadata.append(&#123;&quot;path&quot;:file&#125;)</span><br><span class="line">qdrant.add_texts(texts,metadatas=metadata)</span><br><span class="line">In the provided part of the code, we chunk text into the size of 500 tokens, with a window of 50 overlapping tokens. This way we keep a bit of context on the places where chunks end or begin. In the rest of the code, we create metadata with the document path on the user’s hard disk and add these chunks with metadata to the index.</span><br><span class="line"></span><br><span class="line">However, before we add the content of the files to the index, we need to read it. Even before we read files, we need to get all the files we need to index. For the sake of simplicity, in this project, the user can define a folder that he/she would like to index. The indexer retrieves all the files from that folder and its subfolder in a recursive manner and indexes files that are supported (we will look at how to support PDF, Word, PPT, and TXT).</span><br><span class="line"></span><br><span class="line">We can retrieve all the files in a given folder and its subfolder in a recursive way:</span><br><span class="line"></span><br><span class="line">def get_files(dir):</span><br><span class="line">    file_list = []</span><br><span class="line">    for f in listdir(dir):</span><br><span class="line">        if isfile(join(dir,f)):</span><br><span class="line">            file_list.append(join(dir,f))</span><br><span class="line">        elif isdir(join(dir,f)):</span><br><span class="line">            file_list= file_list + get_files(join(dir,f))</span><br><span class="line">    return file_list</span><br><span class="line">Once all the files are retrieved in the list, we can read the content of files containing text. In this tool, for start, we will support MS Word documents (with extension “.docx”), PDF documents, MS PowerPoint presentations (with extension “.pptx”), and plain text files (with extension “.txt”).</span><br><span class="line"></span><br><span class="line">In order to read MS Word documents, we can use the docx-python library. The function reading documents into a string variable would look something like this:</span><br><span class="line"></span><br><span class="line">import docx</span><br><span class="line">def getTextFromWord(filename):</span><br><span class="line">    doc = docx.Document(filename)</span><br><span class="line">    fullText = []</span><br><span class="line">    for para in doc.paragraphs:</span><br><span class="line">        fullText.append(para.text)</span><br><span class="line">    return &#x27;\n&#x27;.join(fullText)</span><br><span class="line">A similar thing can be done with MS PowerPoint files. For this, we will need to download and install the pptx-python library and write a function like this:</span><br><span class="line"></span><br><span class="line">from pptx import Presentation</span><br><span class="line">def getTextFromPPTX(filename):</span><br><span class="line">    prs = Presentation(filename)</span><br><span class="line">    fullText = []</span><br><span class="line">    for slide in prs.slides:</span><br><span class="line">        for shape in slide.shapes:</span><br><span class="line">            fullText.append(shape.text)</span><br><span class="line">    return &#x27;\n&#x27;.join(fullText)</span><br><span class="line">Reading text files is pretty simple:</span><br><span class="line"></span><br><span class="line">f = open(file,&#x27;r&#x27;)</span><br><span class="line">file_content = f.read()</span><br><span class="line">f.close()</span><br><span class="line">For PDF files we will in this case use the PyPDF2 library:</span><br><span class="line"></span><br><span class="line">reader = PyPDF2.PdfReader(file)</span><br><span class="line">for i in range(0,len(reader.pages)):</span><br><span class="line">    file_content = file_content + &quot; &quot;+reader.pages[i].extract_text()</span><br><span class="line">Finally, the whole indexing function would look something like this:</span><br><span class="line"></span><br><span class="line">file_content = &quot;&quot;</span><br><span class="line">    for file in onlyfiles:</span><br><span class="line">        file_content = &quot;&quot;</span><br><span class="line">        if file.endswith(&quot;.pdf&quot;):</span><br><span class="line">            print(&quot;indexing &quot;+file)</span><br><span class="line">            reader = PyPDF2.PdfReader(file)</span><br><span class="line">            for i in range(0,len(reader.pages)):</span><br><span class="line">                file_content = file_content + &quot; &quot;+reader.pages[i].extract_text()</span><br><span class="line">        elif file.endswith(&quot;.txt&quot;):</span><br><span class="line">            print(&quot;indexing &quot; + file)</span><br><span class="line">            f = open(file,&#x27;r&#x27;)</span><br><span class="line">            file_content = f.read()</span><br><span class="line">            f.close()</span><br><span class="line">        elif file.endswith(&quot;.docx&quot;):</span><br><span class="line">            print(&quot;indexing &quot; + file)</span><br><span class="line">            file_content = getTextFromWord(file)</span><br><span class="line">        elif file.endswith(&quot;.pptx&quot;):</span><br><span class="line">            print(&quot;indexing &quot; + file)</span><br><span class="line">            file_content = getTextFromPPTX(file)</span><br><span class="line">        else:</span><br><span class="line">            continue</span><br><span class="line">        text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=50)</span><br><span class="line">        texts = text_splitter.split_text(file_content)</span><br><span class="line">        metadata = []</span><br><span class="line">        for i in range(0,len(texts)):</span><br><span class="line">            metadata.append(&#123;&quot;path&quot;:file&#125;)</span><br><span class="line">        qdrant.add_texts(texts,metadatas=metadata)</span><br><span class="line">    print(onlyfiles)</span><br><span class="line">    print(&quot;Finished indexing!&quot;)</span><br><span class="line">As we stated, we use TokenTextSplitter from LangChain to create chunks of 500 tokens with 50 token overlap. Now, when we have created an index, we can create a web service for querying it and generating answers.</span><br><span class="line"></span><br><span class="line">Generative Search API</span><br><span class="line">We will create a web service using FastAPI to host our generative search engine. The API will access the Qdrant client with the indexed data we created in the previous section, perform a search using a vector similarity metric, use the top chunks to generate an answer with the Llama 3 model, and finally provide the answer back to the user.</span><br><span class="line"></span><br><span class="line">In order to initialize and import libraries for the generative search component, we can use the following code:</span><br><span class="line"></span><br><span class="line">from fastapi import FastAPI</span><br><span class="line">from langchain_community.embeddings import HuggingFaceEmbeddings</span><br><span class="line">from langchain_qdrant import Qdrant</span><br><span class="line">from qdrant_client import QdrantClient</span><br><span class="line">from pydantic import BaseModel</span><br><span class="line">import torch</span><br><span class="line">from transformers import AutoTokenizer, AutoModelForCausalLM</span><br><span class="line">import environment_var</span><br><span class="line">import os</span><br><span class="line">from openai import OpenAI</span><br><span class="line"></span><br><span class="line">class Item(BaseModel):</span><br><span class="line">    query: str</span><br><span class="line">    def __init__(self, query: str) -&gt; None:</span><br><span class="line">        super().__init__(query=query)</span><br><span class="line">As previously mentioned, we are using FastAPI to create the API interface. We will utilize the qdrant_client library to access the indexed data we created and leverage the langchain_qdrant library for additional support. For embeddings and loading Llama 3 models locally, we will use the PyTorch and Transformers libraries. Additionally, we will make calls to the NVIDIA NIM API using the OpenAI library, with the API keys stored in the environment_var (for both Nvidia and HuggingFace) file we created.</span><br><span class="line"></span><br><span class="line">We create class Item, derived from BaseModel in Pydantic to pass as parameters to request functions. It will have one field, called query.</span><br><span class="line"></span><br><span class="line">Now, we can start initializing our machine-learning models</span><br><span class="line"></span><br><span class="line">model_name = &quot;sentence-transformers/msmarco-bert-base-dot-v5&quot;</span><br><span class="line">model_kwargs = &#123;&#x27;device&#x27;: &#x27;cpu&#x27;&#125;</span><br><span class="line">encode_kwargs = &#123;&#x27;normalize_embeddings&#x27;: True&#125;</span><br><span class="line">hf = HuggingFaceEmbeddings(</span><br><span class="line">    model_name=model_name,</span><br><span class="line">    model_kwargs=model_kwargs,</span><br><span class="line">    encode_kwargs=encode_kwargs</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">os.environ[&quot;HF_TOKEN&quot;] = environment_var.hf_token</span><br><span class="line">use_nvidia_api = False</span><br><span class="line">use_quantized = True</span><br><span class="line">if environment_var.nvidia_key !=&quot;&quot;:</span><br><span class="line">    client_ai = OpenAI(</span><br><span class="line">        base_url=&quot;https://integrate.api.nvidia.com/v1&quot;,</span><br><span class="line">        api_key=environment_var.nvidia_key</span><br><span class="line">    )</span><br><span class="line">    use_nvidia_api = True</span><br><span class="line">elif use_quantized:</span><br><span class="line">    model_id = &quot;Kameshr/LLAMA-3-Quantized&quot;</span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(model_id)</span><br><span class="line">    model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">        model_id,</span><br><span class="line">        torch_dtype=torch.float16,</span><br><span class="line">        device_map=&quot;auto&quot;,</span><br><span class="line">    )</span><br><span class="line">else:</span><br><span class="line">    model_id = &quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(model_id)</span><br><span class="line">    model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">        model_id,</span><br><span class="line">        torch_dtype=torch.float16,</span><br><span class="line">        device_map=&quot;auto&quot;,</span><br><span class="line">    )</span><br><span class="line">In the first few lines, we load weights for the BERT-based model fine-tuned on MSMARCO data that we have also used to index our documents.</span><br><span class="line"></span><br><span class="line">Then, we check whether nvidia_key is provided, and if it is, we use the OpenAI library to call NVIDIA NIM API. When we use NVIDIA NIM API, we can use a big version of the Llama 3 instruct model, with 70B parameters. In case nvidia_key is not provided, we will load Llama 3 locally. However, locally, at least for most consumer electronics, it would not be possible to load the 70B parameters model. Therefore, we will either load the Llama 3 8B parameter model or the Llama 3 8B parameters model that has been additionally quantized. With quantization, we save space and enable model execution on less RAM. For example, Llama 3 8B usually needs about 14GB of GPU RAM, while Llama 3 8B quantized would be able to run on 6GB of GPU RAM. Therefore, we load either a full or quantized model depending on a parameter.</span><br><span class="line"></span><br><span class="line">We can now initialize the Qdrant client</span><br><span class="line"></span><br><span class="line">client = QdrantClient(path=&quot;qdrant/&quot;)</span><br><span class="line">collection_name = &quot;MyCollection&quot;</span><br><span class="line">qdrant = Qdrant(client, collection_name, hf)</span><br><span class="line">Also, FastAPI and create a first mock GET function</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.get(&quot;/&quot;)</span><br><span class="line">async def root():</span><br><span class="line">    return &#123;&quot;message&quot;: &quot;Hello World&quot;&#125;</span><br><span class="line">This function would return JSON in format &#123;“message”:”Hello World”&#125;</span><br><span class="line"></span><br><span class="line">However, for this API to be functional, we will create two functions, one that performs only semantic search, while the other would perform search and then put the top 10 chunks as a context and generate an answer, referencing documents it used.</span><br><span class="line"></span><br><span class="line">@app.post(&quot;/search&quot;)</span><br><span class="line">def search(Item:Item):</span><br><span class="line">    query = Item.query</span><br><span class="line">    search_result = qdrant.similarity_search(</span><br><span class="line">        query=query, k=10</span><br><span class="line">    )</span><br><span class="line">    i = 0</span><br><span class="line">    list_res = []</span><br><span class="line">    for res in search_result:</span><br><span class="line">        list_res.append(&#123;&quot;id&quot;:i,&quot;path&quot;:res.metadata.get(&quot;path&quot;),&quot;content&quot;:res.page_content&#125;)</span><br><span class="line">    return list_res</span><br><span class="line"></span><br><span class="line">@app.post(&quot;/ask_localai&quot;)</span><br><span class="line">async def ask_localai(Item:Item):</span><br><span class="line">    query = Item.query</span><br><span class="line">    search_result = qdrant.similarity_search(</span><br><span class="line">        query=query, k=10</span><br><span class="line">    )</span><br><span class="line">    i = 0</span><br><span class="line">    list_res = []</span><br><span class="line">    context = &quot;&quot;</span><br><span class="line">    mappings = &#123;&#125;</span><br><span class="line">    i = 0</span><br><span class="line">    for res in search_result:</span><br><span class="line">        context = context + str(i)+&quot;\n&quot;+res.page_content+&quot;\n\n&quot;</span><br><span class="line">        mappings[i] = res.metadata.get(&quot;path&quot;)</span><br><span class="line">        list_res.append(&#123;&quot;id&quot;:i,&quot;path&quot;:res.metadata.get(&quot;path&quot;),&quot;content&quot;:res.page_content&#125;)</span><br><span class="line">        i = i +1</span><br><span class="line"></span><br><span class="line">    rolemsg = &#123;&quot;role&quot;: &quot;system&quot;,</span><br><span class="line">               &quot;content&quot;: &quot;Answer user&#x27;s question using documents given in the context. In the context are documents that should contain an answer. Please always reference document id (in squere brackets, for example [0],[1]) of the document that was used to make a claim. Use as many citations and documents as it is necessary to answer question.&quot;&#125;</span><br><span class="line">    messages = [</span><br><span class="line">        rolemsg,</span><br><span class="line">        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Documents:\n&quot;+context+&quot;\n\nQuestion: &quot;+query&#125;,</span><br><span class="line">    ]</span><br><span class="line">    if use_nvidia_api:</span><br><span class="line">        completion = client_ai.chat.completions.create(</span><br><span class="line">            model=&quot;meta/llama3-70b-instruct&quot;,</span><br><span class="line">            messages=messages,</span><br><span class="line">            temperature=0.5,</span><br><span class="line">            top_p=1,</span><br><span class="line">            max_tokens=1024,</span><br><span class="line">            stream=False</span><br><span class="line">        )</span><br><span class="line">        response = completion.choices[0].message.content</span><br><span class="line">    else:</span><br><span class="line">        input_ids = tokenizer.apply_chat_template(</span><br><span class="line">                messages,</span><br><span class="line">                add_generation_prompt=True,</span><br><span class="line">                return_tensors=&quot;pt&quot;</span><br><span class="line">            ).to(model.device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        terminators = [</span><br><span class="line">            tokenizer.eos_token_id,</span><br><span class="line">            tokenizer.convert_tokens_to_ids(&quot;&lt;|eot_id|&gt;&quot;)</span><br><span class="line">            ]</span><br><span class="line"></span><br><span class="line">        outputs = model.generate(</span><br><span class="line">            input_ids,</span><br><span class="line">            max_new_tokens=256,</span><br><span class="line">            eos_token_id=terminators,</span><br><span class="line">            do_sample=True,</span><br><span class="line">            temperature=0.2,</span><br><span class="line">            top_p=0.9,</span><br><span class="line">        )</span><br><span class="line">        response = tokenizer.decode(outputs[0][input_ids.shape[-1]:])</span><br><span class="line">    return &#123;&quot;context&quot;:list_res,&quot;answer&quot;:response&#125;</span><br><span class="line">Both functions are POST methods, and we use our Item class to pass the query via JSON body. The first method returns the 10 most similar document chunks, with the path, and assigns document ID from 0–9. Therefore, it just performs the plain semantic search using dot product as similarity metric (this was defined during indexing in Qdrant — remember line containing distance=Distance.DOT).</span><br><span class="line"></span><br><span class="line">The second function, called ask_localai is slightly more complex. It contains a search mechanism from the first method (therefore it may be easier to go through code there to understand semantic search), but adds a generative part. It creates a prompt for Llama 3, containing instructions in a system prompt message saying:</span><br><span class="line"></span><br><span class="line">Answer the user’s question using the documents given in the context. In the context are documents that should contain an answer. Please always reference the document ID (in square brackets, for example [0],[1]) of the document that was used to make a claim. Use as many citations and documents as it is necessary to answer a question.</span><br><span class="line"></span><br><span class="line">The user’s message contains a list of documents structured as an ID (0–9) followed by the document chunk on the next line. To maintain the mapping between IDs and document paths, we create a list called list_res, which includes the ID, path, and content. The user prompt ends with the word “Question” followed by the user’s query.</span><br><span class="line"></span><br><span class="line">The response contains context and generated answer. However, the answer is again generated by either the Llama 3 70B model (using NVIDIA NIM API), local Llama 3 8B, or local Llama 3 8B quantized depending on the passed parameters.</span><br><span class="line"></span><br><span class="line">The API can be started from a separate file containing the following lines of code (given, that our generative component is in a file called api.py, as the first argument in Uvicorn maps to the file name):</span><br><span class="line"></span><br><span class="line">import uvicorn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    uvicorn.run(&quot;api:app&quot;,host=&#x27;0.0.0.0&#x27;, port=8000, reload=False,  workers=3)</span><br><span class="line">Simple User Interface</span><br><span class="line">The final component of our local generative search engine is the user interface. We will build a simple user interface using Streamlit, which will include an input bar, a search button, a section for displaying the generated answer, and a list of referenced documents that can be opened or downloaded.</span><br><span class="line"></span><br><span class="line">The whole code for the user interface in Streamlit has less than 45 lines of code (44 to be exact):</span><br><span class="line"></span><br><span class="line">import re</span><br><span class="line">import streamlit as st</span><br><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line">st.title(&#x27;_:blue[Local GenAI Search]_ :sunglasses:&#x27;)</span><br><span class="line">question = st.text_input(&quot;Ask a question based on your local files&quot;, &quot;&quot;)</span><br><span class="line">if st.button(&quot;Ask a question&quot;):</span><br><span class="line">    st.write(&quot;The current question is \&quot;&quot;, question+&quot;\&quot;&quot;)</span><br><span class="line">    url = &quot;http://127.0.0.1:8000/ask_localai&quot;</span><br><span class="line"></span><br><span class="line">    payload = json.dumps(&#123;</span><br><span class="line">      &quot;query&quot;: question</span><br><span class="line">    &#125;)</span><br><span class="line">    headers = &#123;</span><br><span class="line">      &#x27;Accept&#x27;: &#x27;application/json&#x27;,</span><br><span class="line">      &#x27;Content-Type&#x27;: &#x27;application/json&#x27;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    response = requests.request(&quot;POST&quot;, url, headers=headers, data=payload)</span><br><span class="line"></span><br><span class="line">    answer = json.loads(response.text)[&quot;answer&quot;]</span><br><span class="line">    rege = re.compile(&quot;\[Document\ [0-9]+\]|\[[0-9]+\]&quot;)</span><br><span class="line">    m = rege.findall(answer)</span><br><span class="line">    num = []</span><br><span class="line">    for n in m:</span><br><span class="line">        num = num + [int(s) for s in re.findall(r&#x27;\b\d+\b&#x27;, n)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    st.markdown(answer)</span><br><span class="line">    documents = json.loads(response.text)[&#x27;context&#x27;]</span><br><span class="line">    show_docs = []</span><br><span class="line">    for n in num:</span><br><span class="line">        for doc in documents:</span><br><span class="line">            if int(doc[&#x27;id&#x27;]) == n:</span><br><span class="line">                show_docs.append(doc)</span><br><span class="line">    a = 1244</span><br><span class="line">    for doc in show_docs:</span><br><span class="line">        with st.expander(str(doc[&#x27;id&#x27;])+&quot; - &quot;+doc[&#x27;path&#x27;]):</span><br><span class="line">            st.write(doc[&#x27;content&#x27;])</span><br><span class="line">            with open(doc[&#x27;path&#x27;], &#x27;rb&#x27;) as f:</span><br><span class="line">                st.download_button(&quot;Downlaod file&quot;, f, file_name=doc[&#x27;path&#x27;].split(&#x27;/&#x27;)[-1],key=a</span><br><span class="line">                )</span><br><span class="line">                a = a + 1</span><br><span class="line">It will all end up looking like this:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">An example of an answered question in the built user interface. Screenshot by author.</span><br><span class="line">Availability</span><br><span class="line">The entire code for the described project is available on GitHub, at https://github.com/nikolamilosevic86/local-genAI-search. In the past, I have worked on several generative search projects, on which there have also been some publications. You can have a look at https://www.thinkmind.org/library/INTERNET/INTERNET_2024/internet_2024_1_10_48001.html or https://arxiv.org/abs/2402.18589.</span><br><span class="line"></span><br><span class="line">Conclusion</span><br><span class="line">This article showed how one can leverage generative AI with semantic search using Qdrant. It is generally a Retrieval-Augmented Generation (RAG) pipeline over local files with instructions to reference claims to the local documents. The whole code is about 300 lines long, and we have even added complexity by giving a choice to the user between 3 different Llama 3 models. For this use case, both 8B and 70B parameter models work quite well.</span><br><span class="line"></span><br><span class="line">I wanted to explain the steps I did, in case this can be helpful for someone in the future. However, if you want to use this particular tool, the easiest way to do so is by just getting it from GitHub, it is all open source!</span><br><span class="line">https://github.com/nikolamilosevic86/local-genAI-search</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.linkedin.com/pulse/intel-unwraps-lunar-lake-ai-pcs-new-cores-gpu-npu-ryan-shrout-jtx4c/?utm_source=share&amp;utm_medium=member_ios&amp;utm_campaign=share_via</span><br><span class="line">Intel Unwraps Lunar Lake for AI PCs: new cores, new GPU, new NPU</span><br><span class="line">Ryan Shrout</span><br><span class="line">Ryan Shrout</span><br><span class="line">Technology and Marketing</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">June 4, 2024</span><br><span class="line">Intel might be the last of the big four silicon providers to present this week at Computex, but they definitely aren’t going to be the least vocal. Many of the press and analyst corp has been in Taiwan with Intel for the better part of a full week, going through two days of briefings and talks about the new Lunar Lake product architecture and its plans for release. And today during the company’s keynote, they let the details out and began to talk about how it sees Lunar Lake changing the game.</span><br><span class="line"></span><br><span class="line">Intel spent multiple days and seemingly 100 different sessions talking to the tech press and media about Lunar Lake, and while I plan to dive into it in more depth in a future story, it’s worth spending a bit of time here to talk about the key points that make Lunar Lake different from Meteor Lake, current shipping Core Ultra processors, and why Intel is confident that they can take on both Qualcomm and AMD in the AI PC segment that has garnered so much attention.</span><br><span class="line"></span><br><span class="line">In short, everything changes with Lunar Lake. New core IP, new power delivery, new GPU, new NPU, new memory system; it’s kind of astounding how different this product is from previous ones. The most visible change is the move to an on-package memory system that supports LPDDR5x, four channels, and up to 32GB of total system memory. This on-package design means that Intel can save a tremendous amount of power on the PHY (up to 40% they claim) while also creating a smaller physical footprint.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The processor itself is broken up into two tiles, a compute tile and a platform controller tile. On the compute tile Intel has built a 4+4 design, with four new Lion Cove P-cores and four new Skymont E-cores. The P-cores have a significant number of architectural changes including an 18 execution port design, 8x wider prediction unit, finer clock intervals, and more. Intel claims this results in a 14% improvement in IPC compared to the Redwood Cove core on MTL.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The E-cores got even more attention this time around, with a significant upgrade that includes larger 4MB L2 cache, deeper queuing, all with the goal of providing a broader workload coverage than the previous gen. The result is a 68% improved single-threaded floating point performance vs Crestmont.</span><br><span class="line"></span><br><span class="line">These are impressive results if they hold, and it means that Intel thinks it has a breakthrough in power and computing efficiency for x86. Clearly the company is targeting the perception that only an Arm-based design like the Snapdragon X Elite can bring the battery life and low power capabilities to compete with the likes of the Apple M-series of CPUs. We’ll be looking to see if this holds true for video playback, real-world workloads, and other uses cases.</span><br><span class="line">Another reason that Intel has confidence in its power story is an improved scheduling system and new iteration of Thread Director that does more to put and keep threads on the E-cores, and in particular, FEWER E-cores. There is a point to be made here about the dual nature of the E-core and hybrid design that Intel has built; on one-hand you can use the E-cores for more multi-threaded performance in less die area for high performance parts (think higher TDP platforms or desktop systems) OR for power efficiency characteristics like the implementation we are seeing on Lunar Lake. This combined efficiency, in an example Intel highlighted, showed a Teams conferencing workload using 35% less power than in the previous methodologies.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Moving to the new GPU, this is the first instance of the new Xe2 Battlemage architecture, and Intel claims that we will see as much as 50% more graphics performance versus Meteor Lake. It adds some interesting new features that are especially interesting like XMX units, that accelerate AI functions to a significant degree, offering 67 TOPS of performance. There are new vector units, improved ray tracing units, and overall, the expectation is that the GPU on Lunar Lake will be outstanding. There was no information on the power or efficiency here, so I do believe that’s an area we’ll want to look at, but the emphasis from Intel on the GPU is strong this time around.</span><br><span class="line">Other tidbits that Intel discussed include an improved video engine, of which Intel already had the industry leading integration, support a brand-new video codec called VVC, or H.266, that offers up to a 10% bitrate reduction over AV1 at the same image quality. They also integrated solid connectivity improvements with Bluetooth 5.4, Wi-Fi 7, and TBT4, all to make sure Lunar Lake is a complete platform package.</span><br><span class="line"></span><br><span class="line">The new NPU, now called NPU 4 as it’s the 4th generation of this technology from Intel, scales from 2 neural engines to 6, increases on-chip bandwidth by 2x, and includes 12 of the SHAVE DSPs that accelerate LLM and transformer operations. The net result is a 48 TOPS integration that is obviously intentional to meet the 40 TOPS requirement of the Microsoft Copilot+ PC program launched in May.</span><br><span class="line"></span><br><span class="line">Intel showed the NPU 4 offering up to 2x the performance at ISO power when compared to NPU 3 (back naming the NPU on Meteor Lake) but also up to 4x the peak performance thanks to the increased compute engine, MAC count and also frequency increase and baseline architecture modifications.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">This brings the total platform AI capability of Lunar Lake to 120 TOPS. That’s an impressive number combined with potentially impressive power efficiency, though even Intel itself will tell you that a TOPS number is wildly ineffective at communicating real-world AI performance. Software, drivers, optimization layers and ISV / developer relations will end up making the difference between the haves and the have nots in this AI PC race.</span><br><span class="line"></span><br><span class="line">Intel hasn’t gotten too specific on the timing of system availability, only stating that it would happen in Q3. In my conversations, Intel is adamant that Q3 will see not just some kind of “shipping” announcement or vague availability of a single SKU in China, but that you would be able to get your hands on designs by the end of September, in plenty of time for the holiday shopping season. And with all the interesting debate around what and when platforms other than the Snapdragon X Elite, will have Copilot+ features will be enabled and running, that availability window will be critically important for Intel to stay relevant and ensure there is not a mind share gap to other silicon platforms.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.microsoft.com/en-us/worklab/our-year-with-copilot-what-microsoft-has-learned-about-ai-at-work</span><br><span class="line">Our Year with Copilot: What Microsoft Has Learned About AI at Work</span><br><span class="line">Getting AI right requires intention, experimentation, and some unexpected heroes. Here’s how you can apply insights from our experience to your own organization.</span><br><span class="line"></span><br><span class="line">A little while back, Jared Spataro got an email from someone he couldn’t immediately place. It’s an experience common for executives: someone reaches out, and it’s clear you have an existing relationship, but you just can’t recall how you know them. So Spataro, Microsoft Corporate Vice President of AI at Work, instinctively turned to Copilot, prompting the chat interface to search across all his meetings, chats, documents, and emails to find out. “It was the most beautiful response I’ve ever seen,” says Spataro, one of the early architects of Copilot for Microsoft 365. It told him who the man was and how he knew him, when they first met, and what they had talked about.</span><br><span class="line"></span><br><span class="line">“That was when I realized, Wow, this is going to change business in a really significant way.”</span><br><span class="line"></span><br><span class="line">Spataro has been using Copilot for a year, along with hundreds of thousands of other Microsoft employees and early customers. The company-wide rollout has been marked by creative experimentation, continual learning, and even a little soul searching about the role of AI within an organization. As our own “customer zero,” we had a lot to learn: How quickly would people develop new skills and AI habits? How was it going to change day-to-day work, entire functions, and even entire teams? And how could we quickly scale those lessons across the company?</span><br><span class="line"></span><br><span class="line">“It’s been a year of learning, but we have started to discover what Copilot can unlock for individual employees and companies as a whole,” Spataro says. “Most days it can feel like we’re on a rocket ship. More specifically, like we’re riding on the rocket ship as we’re building it.”</span><br><span class="line"></span><br><span class="line">As with any rocket launch, this one required multiple test flights. We’ve spent the past year experimenting to see what works and what doesn’t, learning from our experiences, and then sharing what we’ve learned across the company and with our customers. Now, as every leader looks to build the AI-powered organization of the future, we want to share what we’ve learned.</span><br><span class="line"></span><br><span class="line"> A colorful illustration of people and elements like tubes and arrows forming a kind of company “machine” that is getting activatedIllustrations by Tomasz Woźniakowski</span><br><span class="line">Share</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">01</span><br><span class="line"></span><br><span class="line">GO FOR THE BIG WINS</span><br><span class="line"></span><br><span class="line">(AND THE EASY ONES TOO)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Who should get AI first? We prioritized functions that would drive ROI fastest.</span><br><span class="line"></span><br><span class="line">How We Did It</span><br><span class="line"></span><br><span class="line">“Every company will have a slightly different approach,” says Nathalie D’Hers, Corporate Vice President of Microsoft Digital, who oversaw the internal rollout to our more than 200,000 employees. “In our case, we zeroed in first on the roles that we knew would gain a lot of benefit.”</span><br><span class="line"></span><br><span class="line">It made sense for sales to get first access: After all, they need to know the product inside and out to communicate its value to customers. But beyond that, we found that salespeople are uniquely positioned to benefit from Copilot, whether it’s cutting down on email triage to prioritize leads or gathering relevant info ahead of a client meeting. In early results, our salespeople saved 90 minutes of time per week; 83 percent of them felt they were more productive; and 67 percent said they were able to parlay the time savings into more time with customers.</span><br><span class="line"></span><br><span class="line">Next came customer service and support. Nine months ago, they rolled out Copilot to all of their support professionals at once, so they could get the entire organization familiar with the technology fast. They had four objectives: reduce time to expertise for agents, streamline access to knowledge, reduce repetitive administrative tasks (to allow people to focus more on customer support, their key priority), and reduce the high volume of inquiries that come in every day.</span><br><span class="line"></span><br><span class="line">It’s been a year of learning, but we have started to discover what Copilot can unlock for individual employees and companies as a whole. Most days, it can feel like we’re on a rocket ship. More specifically, like we’re riding on the rocket ship as we’re building it.</span><br><span class="line">—Jared Spataro, Microsoft Corporate Vice President of AI at Work</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The investment has paid off. According to a study last year from our Chief Economist’s office of nearly 10,000 Microsoft support agents, several teams saw, on average, a 12 percent reduction in case handling time and a 10 percent boost in case resolution.</span><br><span class="line"></span><br><span class="line">And once HR got access, the department retooled an AI-powered employee resource called Ask HR, which expedited the response time for more complex questions about benefits, payroll, and other HR topics. With HR service advisors using Copilot, employees now get faster and more accurate answers to questions that previously might have taken several days to compile and respond to.</span><br><span class="line"></span><br><span class="line">“Our HR service professionals are able to handle employee inquiries more efficiently,” says Kathleen Hogan, Microsoft Executive Vice President and Chief People Officer. “So far we are seeing a 26 percent reduction in initial response time thanks to Copilot.”</span><br><span class="line"></span><br><span class="line">From there, we used what we learned from those early adopters to help guide the rollout to the rest of our company.</span><br><span class="line"></span><br><span class="line">How You Can Do It Too</span><br><span class="line"></span><br><span class="line">Put Copilot where it’s most useful. Whatever department or role you’re targeting, clearly identifying goals before a rollout helps leaders and employees determine from the start what’s working and what’s not. It also helps set appropriate benchmarks for success, whether that’s response times or more effective meetings or other metrics. For guidance, look to our Copilot Scenario Library, which includes suggested use cases and key performance indicators to help orgs determine how Copilot can help.</span><br><span class="line"></span><br><span class="line">Go for easy wins too. As you’re going after function-level transformation, use AI to improve simple tasks as well. Gaining confidence and ability early on (for example, asking Copilot to recap a meeting) helps users maintain a healthy growth mindset when they hit the inevitable road bumps.</span><br><span class="line"></span><br><span class="line">Give it to entire teams. Rolling out Copilot to entire teams at once—even if they’re small ones—is crucial in promoting peer-to-peer learning: It encourages sharing and learning among the group members, multiplying the impact of the technology. It also allows organizations to see patterns to help identify what’s working (or what’s not).</span><br><span class="line"></span><br><span class="line">Make sure to track the impact. To understand how AI is transforming workplace behavior, you’ll need a way to measure its usage. A platform like our Copilot Dashboard can help you plan and measure the impact.</span><br><span class="line"></span><br><span class="line">02</span><br><span class="line"></span><br><span class="line">FIND YOUR INTERNAL</span><br><span class="line"></span><br><span class="line">CHAMPIONS</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Their enthusiasm and knack for sharing their AI skills with others will encourage use across the organization.</span><br><span class="line"></span><br><span class="line">How We Did It</span><br><span class="line"></span><br><span class="line">Many of our employees went through a period of experimentation and playing around with Copilot before they started to drill down on what it could do. That’s where internal champions come in. “They don’t need to be AI experts,” says Callie August, a Copilot champion in the marketing organization. “Just people who are willing to test, learn, and be okay with being wrong.”</span><br><span class="line"></span><br><span class="line">Through managers and rollout leaders, we identified people who were most excited to dive into the technology and share what they learned with their peers. We then empowered them to lead internal trainings and create quick demo videos to share their skills. That grassroots approach allows others to see the potential—and inspires them to explore the technology for themselves.</span><br><span class="line"></span><br><span class="line">New Words for a New Way of Working</span><br><span class="line"></span><br><span class="line">Essential AI terms every leader should know</span><br><span class="line"></span><br><span class="line">AI Aptitude</span><br><span class="line">The ability to work alongside AI naturally, including writing great prompts, evaluating creative work, and checking for bias. Take action: Encourage everyone in your organization to always be asking, “How can AI help me?”</span><br><span class="line"></span><br><span class="line">Context</span><br><span class="line">The Copilot System</span><br><span class="line">Delegate</span><br><span class="line">Digital Artifact</span><br><span class="line">The 11-by-11-Tipping Point</span><br><span class="line">Internal Champion</span><br><span class="line">Islands of Intelligence</span><br><span class="line">Post-processing</span><br><span class="line">How You Can Do It Too</span><br><span class="line"></span><br><span class="line">Employ champions at every level. An early-in-career employee is going to use Copilot in a very different way than someone who’s been managing a team for 20 years. With advocates at all levels of the organization, everyone from individual contributors to the C-suite can see relevant prompts and use cases.</span><br><span class="line"></span><br><span class="line">Find the connectors. While technical expertise is great, it’s not a must. Look for people with a natural aptitude for leadership who can take complex information and distill it down in a relatable way. After all, your internal champions will be spending most of their time teaching and interacting with other people, not programming.</span><br><span class="line"></span><br><span class="line">Make it official. Once you’ve identified your champions, establish an AI council. As we describe in our adoption playbook, the makeup of that group will be unique to what your company needs, but it should include people from your IT enablement team, your change management team, an executive sponsor, and a representative from risk management. And it should meet regularly to ensure that organizational insights are shared effectively.</span><br><span class="line"></span><br><span class="line">Recognize and incentivize. “You have to celebrate people who are adopting AI and showcase their efforts,” says Hossein Nowbar, Chief Legal Officer at Microsoft. “We had early adopters of AI join me onstage during our department-wide summit to talk about how they are leveraging AI and the efficiencies they gained.” This recognition inspires others to join the AI journey.</span><br><span class="line"></span><br><span class="line">03</span><br><span class="line"></span><br><span class="line">DOUBLE DOWN</span><br><span class="line"></span><br><span class="line">ON SKILLING UP</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Make employee training a priority from the start; the training will evolve over time as both trainers and learners become more comfortable with Copilot.</span><br><span class="line"></span><br><span class="line">How We Did It</span><br><span class="line"></span><br><span class="line">We held live one-on-one and group training sessions where people could ask questions and practice prompting in a variety of different work situations. Internal champions created self-guided courses that employees could access on a SharePoint site and answered questions and offered guidance to employees on Viva Engage.</span><br><span class="line"></span><br><span class="line">We also offered employees training that accommodated different work schedules and learning preferences: Some people might not have time to join an in-person session, so they can watch videos or snapshot demos. Others may want to join big interactive sessions where they can ask questions of an expert in a live environment. And we created incentives for taking and passing training courses—like digital badges that declare one a “Copilot Champ.”</span><br><span class="line"></span><br><span class="line">Our trainings evolved as we learned what worked and what didn’t. “In the beginning, I usually did 30-minute sessions where we’d focus on one app at a time,” August says. “Now we’ll do more comprehensive training where we show one piece of every app.” August eventually took her training sessions public, with a series of short videos explaining everything from how to mitigate writer’s block to what to do if you’re late to a meeting. “I thought about pain points. What are the things I hate to do at work, and are there Copilot prompts that can solve those tasks?”</span><br><span class="line"></span><br><span class="line">Like any new routine, building the Copilot habit takes time. Our internal research has found that a time savings of just 11 minutes a day is all it takes for users to see the value from Copilot.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">How You Can Do It Too</span><br><span class="line"></span><br><span class="line">Don’t reinvent the wheel. Because we created a variety of training materials for our own people, organizations looking to roll out Copilot now have resources available. Look to our adoption playbook and guidance for support on both technical readiness and getting your people prepared.</span><br><span class="line"></span><br><span class="line">But also, use what works best for you. Orgs can create interactive libraries of prompts tailored to the work they do, along with recommendations on which app or apps to use, so that everyone can share what works with other teams across the organization.</span><br><span class="line"></span><br><span class="line">Remember your managers. “One of our early learnings was that we need to be sure we are engaging with managers as a direct leader of employees,” says Sandeep Bhanot, Microsoft Corporate Vice President of Engineering &amp; Data, who leads the team that supports our commercial sales organization. “We found that unless managers were fully bought in and saw the value of Copilot, they weren’t able to be champions of Copilot for their teams, which is critical to success. This uncovered the need for manager training, too, getting them engaged, skilled, and bought in to the value of Copilot so they could lead by example.”</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">04</span><br><span class="line"></span><br><span class="line">BUILD THE</span><br><span class="line"></span><br><span class="line">AI HABIT</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In any AI rollout, some people will be eager to adopt the new technology, and others less so. Embrace a growth mindset when it comes to experimenting with AI and then using it regularly.</span><br><span class="line"></span><br><span class="line">How We Did It</span><br><span class="line"></span><br><span class="line">Throughout our rollout, leaders asked their teams to consider how AI could help them do whatever task they were setting out to do, big or small, before they set out to do it. “When it came to Copilot, we asked ourselves two questions,” D’Hers says. “Number one, how can an AI tool help us be more efficient in this task? And number two, is this something that artificial intelligence can just help us do better?”</span><br><span class="line"></span><br><span class="line">Soon enough, users across the organization were developing their own new work habits, based upon early victories and time-saving hacks. After every meeting, they might ask Copilot what their action items are. Or they’ll use Copilot to find material that might live in an email, a chat, or a PowerPoint deck.</span><br><span class="line"></span><br><span class="line">Then it clicks: “When people see that this is a way to enhance their work, not a usurping of their work, there’s this spark of realization,” says Chris Fernandez, Microsoft Corporate Vice President of HR Services and Digital Employee Experiences.</span><br><span class="line"></span><br><span class="line">Like any new routine, building the Copilot habit takes time. Our internal research has found that a time savings of just 11 minutes a day is all it takes for users to see the value from Copilot. And it takes about a business quarter, or 11 weeks, for most people using Copilot to see improvement in four key areas: productivity, work enjoyment, work-life balance, and the ability to attend fewer meetings.</span><br><span class="line"></span><br><span class="line">How You Can Do It Too</span><br><span class="line"></span><br><span class="line">Remember that it’s an organizational challenge, not only an IT challenge. “When I talk to customers,” says Colette Stallbaumer, General Manager of Copilot, “one predictor of success is if they have involvement at every level of the organization—from senior leadership to functional leaders to grassroots employee activation.” This approach signifies that a company is thinking of it as a new way of working, and not just a new technology.</span><br><span class="line"></span><br><span class="line">Start small. To start building the habit, encourage your teams to find the immediate wins in their workday that deliver from the start. Instead of searching through folders for a deck, for example, encourage your people to use Copilot to locate the file. Executives, meanwhile, can use it to summarize long documents or drawn-out email chains.</span><br><span class="line"></span><br><span class="line">Understand that this is new—really new. Unlike other new technology, there’s an emotional component to adopting AI. The shift can be unsettling, so it’s important to help people understand how AI can be valuable—to their time, for instance, or the quality and purpose of their work. Consider the note-taking ability in Microsoft Teams. “Someone might say, ‘But I usually take the notes in meetings!’” says Claire Sisson, Principal Group Product Manager, Microsoft Digital, who helped lead the company-wide rollout. “So we tell them, ‘Instead of taking notes, you can be a full participant in the meeting. Now you can focus your attention on the critical thinking you can bring.’”</span><br><span class="line"></span><br><span class="line">Our biggest lesson over the past year? We all have to be thoughtful, iterative, and willing to evolve. And while a project this intricate might seem daunting, it’s so valuable that you can’t afford to put it off. “Leaders who see the opportunity,” Spataro says, “who are able to think creatively about what AI can do to rewire every aspect of the organization, are going to be the ones who gain a competitive edge—and that will set them apart in this next era of work.”</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://applied-llms.org/</span><br><span class="line">What We’ve Learned From A Year of Building with LLMs</span><br><span class="line">A practical guide to building successful LLM products, covering the tactical, operational, and strategic.</span><br><span class="line">AUTHORS</span><br><span class="line">Eugene Yan</span><br><span class="line"></span><br><span class="line">Bryan Bischof</span><br><span class="line"></span><br><span class="line">Charles Frye</span><br><span class="line"></span><br><span class="line">Hamel Husain</span><br><span class="line"></span><br><span class="line">Jason Liu</span><br><span class="line"></span><br><span class="line">Shreya Shankar</span><br><span class="line"></span><br><span class="line">PUBLISHED</span><br><span class="line">June 8, 2024</span><br><span class="line"></span><br><span class="line">Also published on O’Reilly Media in three parts: Tactical, Operational, Strategic. Also see podcast.</span><br><span class="line"></span><br><span class="line">It’s an exciting time to build with large language models (LLMs). Over the past year, LLMs have become “good enough” for real-world applications. And they’re getting better and cheaper every year. Coupled with a parade of demos on social media, there will be an estimated $200B investment in AI by 2025. Furthermore, provider APIs have made LLMs more accessible, allowing everyone, not just ML engineers and scientists, to build intelligence into their products. Nonetheless, while the barrier to entry for building with AI has been lowered, creating products and systems that are effective—beyond a demo—remains deceptively difficult.</span><br><span class="line"></span><br><span class="line">We’ve spent the past year building, and have discovered many sharp edges along the way. While we don’t claim to speak for the entire industry, we’d like to share what we’ve learned to help you avoid our mistakes and iterate faster. These are organized into three sections:</span><br><span class="line"></span><br><span class="line">Tactical: Some practices for prompting, RAG, flow engineering, evals, and monitoring. Whether you’re a practitioner building with LLMs, or hacking on weekend projects, this section was written for you.</span><br><span class="line">Operational: The organizational, day-to-day concerns of shipping products, and how to build an effective team. For product/technical leaders looking to deploy sustainably and reliably.</span><br><span class="line">Strategic: The long-term, big-picture view, with opinionated takes such as “no GPU before PMF” and “focus on the system not the model”, and how to iterate. Written with founders and executives in mind.</span><br><span class="line">We intend to make this a practical guide to building successful products with LLMs, drawing from our own experiences and pointing to examples from around the industry.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-06-10</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/06/10/2024-6-10-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 6월 10일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/06/11/WWDC-2024-Apple-Intelligence/" title="WWDC 2024 Apple Intelligence with ChatGPT">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/06/07/2024-6-7-AI-NEWS/" title="2024년 6월 7일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>