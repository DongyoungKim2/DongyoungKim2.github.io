<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 6월 17일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="Summary엔비디아에서는 Nemotron-4 340B 모델군을 발표했습니다. 이 모델은 Nemotron-4-340B-Base, Nemotron-4-340B-Instruct, Nemotron-4-340B-Reward로 구성되어 있으며, 연구 및 상업적 응용을 위해 오픈"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 6월 17일 AI 소식</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>엔비디아에서는 Nemotron-4 340B 모델군을 발표했습니다. 이 모델은 Nemotron-4-340B-Base, Nemotron-4-340B-Instruct, Nemotron-4-340B-Reward로 구성되어 있으며, 연구 및 상업적 응용을 위해 오픈 액세스 모델로 제공됩니다. Google Research에서는 Human I&#x2F;O를 소개했으며, 이는 대규모 언어 모델을 활용하여 상황적 장애를 감지하는 시스템입니다. Lamini는 새로운 메모리 튜닝 방법을 발표하여 사실 정확도를 95%까지 향상시켰습니다.</p>
<h2 id="Nemotron-4-340B-발표"><a href="#Nemotron-4-340B-발표" class="headerlink" title="Nemotron-4 340B 발표,"></a>Nemotron-4 340B 발표,</h2><h3 id="Nemotron-4-340B"><a href="#Nemotron-4-340B" class="headerlink" title="Nemotron-4 340B"></a>Nemotron-4 340B</h3><p><a target="_blank" rel="noopener" href="https://research.nvidia.com/publication/2024-06_nemotron-4-340b">링크</a>, 2024년 6월 14일,<br>엔비디아</p>
<ul>
<li>Nemotron-4 340B 모델군 발표 (Nemotron-4-340B-Base, Nemotron-4-340B-Instruct, Nemotron-4-340B-Reward)</li>
<li>NVIDIA Open Model License Agreement 하에 오픈 액세스 제공</li>
<li>8개의 GPU를 사용하는 DGX H100에서 FP8 정밀도로 배포 가능</li>
<li>모델 정렬 과정에서 사용된 데이터의 98% 이상이 합성 데이터</li>
<li>모델 개발을 지원하기 위해 합성 데이터 생성 파이프라인도 오픈 소스 제공</li>
</ul>
<h2 id="Human-I-O-Detecting-situational-impairments-with-large-language-models"><a href="#Human-I-O-Detecting-situational-impairments-with-large-language-models" class="headerlink" title="Human I&#x2F;O: Detecting situational impairments with large language models,"></a>Human I&#x2F;O: Detecting situational impairments with large language models,</h2><h3 id="Human-I-O-대규모-언어-모델을-활용한-상황적-장애-감지"><a href="#Human-I-O-대규모-언어-모델을-활용한-상황적-장애-감지" class="headerlink" title="Human I&#x2F;O: 대규모 언어 모델을 활용한 상황적 장애 감지"></a>Human I&#x2F;O: 대규모 언어 모델을 활용한 상황적 장애 감지</h3><p><a target="_blank" rel="noopener" href="https://research.google/blog/human-io-detecting-situational-impairments-with-large-language-models/">링크</a>, 2024년 6월 14일,<br>Google Research</p>
<ul>
<li>SIIDs(상황적 장애) 감지를 위한 Human I&#x2F;O 시스템 소개</li>
<li>egocentric vision, 멀티모달 센싱, LLM reasoning 활용</li>
<li>60개의 실제 시나리오에서 82%의 정확도로 사용자의 상호작용 능력 평가</li>
<li>사용자의 시각, 청각, 발성, 손 사용 가능성을 예측하여 디바이스 적응</li>
</ul>
<h2 id="Lamini-Memory-Tuning-95-LLM-Accuracy-10x-Fewer-Hallucinations"><a href="#Lamini-Memory-Tuning-95-LLM-Accuracy-10x-Fewer-Hallucinations" class="headerlink" title="Lamini Memory Tuning: 95% LLM Accuracy, 10x Fewer Hallucinations,"></a>Lamini Memory Tuning: 95% LLM Accuracy, 10x Fewer Hallucinations,</h2><h3 id="Lamini-메모리-튜닝-95-LLM-정확도-10배-감소된-환각"><a href="#Lamini-메모리-튜닝-95-LLM-정확도-10배-감소된-환각" class="headerlink" title="Lamini 메모리 튜닝: 95% LLM 정확도, 10배 감소된 환각"></a>Lamini 메모리 튜닝: 95% LLM 정확도, 10배 감소된 환각</h3><p><a target="_blank" rel="noopener" href="https://www.lamini.ai/blog/lamini-memory-tuning">링크</a>, 2024년 6월 14일,<br>Lamini</p>
<ul>
<li>새로운 메모리 튜닝 방법 발표</li>
<li>사실 정확도를 95%까지 향상시키고 환각을 10% 이하로 감소</li>
<li>정보 검색에서 영감을 받아 인덱스에서 관련 전문가만 선택하여 사용</li>
<li>Fortune 500 고객 사례에서 텍스트-SQL 에이전트의 정확도가 50%에서 95%로 향상</li>
</ul>
<h2 id="HelpSteer2-Open-source-dataset-for-training-top-performing-reward-models"><a href="#HelpSteer2-Open-source-dataset-for-training-top-performing-reward-models" class="headerlink" title="HelpSteer2: Open-source dataset for training top-performing reward models,"></a>HelpSteer2: Open-source dataset for training top-performing reward models,</h2><h3 id="HelpSteer2-최상위-성능의-보상-모델-훈련을-위한-오픈-소스-데이터셋"><a href="#HelpSteer2-최상위-성능의-보상-모델-훈련을-위한-오픈-소스-데이터셋" class="headerlink" title="HelpSteer2: 최상위 성능의 보상 모델 훈련을 위한 오픈 소스 데이터셋"></a>HelpSteer2: 최상위 성능의 보상 모델 훈련을 위한 오픈 소스 데이터셋</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.08673">링크</a>, 2024년 6월 12일,<br>Zhilin Wang 외</p>
<ul>
<li>HelpSteer2 데이터셋 발표, CC-BY-4.0 라이선스로 제공</li>
<li>기존 데이터셋 대비 10배 적은 10,000개의 응답 쌍으로 구성</li>
<li>Reward-Bench의 주요 데이터셋에서 SOTA 점수(92.0%) 달성</li>
<li>SteerLM 2.0 모델 정렬 방법 제안, 보상 모델의 다중 속성 점수 활용</li>
</ul>
<h2 id="ChatGPT-is-bullshit"><a href="#ChatGPT-is-bullshit" class="headerlink" title="ChatGPT is bullshit,"></a>ChatGPT is bullshit,</h2><h3 id="ChatGPT는-허튼소리다"><a href="#ChatGPT는-허튼소리다" class="headerlink" title="ChatGPT는 허튼소리다"></a>ChatGPT는 허튼소리다</h3><p><a target="_blank" rel="noopener" href="https://link.springer.com/content/pdf/10.1007/s10676-024-09775-5.pdf">링크</a>, 2024년 6월 14일,<br>Michael Townsen Hicks 외</p>
<ul>
<li>대규모 언어 모델이 진실에 무관심하다는 관점에서 “bullshit”이라고 주장</li>
<li>AI 허위 정보의 특성을 설명하고, 이를 예측하고 논의하는 더 유용한 방법 제시</li>
</ul>
<h2 id="Multi-AI-Agent-Systems-101"><a href="#Multi-AI-Agent-Systems-101" class="headerlink" title="Multi AI Agent Systems 101,"></a>Multi AI Agent Systems 101,</h2><h3 id="다중-AI-에이전트-시스템-101d"><a href="#다중-AI-에이전트-시스템-101d" class="headerlink" title="다중 AI 에이전트 시스템 101d"></a>다중 AI 에이전트 시스템 101d</h3><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/multi-ai-agent-systems-101-bac58e3bcc47">링크</a>, 2024년 6월 17일,<br>Mariya Mansurova</p>
<ul>
<li>다중 AI 에이전트 시스템의 개요 설명</li>
<li>AutoGen, LangGraph, CrewAI 등 다양한 프레임워크 소개</li>
<li>CrewAI를 사용하여 데이터 소스 관리 자동화 예제 제시</li>
</ul>
<details>
  <summary>Sources</summary>
This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:

<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h2 id="Title"><a href="#Title" class="headerlink" title="Title,"></a>Title,</h2><h3 id="한글제목"><a href="#한글제목" class="headerlink" title="한글제목"></a>한글제목</h3><p><a href="link">링크</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2,  (개조식 문체 사용)<br>…</li>
<li>detailed summary N,  (개조식 문체 사용)</li>
</ul>
<h2 id="Title-1"><a href="#Title-1" class="headerlink" title="Title,"></a>Title,</h2><h3 id="한글제목-1"><a href="#한글제목-1" class="headerlink" title="한글제목"></a>한글제목</h3><p><a href="link">링크</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2,  (개조식 문체 사용)<br>…</li>
<li>detailed summary N,  (개조식 문체 사용)<br>…<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br><span class="line">936</span><br><span class="line">937</span><br><span class="line">938</span><br><span class="line">939</span><br><span class="line">940</span><br><span class="line">941</span><br><span class="line">942</span><br><span class="line">943</span><br><span class="line">944</span><br><span class="line">945</span><br><span class="line">946</span><br><span class="line">947</span><br><span class="line">948</span><br><span class="line">949</span><br><span class="line">950</span><br><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br><span class="line">954</span><br><span class="line">955</span><br><span class="line">956</span><br><span class="line">957</span><br><span class="line">958</span><br><span class="line">959</span><br><span class="line">960</span><br><span class="line">961</span><br><span class="line">962</span><br><span class="line">963</span><br><span class="line">964</span><br><span class="line">965</span><br><span class="line">966</span><br><span class="line">967</span><br><span class="line">968</span><br><span class="line">969</span><br><span class="line">970</span><br><span class="line">971</span><br><span class="line">972</span><br><span class="line">973</span><br><span class="line">974</span><br><span class="line">975</span><br><span class="line">976</span><br><span class="line">977</span><br><span class="line">978</span><br><span class="line">979</span><br><span class="line">980</span><br><span class="line">981</span><br><span class="line">982</span><br><span class="line">983</span><br><span class="line">984</span><br><span class="line">985</span><br><span class="line">986</span><br><span class="line">987</span><br><span class="line">988</span><br><span class="line">989</span><br><span class="line">990</span><br><span class="line">991</span><br><span class="line">992</span><br><span class="line">993</span><br><span class="line">994</span><br><span class="line">995</span><br><span class="line">996</span><br><span class="line">997</span><br><span class="line">998</span><br><span class="line">999</span><br><span class="line">1000</span><br><span class="line">1001</span><br><span class="line">1002</span><br><span class="line">1003</span><br><span class="line">1004</span><br><span class="line">1005</span><br><span class="line">1006</span><br><span class="line">1007</span><br><span class="line">1008</span><br><span class="line">1009</span><br><span class="line">1010</span><br><span class="line">1011</span><br><span class="line">1012</span><br><span class="line">1013</span><br><span class="line">1014</span><br><span class="line">1015</span><br><span class="line">1016</span><br><span class="line">1017</span><br><span class="line">1018</span><br><span class="line">1019</span><br><span class="line">1020</span><br><span class="line">1021</span><br><span class="line">1022</span><br><span class="line">1023</span><br><span class="line">1024</span><br><span class="line">1025</span><br><span class="line">1026</span><br><span class="line">1027</span><br><span class="line">1028</span><br><span class="line">1029</span><br><span class="line">1030</span><br><span class="line">1031</span><br><span class="line">1032</span><br><span class="line">1033</span><br><span class="line">1034</span><br><span class="line">1035</span><br><span class="line">1036</span><br><span class="line">1037</span><br><span class="line">1038</span><br><span class="line">1039</span><br><span class="line">1040</span><br><span class="line">1041</span><br><span class="line">1042</span><br><span class="line">1043</span><br><span class="line">1044</span><br><span class="line">1045</span><br><span class="line">1046</span><br><span class="line">1047</span><br><span class="line">1048</span><br><span class="line">1049</span><br><span class="line">1050</span><br><span class="line">1051</span><br><span class="line">1052</span><br><span class="line">1053</span><br><span class="line">1054</span><br><span class="line">1055</span><br><span class="line">1056</span><br><span class="line">1057</span><br><span class="line">1058</span><br><span class="line">1059</span><br><span class="line">1060</span><br><span class="line">1061</span><br><span class="line">1062</span><br><span class="line">1063</span><br><span class="line">1064</span><br><span class="line">1065</span><br><span class="line">1066</span><br><span class="line">1067</span><br><span class="line">1068</span><br><span class="line">1069</span><br><span class="line">1070</span><br><span class="line">1071</span><br><span class="line">1072</span><br><span class="line">1073</span><br><span class="line">1074</span><br><span class="line">1075</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://research.nvidia.com/publication/2024-06_nemotron-4-340b</span><br><span class="line"></span><br><span class="line">Publications</span><br><span class="line">Nemotron-4 340B</span><br><span class="line">Nemotron-4 340B</span><br><span class="line"></span><br><span class="line">We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base, Nemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. Our models are open access under the NVIDIA Open Model License Agreement, a permissive model license that allows the distribution, modification, and use of the models and their outputs. These models perform competitively to open access models on a wide range of evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in FP8 precision. We believe that the community can benefit from these models in various research studies and commercial applications, especially for generating synthetic data to train smaller language models. Notably, over 98% of data used in our model alignment process is synthetically generated, showcasing the effectiveness of these models in generating synthetic data. To further support open research and facilitate model development, we are also open-sourcing the synthetic data generation pipeline used in our model alignment process.</span><br><span class="line"></span><br><span class="line">Publication Date</span><br><span class="line">Friday, June 14, 2024</span><br><span class="line">Research Area</span><br><span class="line">Artificial Intelligence and Machine Learning</span><br><span class="line">Generative AI</span><br><span class="line"></span><br><span class="line">Model Overview</span><br><span class="line">Nemotron-4-340B-Instruct is a large language model (LLM) that can be used as part of a synthetic data generation pipeline to create training data that helps researchers and developers build their own LLMs. It is a fine-tuned version of the Nemotron-4-340B-Base model, optimized for English-based single and multi-turn chat use-cases. It supports a context length of 4,096 tokens.</span><br><span class="line"></span><br><span class="line">The base model was pre-trained on a corpus of 9 trillion tokens consisting of a diverse assortment of English based texts, 50+ natural languages, and 40+ coding languages. Subsequently the Nemotron-4-340B-Instruct model went through additional alignment steps including:</span><br><span class="line"></span><br><span class="line">Supervised Fine-tuning (SFT)</span><br><span class="line">Direct Preference Optimization (DPO)</span><br><span class="line">Reward-aware Preference Optimization (RPO) (Additional in-house alignment technique)</span><br><span class="line">Throughout the alignment process, we relied on only approximately 20K human-annotated data while our data generation pipeline synthesized over 98% of the data used for supervised fine-tuning and preference fine-tuning (DPO &amp; RPO). We provide comprehensive details about our synthetic data generation pipeline in the technical report.</span><br><span class="line"></span><br><span class="line">This results in a model that is aligned for human chat preferences, improvements in mathematical reasoning, coding and instruction-following, and is capable of generating high quality synthetic data for a variety of use cases.</span><br><span class="line"></span><br><span class="line">Under the NVIDIA Open Model License, NVIDIA confirms:</span><br><span class="line"></span><br><span class="line">Models are commercially usable.</span><br><span class="line">You are free to create and distribute Derivative Models.</span><br><span class="line">NVIDIA does not claim ownership to any outputs generated using the Models or Derivative Models.</span><br><span class="line">License:</span><br><span class="line">NVIDIA Open Model License</span><br><span class="line"></span><br><span class="line">Intended use</span><br><span class="line">Nemotron-4-340B-Instruct is a chat model intended for use for the English language.</span><br><span class="line"></span><br><span class="line">Nemotron-4-340B-Instruct is designed for Synthetic Data Generation to enable developers and enterprises for building and customizing their own large language models and LLM applications.</span><br><span class="line"></span><br><span class="line">The instruct model itself can be further customized using the NeMo Framework suite of customization tools including Parameter-Efficient Fine-Tuning (P-tuning, Adapters, LoRA, and more), and Model Alignment (SFT, SteerLM, RLHF, and more) using NeMo-Aligner.</span><br><span class="line"></span><br><span class="line">Model Developer: NVIDIA</span><br><span class="line"></span><br><span class="line">Model Dates: Nemotron-4-340B-Instruct was trained between December 2023 and May 2024.</span><br><span class="line"></span><br><span class="line">Data Freshness: The pretraining data has a cutoff of June 2023.</span><br><span class="line"></span><br><span class="line">Required Hardware</span><br><span class="line">BF16 Inference:</span><br><span class="line"></span><br><span class="line">8x H200 (1x H200 node)</span><br><span class="line">16x H100 (2x H100 nodes)</span><br><span class="line">16x A100 80GB (2x A100 80GB nodes)</span><br><span class="line">Model Architecture:</span><br><span class="line">Nemotron-4-340B-Instruct is standard decoder-only Transformer, trained with a sequence length of 4096 tokens, uses Grouped-Query Attention (GQA), and Rotary Position Embeddings (RoPE).</span><br><span class="line"></span><br><span class="line">Architecture Type: Transformer Decoder (auto-regressive language model)</span><br><span class="line"></span><br><span class="line">Network Architecture: Nemotron-4</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/</span><br><span class="line">NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models</span><br><span class="line">Nemotron-4 340B, a family of models optimized for NVIDIA NeMo and NVIDIA TensorRT-LLM, includes cutting-edge instruct and reward models, and a dataset for generative AI training.</span><br><span class="line">June 14, 2024 by Ankit Patel</span><br><span class="line"> Share</span><br><span class="line">  </span><br><span class="line">NVIDIA today announced Nemotron-4 340B, a family of open models that developers can use to generate synthetic data for training large language models (LLMs) for commercial applications across healthcare, finance, manufacturing, retail and every other industry.</span><br><span class="line"></span><br><span class="line">High-quality training data plays a critical role in the performance, accuracy and quality of responses from a custom LLM — but robust datasets can be prohibitively expensive and difficult to access.</span><br><span class="line"></span><br><span class="line">Through a uniquely permissive open model license, Nemotron-4 340B gives developers a free, scalable way to generate synthetic data that can help build powerful LLMs.</span><br><span class="line"></span><br><span class="line">The Nemotron-4 340B family includes base, instruct and reward models that form a pipeline to generate synthetic data used for training and refining LLMs. The models are optimized to work with NVIDIA NeMo, an open-source framework for end-to-end model training, including data curation, customization and evaluation. They’re also optimized for inference with the open-source NVIDIA TensorRT-LLM library.</span><br><span class="line"></span><br><span class="line">Nemotron-4 340B can be downloaded now from Hugging Face. Developers will soon be able to access the models at ai.nvidia.com, where they’ll be packaged as an NVIDIA NIM microservice with a standard application programming interface that can be deployed anywhere.</span><br><span class="line"></span><br><span class="line">Navigating Nemotron to Generate Synthetic Data</span><br><span class="line">LLMs can help developers generate synthetic training data in scenarios where access to large, diverse labeled datasets is limited.</span><br><span class="line"></span><br><span class="line">The Nemotron-4 340B Instruct model creates diverse synthetic data that mimics the characteristics of real-world data, helping improve data quality to increase the performance and robustness of custom LLMs across various domains.</span><br><span class="line"></span><br><span class="line">Then, to boost the quality of the AI-generated data, developers can use the Nemotron-4 340B Reward model to filter for high-quality responses. Nemotron-4 340B Reward grades responses on five attributes: helpfulness, correctness, coherence, complexity and verbosity. It’s currently first place on the Hugging Face RewardBench leaderboard, created by AI2, for evaluating the capabilities, safety and pitfalls of reward models.</span><br><span class="line"></span><br><span class="line">nemotron synthetic data generation pipeline diagram</span><br><span class="line">In this synthetic data generation pipeline, (1) the Nemotron-4 340B Instruct model is first used to produce synthetic text-based output. An evaluator model, (2) Nemotron-4 340B Reward, then assesses this generated text — providing feedback that guides iterative improvements and ensures the synthetic data is accurate, relevant and aligned with specific requirements.</span><br><span class="line">Researchers can also create their own instruct or reward models by customizing the Nemotron-4 340B Base model using their proprietary data, combined with the included HelpSteer2 dataset.</span><br><span class="line"></span><br><span class="line">Fine-Tuning With NeMo, Optimizing for Inference With TensorRT-LLM</span><br><span class="line">Using open-source NVIDIA NeMo and NVIDIA TensorRT-LLM, developers can optimize the efficiency of their instruct and reward models to generate synthetic data and to score responses.</span><br><span class="line"></span><br><span class="line">All Nemotron-4 340B models are optimized with TensorRT-LLM to take advantage of tensor parallelism, a type of model parallelism in which individual weight matrices are split across multiple GPUs and servers, enabling efficient inference at scale.</span><br><span class="line"></span><br><span class="line">Nemotron-4 340B Base, trained on 9 trillion tokens, can be customized using the NeMo framework to adapt to specific use cases or domains. This fine-tuning process benefits from extensive pretraining data and yields more accurate outputs for specific downstream tasks.</span><br><span class="line"></span><br><span class="line">A variety of customization methods are available through the NeMo framework, including supervised fine-tuning and parameter-efficient fine-tuning methods such as low-rank adaptation, or LoRA.</span><br><span class="line"></span><br><span class="line">To boost model quality, developers can align their models with NeMo Aligner and datasets annotated by Nemotron-4 340B Reward. Alignment is a key step in training LLMs, where a model’s behavior is fine-tuned using algorithms like reinforcement learning from human feedback (RLHF) to ensure its outputs are safe, accurate, contextually appropriate and consistent with its intended goals.</span><br><span class="line"></span><br><span class="line">Businesses seeking enterprise-grade support and security for production environments can also access NeMo and TensorRT-LLM through the cloud-native NVIDIA AI Enterprise software platform, which provides accelerated and efficient runtimes for generative AI foundation models.</span><br><span class="line"></span><br><span class="line">Evaluating Model Security and Getting Started</span><br><span class="line">The Nemotron-4 340B Instruct model underwent extensive safety evaluation, including adversarial tests, and performed well across a wide range of risk indicators. Users should still perform careful evaluation of the model’s outputs to ensure the synthetically generated data is suitable, safe and accurate for their use case.</span><br><span class="line"></span><br><span class="line">For more information on model security and safety evaluation, read the model card.</span><br><span class="line"></span><br><span class="line">Download Nemotron-4 340B models via Hugging Face. For more details, read the research papers on the model and dataset.</span><br><span class="line"></span><br><span class="line">See notice regarding software product information.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2406.08673</span><br><span class="line">HelpSteer2: Open-source dataset for training top-performing reward models</span><br><span class="line">Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang, Makesh Narsimhan Sreedhar, Oleksii Kuchaiev</span><br><span class="line">High-quality preference datasets are essential for training reward models that can effectively guide large language models (LLMs) in generating high-quality responses aligned with human preferences. As LLMs become stronger and better aligned, permissively licensed preference datasets, such as Open Assistant, HH-RLHF, and HelpSteer need to be updated to remain effective for reward modeling. Methods that distil preference data from proprietary LLMs such as GPT-4 have restrictions on commercial usage imposed by model providers. To improve upon both generated responses and attribute labeling quality, we release HelpSteer2, a permissively licensed preference dataset (CC-BY-4.0). Using a powerful internal base model trained on HelpSteer2, we are able to achieve the SOTA score (92.0%) on Reward-Bench&#x27;s primary dataset, outperforming currently listed open and proprietary models, as of June 12th, 2024. Notably, HelpSteer2 consists of only ten thousand response pairs, an order of magnitude fewer than existing preference datasets (e.g., HH-RLHF), which makes it highly efficient for training reward models. Our extensive experiments demonstrate that reward models trained with HelpSteer2 are effective in aligning LLMs. In particular, we propose SteerLM 2.0, a model alignment approach that can effectively make use of the rich multi-attribute score predicted by our reward models. HelpSteer2 is available at this https URL and code is available at this https URL</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://research.google/blog/human-io-detecting-situational-impairments-with-large-language-models/</span><br><span class="line">Home  Blog </span><br><span class="line">Human I/O: Detecting situational impairments with large language models</span><br><span class="line">June 14, 2024</span><br><span class="line">Google Research</span><br><span class="line"></span><br><span class="line">Xingyu Bruce Liu, Student Researcher, and Ruofei Du, Interactive Perception &amp; Graphics Lead, Google Augmented Reality</span><br><span class="line"></span><br><span class="line">Human I/O is a unified approach that uses egocentric vision, multimodal sensing, and LLM reasoning to detect situational impairments and assess a user’s ability to interact with their hands, vision, hearing or speech in a given situation.</span><br><span class="line"></span><br><span class="line">Every day, we encounter temporary challenges that can affect our abilities to respond to different situations. These challenges, known as situationally induced impairments and disabilities (SIIDs), can be caused by various environmental factors like noise, lighting, temperature, stress, and even social norms. For example, imagine you&#x27;re in a loud restaurant and you miss an important phone call because you simply could not hear your phone ring. Or picture yourself trying to respond to a text message while washing dishes; your wet hands and the task at hand make it hard to type a reply. These everyday scenarios show how our surroundings can momentarily reduce our physical, cognitive, or emotional abilities, leading to frustrating experiences.</span><br><span class="line"></span><br><span class="line">In addition, situational impairments can vary greatly and change frequently, which makes it difficult to apply one-size-fits-all solutions that help users with their needs in real-time. For example, think about a typical morning routine: while brushing their teeth, someone might not be able to use voice commands with their smart devices. When washing their face, it could be hard to see and respond to important text messages. And while using a hairdryer, it might be difficult to hear any phone notifications. Even though various efforts have created solutions tailored for specific situations like these, creating manual solutions for every possible situation and combination of challenges isn&#x27;t really feasible and doesn&#x27;t work well on a large scale.</span><br><span class="line"></span><br><span class="line">In “Human I/O: Towards a Unified Approach to Detecting Situational Impairments”, which received a Best Paper Honorable Mention Award at CHI 2024, we introduce a generalizable and extensible framework for detecting SIIDs. Rather than devising individual models for activities like face-washing, tooth-brushing, or hair-drying, Human Input/Output (Human I/O) universally assesses the availability of a user’s vision (e.g., to read text messages, watch videos), hearing (e.g., to hear notifications, phone calls), vocal (e.g., to have a conversation, use Google Assistant), and hand (e.g., to use touch screen, gesture control) input/output interaction channels. We describe how Human I/O leverages egocentric vision, multimodal sensing, and reasoning with large language models (LLMs) to achieve an 82% accuracy in availability prediction across 60 in-the-wild egocentric video recordings in 32 different scenarios, and validate it as an interactive system in a lab study with ten participants. We also open-sourced the code.</span><br><span class="line"></span><br><span class="line">Human-I:O-Figure1</span><br><span class="line">Leveraging multi-modal AI and large language models, we built a pipeline to computationally model these human input/output channels and achieved good real-time performance.</span><br><span class="line"></span><br><span class="line">Formative study</span><br><span class="line">Prior to developing Human I/O, we conducted a formative study involving ten participants to better understand how different impairments affected their interaction with technology. Our findings highlighted the need for a system that could dynamically adapt to the varying levels of channel availability rather than treating availability as a binary state. Specifically, Human I/O distinguishes between four levels of channel availability, which are crucial for understanding the degree to which a user can engage with their device. These levels include:</span><br><span class="line"></span><br><span class="line">Available</span><br><span class="line">The channel is currently not involved in any activity, or constrained by any environmental factors. It takes low to zero effort to use the channel to complete a new task.</span><br><span class="line">Example: A user is sitting at their desk with their hands free, eyes not engaged in a task, and no background noise interfering with their hearing or speech.</span><br><span class="line">Slightly affected</span><br><span class="line">The channel is engaged in an activity or constrained by an environmental factor. Given a new task that requires the channel, users can multitask, easily pause and resume the current activity, or easily overcome the situation.</span><br><span class="line">Example: A user is holding a remote control, which can be set aside to free up their hand for another task.</span><br><span class="line">Affected</span><br><span class="line">The channel is involved in an activity or constrained by an environmental factor. Given a new task, the user may experience inconvenience or require some effort to use the channel.</span><br><span class="line">Example: A user is using both hands to carry groceries, making it challenging to use their hands for other tasks without putting the bags down first.</span><br><span class="line">Unavailable</span><br><span class="line">The channel is completely unavailable due to an activity or environmental factor, and the user cannot use it for a new task without substantial changes, significant adaptation or changing the environment.</span><br><span class="line">Example: A user is attending a loud concert, making it impossible for them to hear incoming notifications or have a conversation.</span><br><span class="line">Human I/O system pipeline</span><br><span class="line">The Human I/O system incorporates a pipeline that streams real-time data, processes it to understand the context, and applies reasoning with LLMs to predict channel availability. Here’s a deeper look into the three core components of the system: data streaming, processing module, and reasoning module.</span><br><span class="line"></span><br><span class="line">Data streaming</span><br><span class="line">The system begins by streaming real-time video and audio data through an egocentric device that has a camera and microphone. This setup provides a first-person view of the user&#x27;s environment, capturing both visual and auditory details necessary for assessing the context.</span><br><span class="line"></span><br><span class="line">Processing module</span><br><span class="line">This module processes the raw data to extract the following information:</span><br><span class="line"></span><br><span class="line">Activity recognition: Utilizes computer vision to identify the user&#x27;s current activities, such as cooking or washing dishes.</span><br><span class="line">Environment assessment: Determines the setting, such as noisy or quiet environments, using both audio and visual data.</span><br><span class="line">Direct sensing: Detects finer details, such as whether the user’s hands are occupied, ambient noise levels, and lighting conditions.</span><br><span class="line">Reasoning module</span><br><span class="line">The final stage involves analyzing the structured data from the processing module to predict the availability of input/output channels using an LLM with chain-of-thought reasoning. This module processes contextual information and determines how impaired each channel is, guiding how the device should adapt its interactions accordingly. By integrating data streaming, processing, and reasoning, Human I/O dynamically predicts the availability of the user’s input and output channels. We further incorporate a smoothing algorithm for enhanced system stability.</span><br><span class="line"></span><br><span class="line">Human-I:O-Figure2</span><br><span class="line">The Human I/O pipeline comprises an egocentric device, processing modules that obtains activity, environment, and sensor data, and reasoning module that predicts human channel availability.</span><br><span class="line"></span><br><span class="line">Evaluation</span><br><span class="line">To validate Human I/O, we conducted an evaluation with 300 clips selected from 60 in-the-wild egocentric video recordings. The system archives a 0.22 mean absolute error (MAE) and an 82% accuracy in predicting channel availability, with 96% of predictions within one step of the actual availability level. These low MAE values indicate that our system’s predictions closely align with the actual availability, with deviations being less than a third of the actual level on average.</span><br><span class="line"></span><br><span class="line">In addition, we conducted an ablation study by introducing Human I/O Lite, which replaces the chain-of-thought reasoning module with a one-shot prompt. For Human I/O Lite, we observe overall slightly inferior performance compared to the full model. However, the MAE for Human I/O Lite is still at a low level around 0.44, showing a promising ability to predict SIIDs even with reduced computational resources.</span><br><span class="line"></span><br><span class="line">Human-I:O-Figure3</span><br><span class="line">Technical evaluation of Human I/O and Human I/O Lite. We report the MAE, average classification accuracy (ACC), and average intra-video variance (VAR) for four channels and the overall results. Our system estimates availability levels with small margins of error and variance. In Human I/O, 96.0% of predictions are within a discrepancy of 1 step from the actual availability value.</span><br><span class="line"></span><br><span class="line">Additionally, a user study with ten participants showed that Human I/O significantly reduced effort and enhanced user experience in the presence of SIIDs. Participants particularly valued how the system adapted to their real-time context, making digital interactions more seamless and less disruptive. Participants completed the NASA Task Load Index questionnaire, assessing mental demand, physical demand, temporal demand, overall performance, effort, and frustration level on a 7-point scale (from 1–Lowest to 7–Highest). Results suggest that Human I/O significantly reduces effort and improves user experience in the presence of SIIDs. Furthermore, Human I/O users reported a raised awareness of SIIDs that opened up new interaction possibilities.</span><br><span class="line"></span><br><span class="line">Human-I:O-Figure4</span><br><span class="line">Participants’ ratings to Task Load Index questions (on a scale of 1–low to 7–high) for their experience with SIIDs with and without Human I/O in user study. All rating differences are statistically significant with 𝑝 &lt; 0.001 via Wilcoxon signed-rank tests.</span><br><span class="line"></span><br><span class="line">Conclusion &amp; future work</span><br><span class="line">Human I/O represents a leap forward in our ability to interact with technology in a context-aware and adaptive manner. By understanding and predicting the availability of our input and output channels, it paves the way for smarter, more intuitive user interfaces that can enhance productivity and accessibility for everyone, regardless of the situational challenges they face.</span><br><span class="line"></span><br><span class="line">Meanwhile, maintaining privacy and upholding ethical standards are crucial in the design and deployment of SIIDs systems with active cameras and microphones on wearables. For example, a combination of on-device inference (e.g., Gemini Nano) and federated learning can forestall potential data breaches. Future research may incorporate more sensing techniques, such as depth sensing, ultra-wideband, and eye tracking, to provide users with finer controls over how their devices adapt to their changing needs and situations.</span><br><span class="line"></span><br><span class="line">We envision this technology not only improving individual device interactions but also serving as a foundation for future developments in ubiquitous computing.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.lamini.ai/blog/lamini-memory-tuning</span><br><span class="line">Introducing Lamini Memory Tuning: 95% LLM Accuracy, 10x Fewer Hallucinations</span><br><span class="line">TLDR:</span><br><span class="line">Lamini Memory Tuning is a new way to embed facts into LLMs that improves factual accuracy and reduces hallucinations to previously unachievable levels — for one Fortune 500 customer, Lamini Memory Tuning led to 95% accuracy compared to 50% with other approaches. Hallucinations were reduced from 50% to 5%.</span><br><span class="line">Lamini Memory Tuning is a research breakthrough that overcomes a seeming paradox in the AI world: achieving precise factual accuracy (i.e. no hallucinations) while upholding the generalization capabilities that make LLMs valuable in the first place.</span><br><span class="line">The method entails tuning millions of expert adapters (e.g. LoRAs) with precise facts on top of any open-source LLM, like Llama 3 or Mistral 3. If the goal is to get Roman Empire facts exactly right, Lamini Memory Tuning would create experts on Caesar, aqueducts, legions, and any other facts you provide. Inspired by information retrieval, the model retrieves only the most relevant experts from an index at inference time — not all the model weights — so latency and cost are dramatically lower. High accuracy, high speed, low cost: with Lamini Memory Tuning, you don’t have to choose.</span><br><span class="line">Contact us to try Lamini Memory Tuning.</span><br><span class="line">‍</span><br><span class="line"></span><br><span class="line">‍Accuracy matters immensely</span><br><span class="line">Yet, general-purpose LLMs are designed to hallucinate, because they are trained to reduce the average error across the examples they’ve seen. They’re pretty good at everything, but perfect at nothing. They can produce fluent English prose because they’ve seen so much of it across the internet, but specific facts—like a date, a revenue number, or a variable name—get muddled in probabilities. As a result, companies have not been able to count on LLMs for the most critical and most valuable use cases – until now.</span><br><span class="line"></span><br><span class="line">Introducing Lamini Memory Tuning</span><br><span class="line">Lamini Memory Tuning is a completely new way to fine-tune any existing LLM by tuning millions of LoRA adapters and selecting across them in a wide Mixture of Experts at inference time.</span><br><span class="line">Instead of optimizing average error on everything, Lamini Memory Tuning optimizes for zero error on the specific facts you tell it to remember, so it recalls those facts nearly perfectly. That’s not special on its own. This approach is particularly groundbreaking because it preserves the LLM’s ability to generalize with average error on everything else, and thus continue to produce fluent prose around those facts. Lamini Memory Tuning is a systematic tool for eliminating hallucinations on the facts you care about.</span><br><span class="line"></span><br><span class="line">Fortune 500 customers are already using Lamini Memory Tuning to achieve 95% factual accuracy on critical use cases where previous state-of-the-art approaches peaked at 50%.</span><br><span class="line"></span><br><span class="line">‍</span><br><span class="line"></span><br><span class="line">THE PROBLEM</span><br><span class="line"></span><br><span class="line">Prompting and RAG: necessary but not sufficient</span><br><span class="line">Prompting and Retrieval Augmented Generation (RAG) are important methods for surfacing relevant information to the model, shifting its probabilities to consider similar information. This is an important step to getting the model to condition on the right concepts and information, because the model has been trained on so many tasks. Good prompt-engineering and RAG pipelines are critical to improve the overall accuracy of the model.</span><br><span class="line"></span><br><span class="line">At times, this is all you need. But other times, you provide the relevant information and the response is still wrong but so close to right — leading to hallucinations.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Why do hallucinations happen with the right data? In the model’s internal representation, the right answer is likely clustered with similar, but wrong, options. The right context increases the probabilities of the right answer and nearby wrong options. The model doesn’t know that a nearly right answer is still wrong, because general models don’t distinguish between exactly right and nearly right — they never learned to take the loss on those answers to zero. Prompting and RAG don’t change that.</span><br><span class="line"></span><br><span class="line">Lamini Memory Tuning addresses this directly, by combining methods from information retrieval and AI to teach the model that getting the answer nearly right is the same as getting it totally wrong.</span><br><span class="line"></span><br><span class="line">Instruction fine-tuning: the wrong tool for the job</span><br><span class="line">Many teams turn to instruction fine-tuning when other techniques hit a wall on factual accuracy, but instruction fine-tuning, with or without LoRAs, lead to the same issue that pre-training has: it gets to be pretty good at a more narrow dataset, but still perfect at nothing, while being finicky to work with (losing the ability to perform on some general tasks, if you do it wrong).</span><br><span class="line"></span><br><span class="line">As a result, teams struggle with unclear choices, long feedback loops, high compute bills, and ultimately underwhelming performance improvements. While instruction fine-tuning can be really valuable (it’s what turned GPT-3 into ChatGPT), it doesn&#x27;t make models perfect at the facts that matter. In other words, traditional fine-tuning does not ensure that the model&#x27;s answers are faithful to facts in its training data.</span><br><span class="line"></span><br><span class="line">This is why we developed Lamini Memory Tuning.</span><br><span class="line"></span><br><span class="line">‍</span><br><span class="line"></span><br><span class="line">OUR INNOVATION</span><br><span class="line"></span><br><span class="line">Lamini Memory Tuning: near-perfect fact recall via 1 million-way MoE</span><br><span class="line">Lamini Memory Tuning is a fundamentally different fine-tuning approach that effectively teaches any open-source LLM to be near-perfect on facts, while still maintaining its ability to be pretty good at everything else. When the model is supposed to recall a specific fact, Lamini Memory Tuning shifts the entire probability mass to that particular fact (i.e. specific tokens within a particular context), such as the exact SQL schema for your database. This results in output probabilities that are not just closer to the right result, but exactly there.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">To do this, Lamini Memory Tuning tunes a massive mixture of memory experts on any open-source LLM. Each memory expert acts like a LoRA adapter that functionally operates as memory for the model. Together, the memory experts specialize in a million different ways to ensure faithful and factual accuracy to the data that it was tuned on. Inspired by information retrieval, these million memory experts are equivalent to indices from which the model intelligently retrieves and routes. At inference time, the model retrieves the most relevant experts at each layer and merges back into the base model to respond to the user query.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The result is a sparsely activated model, called a Mixture of Memory Experts (MoME), that can scale to an enormous number of parameters at a fixed computational inference cost. This means MoMEs have extremely high capacity for the number of facts that can be learned, bounded only by the total size of the training data set. Llama 3 was trained on 15 trillion tokens. Realistically, you will run out of system memory before you run out of memory capacity in a MoME.</span><br><span class="line"></span><br><span class="line">Ultimately, this approach makes what were impossible use cases that critically suffer from hallucinations within reach, and drastically improves LLM time-to-accuracy and thus time-to-market.</span><br><span class="line"></span><br><span class="line">Read more details in our research paper.</span><br><span class="line"></span><br><span class="line">Results</span><br><span class="line">Lamini Memory Tuning has been a game-changing capability with Lamini’s Fortune 500 clients, who are deploying it for the following use cases:</span><br><span class="line"></span><br><span class="line">High precision text-to-SQL</span><br><span class="line">Client need: Democratize data access by using LLMs to turn natural language questions into database queries.</span><br><span class="line">Challenge: The relevant databases had unique internal names and large, messy schemas.</span><br><span class="line">Result: We achieved 95% accuracy with Lamini Memory Tuning after 50% accuracy with RAG.</span><br><span class="line">High precision classification</span><br><span class="line">Client need: Save thousands of hours by automatically labeling data accurately.</span><br><span class="line">Challenge: We had to adhere to an exact taxonomy of 900 categories.</span><br><span class="line">Result: We achieved 100% accuracy across thousands of documents.</span><br><span class="line">High precision recommendations</span><br><span class="line">Client need: Increase cart size and revenue with AI-powered product suggestions.</span><br><span class="line">Challenge: Applications break when product IDs are hallucinated.</span><br><span class="line">Result: We achieved 88% accuracy across a 50,000 product database.</span><br><span class="line">A new frontier</span><br><span class="line"></span><br><span class="line">Lamini Memory Tuning changes several of the fundamental dynamics and tradeoffs governing how we work with LLMs. We’re in the early days of this new paradigm, and we’re still learning alongside our customers what’s possible. Summarizing a few areas we’re most excited about:</span><br><span class="line"></span><br><span class="line">Higher accuracy enables full automation as opposed to copiloting.</span><br><span class="line">Lower costs let you take your product from internal demos to a wider production audience.</span><br><span class="line">Lower latency enables seamless user experiences.</span><br><span class="line">Smaller models mean faster development and improvement cycles.</span><br><span class="line">What could you do with models that ran faster, were more accurate, and cost less to develop and run?</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Start using Lamini Memory Tuning</span><br><span class="line">Because Lamini Memory Tuning is a cutting-edge technique that embeds your unique data in a new model architecture, we’re exclusively working with select partners.</span><br><span class="line"></span><br><span class="line">Contact us to try Lamini Memory Tuning.</span><br><span class="line"></span><br><span class="line">Want to learn more?</span><br><span class="line"></span><br><span class="line">Read the research paper for details on the underlying approach.</span><br><span class="line">Read the case study to see how a Fortune 500 company is using Lamini Memory Tuning for a 95% accurate text-to-SQL agent.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://link.springer.com/content/pdf/10.1007/s10676-024-09775-5.pdf</span><br><span class="line">ChatGPT is bullshit</span><br><span class="line">Michael Townsen Hicks1  · James Humphries1</span><br><span class="line"> · Joe Slater1</span><br><span class="line"></span><br><span class="line"> t</span><br><span class="line">Recently, there has been considerable interest in large language models: machine learning systems which produce humanlike text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are</span><br><span class="line">often called “AI hallucinations”. We argue that these falsehoods, and the overall activity of large language models, is better</span><br><span class="line">understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important</span><br><span class="line">way indifferent to the truth of their outputs. We distinguish two ways in which the models can be said to be bullshitters,</span><br><span class="line">and argue that they clearly meet at least one of these definitions. We further argue that describing AI misrepresentations</span><br><span class="line">as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://towardsdatascience.com/multi-ai-agent-systems-101-bac58e3bcc47</span><br><span class="line">Multi AI Agent Systems 101</span><br><span class="line">Automating Routine Tasks in Data Source Management with CrewAI</span><br><span class="line">Mariya Mansurova</span><br><span class="line">Towards Data Science</span><br><span class="line">Mariya Mansurova</span><br><span class="line"></span><br><span class="line">·</span><br><span class="line">Follow</span><br><span class="line"></span><br><span class="line">Published in</span><br><span class="line">Towards Data Science</span><br><span class="line"></span><br><span class="line">·</span><br><span class="line">26 min read</span><br><span class="line">·</span><br><span class="line">7 hours ago</span><br><span class="line">100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Image by DALL-E 3</span><br><span class="line">Initially, when ChatGPT just appeared, we used simple prompts to get answers to our questions. Then, we encountered issues with hallucinations and began using RAG (Retrieval Augmented Generation) to provide more context to LLMs. After that, we started experimenting with AI agents, where LLMs act as a reasoning engine and can decide what to do next, which tools to use, and when to return the final answer.</span><br><span class="line"></span><br><span class="line">The next evolutionary step is to create teams of such agents that can collaborate with each other. This approach is logical as it mirrors human interactions. We work in teams where each member has a specific role:</span><br><span class="line"></span><br><span class="line">The product manager proposes the next project to work on.</span><br><span class="line">The designer creates its look and feel.</span><br><span class="line">The software engineer develops the solution.</span><br><span class="line">The analyst examines the data to ensure it performs as expected and identifies ways to improve the product for customers.</span><br><span class="line">Similarly, we can create a team of AI agents, each focusing on one domain. They can collaborate and reach a final conclusion together. Just as specialization enhances performance in real life, it could also benefit the performance of AI agents.</span><br><span class="line"></span><br><span class="line">Another advantage of this approach is increased flexibility. Each agent can operate with its own prompt, set of tools and even LLM. For instance, we can use different models for different parts of our system. You can use GPT-4 for the agent that needs more reasoning and GPT-3.5 for the one that does only simple extraction. We can even fine-tune the model for small specific tasks and use it in our crew of agents.</span><br><span class="line"></span><br><span class="line">The potential drawbacks of this approach are time and cost. Multiple interactions and knowledge sharing between agents require more calls to LLM and consume additional tokens. This could result in longer wait times and increased expenses.</span><br><span class="line"></span><br><span class="line">There are several frameworks available for multi-agent systems today.</span><br><span class="line">Here are some of the most popular ones:</span><br><span class="line"></span><br><span class="line">AutoGen: Developed by Microsoft, AutoGen uses a conversational approach and was one of the earliest frameworks for multi-agent systems,</span><br><span class="line">LangGraph: While not strictly a multi-agent framework, LangGraph allows for defining complex interactions between actors using a graph structure. So, it can also be adapted to create multi-agent systems.</span><br><span class="line">CrewAI: Positioned as a high-level framework, CrewAI facilitates the creation of “crews” consisting of role-playing agents capable of collaborating in various ways.</span><br><span class="line">I’ve decided to start experimenting with multi-agent frameworks from CrewAI since it’s quite widely popular and user friendly. So, it looks like a good option to begin with.</span><br><span class="line"></span><br><span class="line">In this article, I will walk you through how to use CrewAI. As analysts, we’re the domain experts responsible for documenting various data sources and addressing related questions. We’ll explore how to automate these tasks using multi-agent frameworks.</span><br><span class="line"></span><br><span class="line">Setting up the environment</span><br><span class="line">Let’s start with setting up the environment. First, we need to install the CrewAI main package and an extension to work with tools.</span><br><span class="line"></span><br><span class="line">pip install crewai</span><br><span class="line">pip install &#x27;crewai[tools]&#x27;</span><br><span class="line">CrewAI was developed to work primarily with OpenAI API, but I would also like to try it with a local model. According to the ChatBot Arena Leaderboard, the best model you can run on your laptop is Llama 3 (8b parameters). It will be the most feasible option for our use case.</span><br><span class="line"></span><br><span class="line">We can access Llama models using Ollama. Installation is pretty straightforward. You need to download Ollama from the website and then go through the installation process. That’s it.</span><br><span class="line"></span><br><span class="line">Now, you can test the model in CLI by running the following command.</span><br><span class="line"></span><br><span class="line">ollama run llama3</span><br><span class="line">For example, you can ask something like this.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Let’s create a custom Ollama model to use later in CrewAI.</span><br><span class="line"></span><br><span class="line">We will start with a ModelFile (documentation). I only specified the base model (llama3), temperature and stop sequence. However, you might add more features. For example, you can determine the system message using SYSTEM keyword.</span><br><span class="line"></span><br><span class="line">FROM llama3</span><br><span class="line"></span><br><span class="line"># set parameters</span><br><span class="line">PARAMETER temperature 0.5</span><br><span class="line">PARAMETER stop Result</span><br><span class="line">I’ve saved it into a Llama3ModelFile file.</span><br><span class="line"></span><br><span class="line">Let’s create a bash script to load the base model for Ollama and create the custom model we defined in ModelFile.</span><br><span class="line"></span><br><span class="line">#!/bin/zsh</span><br><span class="line"></span><br><span class="line"># define variables</span><br><span class="line">model_name=&quot;llama3&quot;</span><br><span class="line">custom_model_name=&quot;crewai-llama3&quot;</span><br><span class="line"></span><br><span class="line"># load the base model</span><br><span class="line">ollama pull $model_name</span><br><span class="line"></span><br><span class="line"># create the model file</span><br><span class="line">ollama create $custom_model_name -f ./Llama3ModelFile</span><br><span class="line">Let’s execute this file.</span><br><span class="line"></span><br><span class="line">chmod +x ./llama3_setup.sh</span><br><span class="line">./llama3_setup.sh</span><br><span class="line">You can find both files on GitHub: Llama3ModelFile and llama3_setup.sh</span><br><span class="line"></span><br><span class="line">We need to initialise the following environmental variables to use the local Llama model with CrewAI.</span><br><span class="line"></span><br><span class="line">os.environ[&quot;OPENAI_API_BASE&quot;]=&#x27;http://localhost:11434/v1&#x27;</span><br><span class="line"></span><br><span class="line">os.environ[&quot;OPENAI_MODEL_NAME&quot;]=&#x27;crewai-llama3&#x27; </span><br><span class="line"># custom_model_name from the bash script</span><br><span class="line"></span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;NA&quot;</span><br><span class="line">We’ve finished the setup and are ready to continue our journey.</span><br><span class="line"></span><br><span class="line">Use cases: working with documentation</span><br><span class="line">As analysts, we often play the role of subject matter experts for data and some data-related tools. In my previous team, we used to have a channel with almost 1K participants, where we were answering lots of questions about our data and the ClickHouse database we used as storage. It took us quite a lot of time to manage this channel. It would be interesting to see whether such tasks can be automated with LLMs.</span><br><span class="line"></span><br><span class="line">For this example, I will use the ClickHouse database. If you’re interested, You can learn more about ClickHouse and how to set it up locally in my previous article. However, we won’t utilise any ClickHouse-specific features, so feel free to stick to the database you know.</span><br><span class="line"></span><br><span class="line">I’ve created a pretty simple data model to work with. There are just two tables in our DWH (Data Warehouse): ecommerce_db.users and ecommerce_db.sessions. As you might guess, the first table contains information about the users of our service.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The ecommerce_db.sessions table stores information about user sessions.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Regarding data source management, analysts typically handle tasks like writing and updating documentation and answering questions about this data. So, we will use LLM to write documentation for the table in the database and teach it to answer questions about data or ClickHouse.</span><br><span class="line"></span><br><span class="line">But before moving on to the implementation, let’s learn more about the CrewAI framework and its core concepts.</span><br><span class="line"></span><br><span class="line">CrewAI basic concepts</span><br><span class="line">The cornerstone of a multi-agent framework is an agent concept. In CrewAI, agents are powered by role-playing. Role-playing is a tactic when you ask an agent to adopt a persona and behave like a top-notch backend engineer or helpful customer support agent. So, when creating a CrewAI agent, you need to specify each agent&#x27;s role, goal, and backstory so that LLM knows enough to play this role.</span><br><span class="line"></span><br><span class="line">The agents’ capabilities are limited without tools (functions that agents can execute and get results). With CrewAI, you can use one of the predefined tools (for example, to search the Internet, parse a website, or do RAG on a document), create a custom tool yourself or use LangChain tools. So, it’s pretty easy to create a powerful agent.</span><br><span class="line"></span><br><span class="line">Let’s move on from agents to the work they are doing. Agents are working on tasks (specific assignments). For each task, we need to define a description, expected output (definition of done), set of available tools and assigned agent. I really like that these frameworks follow the managerial best practices like a clear definition of done for the tasks.</span><br><span class="line"></span><br><span class="line">The next question is how to define the execution order for tasks: which one to work on first, which ones can run in parallel, etc. CrewAI implemented processes to orchestrate the tasks. It provides a couple of options:</span><br><span class="line"></span><br><span class="line">Sequential —the most straightforward approach when tasks are called one after another.</span><br><span class="line">Hierarchical — when there’s a manager (specified as LLM model) that creates and delegates tasks to the agents.</span><br><span class="line">Also, CrewAI is working on a consensual process. In such a process, agents will be able to make decisions collaboratively with a democratic approach.</span><br><span class="line"></span><br><span class="line">There are other levers you can use to tweak the process of tasks’ execution:</span><br><span class="line"></span><br><span class="line">You can mark tasks as “asynchronous”, then they will be executed in parallel, so you will be able to get an answer faster.</span><br><span class="line">You can use the “human input” flag on a task, and then the agent will ask for human approval before finalising the output of this task. It can allow you to add an oversight to the process.</span><br><span class="line">We’ve defined all the primary building blocks and can discuss the holly grail of CrewAI — crew concept. The crew represents the team of agents and the set of tasks they will be working on. The approach for collaboration (processes we discussed above) can also be defined at the crew level.</span><br><span class="line"></span><br><span class="line">Also, we can set up the memory for a crew. Memory is crucial for efficient collaboration between the agents. CrewAI supports three levels of memory:</span><br><span class="line"></span><br><span class="line">Short-term memory stores information related to the current execution. It helps agents to work together on the current task.</span><br><span class="line">Long-term memory is data about the previous executions stored in the local database. This type of memory allows agents to learn from earlier iterations and improve over time.</span><br><span class="line">Entity memory captures and structures information about entities (like personas, cities, etc.)</span><br><span class="line">Right now, you can only switch on all types of memory for a crew without any further customisation. However, it doesn’t work with the Llama models.</span><br><span class="line"></span><br><span class="line">We’ve learned enough about the CrewAI framework, so it’s time to start using this knowledge in practice.</span><br><span class="line"></span><br><span class="line">Use case: writing documentation</span><br><span class="line">Let’s start with a simple task: putting together the documentation for our DWH. As we discussed before, there are two tables in our DWH, and I would like to create a detailed description for them using LLMs.</span><br><span class="line"></span><br><span class="line">First approach</span><br><span class="line">In the beginning, we need to think about the team structure. Think of this as a typical managerial task. Who would you hire for such a job?</span><br><span class="line"></span><br><span class="line">I would break this task into two parts: retrieving data from a database and writing documentation. So, we need a database specialist and a technical writer. The database specialist needs access to a database, while the writer won’t need any special tools.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Now, we have a high-level plan. Let’s create the agents.</span><br><span class="line"></span><br><span class="line">For each agent, I’ve specified the role, goal and backstory. I’ve tried my best to provide agents with all the needed context.</span><br><span class="line"></span><br><span class="line">database_specialist_agent = Agent(</span><br><span class="line">  role = &quot;Database specialist&quot;,</span><br><span class="line">  goal = &quot;Provide data to answer business questions using SQL&quot;,</span><br><span class="line">  backstory = &#x27;&#x27;&#x27;You are an expert in SQL, so you can help the team </span><br><span class="line">  to gather needed data to power their decisions. </span><br><span class="line">  You are very accurate and take into account all the nuances in data.&#x27;&#x27;&#x27;,</span><br><span class="line">  allow_delegation = False,</span><br><span class="line">  verbose = True</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tech_writer_agent = Agent(</span><br><span class="line">  role = &quot;Technical writer&quot;,</span><br><span class="line">  goal = &#x27;&#x27;&#x27;Write engaging and factually accurate technical documentation </span><br><span class="line">    for data sources or tools&#x27;&#x27;&#x27;,</span><br><span class="line">  backstory = &#x27;&#x27;&#x27; </span><br><span class="line">  You are an expert in both technology and communications, so you can easily explain even sophisticated concepts.</span><br><span class="line">  You base your work on the factual information provided by your colleagues.</span><br><span class="line">  Your texts are concise and can be easily understood by a wide audience. </span><br><span class="line">  You use professional but rather an informal style in your communication.</span><br><span class="line">  &#x27;&#x27;&#x27;,</span><br><span class="line">  allow_delegation = False,</span><br><span class="line">  verbose = True</span><br><span class="line">)</span><br><span class="line">We will use a simple sequential process, so there’s no need for agents to delegate tasks to each other. That’s why I specified allow_delegation = False.</span><br><span class="line"></span><br><span class="line">The next step is setting the tasks for agents. But before moving to them, we need to create a custom tool to connect to the database.</span><br><span class="line"></span><br><span class="line">First, I put together a function to execute ClickHouse queries using HTTP API.</span><br><span class="line"></span><br><span class="line">CH_HOST = &#x27;http://localhost:8123&#x27; # default address </span><br><span class="line"></span><br><span class="line">def get_clickhouse_data(query, host = CH_HOST, connection_timeout = 1500):</span><br><span class="line">  r = requests.post(host, params = &#123;&#x27;query&#x27;: query&#125;, </span><br><span class="line">    timeout = connection_timeout)</span><br><span class="line">  if r.status_code == 200:</span><br><span class="line">      return r.text</span><br><span class="line">  else: </span><br><span class="line">      return &#x27;Database returned the following error:\n&#x27; + r.text</span><br><span class="line">When working with LLM agents, it’s important to make tools fault-tolerant. For example, if the database returns an error (status_code != 200), my code won’t throw an exception. Instead, it will return the error description to the LLM so it can attempt to resolve the issue.</span><br><span class="line"></span><br><span class="line">To create a CrewAI custom tool, we need to derive our class from crewai_tools.BaseTool, implement the _run method and then create an instance of this class.</span><br><span class="line"></span><br><span class="line">from crewai_tools import BaseTool</span><br><span class="line"></span><br><span class="line">class DatabaseQuery(BaseTool):</span><br><span class="line">  name: str = &quot;Database Query&quot;</span><br><span class="line">  description: str = &quot;Returns the result of SQL query execution&quot;</span><br><span class="line"></span><br><span class="line">  def _run(self, sql_query: str) -&gt; str:</span><br><span class="line">      # Implementation goes here</span><br><span class="line">      return get_clickhouse_data(sql_query)</span><br><span class="line"></span><br><span class="line">database_query_tool = DatabaseQuery()</span><br><span class="line">Now, we can set the tasks for the agents. Again, providing clear instructions and all the context to LLM is crucial.</span><br><span class="line"></span><br><span class="line">table_description_task = Task(</span><br><span class="line">  description = &#x27;&#x27;&#x27;Provide the comprehensive overview for the data </span><br><span class="line">  in table &#123;table&#125;, so that it&#x27;s easy to understand the structure </span><br><span class="line">  of the data. This task is crucial to put together the documentation </span><br><span class="line">  for our database&#x27;&#x27;&#x27;,</span><br><span class="line">  expected_output = &#x27;&#x27;&#x27;The comprehensive overview of &#123;table&#125; in the md format. </span><br><span class="line">  Include 2 sections: columns (list of columns with their types) </span><br><span class="line">  and examples (the first 30 rows from table).&#x27;&#x27;&#x27;,</span><br><span class="line">  tools = [database_query_tool],</span><br><span class="line">  agent = database_specialist_agent</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">table_documentation_task = Task(</span><br><span class="line">  description = &#x27;&#x27;&#x27;Using provided information about the table, </span><br><span class="line">  put together the detailed documentation for this table so that </span><br><span class="line">  people can use it in practice&#x27;&#x27;&#x27;,</span><br><span class="line">  expected_output = &#x27;&#x27;&#x27;Well-written detailed documentation describing </span><br><span class="line">  the data scheme for the table &#123;table&#125; in markdown format, </span><br><span class="line">  that gives the table overview in 1-2 sentences then then </span><br><span class="line">  describes each columm. Structure the columns description </span><br><span class="line">  as a markdown table with column name, type and description.&#x27;&#x27;&#x27;,</span><br><span class="line">  tools = [],</span><br><span class="line">  output_file=&quot;table_documentation.md&quot;,</span><br><span class="line">  agent = tech_writer_agent</span><br><span class="line">)</span><br><span class="line">You might have noticed that I’ve used &#123;table&#125; placeholder in the tasks’ descriptions. We will use table as an input variable when executing the crew, and this value will be inserted into all placeholders.</span><br><span class="line"></span><br><span class="line">Also, I’ve specified the output file for the table documentation task to save the final result locally.</span><br><span class="line"></span><br><span class="line">We have all we need. Now, it’s time to create a crew and execute the process, specifying the table we are interested in. Let’s try it with the users table.</span><br><span class="line"></span><br><span class="line">crew = Crew(</span><br><span class="line">  agents = [database_specialist_agent, tech_writer_agent],</span><br><span class="line">  tasks = [table_description_task,  table_documentation_task],</span><br><span class="line">  verbose = 2</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">result = crew.kickoff(&#123;&#x27;table&#x27;: &#x27;ecommerce_db.users&#x27;&#125;)</span><br><span class="line">It’s an exciting moment, and I’m really looking forward to seeing the result. Don’t worry if execution takes some time. Agents make multiple LLM calls, so it’s perfectly normal for it to take a few minutes. It took 2.5 minutes on my laptop.</span><br><span class="line"></span><br><span class="line">We asked LLM to return the documentation in markdown format. We can use the following code to see the formatted result in Jupyter Notebook.</span><br><span class="line"></span><br><span class="line">from IPython.display import Markdown</span><br><span class="line">Markdown(result)</span><br><span class="line">At first glance, it looks great. We’ve got the valid markdown file describing the users&#x27; table.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">But wait, it’s incorrect. Let’s see what data we have in our table.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The columns listed in the documentation are completely different from what we have in the database. It’s a case of LLM hallucinations.</span><br><span class="line"></span><br><span class="line">We’ve set verbose = 2 to get the detailed logs from CrewAI. Let’s read through the execution logs to identify the root cause of the problem.</span><br><span class="line"></span><br><span class="line">First, the database specialist couldn’t query the database due to complications with quotes.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The specialist didn’t manage to resolve this problem. Finally, this chain has been terminated by CrewAI with the following output: Agent stopped due to iteration limit or time limit.</span><br><span class="line"></span><br><span class="line">This means the technical writer didn’t receive any factual information about the data. However, the agent continued and produced completely fake results. That’s how we ended up with incorrect documentation.</span><br><span class="line"></span><br><span class="line">Fixing the issues</span><br><span class="line">Even though our first iteration wasn’t successful, we’ve learned a lot. We have (at least) two areas for improvement:</span><br><span class="line"></span><br><span class="line">Our database tool is too difficult for the model, and the agent struggles to use it. We can make the tool more tolerant by removing quotes from the beginning and end of the queries. This solution is not ideal since valid SQL can end with a quote, but let’s try it.</span><br><span class="line">Our technical writer isn’t basing its output on the input from the database specialist. We need to tweak the prompt to highlight the importance of providing only factual information.</span><br><span class="line">So, let’s try to fix these problems. First, we will fix the tool — we can leverage strip to eliminate quotes.</span><br><span class="line"></span><br><span class="line">CH_HOST = &#x27;http://localhost:8123&#x27; # default address </span><br><span class="line"></span><br><span class="line">def get_clickhouse_data(query, host = CH_HOST, connection_timeout = 1500):</span><br><span class="line">  r = requests.post(host, params = &#123;&#x27;query&#x27;: query.strip(&#x27;&quot;&#x27;).strip(&quot;&#x27;&quot;)&#125;, </span><br><span class="line">    timeout = connection_timeout)</span><br><span class="line">  if r.status_code == 200:</span><br><span class="line">    return r.text</span><br><span class="line">  else: </span><br><span class="line">    return &#x27;Database returned the following error:\n&#x27; + r.text</span><br><span class="line">Then, it’s time to update the prompt. I’ve included statements emphasizing the importance of sticking to the facts in both the agent and task definitions.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tech_writer_agent = Agent(</span><br><span class="line">  role = &quot;Technical writer&quot;,</span><br><span class="line">  goal = &#x27;&#x27;&#x27;Write engaging and factually accurate technical documentation </span><br><span class="line">  for data sources or tools&#x27;&#x27;&#x27;,</span><br><span class="line">  backstory = &#x27;&#x27;&#x27; </span><br><span class="line">  You are an expert in both technology and communications, so you </span><br><span class="line">  can easily explain even sophisticated concepts.</span><br><span class="line">  Your texts are concise and can be easily understood by wide audience. </span><br><span class="line">  You use professional but rather informal style in your communication.</span><br><span class="line">  You base your work on the factual information provided by your colleagues. </span><br><span class="line">  You stick to the facts in the documentation and use ONLY </span><br><span class="line">  information provided by the colleagues not adding anything.&#x27;&#x27;&#x27;,</span><br><span class="line">  allow_delegation = False,</span><br><span class="line">  verbose = True</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">table_documentation_task = Task(</span><br><span class="line">  description = &#x27;&#x27;&#x27;Using provided information about the table, </span><br><span class="line">  put together the detailed documentation for this table so that </span><br><span class="line">  people can use it in practice&#x27;&#x27;&#x27;,</span><br><span class="line">  expected_output = &#x27;&#x27;&#x27;Well-written detailed documentation describing </span><br><span class="line">  the data scheme for the table &#123;table&#125; in markdown format, </span><br><span class="line">  that gives the table overview in 1-2 sentences then then </span><br><span class="line">  describes each columm. Structure the columns description </span><br><span class="line">  as a markdown table with column name, type and description.</span><br><span class="line">  The documentation is based ONLY on the information provided </span><br><span class="line">  by the database specialist without any additions.&#x27;&#x27;&#x27;,</span><br><span class="line">  tools = [],</span><br><span class="line">  output_file = &quot;table_documentation.md&quot;,</span><br><span class="line">  agent = tech_writer_agent</span><br><span class="line">)</span><br><span class="line">Let’s execute our crew once again and see the results.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">We’ve achieved a bit better result. Our database specialist was able to execute queries and view the data, which is a significant win for us. Additionally, we can see all the relevant fields in the result table, though there are lots of other fields as well. So, it’s still not entirely correct.</span><br><span class="line"></span><br><span class="line">I once again looked through the CrewAI execution log to figure out what went wrong. The issue lies in getting the list of columns. There’s no filter by database, so it returns some unrelated columns that appear in the result.</span><br><span class="line"></span><br><span class="line">SELECT column_name </span><br><span class="line">FROM information_schema.columns </span><br><span class="line">WHERE table_name = &#x27;users&#x27;</span><br><span class="line">Also, after looking at multiple attempts, I noticed that the database specialist, from time to time, executes select * from &lt;table&gt; query. It might cause some issues in production as it might generate lots of data and send it to LLM.</span><br><span class="line"></span><br><span class="line">More specialised tools</span><br><span class="line">We can provide our agent with more specialised tools to improve our solution. Currently, the agent has a tool to execute any SQL query, which is flexible and powerful but prone to errors. We can create more focused tools, such as getting table structure and top-N rows from the table. Hopefully, it will reduce the number of mistakes.</span><br><span class="line"></span><br><span class="line">class TableStructure(BaseTool):</span><br><span class="line">  name: str = &quot;Table structure&quot;</span><br><span class="line">  description: str = &quot;Returns the list of columns and their types&quot;</span><br><span class="line"></span><br><span class="line">  def _run(self, table: str) -&gt; str:</span><br><span class="line">    table = table.strip(&#x27;&quot;&#x27;).strip(&quot;&#x27;&quot;)</span><br><span class="line">    return get_clickhouse_data(</span><br><span class="line">      &#x27;describe &#123;table&#125; format TabSeparatedWithNames&#x27;\</span><br><span class="line">        .format(table = table)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">class TableExamples(BaseTool):</span><br><span class="line">  name: str = &quot;Table examples&quot;</span><br><span class="line">  description: str = &quot;Returns the first N rows from the table&quot;</span><br><span class="line"></span><br><span class="line">  def _run(self, table: str, n: int = 30) -&gt; str:</span><br><span class="line">    table = table.strip(&#x27;&quot;&#x27;).strip(&quot;&#x27;&quot;)</span><br><span class="line">    return get_clickhouse_data(</span><br><span class="line">      &#x27;select * from &#123;table&#125; limit &#123;n&#125; format TabSeparatedWithNames&#x27;</span><br><span class="line">        \.format(table = table, n = n)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">table_structure_tool = TableStructure()</span><br><span class="line">table_examples_tool = TableExamples()</span><br><span class="line">Now, we need to specify these tools in the task and re-run our script. After the first attempt, I got the following output from the Technical Writer.</span><br><span class="line"></span><br><span class="line">Task output: This final answer provides a detailed and factual description </span><br><span class="line">of the ecommerce_db.users table structure, including column names, types, </span><br><span class="line">and descriptions. The documentation adheres to the provided information </span><br><span class="line">from the database specialist without any additions or modifications.</span><br><span class="line">More focused tools helped the database specialist retrieve the correct table information. However, even though the writer had all the necessary information, we didn’t get the expected result.</span><br><span class="line"></span><br><span class="line">As we know, LLMs are probabilistic, so I gave it another try. And hooray, this time, the result was pretty good.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">It’s not perfect since it still includes some irrelevant comments and lacks the overall description of the table. However, providing more specialised tools has definitely paid off. It also helped to prevent issues when the agent tried to load all the data from the table.</span><br><span class="line"></span><br><span class="line">Quality assurance specialist</span><br><span class="line">We’ve achieved pretty good results, but let’s see if we can improve them further. A common practice in multi-agent setups is quality assurance, which adds the final review stage before finalising the results.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Let’s create a new agent — a Quality Assurance Specialist, who will be in charge of review.</span><br><span class="line"></span><br><span class="line">qa_specialist_agent = Agent(</span><br><span class="line">  role = &quot;Quality Assurance specialist&quot;,</span><br><span class="line">  goal = &quot;&quot;&quot;Ensure the highest quality of the documentation we provide </span><br><span class="line">  (that it&#x27;s correct and easy to understand)&quot;&quot;&quot;,</span><br><span class="line">  backstory = &#x27;&#x27;&#x27;</span><br><span class="line">  You work as a Quality Assurance specialist, checking the work </span><br><span class="line">  from the technical writer and ensuring that it&#x27;s inline </span><br><span class="line">  with our highest standards.</span><br><span class="line">  You need to check that the technical writer provides the full complete </span><br><span class="line">  answers and make no assumptions. </span><br><span class="line">  Also, you need to make sure that the documentation addresses </span><br><span class="line">  all the questions and is easy to understand.</span><br><span class="line">  &#x27;&#x27;&#x27;,</span><br><span class="line">  allow_delegation = False,</span><br><span class="line">  verbose = True</span><br><span class="line">)</span><br><span class="line">Now, it’s time to describe the review task. I’ve used the context parameter to specify that this task requires outputs from both table_description_task and table_documentation_task.</span><br><span class="line"></span><br><span class="line">qa_review_task = Task(</span><br><span class="line">  description = &#x27;&#x27;&#x27;</span><br><span class="line">  Review the draft documentation provided by the technical writer.</span><br><span class="line">  Ensure that the documentation fully answers all the questions: </span><br><span class="line">  the purpose of the table and its structure in the form of table. </span><br><span class="line">  Make sure that the documentation is consistent with the information </span><br><span class="line">  provided by the database specialist. </span><br><span class="line">  Double check that there are no irrelevant comments in the final version </span><br><span class="line">  of documentation.</span><br><span class="line">  &#x27;&#x27;&#x27;,</span><br><span class="line">  expected_output = &#x27;&#x27;&#x27;</span><br><span class="line">  The final version of the documentation in markdown format </span><br><span class="line">  that can be published. </span><br><span class="line">  The documentation should fully address all the questions, be consistent </span><br><span class="line">  and follow our professional but informal tone of voice.</span><br><span class="line">  &#x27;&#x27;&#x27;,</span><br><span class="line">  tools = [],</span><br><span class="line">  context = [table_description_task, table_documentation_task],</span><br><span class="line">  output_file=&quot;checked_table_documentation.md&quot;,</span><br><span class="line">  agent = qa_specialist_agent</span><br><span class="line">)</span><br><span class="line">Let’s update our crew and run it.</span><br><span class="line"></span><br><span class="line">full_crew = Crew(</span><br><span class="line">  agents=[database_specialist_agent, tech_writer_agent, qa_specialist_agent],</span><br><span class="line">  tasks=[table_description_task,  table_documentation_task, qa_review_task],</span><br><span class="line">  verbose = 2,</span><br><span class="line">  memory = False # don&#x27;t work with Llama</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">full_result = full_crew.kickoff(&#123;&#x27;table&#x27;: &#x27;ecommerce_db.users&#x27;&#125;)</span><br><span class="line">We now have more structured and detailed documentation thanks to the addition of the QA stage.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Delegation</span><br><span class="line">With the addition of the QA specialist, it would be interesting to test the delegation mechanism. The QA specialist agent might have questions or requests that it could delegate to other agents.</span><br><span class="line"></span><br><span class="line">I tried using the delegation with Llama 3, but it didn’t go well. Llama 3 struggled to call the co-worker tool correctly. It couldn’t specify the correct co-worker’s name.</span><br><span class="line"></span><br><span class="line">We achieved pretty good results with a local model that can run on any laptop, but now it’s time to switch gears and use a way more powerful model — GPT-4o.</span><br><span class="line"></span><br><span class="line">To do it, we just need to update the following environment variables.</span><br><span class="line"></span><br><span class="line">os.environ[&quot;OPENAI_MODEL_NAME&quot;] = &#x27;gpt-4o&#x27;  </span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = config[&#x27;OPENAI_API_KEY&#x27;] # your OpenAI key</span><br><span class="line">To switch on the delegation, we should specify allow_delegation = True for the QA specialist agent.</span><br><span class="line"></span><br><span class="line">Also, we can use handy memory functionality for OpenAI models, as mentioned above. The memory will allow our agents to share their knowledge with each other during execution and leverage long-term memory to get information from previous executions.</span><br><span class="line"></span><br><span class="line">full_crew = Crew(</span><br><span class="line">  agents = [database_specialist_agent, tech_writer_agent, qa_specialist_agent],</span><br><span class="line">  tasks = [table_description_task,  table_documentation_task, qa_review_task],</span><br><span class="line">  verbose = 2,</span><br><span class="line">  memory = True </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">full_result = full_crew.kickoff(&#123;&#x27;table&#x27;: &#x27;ecommerce_db.users&#x27;&#125;)</span><br><span class="line">Let’s see the CrewAI execution logs to understand how delegation works. Here are all the logs for the QA specialist. We can see that it reached out to the database specialist to double-check the information.</span><br><span class="line"></span><br><span class="line">Thought: I need to review the provided draft documentation to ensure it </span><br><span class="line">fully answers all the questions and is complete, correct, and easy </span><br><span class="line">to understand. Specifically, I need to ensure that it explains the purpose </span><br><span class="line">of the table, its structure, and includes relevant examples without </span><br><span class="line">any irrelevant comments.</span><br><span class="line"></span><br><span class="line">First, I will compare the two drafts to identify any discrepancies and </span><br><span class="line">missing information. Then, I will verify the accuracy and completeness of </span><br><span class="line">the information provided.</span><br><span class="line"></span><br><span class="line">**Comparison:**</span><br><span class="line">1. **Introduction:**</span><br><span class="line">   - First draft: Includes an introduction and examples.</span><br><span class="line">   - Second draft: Adds a more detailed introduction about the purpose and importance of the table.</span><br><span class="line"></span><br><span class="line">2. **Columns:**</span><br><span class="line">   - Both drafts have the same column structure and descriptions.</span><br><span class="line"></span><br><span class="line">3. **Additional Information:**</span><br><span class="line">   - Both drafts include similar additional information, but the second draft is more concise.</span><br><span class="line"></span><br><span class="line">4. **Examples:**</span><br><span class="line">   - The first draft includes examples, which are missing in the second draft.</span><br><span class="line"></span><br><span class="line">5. **Final Checks:**</span><br><span class="line">   - Ensure there are no irrelevant comments.</span><br><span class="line">   - Verify consistency with information provided by the database specialist.</span><br><span class="line"></span><br><span class="line">Action: Ask question to co-worker</span><br><span class="line">Action Input: &#123;</span><br><span class="line">&quot;question&quot;: &quot;Can you confirm the accuracy of the provided column descriptions </span><br><span class="line">  and examples for the `ecommerce_db.users` table?&quot;, </span><br><span class="line">&quot;context&quot;: &quot;We are finalizing the documentation for the `ecommerce_db.users`</span><br><span class="line"> table. The table includes columns `user_id`, `country`, `is_active`, and </span><br><span class="line">  `age`. Here are the examples provided:</span><br><span class="line"></span><br><span class="line">  | user_id | country        | is_active | age |</span><br><span class="line">  |---------|----------------|-----------|-----|</span><br><span class="line">  | 1000001 | United Kingdom | 0         | 70  |</span><br><span class="line">  | 1000002 | France         | 1         | 87  |</span><br><span class="line">  | 1000003 | France         | 1         | 88  |</span><br><span class="line">  | 1000004 | Germany        | 1         | 25  |</span><br><span class="line">  | 1000005 | Germany        | 1         | 48  |</span><br><span class="line">  | 1000006 | France         | 1         | 78  |</span><br><span class="line">  | 1000007 | Netherlands    | 1         | 65  |</span><br><span class="line">  | 1000008 | Germany        | 1         | 31  |</span><br><span class="line">  | 1000009 | Germany        | 1         | 66  |</span><br><span class="line">  | 1000010 | Germany        | 1         | 73  |&quot;, </span><br><span class="line">&quot;coworker&quot;: &quot;database specialist&quot;&#125;</span><br><span class="line"></span><br><span class="line">&lt;-- omitted logs from the database specialist --&gt;</span><br><span class="line"></span><br><span class="line">Based on the provided context and examples for the `ecommerce_db.users` table, </span><br><span class="line">the column descriptions and examples appear accurate. Here are the details:</span><br><span class="line">- `user_id`: This column contains a unique identifier for each user. </span><br><span class="line">  The provided examples (e.g., 1000001, 1000002) align with this description </span><br><span class="line">  as they are unique numeric identifiers.</span><br><span class="line"></span><br><span class="line">- `country`: This column specifies the country of the user. The provided </span><br><span class="line">  examples (e.g., United Kingdom, France, Germany, Netherlands) are </span><br><span class="line">  all valid country names.</span><br><span class="line"></span><br><span class="line">- `is_active`: This column indicates whether the user is active (1) or not (0). The provided examples (e.g., 0, 1) correctly represent this binary status.</span><br><span class="line"></span><br><span class="line">- `age`: This column shows the age of the user. The provided examples (e.g., 70, </span><br><span class="line">  87, 88, 25, 48, 78, 65, 31, 66, 73) are all valid ages.</span><br><span class="line"></span><br><span class="line">Therefore, the column descriptions and examples provided for the </span><br><span class="line"> `ecommerce_db.users` table are accurate and meet the expected criteria.</span><br><span class="line"></span><br><span class="line">Thought: Based on the observation from the database specialist, </span><br><span class="line">the column descriptions and examples provided for the `ecommerce_db.users` </span><br><span class="line">table are accurate. Now, I will consolidate the best elements from both drafts,</span><br><span class="line"> ensuring the documentation is complete, clear, and free of irrelevant comments.</span><br><span class="line"></span><br><span class="line">Action: Finalize the documentation in markdown format, incorporating </span><br><span class="line">the detailed introduction, examples, and ensuring the tone is professional </span><br><span class="line">but informal.</span><br><span class="line">When I tried the delegation for the first time, I didn’t enable memory, which led to incorrect results. The data specialist and the technical writer initially returned the correct information. However, when the QA specialist returned with the follow-up questions, they started to hallucinate. So, it looks like delegation works better when memory is enabled.</span><br><span class="line"></span><br><span class="line">Here’s the final output from GPT-4o. The result looks pretty nice now. We definitely can use LLMs to automate documentation.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">So, the first task has been solved!</span><br><span class="line"></span><br><span class="line">I used the same script to generate documentation for the ecommerce_db.sessions table as well. It will be handy for our next task. So, let’s not waste any time and move on.</span><br><span class="line"></span><br><span class="line">Use case: answering questions</span><br><span class="line">Our next task is answering questions based on the documentation since it’s common for many data analysts (and other specialists).</span><br><span class="line"></span><br><span class="line">We will start simple and will create just two agents:</span><br><span class="line"></span><br><span class="line">The documentation support specialist will be answering questions based on the docs,</span><br><span class="line">The support QA agent will review the answer before sharing it with the customer.</span><br><span class="line"></span><br><span class="line">We will need to empower the documentation specialist with a couple of tools that will allow them to see all the files stored in the directory and read the files. It’s pretty straightforward since CrewAI has implemented such tools.</span><br><span class="line"></span><br><span class="line">from crewai_tools import DirectoryReadTool, FileReadTool</span><br><span class="line"></span><br><span class="line">documentation_directory_tool = DirectoryReadTool(</span><br><span class="line">    directory = &#x27;~/crewai_project/ecommerce_documentation&#x27;)</span><br><span class="line"></span><br><span class="line">base_file_read_tool = FileReadTool()</span><br><span class="line">However, since Llama 3 keeps struggling with quotes when calling tools, I had to create a custom tool on top of the FileReaderTool to overcome this issue.</span><br><span class="line"></span><br><span class="line">from crewai_tools import BaseTool</span><br><span class="line"></span><br><span class="line">class FileReadToolUPD(BaseTool):</span><br><span class="line">    name: str = &quot;Read a file&#x27;s content&quot;</span><br><span class="line">    description: str = &quot;A tool that can be used to read a file&#x27;s content.&quot;</span><br><span class="line"></span><br><span class="line">    def _run(self, file_path: str) -&gt; str:</span><br><span class="line">        # Implementation goes here</span><br><span class="line">        return base_file_read_tool._run(file_path = file_path.strip(&#x27;&quot;&#x27;).strip(&quot;&#x27;&quot;))</span><br><span class="line">        </span><br><span class="line">file_read_tool = FileReadToolUPD()</span><br><span class="line">Next, as we did before, we need to create agents, tasks and crew.</span><br><span class="line"></span><br><span class="line">data_support_agent = Agent(</span><br><span class="line">  role = &quot;Senior Data Support Agent&quot;,</span><br><span class="line">  goal = &quot;Be the most helpful support for you colleagues&quot;,</span><br><span class="line">  backstory = &#x27;&#x27;&#x27;You work as a support for data-related questions </span><br><span class="line">  in the company. </span><br><span class="line">  Even though you&#x27;re a big expert in our data warehouse, you double check </span><br><span class="line">  all the facts in documentation. </span><br><span class="line">  Our documentation is absolutely up-to-date, so you can fully rely on it </span><br><span class="line">  when answering questions (you don&#x27;t need to check the actual data </span><br><span class="line">  in database).</span><br><span class="line">  Your work is very important for the team success. However, remember </span><br><span class="line">  that examples of table rows don&#x27;t show all the possible values. </span><br><span class="line">  You need to ensure that you provide the best possible support: answering </span><br><span class="line">  all the questions, making no assumptions and sharing only the factual data.</span><br><span class="line">  Be creative try your best to solve the customer problem. </span><br><span class="line">  &#x27;&#x27;&#x27;,</span><br><span class="line">  allow_delegation = False,</span><br><span class="line">  verbose = True</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">qa_support_agent = Agent(</span><br><span class="line">  role = &quot;Support Quality Assurance Agent&quot;,</span><br><span class="line">  goal = &quot;&quot;&quot;Ensure the highest quality of the answers we provide </span><br><span class="line">  to the customers&quot;&quot;&quot;,</span><br><span class="line">  backstory = &#x27;&#x27;&#x27;You work as a Quality Assurance specialist, checking the work </span><br><span class="line">  from support agents and ensuring that it&#x27;s inline with our highest standards.</span><br><span class="line">  You need to check that the agent provides the full complete answers </span><br><span class="line">  and make no assumptions. </span><br><span class="line">  Also, you need to make sure that the documentation addresses all </span><br><span class="line">  the questions and is easy to understand.</span><br><span class="line">  &#x27;&#x27;&#x27;,</span><br><span class="line">  allow_delegation = False,</span><br><span class="line">  verbose = True</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">draft_data_answer = Task(</span><br><span class="line">  description = &#x27;&#x27;&#x27;Very important customer &#123;customer&#125; reached out to you </span><br><span class="line">  with the following question:</span><br><span class="line"></span><br><span class="line">  &#123;question&#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  Your task is to provide the best answer to all the points in the question </span><br><span class="line">  using all available information and not making any assumprions. </span><br><span class="line">  If you don&#x27;t have enough information to answer the question, just say </span><br><span class="line">  that you don&#x27;t know.&#x27;&#x27;&#x27;,</span><br><span class="line">  expected_output = &#x27;&#x27;&#x27;The detailed informative answer to the customer&#x27;s </span><br><span class="line">  question that addresses all the point mentioned. </span><br><span class="line">  Make sure that answer is complete and stict to facts </span><br><span class="line">  (without any additional information not based on the factual data)&#x27;&#x27;&#x27;,</span><br><span class="line">  tools = [documentation_directory_tool, file_read_tool], </span><br><span class="line">  agent = data_support_agent</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">answer_review = Task(</span><br><span class="line">  description = &#x27;&#x27;&#x27;</span><br><span class="line">  Review the draft answer provided by the support agent.</span><br><span class="line">  Ensure that the it fully answers all the questions mentioned </span><br><span class="line">  in the initial inquiry. </span><br><span class="line">  Make sure that the answer is consistent and doesn&#x27;t include any assumptions.</span><br><span class="line">  &#x27;&#x27;&#x27;,</span><br><span class="line">  expected_output = &#x27;&#x27;&#x27;</span><br><span class="line">  The final version of the answer in markdown format that can be shared </span><br><span class="line">  with the customer. </span><br><span class="line">  The answer should fully address all the questions, be consistent </span><br><span class="line">  and follow our professional but informal tone of voice. </span><br><span class="line">  We are very chill and friendly company, so don&#x27;t forget to include </span><br><span class="line">  all the polite phrases.</span><br><span class="line">  &#x27;&#x27;&#x27;,</span><br><span class="line">  tools = [], </span><br><span class="line">  agent = qa_support_agent</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">qna_crew = Crew(</span><br><span class="line">  agents = [data_support_agent, qa_support_agent],</span><br><span class="line">  tasks = [draft_data_answer,  answer_review],</span><br><span class="line">  verbose = 2,</span><br><span class="line">  memory = False # don&#x27;t work with Llama</span><br><span class="line">)</span><br><span class="line">Let’s see how it works in practice.</span><br><span class="line"></span><br><span class="line">result = qna_crew.kickoff(</span><br><span class="line">  &#123;&#x27;customer&#x27;: &quot;Max&quot;, </span><br><span class="line">   &#x27;question&#x27;: &quot;&quot;&quot;Hey team, I hope you&#x27;re doing well. I need to find </span><br><span class="line">    the numbers before our CEO presentation tomorrow, so I will really </span><br><span class="line">    appreciate your help.</span><br><span class="line">    I need to calculate the number of sessions from our Windows users in 2023. I&#x27;ve tried to find the table with such data in our data warehouse, but wasn&#x27;t able to. </span><br><span class="line">    Do you have any ideas whether we store the needed data somewhere, </span><br><span class="line">    so that I can query it? &quot;&quot;&quot;</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line">We’ve got a polite, practical and helpful answer in return. That’s really great.</span><br><span class="line"></span><br><span class="line">**Hello Max,**</span><br><span class="line"></span><br><span class="line">Thank you for reaching out with your question! I&#x27;m happy to help you </span><br><span class="line">find the number of sessions from Windows users in 2023. </span><br><span class="line">After reviewing our documentation, I found that we do store data </span><br><span class="line">related to sessions and users in our ecommerce database, specifically in </span><br><span class="line">the `ecommerce_db.sessions` table.</span><br><span class="line"></span><br><span class="line">To answer your question, I can provide you with a step-by-step guide </span><br><span class="line">on how to query this table using SQL. First, you can use the `session_id` </span><br><span class="line">column along with the `os` column filtering for &quot;Windows&quot; and </span><br><span class="line">the `action_date` column filtering for dates in 2023. </span><br><span class="line">Then, you can group the results by `os` using the `GROUP BY` clause </span><br><span class="line">to count the number of sessions that meet these conditions.</span><br><span class="line"></span><br><span class="line">Here&#x27;s a sample SQL query that should give you the desired output:</span><br><span class="line"></span><br><span class="line">SELECT COUNT(*) </span><br><span class="line">FROM ecommerce_db.sessions </span><br><span class="line">WHERE os = &#x27;Windows&#x27; </span><br><span class="line">AND action_date BETWEEN &#x27;2023-01-01&#x27; AND &#x27;2023-12-31&#x27;</span><br><span class="line">GROUP BY os;</span><br><span class="line"></span><br><span class="line">This query will return the total number of sessions from Windows </span><br><span class="line">users in 2023. I hope this helps! If you have any further questions or </span><br><span class="line">need more assistance, please don&#x27;t hesitate to ask.</span><br><span class="line">Let’s complicate the task a bit. Suppose we can get not only questions about our data but also about our tool (ClickHouse). So, we will have another agent in the crew — ClickHouse Guru. To give our CH agent some knowledge, I will share a documentation website with it.</span><br><span class="line"></span><br><span class="line">from crewai_tools import ScrapeWebsiteTool, WebsiteSearchTool</span><br><span class="line">ch_documenation_tool = ScrapeWebsiteTool(</span><br><span class="line">  &#x27;https://clickhouse.com/docs/en/guides/creating-tables&#x27;)</span><br><span class="line">If you need to work with a lengthy document, you might try using RAG (Retrieval Augmented generation) — WebsiteSearchTool. It will calculate embeddings and store them locally in ChromaDB. In our case, we will stick to a simple website scraper tool.</span><br><span class="line"></span><br><span class="line">Now that we have two subject matter experts, we need to decide who will be working on the questions. So, it’s time to use a hierarchical process and add a manager to orchestrate all the tasks.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CrewAI provides the manager implementation, so we only need to specify the LLM model. I’ve picked the GPT-4o.</span><br><span class="line"></span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">from crewai import Process</span><br><span class="line"></span><br><span class="line">complext_qna_crew = Crew(</span><br><span class="line">  agents = [ch_support_agent, data_support_agent, qa_support_agent],</span><br><span class="line">  tasks = [draft_ch_answer, draft_data_answer, answer_review],</span><br><span class="line">  verbose = 2,</span><br><span class="line">  manager_llm = ChatOpenAI(model=&#x27;gpt-4o&#x27;, temperature=0),  </span><br><span class="line">  process = Process.hierarchical,  </span><br><span class="line">  memory = False </span><br><span class="line">)</span><br><span class="line">At this point, I had to switch from Llama 3 to OpenAI models again to run a hierarchical process since it hasn’t worked for me with Llama (similar to this issue).</span><br><span class="line"></span><br><span class="line">Now, we can try our new crew with different types of questions (either related to our data or ClickHouse database).</span><br><span class="line"></span><br><span class="line">ch_result = complext_qna_crew.kickoff(</span><br><span class="line">  &#123;&#x27;customer&#x27;: &quot;Maria&quot;, </span><br><span class="line">   &#x27;question&#x27;: &quot;&quot;&quot;Good morning, team. I&#x27;m using ClickHouse to calculate </span><br><span class="line">   the number of customers. </span><br><span class="line">   Could you please remind whether there&#x27;s an option to add totals </span><br><span class="line">   in ClickHouse?&quot;&quot;&quot;</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">doc_result = complext_qna_crew.kickoff(</span><br><span class="line">  &#123;&#x27;customer&#x27;: &quot;Max&quot;, </span><br><span class="line">   &#x27;question&#x27;: &quot;&quot;&quot;Hey team, I hope you&#x27;re doing well. I need to find </span><br><span class="line">    the numbers before our CEO presentation tomorrow, so I will really </span><br><span class="line">    appreciate your help.</span><br><span class="line">    I need to calculate the number of sessions from our Windows users </span><br><span class="line">    in 2023. I&#x27;ve tried to find the table with such data </span><br><span class="line">    in our data warehouse, but wasn&#x27;t able to. </span><br><span class="line">    Do you have any ideas whether we store the needed data somewhere, </span><br><span class="line">    so that I can query it. &quot;&quot;&quot;</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line">If we look at the final answers and logs (I’ve omitted them here since they are quite lengthy, but you can find them and full logs on GitHub), we will see that the manager was able to orchestrate correctly and delegate tasks to co-workers with relevant knowledge to address the customer&#x27;s question. For the first (ClickHouse-related) question, we got a detailed answer with examples and possible implications of using WITH TOTALS functionality. For the data-related question, models returned roughly the same information as we’ve seen above.</span><br><span class="line"></span><br><span class="line">So, we’ve built a crew that can answer various types of questions based on the documentation, whether from a local file or a website. I think it’s an excellent result.</span><br><span class="line"></span><br><span class="line">You can find all the code on GitHub.</span><br><span class="line"></span><br><span class="line">Summary</span><br><span class="line">In this article, we’ve explored using the CrewAI multi-agent framework to create a solution for writing documentation based on tables and answering related questions.</span><br><span class="line"></span><br><span class="line">Given the extensive functionality we’ve utilised, it’s time to summarise the strengths and weaknesses of this framework.</span><br><span class="line"></span><br><span class="line">Overall, I find CrewAI to be an incredibly useful framework for multi-agent systems:</span><br><span class="line"></span><br><span class="line">It’s straightforward, and you can build your first prototype quickly.</span><br><span class="line">Its flexibility allows to solve quite sophisticated business problems.</span><br><span class="line">It encourages good practices like role-playing.</span><br><span class="line">It provides many handy tools out of the box, such as RAG and a website parser.</span><br><span class="line">The support of different types of memory enhances the agents’ collaboration.</span><br><span class="line">Built-in guardrails help prevent agents from getting stuck in repetitive loops.</span><br><span class="line">However, there are areas that could be improved:</span><br><span class="line"></span><br><span class="line">While the framework is simple and easy to use, it’s not very customisable. For instance, you currently can’t create your own LLM manager to orchestrate the processes.</span><br><span class="line">Sometimes, it’s quite challenging to get the full detailed information from the documentation. For example, it’s clear that CrewAI implemented some guardrails to prevent repetitive function calls, but the documentation doesn’t fully explain how it works.</span><br><span class="line">Another improvement area is transparency. I like to understand how frameworks work under the hood. For example, in Langchain, you can use langchain.debug = True to see all the LLM calls. However, I haven’t figured out how to get the same level of detail with CrewAI.</span><br><span class="line">The full support for the local models would be a great addition, as the current implementation either lacks some features or is difficult to get working properly.</span><br><span class="line">The domain and tools for LLMs are evolving rapidly, so I’m hopeful that we’ll see a lot of progress in the near future.</span><br><span class="line"></span><br><span class="line">Thank you a lot for reading this article. I hope this article was insightful for you. If you have any follow-up questions or comments, please leave them in the comments section.</span><br><span class="line"></span><br><span class="line">Reference</span><br><span class="line">This article is inspired by the “Multi AI Agent Systems with CrewAI” short course from DeepLearning.AI.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</details></li>
</ul>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-06-17</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/06/17/2024-6-17-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 6월 17일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/06/18/2024-6-18-AI-NEWS/" title="2024년 6월 18일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/06/13/2024-6-13-AI-NEWS/" title="2024년 6월 13일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>