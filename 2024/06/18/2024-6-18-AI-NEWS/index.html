<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 6월 18일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="SummaryOpenAI에서는 박사급 AI 연구원의 초봉이 업계 최고 수준인 86만5000달러에 달하는 것으로 나타났습니다. 또한, Apple은 4M(4M: Massively Multimodal Masked Modeling)이라는 새로운 멀티모달 학습 프레임워크를 발표"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 6월 18일 AI 소식</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>OpenAI에서는 박사급 AI 연구원의 초봉이 업계 최고 수준인 86만5000달러에 달하는 것으로 나타났습니다. 또한, Apple은 4M(4M: Massively Multimodal Masked Modeling)이라는 새로운 멀티모달 학습 프레임워크를 발표했습니다. DeepSeek는 DeepSeek-Coder-V2라는 새로운 코드 언어 모델을 공개하였으며, Microsoft는 AutoGen Studio라는 멀티 에이전트 워크플로우 구축을 위한 저코드 인터페이스를 소개했습니다. 마지막으로 Google은 비디오 생성 모델과의 연동을 통해 동영상을 위한 오디오 생성 기술을 발표했습니다.</p>
<h2 id="4M-Massively-Multimodal-Masked-Modeling"><a href="#4M-Massively-Multimodal-Masked-Modeling" class="headerlink" title="4M: Massively Multimodal Masked Modeling,"></a>4M: Massively Multimodal Masked Modeling,</h2><h3 id="다중-모달-마스크-모델링"><a href="#다중-모달-마스크-모델링" class="headerlink" title="다중 모달 마스크 모델링"></a>다중 모달 마스크 모델링</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.06647">링크</a>, 2024-06-17,<br>Apple</p>
<ul>
<li>Apple과 EPFL은 4M이라는 새로운 멀티모달 학습 프레임워크를 발표</li>
<li>4M-7과 4M-21 모델 체크포인트 공개</li>
<li>모델 체크포인트는 RGB, Edge, Geometric, Text, Semantic, Feature map 등의 모달리티 포함</li>
<li>Apache 2.0 라이선스로 코드와 가중치 배포</li>
<li>단일 Transformer 인코더-디코더 모델을 사용한 학습</li>
<li>다양한 비전 작업을 수행할 수 있는 다재다능한 모델 구현</li>
</ul>
<h2 id="DeepSeek-Coder-V2-Breaking-the-Barrier-of-Closed-Source-Models-in-Code-Intelligence"><a href="#DeepSeek-Coder-V2-Breaking-the-Barrier-of-Closed-Source-Models-in-Code-Intelligence" class="headerlink" title="DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence,"></a>DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence,</h2><h3 id="코드-인텔리전스에서-폐쇄형-모델의-장벽을-허물다"><a href="#코드-인텔리전스에서-폐쇄형-모델의-장벽을-허물다" class="headerlink" title="코드 인텔리전스에서 폐쇄형 모델의 장벽을 허물다"></a>코드 인텔리전스에서 폐쇄형 모델의 장벽을 허물다</h3><p><a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/paper.pdf">링크</a>, 2024-06-17,<br>DeepSeek</p>
<ul>
<li>DeepSeek-Coder-V2는 GPT-4 Turbo와 유사한 성능을 자랑하는 오픈소스 코드 언어 모델</li>
<li>DeepSeek-Coder-V2-Base에서 6조 개의 토큰을 추가로 학습하여 성능 향상</li>
<li>코딩 및 수학적 추론 능력 대폭 강화</li>
<li>지원 프로그래밍 언어를 86개에서 338개로 확장</li>
<li>컨텍스트 길이를 16K에서 128K로 확장</li>
<li>연구 및 상업적 사용을 위한 허가 라이선스 포함</li>
</ul>
<h2 id="“오픈AI-박사급-연구원-초봉-11억”…급여-순위-공개"><a href="#“오픈AI-박사급-연구원-초봉-11억”…급여-순위-공개" class="headerlink" title="“오픈AI, 박사급 연구원 초봉 11억”…급여 순위 공개,"></a>“오픈AI, 박사급 연구원 초봉 11억”…급여 순위 공개,</h2><h3 id="오픈AI의-박사급-연구원-초봉-11억-공개"><a href="#오픈AI의-박사급-연구원-초봉-11억-공개" class="headerlink" title="오픈AI의 박사급 연구원 초봉 11억 공개"></a>오픈AI의 박사급 연구원 초봉 11억 공개</h3><p><a target="_blank" rel="noopener" href="https://www.aitimes.com/news/articleViewAmp.html?idxno=156265">링크</a>, 2024-01-03,<br>로라</p>
<ul>
<li>오픈AI의 박사급 AI 연구원 초봉이 86만5000달러로 업계 최고 수준</li>
<li>앤트로픽이 85만5000달러로 두 번째로 높은 초봉 제공</li>
<li>인플렉션 AI, 테슬라, 아마존, 구글 브레인 등의 기업도 높은 초봉 제공</li>
<li>AI 기술 수요가 공급을 초과하여 초봉이 높아짐</li>
<li>박사 학위 논문 출판 기록이 중요한 평가 요소로 작용</li>
</ul>
<h2 id="Generating-audio-for-video"><a href="#Generating-audio-for-video" class="headerlink" title="Generating audio for video,"></a>Generating audio for video,</h2><h3 id="비디오를-위한-오디오-생성"><a href="#비디오를-위한-오디오-생성" class="headerlink" title="비디오를 위한 오디오 생성"></a>비디오를 위한 오디오 생성</h3><p><a target="_blank" rel="noopener" href="https://deepmind.google/discover/blog/generating-audio-for-video/">링크</a>, 2024-06-17,<br>Google Research</p>
<ul>
<li>Google은 비디오 픽셀과 텍스트 프롬프트를 사용하여 풍부한 사운드트랙을 생성하는 V2A 기술 발표</li>
<li>V2A는 비디오 생성 모델과 결합하여 영화의 사운드트랙, 현실적인 사운드 효과 또는 대화를 생성 가능</li>
<li>다양한 비디오 자료에 사운드트랙 생성 가능</li>
<li>오디오 출력의 품질을 높이기 위해 추가 정보로 훈련 과정 개선</li>
</ul>
<h2 id="Introducing-AutoGen-Studio-A-low-code-interface-for-building-multi-agent-workflows"><a href="#Introducing-AutoGen-Studio-A-low-code-interface-for-building-multi-agent-workflows" class="headerlink" title="Introducing AutoGen Studio: A low-code interface for building multi-agent workflows,"></a>Introducing AutoGen Studio: A low-code interface for building multi-agent workflows,</h2><h3 id="멀티-에이전트-워크플로우-구축을-위한-저코드-인터페이스-AutoGen-Studio-소개"><a href="#멀티-에이전트-워크플로우-구축을-위한-저코드-인터페이스-AutoGen-Studio-소개" class="headerlink" title="멀티 에이전트 워크플로우 구축을 위한 저코드 인터페이스 AutoGen Studio 소개"></a>멀티 에이전트 워크플로우 구축을 위한 저코드 인터페이스 AutoGen Studio 소개</h3><p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/blog/introducing-autogen-studio-a-low-code-interface-for-building-multi-agent-workflows/">링크</a>, 2024-06-17,<br>Microsoft Research</p>
<ul>
<li>AutoGen Studio는 멀티 에이전트 애플리케이션을 구축하기 위한 저코드 인터페이스 제공</li>
<li>사용자는 간단한 그래픽 인터페이스를 통해 에이전트를 구성하고 워크플로우 작성 가능</li>
<li>에이전트 워크플로우를 테스트하고 디버그할 수 있는 기능 제공</li>
<li>워크플로우를 JSON 파일로 내보내어 다른 애플리케이션에서 사용 가능</li>
</ul>
<h2 id="Pre-translation-vs-direct-inference-in-multilingual-LLM-applications"><a href="#Pre-translation-vs-direct-inference-in-multilingual-LLM-applications" class="headerlink" title="Pre-translation vs. direct inference in multilingual LLM applications,"></a>Pre-translation vs. direct inference in multilingual LLM applications,</h2><h3 id="다국어-LLM-애플리케이션에서-사전-번역-대-직접-추론"><a href="#다국어-LLM-애플리케이션에서-사전-번역-대-직접-추론" class="headerlink" title="다국어 LLM 애플리케이션에서 사전 번역 대 직접 추론"></a>다국어 LLM 애플리케이션에서 사전 번역 대 직접 추론</h3><p><a target="_blank" rel="noopener" href="https://research.google/blog/pre-translation-vs-direct-inference-in-multilingual-llm-applications/">링크</a>, 2024-06-14,<br>Google Research</p>
<ul>
<li>PaLM2는 다국어 작업에서 사전 번역 없이 직접 추론이 더 나은 성능을 보임</li>
<li>108개 언어 중 94개 언어에서 직접 추론이 사전 번역보다 우수한 결과</li>
<li>다국어 LLM의 효율성과 효과성을 향상시키기 위한 연구 지속</li>
</ul>
<h2 id="Introducing-Gen-3-Alpha"><a href="#Introducing-Gen-3-Alpha" class="headerlink" title="Introducing Gen-3 Alpha,"></a>Introducing Gen-3 Alpha,</h2><h3 id="Gen-3-Alpha-소개"><a href="#Gen-3-Alpha-소개" class="headerlink" title="Gen-3 Alpha 소개"></a>Gen-3 Alpha 소개</h3><p><a target="_blank" rel="noopener" href="https://runwayml.com/blog/introducing-gen-3-alpha/">링크</a>, 2024-06-17,<br>Runway</p>
<ul>
<li>Gen-3 Alpha는 높은 충실도와 일관성을 갖춘 비디오 생성 모델</li>
<li>텍스트에서 비디오, 이미지에서 비디오, 텍스트에서 이미지 도구 제공</li>
<li>사용자 정의 버전 제공, 예술적 및 내러티브 요구사항에 맞춘 모델 생성 가능</li>
<li>새로운 인프라를 통해 대규모 멀티모달 학습 가능</li>
</ul>
<details>
  <summary>Sources</summary>
This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:

<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h2 id="Title"><a href="#Title" class="headerlink" title="Title,"></a>Title,</h2><h3 id="한글제목"><a href="#한글제목" class="headerlink" title="한글제목"></a>한글제목</h3><p><a href="link">링크</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2,  (개조식 문체 사용)<br>…</li>
<li>detailed summary N,  (개조식 문체 사용)</li>
</ul>
<h2 id="Title-1"><a href="#Title-1" class="headerlink" title="Title,"></a>Title,</h2><h3 id="한글제목-1"><a href="#한글제목-1" class="headerlink" title="한글제목"></a>한글제목</h3><p><a href="link">링크</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2,  (개조식 문체 사용)<br>…</li>
<li>detailed summary N,  (개조식 문체 사용)<br>…<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2312.06647</span><br><span class="line"></span><br><span class="line">Apple dropped 4M: Massively Multilingual Masked Modeling! 🔥</span><br><span class="line">Is this what powers the on-device vision-text backbone?</span><br><span class="line">&gt; A framework for training any-to-any multimodal foundational models. Training/ Finetuning/ Inference.</span><br><span class="line">&gt; Release 4M-7 and 4M-21 model checkpoints (trained across tens of tasks and modalities).</span><br><span class="line">&gt; 198M, 705M and 2.8B model checkpoints.</span><br><span class="line">&gt; Release specialised Text to Image and image super-resolution specialist model checkpoints.</span><br><span class="line">&gt; Apache 2.0 license for code and weights!</span><br><span class="line">&gt; A unified transformer encoder-decoder model is trained on a masked modelling objective.</span><br><span class="line">&gt; Spread across RGB, Edge, Geometric, Text, Semantic, Feature map, and more modalities.</span><br><span class="line">&gt; Model checkpoints on the Hub 🤗</span><br><span class="line"></span><br><span class="line">Kudos to EPFL and Apple. I especially liked the any-to-any generation bit paired with multimodal chained generation! ⚡</span><br><span class="line"></span><br><span class="line">4M: Massively Multimodal Masked Modeling</span><br><span class="line">David Mizrahi, Roman Bachmann, Oğuzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, Amir Zamir</span><br><span class="line">Current machine learning models for vision are often highly specialized and limited to a single modality and task. In contrast, recent large language models exhibit a wide range of capabilities, hinting at a possibility for similarly versatile models in computer vision. In this paper, we take a step in this direction and propose a multimodal training scheme called 4M. It consists of training a single unified Transformer encoder-decoder using a masked modeling objective across a wide range of input/output modalities - including text, images, geometric, and semantic modalities, as well as neural network feature maps. 4M achieves scalability by unifying the representation space of all modalities through mapping them into discrete tokens and performing multimodal masked modeling on a small randomized subset of tokens.</span><br><span class="line">4M leads to models that exhibit several key capabilities: (1) they can perform a diverse set of vision tasks out of the box, (2) they excel when fine-tuned for unseen downstream tasks or new input modalities, and (3) they can function as a generative model that can be conditioned on arbitrary modalities, enabling a wide variety of expressive multimodal editing capabilities with remarkable flexibility.</span><br><span class="line">Through experimental analyses, we demonstrate the potential of 4M for training versatile and scalable foundation models for vision tasks, setting the stage for further exploration in multimodal learning for vision and other domains.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/paper.pdf</span><br><span class="line">DeepSeek-V2</span><br><span class="line">Homepage Chat Hugging Face</span><br><span class="line">Discord Wechat Twitter Follow</span><br><span class="line">Code License Model License</span><br><span class="line">Model Download | Evaluation Results | API Platform | How to Use | License | Citation</span><br><span class="line"></span><br><span class="line">Paper Link👁️</span><br><span class="line"></span><br><span class="line">DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence</span><br><span class="line">1. Introduction</span><br><span class="line">We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from DeepSeek-Coder-V2-Base with 6 trillion tokens sourced from a high-quality and multi-source corpus. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-Coder-V2-Base, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K.</span><br><span class="line"></span><br><span class="line">90.2% on HumanEval and 75.7% on MATH. These are higher numbers than GPT-4-Turbo-0409 according to their technical report.</span><br><span class="line">More:</span><br><span class="line">&gt; includes 16B and 236B parameter models</span><br><span class="line">&gt; further pretrained from DeepSeek-V2 checkpoint</span><br><span class="line">&gt; uses an additional 6 trillion tokens</span><br><span class="line">&gt; expands to 338 programming languages</span><br><span class="line">&gt; context length extended from 16K to 128K</span><br><span class="line">&gt; permissive license allows for both research and unrestricted commercial use</span><br><span class="line">Still not quite there for instruction-following capabilities as compared to GPT-4 Turbo but has huge potential to improve.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www-aitimes-com.cdn.ampproject.org/c/s/www.aitimes.com/news/articleViewAmp.html?idxno=156265</span><br><span class="line">”오픈AI, 박사급 연구원 초봉 11억”...급여 순위 공개</span><br><span class="line">급여 협상 서비스 기업 로라 집계</span><br><span class="line">2024-01-03     박찬 기자</span><br><span class="line">박사급 AI 연구원의 초기 보상 제안과 최종 보상 제안 비교(사진=로라)</span><br><span class="line">박사급 AI 연구원의 초기 보상 제안과 최종 보상 제안 비교(사진=로라)</span><br><span class="line">오픈AI의 박사급 인공지능(AI) 연구원 초봉이 86만5000달러(약 11억3000만원)로 업계 최고 수준인 것으로 나타났다. 최고급 스타트업과 빅테크의 초봉도 9억~10억원에 달하는 것으로 알려졌다. 그만큼 AI 연구원이 부족하다는 설명이다.</span><br><span class="line"></span><br><span class="line">리드라이트는 2일(현지시간) 급여 협상 서비스 기업인 로라의 집계를 인용, 신규 박사급 AI 연구원을 채용한 600여개 기업 중 오픈AI와 앤트로픽이 각각 86만5000달러와 85만5000달러(약 11억2000만원)로 가장 높은 초봉을 제공했다고 보도했다. 초봉에는 기본급과 보너스, 주식 등이 포함된다. </span><br><span class="line"></span><br><span class="line">이에 따르면 오픈AI와 앤트로픽의 라이벌로 꼽히는 인플렉션 AI가 82만5000달러(약 10억8000만원)로 3위를 차지했다.</span><br><span class="line"></span><br><span class="line">이어 테슬라 78만달러(약 10억2000만원), 아마존 71만9000달러(약 9억4000만원), 구글 브레인 69만5000달러(약 9억1000만원) 등으로 빅테크보다 전문 스타트업의 인재 확보 경쟁이 더 치열한 것으로 나타났다.</span><br><span class="line"></span><br><span class="line">그러나 초기 제안과 최종 제안 사이의 협상폭은 구글 리서치가 평균 77%로 가장 높았으며, 마이크로소프트 리서치, 블룸버그 AI, IBM 리서치, 틱톡 등의 순이었다. 구글 리서치의 한 연구원은 초기 제안으로 21만6000달러(약 2억8000만원)를 받았으나, 협상을 통해 243% 증가한 최종 52만6000달러(약 6억9000만원)의 연봉을 받게 됐다.</span><br><span class="line"></span><br><span class="line">박사급 AI 연구원의 초봉 순위(사진=로라) </span><br><span class="line">박사급 AI 연구원의 초봉 순위(사진=로라) </span><br><span class="line">이처럼 박사급 AI 연구원의 연봉 수준이 높은 이유는 AI 기술에 대한 전 세계 수요가 실제 공급보다 훨씬 더 크기 때문이다.</span><br><span class="line"></span><br><span class="line">톨비 서베이의 설문조사에 따르면 2021년에는 컴퓨팅 연구 분야에서 수여된 박사 학위가 1691명에 불과했다. 미국에서만 3만35000명의 컴퓨터 및 정보 연구원이 필요하며 수요는 연간 21% 증가하고 있다. 즉 매년 필요한 연구원보다 일자리가  5000개 이상 많다는 것을 의미한다. </span><br><span class="line"></span><br><span class="line">현재 가장 수요가 높은 분야는 컴퓨터 비전, 로봇공학, 자연어 처리(NLP), 생물학, 신경과학 등에 AI를 적용하는 분야다. &#x27;챗GPT&#x27;가 도입되면서 대형언어모델(LLM)에 대한 전문성은 최고 인기 기술이 됐다. </span><br><span class="line"></span><br><span class="line">리드라이트는 AI 연구원에게는 검증된 연구 능력이 무엇보다 중요하다고 지적했다. 이를 입증하는 것 중 하나를 논문 출판 기록으로 꼽았다.</span><br><span class="line"></span><br><span class="line">업계 최고 수준의 연구원들은 박사 학위 논문만으로 최대 2000번의 인용과 &#x27;H-지수(H-index) 10&#x27;을 보유하게 된다고 전했다. H-지수 10은 논문 인용횟수가 10이 넘는 논문이 적어도 10편이 된다는 것을 의미한다. </span><br><span class="line"></span><br><span class="line">이 정도 능력이면 높은 직위와 최고 보상을 요구할 수 있는 최고 연구원급 영향력을 가진다는 설명이다.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://deepmind.google/discover/blog/generating-audio-for-video/</span><br><span class="line">google research</span><br><span class="line"></span><br><span class="line">RESEARCH</span><br><span class="line"></span><br><span class="line">Generating audio for video</span><br><span class="line">Published</span><br><span class="line">17 JUNE 2024</span><br><span class="line">Authors</span><br><span class="line">Generative Media team</span><br><span class="line"></span><br><span class="line">Share</span><br><span class="line"></span><br><span class="line">Video-to-audio research uses video pixels and text prompts to generate rich soundtracks</span><br><span class="line"></span><br><span class="line">Video generation models are advancing at an incredible pace, but many current systems can only generate silent output. One of the next major steps toward bringing generated movies to life is creating soundtracks for these silent videos.</span><br><span class="line"></span><br><span class="line">Today, we&#x27;re sharing progress on our video-to-audio (V2A) technology, which makes synchronized audiovisual generation possible. V2A combines video pixels with natural language text prompts to generate rich soundscapes for the on-screen action.</span><br><span class="line"></span><br><span class="line">Our V2A technology is pairable with video generation models like Veo to create shots with a dramatic score, realistic sound effects or dialogue that matches the characters and tone of a video.</span><br><span class="line"></span><br><span class="line">It can also generate soundtracks for a range of traditional footage, including archival material, silent films and more — opening a wider range of creative opportunities.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:13</span><br><span class="line">Prompt for audio: Cinematic, thriller, horror film, music, tension, ambience, footsteps on concrete</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:08</span><br><span class="line">Prompt for audio: Cute baby dinosaur chirps, jungle ambience, egg cracking</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:09</span><br><span class="line">Prompt for audio: jellyfish pulsating under water, marine life, ocean</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:09</span><br><span class="line">Prompt for audio: A drummer on a stage at a concert surrounded by flashing lights and a cheering crowd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:12</span><br><span class="line">Prompt for audio: cars skidding, car engine throttling, angelic electronic music</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:08</span><br><span class="line">Prompt for audio: a slow mellow harmonica plays as the sun goes down on the prairie</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:07</span><br><span class="line">Prompt for audio: Wolf howling at the moon</span><br><span class="line"></span><br><span class="line">Enhanced creative control</span><br><span class="line">Importantly, V2A can generate an unlimited number of soundtracks for any video input. Optionally, a ‘positive prompt’ can be defined to guide the generated output toward desired sounds, or a ‘negative prompt’ to guide it away from undesired sounds.</span><br><span class="line"></span><br><span class="line">This flexibility gives users more control over V2A’s audio output, making it possible to rapidly experiment with different audio outputs and choose the best match.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:08</span><br><span class="line">Prompt for audio: A spaceship hurtles through the vastness of space, stars streaking past it, high speed, Sci-fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:08</span><br><span class="line">Prompt for audio: Ethereal cello atmosphere</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:08</span><br><span class="line">Prompt for audio: A spaceship hurtles through the vastness of space, stars streaking past it, high speed, Sci-fi</span><br><span class="line"></span><br><span class="line">How it works</span><br><span class="line">We experimented with autoregressive and diffusion approaches to discover the most scalable AI architecture, and the diffusion-based approach for audio generation gave the most realistic and compelling results for synchronizing video and audio information.</span><br><span class="line"></span><br><span class="line">Our V2A system starts by encoding video input into a compressed representation. Then, the diffusion model iteratively refines the audio from random noise. This process is guided by the visual input and natural language prompts given to generate synchronized, realistic audio that closely aligns with the prompt. Finally, the audio output is decoded, turned into an audio waveform and combined with the video data.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Diagram of our V2A system, taking video pixel and audio prompt input to generate an audio waveform synchronized to the underlying video. First, V2A encodes the video and audio prompt input and iteratively runs it through the diffusion model. Then it generates compressed audio, which is decoded into an audio waveform.</span><br><span class="line"></span><br><span class="line">To generate higher quality audio and add the ability to guide the model towards generating specific sounds, we added more information to the training process, including AI-generated annotations with detailed descriptions of sound and transcripts of spoken dialogue.</span><br><span class="line"></span><br><span class="line">By training on video, audio and the additional annotations, our technology learns to associate specific audio events with various visual scenes, while responding to the information provided in the annotations or transcripts.</span><br><span class="line"></span><br><span class="line">Further research underway</span><br><span class="line">Our research stands out from existing video-to-audio solutions because it can understand raw pixels and adding a text prompt is optional.</span><br><span class="line"></span><br><span class="line">Also, the system doesn&#x27;t need manual alignment of the generated sound with the video, which involves tediously adjusting different elements of sounds, visuals and timings.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:09</span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:09</span><br><span class="line">Still, there are a number of other limitations we’re trying to address and further research is underway.</span><br><span class="line"></span><br><span class="line">Since the quality of the audio output is dependent on the quality of the video input, artifacts or distortions in the video, which are outside the model’s training distribution, can lead to a noticeable drop in audio quality.</span><br><span class="line"></span><br><span class="line">We’re also improving lip synchronization for videos that involve speech. V2A attempts to generate speech from the input transcripts and synchronize it with characters&#x27; lip movements. But the paired video generation model may not be conditioned on transcripts. This creates a mismatch, often resulting in uncanny lip-syncing, as the video model doesn’t generate mouth movements that match the transcript.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:09</span><br><span class="line">Prompt for audio: Music, Transcript: “this turkey looks amazing, I’m so hungry”</span><br><span class="line"></span><br><span class="line">Our commitment to safety and transparency</span><br><span class="line">We’re committed to developing and deploying AI technologies responsibly. To make sure our V2A technology can have a positive impact on the creative community, we’re gathering diverse perspectives and insights from leading creators and filmmakers, and using this valuable feedback to inform our ongoing research and development.</span><br><span class="line"></span><br><span class="line">We’ve also incorporated our SynthID toolkit into our V2A research to watermark all AI-generated content to help safeguard against the potential for misuse of this technology.</span><br><span class="line"></span><br><span class="line">Before we consider opening access to it to the wider public, our V2A technology will undergo rigorous safety assessments and testing. Initial results are showing this technology will become a promising approach for bringing generated movies to life.</span><br><span class="line"></span><br><span class="line">Note: All examples are generated by our V2A technology, which is paired with Veo, our most capable generative video model.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.microsoft.com/en-us/research/blog/introducing-autogen-studio-a-low-code-interface-for-building-multi-agent-workflows/</span><br><span class="line">Microsoft Research Blog</span><br><span class="line">Introducing AutoGen Studio: A low-code interface for building multi-agent workflows</span><br><span class="line">Published June 17, 2024</span><br><span class="line"></span><br><span class="line">By Victor Dibia , Principal Research Software Engineer  Gagan Bansal , Senior Researcher  Jingya Chen , UX Designer  Suff Syed , Principal Design Director  Adam Fourney , Principal Researcher  Erkang (Eric) Zhu , Senior Researcher  Chi Wang , Principal Researcher  Saleema Amershi , Senior Principal Research Manager</span><br><span class="line"></span><br><span class="line">Share this page</span><br><span class="line"></span><br><span class="line">Share on Facebook</span><br><span class="line">Share on Twitter</span><br><span class="line">Share on LinkedIn</span><br><span class="line">Share on Reddit</span><br><span class="line">Subscribe to our RSS feed</span><br><span class="line">White icons representing (from left to right) agents (multi), workflow, tasks, and coding on a blue to purple to pink gradient background.</span><br><span class="line">Multi-agent approaches to AI applications, where multiple foundation model-based agents collaborate to solve problems, are emerging as a powerful paradigm for accomplishing increasingly complex tasks. In September 2023, we released AutoGen – a flexible and open-source Python-based framework for defining, configuring, and composing AI agents to drive multi-agent applications. Today, we are introducing AutoGen Studio (version 0.1.0) – a low-code interface for rapidly building, testing, and sharing multi-agent solutions. AutoGen Studio is built on AutoGen and inherits its features and functionalities, while providing a user-friendly and intuitive interface to create and customize agents, with little to no coding required.</span><br><span class="line"></span><br><span class="line">PROJECT</span><br><span class="line">AutoGen </span><br><span class="line">During the nine months since it was released, AutoGen(opens in new tab) has been widely adopted by researchers, developers, and enthusiasts who have created a variety of novel and exciting applications(opens in new tab) – from market research to interactive educational tools to data analysis pipelines in the medical domain.  With more than 290 community contributors on GitHub and 890,000 downloads of the Python package (as of May 2024), AutoGen continues to be a leading framework for building and researching multi-agent AI applications.</span><br><span class="line"></span><br><span class="line">AutoGen Studio user interface: PDF Book Gen Session</span><br><span class="line">A screenshot of the AutoGen Studio interface shows results when two agents are used to address the task, “Create a 4-page kids’ .pdf book with details and pictures about weather patterns in Seattle”.</span><br><span class="line">AutoGen Studio is the next step forward in enabling developers to advance the multi-agent paradigm. We want to make multi-agent solutions responsibly available to diverse audiences – from academic researchers to professional developers across industries – who want to build multi-agent applications to solve real-world problems. Imagine having access to agents that can automate your vacation planning and grocery shopping, manage your personal finances, help you accomplish your learning goals, or perform any other task you care about. How would you build such agents? What capabilities would you give them? How would you make them work together? How would you ensure they are working as intended?</span><br><span class="line"></span><br><span class="line">DOWNLOAD</span><br><span class="line">AutoGen Studio </span><br><span class="line">These questions motivated us to build AutoGen Studio. With AutoGen Studio, developers can rapidly build, test, deploy, and share agents and agent-teams (workflows), with the community. </span><br><span class="line"></span><br><span class="line">Note: AutoGen is primarily a developer tool to enable rapid prototyping and research. It is not a production ready tool. Please see the GitHub repository(opens in new tab) and documentation(opens in new tab) for instructions on how to get started.</span><br><span class="line"></span><br><span class="line">What can you do with AutoGen Studio right now?</span><br><span class="line">We built AutoGen Studio with the following goals in mind:  </span><br><span class="line"></span><br><span class="line">Lower the barrier to entry in building multi-agent applications  </span><br><span class="line">Facilitate rapid prototyping and testing of multi-agent solutions</span><br><span class="line">Cultivate expertise and community by allowing users to share and re-use this technology </span><br><span class="line">With AutoGen Studio’s early release (v 0.1.0), users can rapidly author agent workflows via a user interface, interactively test and debug agents, reuse artifacts, and deploy workflows.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The video above shows how users can create skills and models, attach them to agents, create agent workflows, test and deploy them in AutoGen Studio. All in a few clicks.</span><br><span class="line">Rapidly author agent workflows</span><br><span class="line">AutoGen Studio provides a “Build” section where users can choose from a library of pre-defined agents and compose them into teams (workflows) that can address tasks in minutes. Furthermore, users can customize agents and agent teams with foundation models, prompts, skills (python functions that accomplish a specific task e.g., fetching the weather from a weather provider), and workflows via a graphical user interface.  Workflows may be sequential (where agents act in a predefined sequential order) or autonomous chat (where the order in which agents act may be driven by a large language model, custom logic, all based on the state of the task).</span><br><span class="line"></span><br><span class="line">AutoGen Studio user interface: agent configuration</span><br><span class="line">In AutoGen Studio, agents can be configured via the user interface. Models and skills can be associated with agents, and agents can be composed into autonomous chat and sequential workflows.</span><br><span class="line">Debug and test agents</span><br><span class="line">AutoGen Studio allows developers to immediately test workflows on a variety of tasks and review resulting artifacts (such as images, code, and documents). Developers can also review the “inner monologue” of agent workflows as they address tasks, and view profiling information such as costs associated with the run (such as number of turns and number of tokens), and agent actions (such as whether tools were called and the outcomes of code execution).</span><br><span class="line"></span><br><span class="line">AutoGen Studio user interface: profile sample workflow</span><br><span class="line">AutoGen Studio user interface: sample workflow</span><br><span class="line">In AutoGen Studio, users can test workflows, see results, and view visualizations that profile agent actions (such as how often tools were used or code was executed).</span><br><span class="line">Artifact reuse and deployment</span><br><span class="line">Users can download the skills, agents, and workflow configurations they create as well as share and reuse these artifacts.  AutoGen Studio also offers a seamless process to export workflows and deploy them as application programming interfaces (APIs) that can be consumed in other applications deploying workflows as APIs.</span><br><span class="line"></span><br><span class="line">Specifically, workflows can be exported as JavaScript Object Notation (JSON) files and loaded into any python application, launched as an API endpoint from the command line or wrapped into a Dockerfile that can be deployed on cloud services like Azure Container Apps or Azure Web Apps.</span><br><span class="line"></span><br><span class="line">AutoGen Studio user interface: export workflow</span><br><span class="line">In AutoGen Studio, users can export agent workflows as a JSON configuration file and then reuse them in any python application, launch it as an API from the command line or deploy on a cloud service like Azure Container Apps and Azure Web Apps.</span><br><span class="line">MICROSOFT RESEARCH PODCAST</span><br><span class="line"></span><br><span class="line">Microsoft Research Podcast | What&#x27;s Your Story | Weishung Liu</span><br><span class="line">What’s Your Story: Weishung Liu</span><br><span class="line">Principal PM Manager Weishung Liu shares how a career delivering products and customer experiences aligns with her love of people and storytelling and how—despite efforts to defy the expectations that come with growing up in Silicon Valley—she landed in tech.</span><br><span class="line"></span><br><span class="line">Listen now</span><br><span class="line">Opens in a new tab</span><br><span class="line">What is the community creating with AutoGen Studio?</span><br><span class="line">Over the last few months, we have shared an early version of AutoGen Studio, which has been downloaded more than 154,000 times on pypi (January – May 2024). Our observations of early usage patterns (based on feedback from social platforms like GitHub discussions(opens in new tab) , Discord(opens in new tab) and Youtube(opens in new tab) (opens in new tab)) suggest that AutoGen Studio is driving a new group of users who have basic technical capabilities (that is, they can install the tool) and are interested in rapidly testing out ideas but have limited programming skills.</span><br><span class="line"></span><br><span class="line">We have seen these users prototype examples covering tasks like travel planning, pdf brochure generation, market research, structured data extraction, video generation, and visualization generation among others. Importantly, these tasks are accomplished simply by defining agents, giving them access to large language models and skills, adding agents to a workflow, and running tasks with these workflows.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Users are exploring early use cases such as report/book generation, as seen in the screenshot above. Here, two agents are defined and given access to skills for generating images. The agents are then composed into a workflow where messages and actions are exchanged to solve the task of generating a pdf report.</span><br><span class="line">Open research questions and next steps</span><br><span class="line">Orchestrating teams of agents that can explore plans, reflect on actions, and collaborate offers opportunities to build tools that address challenging tasks. We believe that we are just scratching the surface of what may be possible with the multi-agent paradigm, and much is unknown about how best to harness foundation models, let alone foundation model-based agents and multi-agent solutions.</span><br><span class="line"></span><br><span class="line">This leaves open many opportunities for further research.</span><br><span class="line"></span><br><span class="line">For example, the sophisticated interplay between agents in multi-agent paradigms, particularly for increasingly more complex and dynamic domains, highlights many opportunities for multi-agent evaluation and tooling. Open questions include:</span><br><span class="line"></span><br><span class="line">How can we measure the performance, reliability, and reusability of agents across tasks?</span><br><span class="line">How can we better understand the strengths and limitations of agents?</span><br><span class="line">How can we explore alternative scenarios and outcomes?</span><br><span class="line">How can we compare different agent architectures and collaboration protocols?</span><br><span class="line">These questions require novel methods and metrics that can capture the multi-faceted aspects of multi-agent paradigms and provide actionable insights for developers and users.</span><br><span class="line"></span><br><span class="line">As our understanding of the multi-agent paradigm matures, another opportunity is in distilling design patterns and best practices for building effective agent teams for different types of tasks. For instance:</span><br><span class="line"></span><br><span class="line">What are the optimal number and composition of agents for a given problem?</span><br><span class="line">What is the best way to distribute responsibilities and coordinate actions among agents?</span><br><span class="line">What are the trade-offs between centralized and decentralized control, or between homogeneous and heterogeneous agents?</span><br><span class="line">How can we leverage human oversight and feedback to improve agent reliability and safety?</span><br><span class="line">These questions require systematic studies and empirical evaluations to discover the key dimensions and principles for designing multi-agent solutions.</span><br><span class="line"></span><br><span class="line">Finally, as agents become more long-lived and ubiquitous in our digital world, an open challenge is in automating and optimizing the agent-creation process itself. For example:</span><br><span class="line"></span><br><span class="line"> How can we dynamically spawn agents based on the task requirements and available resources?</span><br><span class="line">How can we tune agent parameter workflow configurations to achieve the best performance?</span><br><span class="line">How can we adapt agent teams to changing environments and user preferences?</span><br><span class="line">Future design improvements</span><br><span class="line">Naturally, we see AutoGen Studio as a potential vehicle to study many of these research questions – from improvements in the user experience of authoring workflows to a gallery of shareable artifacts to advanced tools for making sense of agent behaviors.</span><br><span class="line"></span><br><span class="line">We are currently working on a new drag-and-drop experience in AutoGen Studio, designed to transform how users’ author multi-agent workflows. Our new visual canvas allows users to easily orchestrate and connect agents, providing an intuitive interface for defining collaboration dynamics.</span><br><span class="line"></span><br><span class="line">AutoGen Studio user interface: visual workflow design</span><br><span class="line">A new visual canvas interface for AutoGen allows users to easily orchestrate and connect agents, providing an intuitive interface for defining collaboration dynamics. Entities such as skills and models can be associated with agents via drag-and-drop interactions.</span><br><span class="line">Visual workflow design: The heart of our enhanced user interface is a visual canvas where you can literally see your workflow come to life. Drag and drop different agents onto the canvas to build complex conversation patterns. This graphical approach not only simplifies the initial setup but also makes the process of modifying agents and workflows more intuitive.</span><br><span class="line"></span><br><span class="line">A new visual canvas interface for AutoGen that allows users to both visualize agent interactions as well as update properties of each agent in the same view pane.</span><br><span class="line">A new visual canvas interface for AutoGen allows users to both visualize agent interactions and update properties of each agent in the same view pane.</span><br><span class="line">Configurable agents, models, and skills: Customize each agent’s role and skills through simple, direct interactions on the canvas. Whether you’re adding new capabilities or tweaking existing ones, the process is straightforward and user-friendly.</span><br><span class="line"></span><br><span class="line">AutoGen Studio user interface: dynamic prototyping and testing</span><br><span class="line">The proposed visual canvas interface for AutoGen will explore updated visualization of agent internal monologues for improved debugging.</span><br><span class="line">Dynamic prototyping and testing: Experimentation is key to perfecting agent workflows. With our new interface, you can prototype various agent configurations and immediately test them in a live environment. This real-time interaction allows you to chat with the workflow, observe all agent messages, and pinpoint areas for improvement on the fly.</span><br><span class="line"></span><br><span class="line">AutoGen Studio community gallery</span><br><span class="line">The new proposed design explores a gallery of curated workflows and entities (such as skills and agents) that can be reused.</span><br><span class="line">Finally, we are developing a community gallery within AutoGen Studio where users can share, discover, and learn from one another. This gallery will allow you to publish your workflows, agents, and skills, fostering a collaborative environment where everyone can benefit from shared knowledge and innovations.</span><br><span class="line"></span><br><span class="line">Note on responsible AI: Promoting safe and ethical multi-agent solutions</span><br><span class="line">AutoGen Studio is designed to provide a low-code environment for rapidly prototyping and testing multi-agent workflows. Our goal is to responsibly advance research and practice in solving problems with multiple agents and to develop tools that contribute to human well-being. Along with AutoGen, AutoGen Studio is committed to implementing features that promote safe and reliable outcomes. For example, AutoGen Studio offers profiling tools to make sense of agent actions and safeguards, such as support for Docker environments for code execution. This feature helps ensure that agents operate within controlled and secure environments, reducing the risk of unintended or harmful actions. For more information on our approach to responsible AI in AutoGen,  please refer to transparency FAQS here: https://github.com/microsoft/autogen/blob/main/TRANSPARENCY_FAQS.md(opens in new tab). Finally, AutoGen Studio is not production ready i.e., it does not focus on implementing authentication and other security measures that are required for production ready deployments.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://research.google/blog/pre-translation-vs-direct-inference-in-multilingual-llm-applications/</span><br><span class="line">google research</span><br><span class="line">Pre-translation vs. direct inference in multilingual LLM applications</span><br><span class="line">June 14, 2024</span><br><span class="line"></span><br><span class="line">Roman Goldenberg, Research Scientist, Verily AI, and Natalie Aizenberg, Research Software Engineer, Google Research &amp; Verily AI</span><br><span class="line"></span><br><span class="line">A comprehensive evaluation comparing pre-translation with direct inference of PaLM2 on multilingual tasks, demonstrating its improved performance using direct inference in the source language, compared to pre-translation to English. PaLM2 models do not need pre-translation to excel in multilingual tasks, as demonstrated in a comprehensive evaluation comparing direct inference with pre-translation.</span><br><span class="line"></span><br><span class="line">Large language models (LLMs) are becoming omnipresent tools for solving a wide range of problems. However, their effectiveness in handling diverse languages has been hampered by inherent limitations in training data, which are often skewed towards English. To address this, pre-translation, where inputs are translated to English before feeding them to the LLM, has become a standard practice.</span><br><span class="line"></span><br><span class="line">Previous research has demonstrated the effectiveness of pre-translation for optimal LLM performance for GPT-3/3.5/4, ChatGPT, PaLM and other models. While pre-translation helps address the language bias issue, it introduces complexities and inefficiencies, and it may lead to information loss. With the introduction of new powerful LLMs trained on massive multilingual datasets, it is time to revisit the assumed necessity of pre-translation.</span><br><span class="line"></span><br><span class="line">In our recent work “Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?”, to be presented at NAACL’24, we re-evaluate the need for pre-translation using PaLM2, which has been established as highly performant in multilingual tasks. Our findings challenge the pre-translation paradigm established in prior research and highlight the advantages of direct inference in PaLM2. Specifically, we demonstrate that PaLM2-L consistently outperforms pre-translation in 94 out of 108 languages, offering a more efficient and effective application in multilingual settings while unlocking linguistic authenticity and alleviating the limitations of pre-translation.</span><br><span class="line"></span><br><span class="line">Rethinking multilingual LLM evaluation</span><br><span class="line">Prior research on evaluating the impact of pre-translation mainly focused on discriminative (close-ended) tasks, such as multiple choice question answering (QA), for which the language of the answer is mostly insignificant. For evaluating generative (open-ended) tasks, such as text summarization or attributed QA, the output needs to be in the source language to compare it to the ground truth (GT). This requires adding an extra post-inference translation step. While for source language inference evaluation (a in the figure below), inference is directly compared to GT in the source language, for pre-translation evaluation (b), LLM inference is translated back to source language (c.1).</span><br><span class="line"></span><br><span class="line">BtLB-1-Source</span><br><span class="line">Comparative evaluation of direct inference vs. pre-translation in source language.</span><br><span class="line"></span><br><span class="line">One of the drawbacks of this evaluation scheme is that comparing model output to GT in different languages using standard lexical metrics, such as ROUGE and F1, is language dependent and introduces inconsistencies. Another problem with this approach is that GT answers in open-ended tasks rely primarily on information present within the provided context. Specifically, in reading comprehension Q&amp;A benchmarks, it is common to have the GT be a substring of the original context. This presents a potential disadvantage for pre-translation, which lacks access to the original context from which the GT was extracted.</span><br><span class="line"></span><br><span class="line">To address both these caveats, we perform a complimentary evaluation in English by translating the GT and direct inference results to English. Here, instead of translating the pre-translated inference back to source language, we translate the direct inference output and GT to English (as illustrated below in panels c.2 and c.3, respectively). Then the evaluation against GT is performed in English.</span><br><span class="line"></span><br><span class="line">BtLB-2-English</span><br><span class="line">Comparative evaluation of direct inference vs. pre-translation in English.</span><br><span class="line"></span><br><span class="line">In addition, we found that averaging LLM accuracy metrics across languages, as done in the prior approaches, can be misleading, masking crucial details. To gain a more nuanced understanding, we introduced the Language Ratio metric as an alternative aggregation over commonly used lexical metrics. It is defined as the percentage of languages for which direct inference yields better results than pre-translation.</span><br><span class="line"></span><br><span class="line">The Language Ratio can be computed for any accuracy score of choice (such as F1 or Rouge) over a single inference mode (direct and pre-translation) and language. By inspecting the proportion of languages where one method outperforms the other, rather than averaging language bias scores, a fairer overall comparison and more detailed understanding of relative strengths and weaknesses across languages is possible.</span><br><span class="line"></span><br><span class="line">Direct inference takes the lead</span><br><span class="line">Our analysis encompassed a variety of tasks and languages. We employed six publicly available benchmarks to evaluate PaLM2&#x27;s performance in both discriminative (XCOPA, XStoryCLoze and BeleBele benchmarks) and generative tasks (XLSum, TyDiQA-GP and XQuAD) across 108 languages. Two variants of PaLM2 were evaluated: PaLM2-S (Small - Bison) and PaLM2-L (Large - Unicorn), while using Google Translation API for pre- and post-translation.</span><br><span class="line"></span><br><span class="line">BtLB-4-Results</span><br><span class="line">PaLM2-S (left) and PaLM2-L (right) evaluation results, comparing pre-translation (blue) and direct inference (red). Model performance for generative (open-ended) tasks is evaluated both in the source language and in English. Top: Accuracy metrics (accuracy, Rouge-L, F1) measured on various benchmarks. Bottom: Language Ratio metric.</span><br><span class="line"></span><br><span class="line">The results were strikingly different from those reported in prior literature for other models.</span><br><span class="line"></span><br><span class="line">PaLM2-L consistently achieved better performance with direct inference in 94 out of 108 languages evaluated. The advantage was observed for both close- and open-ended tasks, on all benchmarks. The results were consistent across all evaluations — in source language and in English, using standard metrics (Accuracy/F1/Rouge) and the Language Ratio.</span><br><span class="line">PaLM2-S also favors direct inference in all but the XQuAD benchmark, where the result is less conclusive. Better average F1 score is achieved using direct inference (due to significant improvements in Chinese and Thai), but the Language Ratio is better for pre-translation, which emphasizes the complimentary value of this metric.</span><br><span class="line">Direct inference yielded superior results even in low-resource languages (LRL). This is particularly significant for fostering communication and information access in under-represented languages.</span><br><span class="line">Language-focused analysis</span><br><span class="line">While PaLM2-L clearly performs better using direct inference for the majority of languages, pre-translation shows consistent superiority (across benchmarks) for 7 languages: Bambara, Cusco-Collao Quechua, Lingala, Oromo, Punjabi, Tigrinya, and Tsonga. All 7 are LRL, 4 out of 7 are African, with Lingala, the largest, spoken by over 40 million people. Interestingly, the majority (85%) of LRL benefit from direct inference with PaLM2.</span><br><span class="line"></span><br><span class="line">BtLB-5-Performance</span><br><span class="line">PaLM2-L average direct inference Lift over pre-translate inference on LRL. The majority of languages (over 85%) benefit from direct inference with PaLM2, with lifts exceeding 5% (dashed line) in 63% of languages.</span><br><span class="line"></span><br><span class="line">The future of multilingual communication</span><br><span class="line">The comprehensive comparative analysis we performed in this study suggests that the new generation of LLMs, trained on massive multilingual datasets, can better handle information and communication across languages, eliminating the need for pre-translation for certain languages.</span><br><span class="line"></span><br><span class="line">We are committed to ongoing research in this area, focusing on improving LLM performance for all languages and fostering a more inclusive future for multilingual communication.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://runwayml.com/blog/introducing-gen-3-alpha/</span><br><span class="line">Introducing Gen-3 Alpha</span><br><span class="line">A new frontier for high-fidelity, controllable video generation.</span><br><span class="line">Anastasis Germanidis | June 17th, 2024</span><br><span class="line">Gen-3 Alpha is the first of an upcoming series of models trained by Runway on a new infrastructure built for large-scale multimodal training. It is a major improvement in fidelity, consistency, and motion over Gen-2, and a step towards building General World Models.</span><br><span class="line">All of the videos on this page were generated with Gen-3 Alpha with no modifications.</span><br><span class="line"></span><br><span class="line">Prompt: Subtle reflections of a woman on the window of a train moving at hyper-speed in a Japanese city.</span><br><span class="line">Prompt: An astronaut running through an alley in Rio de Janeiro.</span><br><span class="line">Prompt: FPV flying through a colorful coral lined streets of an underwater suburban neighborhood.</span><br><span class="line">Prompt: Handheld tracking shot at night, following a dirty blue ballon floating above the ground in abandon old European street.</span><br><span class="line"></span><br><span class="line">Trained jointly on videos and images, Gen-3 Alpha will power Runway&#x27;s Text to Video, Image to Video and Text to Image tools, existing control modes such as Motion Brush, Advanced Camera Controls, Director Mode as well as upcoming tools for more fine-grained control over structure, style, and motion.</span><br><span class="line"></span><br><span class="line">Gen-3 Alpha will be released with a new set of safeguards, including our new and improved in-house visual moderation system and C2PA provenance standards.</span><br><span class="line">Prompt: An empty warehouse dynamically transformed by flora that explode from the ground.</span><br><span class="line">Prompt: Close up shot of a living flame wisp darting through a bustling fantasy market at night.</span><br><span class="line">Prompt: Handheld tracking shot, following a red ballon floating above the ground in abandon street.</span><br><span class="line">Prompt: A FPV shot zooming through a tunnel into a vibrant underwater space.</span><br><span class="line">Prompt: A wide symmetrical shot of a painting in a museum. The camera zooms in close to the painting.</span><br><span class="line">Prompt: Ultra-fast disorienting hyperlapse racing through a tunnel into a labyrinth of rapidly growing vines.</span><br><span class="line">Prompt: FPV, internal locomotive cab of a train moving at hyper-speed in an old European city.</span><br><span class="line">Prompt: Zooming in hyper-fast to a dandelion to reveal macro dream-like abstract world.</span><br><span class="line">Fine-grained temporal control</span><br><span class="line">Gen-3 Alpha has been trained with highly descriptive, temporally dense captions, enabling imaginative transitions and precise key-framing of elements in the scene.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Prompt: An extreme close-up shot of an ant emerging from its nest. The camera pulls back revealing a neighborhood beyond the hill.</span><br><span class="line">Prompt: A tsunami coming through an alley in Bulgaria, dynamic movement.</span><br><span class="line">Prompt: A FPV drone shot through a castle on a cliff.</span><br><span class="line">Prompt: Internal window of a train moving at hyper-speed in an old European city.</span><br><span class="line">Prompt: Handheld camera moving fast, flashlight light, in a white old wall in a old alley at night a black graffiti that spells ‘Runway’.</span><br><span class="line">Prompt: Super fast zoom out from the peak of a frozen mountain where a lonely hiker is arriving to the submit.</span><br><span class="line">Prompt: A first-person POV shot rapidly flies through open doors to reveal a surreal waterfall cascading in the middle of the living room.</span><br><span class="line">Prompt: A first-person POV shot rapidly flies towards a house&#x27;s front door at 10x speed.</span><br><span class="line">Prompt: A pencil drawing an architectural plan.</span><br><span class="line">Photorealistic Humans</span><br><span class="line">Gen-3 Alpha excels at generating expressive human characters with a wide range of actions, gestures, and emotions, unlocking new storytelling opportunities.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Prompt: A cinematic wide portrait of a man with his face lit by the glow of a TV.</span><br><span class="line">Prompt: A close up portrait of a woman lit by the side, the camera pulls back.</span><br><span class="line">Prompt: Zoom in shot to the face of a young woman sitting on a bench in the middle of an empty school gym.</span><br><span class="line">Prompt: A close up of an older man in a warehouse, camera zoom out.</span><br><span class="line">Prompt: An older man playing piano, lit from the side.</span><br><span class="line">Prompt: Macro shot to the face freckles of a young woman trying to look for something.</span><br><span class="line">Prompt: An astronaut walking between stone buildings.</span><br><span class="line">Prompt: A middle-aged sad bald man becomes happy as a wig of curly hair and sunglasses fall suddenly on his head.</span><br><span class="line">For artists, by artists</span><br><span class="line">Training Gen-3 Alpha was a collaborative effort from a cross-disciplinary team of research scientists, engineers, and artists. It was designed to interpret a wide range of styles and cinematic terminology.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Prompt: View out a window of a giant strange creature walking in rundown city at night, one single street lamp dimly lighting the area.</span><br><span class="line">Prompt: A man made of rocks walking in the forest, full-body shot.</span><br><span class="line">Prompt: A slow cinematic push in on an ostrich standing in a 1980s kitchen.</span><br><span class="line">Prompt: A giant humanoid, made of fluffy blue cotton candy, stomping on the ground, and roaring to the sky, clear blue sky behind them.</span><br><span class="line">Prompt: Zooming through a dark forest with neon light flora lighting up.</span><br><span class="line">Prompt: A cyclone of broken glass in an urban alleyway. dynamic movement.</span><br><span class="line">Prompt: A man standing in front of a burning building giving the &#x27;thumbs up&#x27; sign.</span><br><span class="line">Prompt: Highly detailed close up of a bacteria.</span><br><span class="line">Prompt: An ultra-wide shot of a giant stone hand reaching out of a pile of rocks at the base of a mountain.</span><br><span class="line">Prompt: Aerial view shot of a cloaked figure elevating in the sky betweem slyscrapers.</span><br><span class="line">Prompt: An oil painting of a natural forest environment with colorful maple trees and cinematic parallax animation.</span><br><span class="line">Prompt: A Japanese animated film of a young woman standing on a ship and looking back at camera.</span><br><span class="line">Prompt: A close-up shot of a young woman driving a car, looking thoughtful, blurred green forest visible through the rainy car window.</span><br><span class="line">Prompt: Aerial shot of a drone moving fast in a dense green jungle.</span><br><span class="line">Prompt: Hyperlapse shot through a corridor with flashing lights. A silver fabric flies through the entire corridor.</span><br><span class="line">Prompt: Aerial shot of the ocean. a maelstrom forms in the water swirling around until it reveals the fiery depths below.</span><br><span class="line">Prompt: A push through an ocean research outpost.</span><br><span class="line">Prompt: A woman singing and standing in a concert stage with a bright light in the background.</span><br><span class="line">Industry Customization</span><br><span class="line">As part of the family of Gen-3 models, we have been collaborating and partnering with leading entertainment and media organizations to create custom versions of Gen-3.</span><br><span class="line">Customization of Gen-3 models allows for more stylistically controlled and consistent characters, and targets specific artistic and narrative requirements, among other features.</span><br><span class="line">For companies interested in fine-tuning and custom models, reach out to us using the form in the button below:</span><br></pre></td></tr></table></figure>
</details></li>
</ul>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-06-18</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/06/18/2024-6-18-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 6월 18일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/06/21/2024-6-21-AI-NEWS/" title="2024년 6월 21일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/06/17/2024-6-17-AI-NEWS/" title="2024년 6월 17일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>