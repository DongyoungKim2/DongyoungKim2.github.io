<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 6ì›” 18ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="SummaryOpenAIì—ì„œëŠ” ë°•ì‚¬ê¸‰ AI ì—°êµ¬ì›ì˜ ì´ˆë´‰ì´ ì—…ê³„ ìµœê³  ìˆ˜ì¤€ì¸ 86ë§Œ5000ë‹¬ëŸ¬ì— ë‹¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ë˜í•œ, Appleì€ 4M(4M: Massively Multimodal Masked Modeling)ì´ë¼ëŠ” ìƒˆë¡œìš´ ë©€í‹°ëª¨ë‹¬ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ë°œí‘œ"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 6ì›” 18ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>OpenAIì—ì„œëŠ” ë°•ì‚¬ê¸‰ AI ì—°êµ¬ì›ì˜ ì´ˆë´‰ì´ ì—…ê³„ ìµœê³  ìˆ˜ì¤€ì¸ 86ë§Œ5000ë‹¬ëŸ¬ì— ë‹¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ë˜í•œ, Appleì€ 4M(4M: Massively Multimodal Masked Modeling)ì´ë¼ëŠ” ìƒˆë¡œìš´ ë©€í‹°ëª¨ë‹¬ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. DeepSeekëŠ” DeepSeek-Coder-V2ë¼ëŠ” ìƒˆë¡œìš´ ì½”ë“œ ì–¸ì–´ ëª¨ë¸ì„ ê³µê°œí•˜ì˜€ìœ¼ë©°, MicrosoftëŠ” AutoGen Studioë¼ëŠ” ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° êµ¬ì¶•ì„ ìœ„í•œ ì €ì½”ë“œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì†Œê°œí–ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ Googleì€ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ê³¼ì˜ ì—°ë™ì„ í†µí•´ ë™ì˜ìƒì„ ìœ„í•œ ì˜¤ë””ì˜¤ ìƒì„± ê¸°ìˆ ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.</p>
<h2 id="4M-Massively-Multimodal-Masked-Modeling"><a href="#4M-Massively-Multimodal-Masked-Modeling" class="headerlink" title="4M: Massively Multimodal Masked Modeling,"></a>4M: Massively Multimodal Masked Modeling,</h2><h3 id="ë‹¤ì¤‘-ëª¨ë‹¬-ë§ˆìŠ¤í¬-ëª¨ë¸ë§"><a href="#ë‹¤ì¤‘-ëª¨ë‹¬-ë§ˆìŠ¤í¬-ëª¨ë¸ë§" class="headerlink" title="ë‹¤ì¤‘ ëª¨ë‹¬ ë§ˆìŠ¤í¬ ëª¨ë¸ë§"></a>ë‹¤ì¤‘ ëª¨ë‹¬ ë§ˆìŠ¤í¬ ëª¨ë¸ë§</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.06647">ë§í¬</a>, 2024-06-17,<br>Apple</p>
<ul>
<li>Appleê³¼ EPFLì€ 4Mì´ë¼ëŠ” ìƒˆë¡œìš´ ë©€í‹°ëª¨ë‹¬ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ë°œí‘œ</li>
<li>4M-7ê³¼ 4M-21 ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê³µê°œ</li>
<li>ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ëŠ” RGB, Edge, Geometric, Text, Semantic, Feature map ë“±ì˜ ëª¨ë‹¬ë¦¬í‹° í¬í•¨</li>
<li>Apache 2.0 ë¼ì´ì„ ìŠ¤ë¡œ ì½”ë“œì™€ ê°€ì¤‘ì¹˜ ë°°í¬</li>
<li>ë‹¨ì¼ Transformer ì¸ì½”ë”-ë””ì½”ë” ëª¨ë¸ì„ ì‚¬ìš©í•œ í•™ìŠµ</li>
<li>ë‹¤ì–‘í•œ ë¹„ì „ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë‹¤ì¬ë‹¤ëŠ¥í•œ ëª¨ë¸ êµ¬í˜„</li>
</ul>
<h2 id="DeepSeek-Coder-V2-Breaking-the-Barrier-of-Closed-Source-Models-in-Code-Intelligence"><a href="#DeepSeek-Coder-V2-Breaking-the-Barrier-of-Closed-Source-Models-in-Code-Intelligence" class="headerlink" title="DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence,"></a>DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence,</h2><h3 id="ì½”ë“œ-ì¸í…”ë¦¬ì „ìŠ¤ì—ì„œ-íì‡„í˜•-ëª¨ë¸ì˜-ì¥ë²½ì„-í—ˆë¬¼ë‹¤"><a href="#ì½”ë“œ-ì¸í…”ë¦¬ì „ìŠ¤ì—ì„œ-íì‡„í˜•-ëª¨ë¸ì˜-ì¥ë²½ì„-í—ˆë¬¼ë‹¤" class="headerlink" title="ì½”ë“œ ì¸í…”ë¦¬ì „ìŠ¤ì—ì„œ íì‡„í˜• ëª¨ë¸ì˜ ì¥ë²½ì„ í—ˆë¬¼ë‹¤"></a>ì½”ë“œ ì¸í…”ë¦¬ì „ìŠ¤ì—ì„œ íì‡„í˜• ëª¨ë¸ì˜ ì¥ë²½ì„ í—ˆë¬¼ë‹¤</h3><p><a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/paper.pdf">ë§í¬</a>, 2024-06-17,<br>DeepSeek</p>
<ul>
<li>DeepSeek-Coder-V2ëŠ” GPT-4 Turboì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ìë‘í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ì½”ë“œ ì–¸ì–´ ëª¨ë¸</li>
<li>DeepSeek-Coder-V2-Baseì—ì„œ 6ì¡° ê°œì˜ í† í°ì„ ì¶”ê°€ë¡œ í•™ìŠµí•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ</li>
<li>ì½”ë”© ë° ìˆ˜í•™ì  ì¶”ë¡  ëŠ¥ë ¥ ëŒ€í­ ê°•í™”</li>
<li>ì§€ì› í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ 86ê°œì—ì„œ 338ê°œë¡œ í™•ì¥</li>
<li>ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ 16Kì—ì„œ 128Kë¡œ í™•ì¥</li>
<li>ì—°êµ¬ ë° ìƒì—…ì  ì‚¬ìš©ì„ ìœ„í•œ í—ˆê°€ ë¼ì´ì„ ìŠ¤ í¬í•¨</li>
</ul>
<h2 id="â€œì˜¤í”ˆAI-ë°•ì‚¬ê¸‰-ì—°êµ¬ì›-ì´ˆë´‰-11ì–µâ€â€¦ê¸‰ì—¬-ìˆœìœ„-ê³µê°œ"><a href="#â€œì˜¤í”ˆAI-ë°•ì‚¬ê¸‰-ì—°êµ¬ì›-ì´ˆë´‰-11ì–µâ€â€¦ê¸‰ì—¬-ìˆœìœ„-ê³µê°œ" class="headerlink" title="â€œì˜¤í”ˆAI, ë°•ì‚¬ê¸‰ ì—°êµ¬ì› ì´ˆë´‰ 11ì–µâ€â€¦ê¸‰ì—¬ ìˆœìœ„ ê³µê°œ,"></a>â€œì˜¤í”ˆAI, ë°•ì‚¬ê¸‰ ì—°êµ¬ì› ì´ˆë´‰ 11ì–µâ€â€¦ê¸‰ì—¬ ìˆœìœ„ ê³µê°œ,</h2><h3 id="ì˜¤í”ˆAIì˜-ë°•ì‚¬ê¸‰-ì—°êµ¬ì›-ì´ˆë´‰-11ì–µ-ê³µê°œ"><a href="#ì˜¤í”ˆAIì˜-ë°•ì‚¬ê¸‰-ì—°êµ¬ì›-ì´ˆë´‰-11ì–µ-ê³µê°œ" class="headerlink" title="ì˜¤í”ˆAIì˜ ë°•ì‚¬ê¸‰ ì—°êµ¬ì› ì´ˆë´‰ 11ì–µ ê³µê°œ"></a>ì˜¤í”ˆAIì˜ ë°•ì‚¬ê¸‰ ì—°êµ¬ì› ì´ˆë´‰ 11ì–µ ê³µê°œ</h3><p><a target="_blank" rel="noopener" href="https://www.aitimes.com/news/articleViewAmp.html?idxno=156265">ë§í¬</a>, 2024-01-03,<br>ë¡œë¼</p>
<ul>
<li>ì˜¤í”ˆAIì˜ ë°•ì‚¬ê¸‰ AI ì—°êµ¬ì› ì´ˆë´‰ì´ 86ë§Œ5000ë‹¬ëŸ¬ë¡œ ì—…ê³„ ìµœê³  ìˆ˜ì¤€</li>
<li>ì•¤íŠ¸ë¡œí”½ì´ 85ë§Œ5000ë‹¬ëŸ¬ë¡œ ë‘ ë²ˆì§¸ë¡œ ë†’ì€ ì´ˆë´‰ ì œê³µ</li>
<li>ì¸í”Œë ‰ì…˜ AI, í…ŒìŠ¬ë¼, ì•„ë§ˆì¡´, êµ¬ê¸€ ë¸Œë ˆì¸ ë“±ì˜ ê¸°ì—…ë„ ë†’ì€ ì´ˆë´‰ ì œê³µ</li>
<li>AI ê¸°ìˆ  ìˆ˜ìš”ê°€ ê³µê¸‰ì„ ì´ˆê³¼í•˜ì—¬ ì´ˆë´‰ì´ ë†’ì•„ì§</li>
<li>ë°•ì‚¬ í•™ìœ„ ë…¼ë¬¸ ì¶œíŒ ê¸°ë¡ì´ ì¤‘ìš”í•œ í‰ê°€ ìš”ì†Œë¡œ ì‘ìš©</li>
</ul>
<h2 id="Generating-audio-for-video"><a href="#Generating-audio-for-video" class="headerlink" title="Generating audio for video,"></a>Generating audio for video,</h2><h3 id="ë¹„ë””ì˜¤ë¥¼-ìœ„í•œ-ì˜¤ë””ì˜¤-ìƒì„±"><a href="#ë¹„ë””ì˜¤ë¥¼-ìœ„í•œ-ì˜¤ë””ì˜¤-ìƒì„±" class="headerlink" title="ë¹„ë””ì˜¤ë¥¼ ìœ„í•œ ì˜¤ë””ì˜¤ ìƒì„±"></a>ë¹„ë””ì˜¤ë¥¼ ìœ„í•œ ì˜¤ë””ì˜¤ ìƒì„±</h3><p><a target="_blank" rel="noopener" href="https://deepmind.google/discover/blog/generating-audio-for-video/">ë§í¬</a>, 2024-06-17,<br>Google Research</p>
<ul>
<li>Googleì€ ë¹„ë””ì˜¤ í”½ì…€ê³¼ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í’ë¶€í•œ ì‚¬ìš´ë“œíŠ¸ë™ì„ ìƒì„±í•˜ëŠ” V2A ê¸°ìˆ  ë°œí‘œ</li>
<li>V2AëŠ” ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ê³¼ ê²°í•©í•˜ì—¬ ì˜í™”ì˜ ì‚¬ìš´ë“œíŠ¸ë™, í˜„ì‹¤ì ì¸ ì‚¬ìš´ë“œ íš¨ê³¼ ë˜ëŠ” ëŒ€í™”ë¥¼ ìƒì„± ê°€ëŠ¥</li>
<li>ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ìë£Œì— ì‚¬ìš´ë“œíŠ¸ë™ ìƒì„± ê°€ëŠ¥</li>
<li>ì˜¤ë””ì˜¤ ì¶œë ¥ì˜ í’ˆì§ˆì„ ë†’ì´ê¸° ìœ„í•´ ì¶”ê°€ ì •ë³´ë¡œ í›ˆë ¨ ê³¼ì • ê°œì„ </li>
</ul>
<h2 id="Introducing-AutoGen-Studio-A-low-code-interface-for-building-multi-agent-workflows"><a href="#Introducing-AutoGen-Studio-A-low-code-interface-for-building-multi-agent-workflows" class="headerlink" title="Introducing AutoGen Studio: A low-code interface for building multi-agent workflows,"></a>Introducing AutoGen Studio: A low-code interface for building multi-agent workflows,</h2><h3 id="ë©€í‹°-ì—ì´ì „íŠ¸-ì›Œí¬í”Œë¡œìš°-êµ¬ì¶•ì„-ìœ„í•œ-ì €ì½”ë“œ-ì¸í„°í˜ì´ìŠ¤-AutoGen-Studio-ì†Œê°œ"><a href="#ë©€í‹°-ì—ì´ì „íŠ¸-ì›Œí¬í”Œë¡œìš°-êµ¬ì¶•ì„-ìœ„í•œ-ì €ì½”ë“œ-ì¸í„°í˜ì´ìŠ¤-AutoGen-Studio-ì†Œê°œ" class="headerlink" title="ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° êµ¬ì¶•ì„ ìœ„í•œ ì €ì½”ë“œ ì¸í„°í˜ì´ìŠ¤ AutoGen Studio ì†Œê°œ"></a>ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° êµ¬ì¶•ì„ ìœ„í•œ ì €ì½”ë“œ ì¸í„°í˜ì´ìŠ¤ AutoGen Studio ì†Œê°œ</h3><p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/blog/introducing-autogen-studio-a-low-code-interface-for-building-multi-agent-workflows/">ë§í¬</a>, 2024-06-17,<br>Microsoft Research</p>
<ul>
<li>AutoGen StudioëŠ” ë©€í‹° ì—ì´ì „íŠ¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ì €ì½”ë“œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ</li>
<li>ì‚¬ìš©ìëŠ” ê°„ë‹¨í•œ ê·¸ë˜í”½ ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì„±í•˜ê³  ì›Œí¬í”Œë¡œìš° ì‘ì„± ê°€ëŠ¥</li>
<li>ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê³  ë””ë²„ê·¸í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ ì œê³µ</li>
<li>ì›Œí¬í”Œë¡œìš°ë¥¼ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ì–´ ë‹¤ë¥¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš© ê°€ëŠ¥</li>
</ul>
<h2 id="Pre-translation-vs-direct-inference-in-multilingual-LLM-applications"><a href="#Pre-translation-vs-direct-inference-in-multilingual-LLM-applications" class="headerlink" title="Pre-translation vs. direct inference in multilingual LLM applications,"></a>Pre-translation vs. direct inference in multilingual LLM applications,</h2><h3 id="ë‹¤êµ­ì–´-LLM-ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ-ì‚¬ì „-ë²ˆì—­-ëŒ€-ì§ì ‘-ì¶”ë¡ "><a href="#ë‹¤êµ­ì–´-LLM-ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ-ì‚¬ì „-ë²ˆì—­-ëŒ€-ì§ì ‘-ì¶”ë¡ " class="headerlink" title="ë‹¤êµ­ì–´ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ì „ ë²ˆì—­ ëŒ€ ì§ì ‘ ì¶”ë¡ "></a>ë‹¤êµ­ì–´ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ì „ ë²ˆì—­ ëŒ€ ì§ì ‘ ì¶”ë¡ </h3><p><a target="_blank" rel="noopener" href="https://research.google/blog/pre-translation-vs-direct-inference-in-multilingual-llm-applications/">ë§í¬</a>, 2024-06-14,<br>Google Research</p>
<ul>
<li>PaLM2ëŠ” ë‹¤êµ­ì–´ ì‘ì—…ì—ì„œ ì‚¬ì „ ë²ˆì—­ ì—†ì´ ì§ì ‘ ì¶”ë¡ ì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„</li>
<li>108ê°œ ì–¸ì–´ ì¤‘ 94ê°œ ì–¸ì–´ì—ì„œ ì§ì ‘ ì¶”ë¡ ì´ ì‚¬ì „ ë²ˆì—­ë³´ë‹¤ ìš°ìˆ˜í•œ ê²°ê³¼</li>
<li>ë‹¤êµ­ì–´ LLMì˜ íš¨ìœ¨ì„±ê³¼ íš¨ê³¼ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì—°êµ¬ ì§€ì†</li>
</ul>
<h2 id="Introducing-Gen-3-Alpha"><a href="#Introducing-Gen-3-Alpha" class="headerlink" title="Introducing Gen-3 Alpha,"></a>Introducing Gen-3 Alpha,</h2><h3 id="Gen-3-Alpha-ì†Œê°œ"><a href="#Gen-3-Alpha-ì†Œê°œ" class="headerlink" title="Gen-3 Alpha ì†Œê°œ"></a>Gen-3 Alpha ì†Œê°œ</h3><p><a target="_blank" rel="noopener" href="https://runwayml.com/blog/introducing-gen-3-alpha/">ë§í¬</a>, 2024-06-17,<br>Runway</p>
<ul>
<li>Gen-3 AlphaëŠ” ë†’ì€ ì¶©ì‹¤ë„ì™€ ì¼ê´€ì„±ì„ ê°–ì¶˜ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸</li>
<li>í…ìŠ¤íŠ¸ì—ì„œ ë¹„ë””ì˜¤, ì´ë¯¸ì§€ì—ì„œ ë¹„ë””ì˜¤, í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€ ë„êµ¬ ì œê³µ</li>
<li>ì‚¬ìš©ì ì •ì˜ ë²„ì „ ì œê³µ, ì˜ˆìˆ ì  ë° ë‚´ëŸ¬í‹°ë¸Œ ìš”êµ¬ì‚¬í•­ì— ë§ì¶˜ ëª¨ë¸ ìƒì„± ê°€ëŠ¥</li>
<li>ìƒˆë¡œìš´ ì¸í”„ë¼ë¥¼ í†µí•´ ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ í•™ìŠµ ê°€ëŠ¥</li>
</ul>
<details>
  <summary>Sources</summary>
This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:

<h1 id="todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹"><a href="#todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹" class="headerlink" title="(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,"></a>(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,</h1><h2 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAIì—ì„œëŠ” ~~~ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.)</p>
<h2 id="Title"><a href="#Title" class="headerlink" title="Title,"></a>Title,</h2><h3 id="í•œê¸€ì œëª©"><a href="#í•œê¸€ì œëª©" class="headerlink" title="í•œê¸€ì œëª©"></a>í•œê¸€ì œëª©</h3><p><a href="link">ë§í¬</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2,  (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N,  (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
</ul>
<h2 id="Title-1"><a href="#Title-1" class="headerlink" title="Title,"></a>Title,</h2><h3 id="í•œê¸€ì œëª©-1"><a href="#í•œê¸€ì œëª©-1" class="headerlink" title="í•œê¸€ì œëª©"></a>í•œê¸€ì œëª©</h3><p><a href="link">ë§í¬</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2,  (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N,  (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2312.06647</span><br><span class="line"></span><br><span class="line">Apple dropped 4M: Massively Multilingual Masked Modeling! ğŸ”¥</span><br><span class="line">Is this what powers the on-device vision-text backbone?</span><br><span class="line">&gt; A framework for training any-to-any multimodal foundational models. Training/ Finetuning/ Inference.</span><br><span class="line">&gt; Release 4M-7 and 4M-21 model checkpoints (trained across tens of tasks and modalities).</span><br><span class="line">&gt; 198M, 705M and 2.8B model checkpoints.</span><br><span class="line">&gt; Release specialised Text to Image and image super-resolution specialist model checkpoints.</span><br><span class="line">&gt; Apache 2.0 license for code and weights!</span><br><span class="line">&gt; A unified transformer encoder-decoder model is trained on a masked modelling objective.</span><br><span class="line">&gt; Spread across RGB, Edge, Geometric, Text, Semantic, Feature map, and more modalities.</span><br><span class="line">&gt; Model checkpoints on the Hub ğŸ¤—</span><br><span class="line"></span><br><span class="line">Kudos to EPFL and Apple. I especially liked the any-to-any generation bit paired with multimodal chained generation! âš¡</span><br><span class="line"></span><br><span class="line">4M: Massively Multimodal Masked Modeling</span><br><span class="line">David Mizrahi, Roman Bachmann, OÄŸuzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, Amir Zamir</span><br><span class="line">Current machine learning models for vision are often highly specialized and limited to a single modality and task. In contrast, recent large language models exhibit a wide range of capabilities, hinting at a possibility for similarly versatile models in computer vision. In this paper, we take a step in this direction and propose a multimodal training scheme called 4M. It consists of training a single unified Transformer encoder-decoder using a masked modeling objective across a wide range of input/output modalities - including text, images, geometric, and semantic modalities, as well as neural network feature maps. 4M achieves scalability by unifying the representation space of all modalities through mapping them into discrete tokens and performing multimodal masked modeling on a small randomized subset of tokens.</span><br><span class="line">4M leads to models that exhibit several key capabilities: (1) they can perform a diverse set of vision tasks out of the box, (2) they excel when fine-tuned for unseen downstream tasks or new input modalities, and (3) they can function as a generative model that can be conditioned on arbitrary modalities, enabling a wide variety of expressive multimodal editing capabilities with remarkable flexibility.</span><br><span class="line">Through experimental analyses, we demonstrate the potential of 4M for training versatile and scalable foundation models for vision tasks, setting the stage for further exploration in multimodal learning for vision and other domains.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/paper.pdf</span><br><span class="line">DeepSeek-V2</span><br><span class="line">Homepage Chat Hugging Face</span><br><span class="line">Discord Wechat Twitter Follow</span><br><span class="line">Code License Model License</span><br><span class="line">Model Download | Evaluation Results | API Platform | How to Use | License | Citation</span><br><span class="line"></span><br><span class="line">Paper LinkğŸ‘ï¸</span><br><span class="line"></span><br><span class="line">DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence</span><br><span class="line">1. Introduction</span><br><span class="line">We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from DeepSeek-Coder-V2-Base with 6 trillion tokens sourced from a high-quality and multi-source corpus. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-Coder-V2-Base, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K.</span><br><span class="line"></span><br><span class="line">90.2% on HumanEval and 75.7% on MATH. These are higher numbers than GPT-4-Turbo-0409 according to their technical report.</span><br><span class="line">More:</span><br><span class="line">&gt; includes 16B and 236B parameter models</span><br><span class="line">&gt; further pretrained from DeepSeek-V2 checkpoint</span><br><span class="line">&gt; uses an additional 6 trillion tokens</span><br><span class="line">&gt; expands to 338 programming languages</span><br><span class="line">&gt; context length extended from 16K to 128K</span><br><span class="line">&gt; permissive license allows for both research and unrestricted commercial use</span><br><span class="line">Still not quite there for instruction-following capabilities as compared to GPT-4 Turbo but has huge potential to improve.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www-aitimes-com.cdn.ampproject.org/c/s/www.aitimes.com/news/articleViewAmp.html?idxno=156265</span><br><span class="line">â€ì˜¤í”ˆAI, ë°•ì‚¬ê¸‰ ì—°êµ¬ì› ì´ˆë´‰ 11ì–µâ€...ê¸‰ì—¬ ìˆœìœ„ ê³µê°œ</span><br><span class="line">ê¸‰ì—¬ í˜‘ìƒ ì„œë¹„ìŠ¤ ê¸°ì—… ë¡œë¼ ì§‘ê³„</span><br><span class="line">2024-01-03     ë°•ì°¬ ê¸°ì</span><br><span class="line">ë°•ì‚¬ê¸‰ AI ì—°êµ¬ì›ì˜ ì´ˆê¸° ë³´ìƒ ì œì•ˆê³¼ ìµœì¢… ë³´ìƒ ì œì•ˆ ë¹„êµ(ì‚¬ì§„=ë¡œë¼)</span><br><span class="line">ë°•ì‚¬ê¸‰ AI ì—°êµ¬ì›ì˜ ì´ˆê¸° ë³´ìƒ ì œì•ˆê³¼ ìµœì¢… ë³´ìƒ ì œì•ˆ ë¹„êµ(ì‚¬ì§„=ë¡œë¼)</span><br><span class="line">ì˜¤í”ˆAIì˜ ë°•ì‚¬ê¸‰ ì¸ê³µì§€ëŠ¥(AI) ì—°êµ¬ì› ì´ˆë´‰ì´ 86ë§Œ5000ë‹¬ëŸ¬(ì•½ 11ì–µ3000ë§Œì›)ë¡œ ì—…ê³„ ìµœê³  ìˆ˜ì¤€ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤. ìµœê³ ê¸‰ ìŠ¤íƒ€íŠ¸ì—…ê³¼ ë¹…í…Œí¬ì˜ ì´ˆë´‰ë„ 9ì–µ~10ì–µì›ì— ë‹¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì¡Œë‹¤. ê·¸ë§Œí¼ AI ì—°êµ¬ì›ì´ ë¶€ì¡±í•˜ë‹¤ëŠ” ì„¤ëª…ì´ë‹¤.</span><br><span class="line"></span><br><span class="line">ë¦¬ë“œë¼ì´íŠ¸ëŠ” 2ì¼(í˜„ì§€ì‹œê°„) ê¸‰ì—¬ í˜‘ìƒ ì„œë¹„ìŠ¤ ê¸°ì—…ì¸ ë¡œë¼ì˜ ì§‘ê³„ë¥¼ ì¸ìš©, ì‹ ê·œ ë°•ì‚¬ê¸‰ AI ì—°êµ¬ì›ì„ ì±„ìš©í•œ 600ì—¬ê°œ ê¸°ì—… ì¤‘ ì˜¤í”ˆAIì™€ ì•¤íŠ¸ë¡œí”½ì´ ê°ê° 86ë§Œ5000ë‹¬ëŸ¬ì™€ 85ë§Œ5000ë‹¬ëŸ¬(ì•½ 11ì–µ2000ë§Œì›)ë¡œ ê°€ì¥ ë†’ì€ ì´ˆë´‰ì„ ì œê³µí–ˆë‹¤ê³  ë³´ë„í–ˆë‹¤. ì´ˆë´‰ì—ëŠ” ê¸°ë³¸ê¸‰ê³¼ ë³´ë„ˆìŠ¤, ì£¼ì‹ ë“±ì´ í¬í•¨ëœë‹¤. </span><br><span class="line"></span><br><span class="line">ì´ì— ë”°ë¥´ë©´ ì˜¤í”ˆAIì™€ ì•¤íŠ¸ë¡œí”½ì˜ ë¼ì´ë²Œë¡œ ê¼½íˆëŠ” ì¸í”Œë ‰ì…˜ AIê°€ 82ë§Œ5000ë‹¬ëŸ¬(ì•½ 10ì–µ8000ë§Œì›)ë¡œ 3ìœ„ë¥¼ ì°¨ì§€í–ˆë‹¤.</span><br><span class="line"></span><br><span class="line">ì´ì–´ í…ŒìŠ¬ë¼ 78ë§Œë‹¬ëŸ¬(ì•½ 10ì–µ2000ë§Œì›), ì•„ë§ˆì¡´ 71ë§Œ9000ë‹¬ëŸ¬(ì•½ 9ì–µ4000ë§Œì›), êµ¬ê¸€ ë¸Œë ˆì¸ 69ë§Œ5000ë‹¬ëŸ¬(ì•½ 9ì–µ1000ë§Œì›) ë“±ìœ¼ë¡œ ë¹…í…Œí¬ë³´ë‹¤ ì „ë¬¸ ìŠ¤íƒ€íŠ¸ì—…ì˜ ì¸ì¬ í™•ë³´ ê²½ìŸì´ ë” ì¹˜ì—´í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.</span><br><span class="line"></span><br><span class="line">ê·¸ëŸ¬ë‚˜ ì´ˆê¸° ì œì•ˆê³¼ ìµœì¢… ì œì•ˆ ì‚¬ì´ì˜ í˜‘ìƒí­ì€ êµ¬ê¸€ ë¦¬ì„œì¹˜ê°€ í‰ê·  77%ë¡œ ê°€ì¥ ë†’ì•˜ìœ¼ë©°, ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ ë¦¬ì„œì¹˜, ë¸”ë£¸ë²„ê·¸ AI, IBM ë¦¬ì„œì¹˜, í‹±í†¡ ë“±ì˜ ìˆœì´ì—ˆë‹¤. êµ¬ê¸€ ë¦¬ì„œì¹˜ì˜ í•œ ì—°êµ¬ì›ì€ ì´ˆê¸° ì œì•ˆìœ¼ë¡œ 21ë§Œ6000ë‹¬ëŸ¬(ì•½ 2ì–µ8000ë§Œì›)ë¥¼ ë°›ì•˜ìœ¼ë‚˜, í˜‘ìƒì„ í†µí•´ 243% ì¦ê°€í•œ ìµœì¢… 52ë§Œ6000ë‹¬ëŸ¬(ì•½ 6ì–µ9000ë§Œì›)ì˜ ì—°ë´‰ì„ ë°›ê²Œ ëë‹¤.</span><br><span class="line"></span><br><span class="line">ë°•ì‚¬ê¸‰ AI ì—°êµ¬ì›ì˜ ì´ˆë´‰ ìˆœìœ„(ì‚¬ì§„=ë¡œë¼) </span><br><span class="line">ë°•ì‚¬ê¸‰ AI ì—°êµ¬ì›ì˜ ì´ˆë´‰ ìˆœìœ„(ì‚¬ì§„=ë¡œë¼) </span><br><span class="line">ì´ì²˜ëŸ¼ ë°•ì‚¬ê¸‰ AI ì—°êµ¬ì›ì˜ ì—°ë´‰ ìˆ˜ì¤€ì´ ë†’ì€ ì´ìœ ëŠ” AI ê¸°ìˆ ì— ëŒ€í•œ ì „ ì„¸ê³„ ìˆ˜ìš”ê°€ ì‹¤ì œ ê³µê¸‰ë³´ë‹¤ í›¨ì”¬ ë” í¬ê¸° ë•Œë¬¸ì´ë‹¤.</span><br><span class="line"></span><br><span class="line">í†¨ë¹„ ì„œë² ì´ì˜ ì„¤ë¬¸ì¡°ì‚¬ì— ë”°ë¥´ë©´ 2021ë…„ì—ëŠ” ì»´í“¨íŒ… ì—°êµ¬ ë¶„ì•¼ì—ì„œ ìˆ˜ì—¬ëœ ë°•ì‚¬ í•™ìœ„ê°€ 1691ëª…ì— ë¶ˆê³¼í–ˆë‹¤. ë¯¸êµ­ì—ì„œë§Œ 3ë§Œ35000ëª…ì˜ ì»´í“¨í„° ë° ì •ë³´ ì—°êµ¬ì›ì´ í•„ìš”í•˜ë©° ìˆ˜ìš”ëŠ” ì—°ê°„ 21% ì¦ê°€í•˜ê³  ìˆë‹¤. ì¦‰ ë§¤ë…„ í•„ìš”í•œ ì—°êµ¬ì›ë³´ë‹¤ ì¼ìë¦¬ê°€  5000ê°œ ì´ìƒ ë§ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. </span><br><span class="line"></span><br><span class="line">í˜„ì¬ ê°€ì¥ ìˆ˜ìš”ê°€ ë†’ì€ ë¶„ì•¼ëŠ” ì»´í“¨í„° ë¹„ì „, ë¡œë´‡ê³µí•™, ìì—°ì–´ ì²˜ë¦¬(NLP), ìƒë¬¼í•™, ì‹ ê²½ê³¼í•™ ë“±ì— AIë¥¼ ì ìš©í•˜ëŠ” ë¶„ì•¼ë‹¤. &#x27;ì±—GPT&#x27;ê°€ ë„ì…ë˜ë©´ì„œ ëŒ€í˜•ì–¸ì–´ëª¨ë¸(LLM)ì— ëŒ€í•œ ì „ë¬¸ì„±ì€ ìµœê³  ì¸ê¸° ê¸°ìˆ ì´ ëë‹¤. </span><br><span class="line"></span><br><span class="line">ë¦¬ë“œë¼ì´íŠ¸ëŠ” AI ì—°êµ¬ì›ì—ê²ŒëŠ” ê²€ì¦ëœ ì—°êµ¬ ëŠ¥ë ¥ì´ ë¬´ì—‡ë³´ë‹¤ ì¤‘ìš”í•˜ë‹¤ê³  ì§€ì í–ˆë‹¤. ì´ë¥¼ ì…ì¦í•˜ëŠ” ê²ƒ ì¤‘ í•˜ë‚˜ë¥¼ ë…¼ë¬¸ ì¶œíŒ ê¸°ë¡ìœ¼ë¡œ ê¼½ì•˜ë‹¤.</span><br><span class="line"></span><br><span class="line">ì—…ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ì—°êµ¬ì›ë“¤ì€ ë°•ì‚¬ í•™ìœ„ ë…¼ë¬¸ë§Œìœ¼ë¡œ ìµœëŒ€ 2000ë²ˆì˜ ì¸ìš©ê³¼ &#x27;H-ì§€ìˆ˜(H-index) 10&#x27;ì„ ë³´ìœ í•˜ê²Œ ëœë‹¤ê³  ì „í–ˆë‹¤. H-ì§€ìˆ˜ 10ì€ ë…¼ë¬¸ ì¸ìš©íšŸìˆ˜ê°€ 10ì´ ë„˜ëŠ” ë…¼ë¬¸ì´ ì ì–´ë„ 10í¸ì´ ëœë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. </span><br><span class="line"></span><br><span class="line">ì´ ì •ë„ ëŠ¥ë ¥ì´ë©´ ë†’ì€ ì§ìœ„ì™€ ìµœê³  ë³´ìƒì„ ìš”êµ¬í•  ìˆ˜ ìˆëŠ” ìµœê³  ì—°êµ¬ì›ê¸‰ ì˜í–¥ë ¥ì„ ê°€ì§„ë‹¤ëŠ” ì„¤ëª…ì´ë‹¤.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://deepmind.google/discover/blog/generating-audio-for-video/</span><br><span class="line">google research</span><br><span class="line"></span><br><span class="line">RESEARCH</span><br><span class="line"></span><br><span class="line">Generating audio for video</span><br><span class="line">Published</span><br><span class="line">17 JUNE 2024</span><br><span class="line">Authors</span><br><span class="line">Generative Media team</span><br><span class="line"></span><br><span class="line">Share</span><br><span class="line"></span><br><span class="line">Video-to-audio research uses video pixels and text prompts to generate rich soundtracks</span><br><span class="line"></span><br><span class="line">Video generation models are advancing at an incredible pace, but many current systems can only generate silent output. One of the next major steps toward bringing generated movies to life is creating soundtracks for these silent videos.</span><br><span class="line"></span><br><span class="line">Today, we&#x27;re sharing progress on our video-to-audio (V2A) technology, which makes synchronized audiovisual generation possible. V2A combines video pixels with natural language text prompts to generate rich soundscapes for the on-screen action.</span><br><span class="line"></span><br><span class="line">Our V2A technology is pairable with video generation models like Veo to create shots with a dramatic score, realistic sound effects or dialogue that matches the characters and tone of a video.</span><br><span class="line"></span><br><span class="line">It can also generate soundtracks for a range of traditional footage, including archival material, silent films and more â€” opening a wider range of creative opportunities.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:13</span><br><span class="line">Prompt for audio: Cinematic, thriller, horror film, music, tension, ambience, footsteps on concrete</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:08</span><br><span class="line">Prompt for audio: Cute baby dinosaur chirps, jungle ambience, egg cracking</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:09</span><br><span class="line">Prompt for audio: jellyfish pulsating under water, marine life, ocean</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:09</span><br><span class="line">Prompt for audio: A drummer on a stage at a concert surrounded by flashing lights and a cheering crowd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:12</span><br><span class="line">Prompt for audio: cars skidding, car engine throttling, angelic electronic music</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:08</span><br><span class="line">Prompt for audio: a slow mellow harmonica plays as the sun goes down on the prairie</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:07</span><br><span class="line">Prompt for audio: Wolf howling at the moon</span><br><span class="line"></span><br><span class="line">Enhanced creative control</span><br><span class="line">Importantly, V2A can generate an unlimited number of soundtracks for any video input. Optionally, a â€˜positive promptâ€™ can be defined to guide the generated output toward desired sounds, or a â€˜negative promptâ€™ to guide it away from undesired sounds.</span><br><span class="line"></span><br><span class="line">This flexibility gives users more control over V2Aâ€™s audio output, making it possible to rapidly experiment with different audio outputs and choose the best match.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:08</span><br><span class="line">Prompt for audio: A spaceship hurtles through the vastness of space, stars streaking past it, high speed, Sci-fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:08</span><br><span class="line">Prompt for audio: Ethereal cello atmosphere</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:08</span><br><span class="line">Prompt for audio: A spaceship hurtles through the vastness of space, stars streaking past it, high speed, Sci-fi</span><br><span class="line"></span><br><span class="line">How it works</span><br><span class="line">We experimented with autoregressive and diffusion approaches to discover the most scalable AI architecture, and the diffusion-based approach for audio generation gave the most realistic and compelling results for synchronizing video and audio information.</span><br><span class="line"></span><br><span class="line">Our V2A system starts by encoding video input into a compressed representation. Then, the diffusion model iteratively refines the audio from random noise. This process is guided by the visual input and natural language prompts given to generate synchronized, realistic audio that closely aligns with the prompt. Finally, the audio output is decoded, turned into an audio waveform and combined with the video data.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Diagram of our V2A system, taking video pixel and audio prompt input to generate an audio waveform synchronized to the underlying video. First, V2A encodes the video and audio prompt input and iteratively runs it through the diffusion model. Then it generates compressed audio, which is decoded into an audio waveform.</span><br><span class="line"></span><br><span class="line">To generate higher quality audio and add the ability to guide the model towards generating specific sounds, we added more information to the training process, including AI-generated annotations with detailed descriptions of sound and transcripts of spoken dialogue.</span><br><span class="line"></span><br><span class="line">By training on video, audio and the additional annotations, our technology learns to associate specific audio events with various visual scenes, while responding to the information provided in the annotations or transcripts.</span><br><span class="line"></span><br><span class="line">Further research underway</span><br><span class="line">Our research stands out from existing video-to-audio solutions because it can understand raw pixels and adding a text prompt is optional.</span><br><span class="line"></span><br><span class="line">Also, the system doesn&#x27;t need manual alignment of the generated sound with the video, which involves tediously adjusting different elements of sounds, visuals and timings.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:09</span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:09</span><br><span class="line">Still, there are a number of other limitations weâ€™re trying to address and further research is underway.</span><br><span class="line"></span><br><span class="line">Since the quality of the audio output is dependent on the quality of the video input, artifacts or distortions in the video, which are outside the modelâ€™s training distribution, can lead to a noticeable drop in audio quality.</span><br><span class="line"></span><br><span class="line">Weâ€™re also improving lip synchronization for videos that involve speech. V2A attempts to generate speech from the input transcripts and synchronize it with characters&#x27; lip movements. But the paired video generation model may not be conditioned on transcripts. This creates a mismatch, often resulting in uncanny lip-syncing, as the video model doesnâ€™t generate mouth movements that match the transcript.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Watch</span><br><span class="line"></span><br><span class="line">00:09</span><br><span class="line">Prompt for audio: Music, Transcript: â€œthis turkey looks amazing, Iâ€™m so hungryâ€</span><br><span class="line"></span><br><span class="line">Our commitment to safety and transparency</span><br><span class="line">Weâ€™re committed to developing and deploying AI technologies responsibly. To make sure our V2A technology can have a positive impact on the creative community, weâ€™re gathering diverse perspectives and insights from leading creators and filmmakers, and using this valuable feedback to inform our ongoing research and development.</span><br><span class="line"></span><br><span class="line">Weâ€™ve also incorporated our SynthID toolkit into our V2A research to watermark all AI-generated content to help safeguard against the potential for misuse of this technology.</span><br><span class="line"></span><br><span class="line">Before we consider opening access to it to the wider public, our V2A technology will undergo rigorous safety assessments and testing. Initial results are showing this technology will become a promising approach for bringing generated movies to life.</span><br><span class="line"></span><br><span class="line">Note: All examples are generated by our V2A technology, which is paired with Veo, our most capable generative video model.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.microsoft.com/en-us/research/blog/introducing-autogen-studio-a-low-code-interface-for-building-multi-agent-workflows/</span><br><span class="line">Microsoft Research Blog</span><br><span class="line">Introducing AutoGen Studio: A low-code interface for building multi-agent workflows</span><br><span class="line">Published June 17, 2024</span><br><span class="line"></span><br><span class="line">By Victor Dibia , Principal Research Software Engineer  Gagan Bansal , Senior Researcher  Jingya Chen , UX Designer  Suff Syed , Principal Design Director  Adam Fourney , Principal Researcher  Erkang (Eric) Zhu , Senior Researcher  Chi Wang , Principal Researcher  Saleema Amershi , Senior Principal Research Manager</span><br><span class="line"></span><br><span class="line">Share this page</span><br><span class="line"></span><br><span class="line">Share on Facebook</span><br><span class="line">Share on Twitter</span><br><span class="line">Share on LinkedIn</span><br><span class="line">Share on Reddit</span><br><span class="line">Subscribe to our RSS feed</span><br><span class="line">White icons representing (from left to right) agents (multi), workflow, tasks, and coding on a blue to purple to pink gradient background.</span><br><span class="line">Multi-agent approaches to AI applications, where multiple foundation model-based agents collaborate to solve problems, are emerging as a powerful paradigm for accomplishing increasingly complex tasks. In September 2023, we released AutoGen â€“ a flexible and open-source Python-based framework for defining, configuring, and composing AI agents to drive multi-agent applications. Today, we are introducing AutoGen Studio (version 0.1.0) â€“ a low-code interface for rapidly building, testing, and sharing multi-agent solutions. AutoGen Studio is built on AutoGen and inherits its features and functionalities, while providing a user-friendly and intuitive interface to create and customize agents, with little to no coding required.</span><br><span class="line"></span><br><span class="line">PROJECT</span><br><span class="line">AutoGen </span><br><span class="line">During the nine months since it was released, AutoGen(opens in new tab) has been widely adopted by researchers, developers, and enthusiasts who have created a variety of novel and exciting applications(opens in new tab) â€“ from market research to interactive educational tools to data analysis pipelines in the medical domain.  With more than 290 community contributors on GitHub and 890,000 downloads of the Python package (as of May 2024), AutoGen continues to be a leading framework for building and researching multi-agent AI applications.</span><br><span class="line"></span><br><span class="line">AutoGen Studio user interface: PDF Book Gen Session</span><br><span class="line">A screenshot of the AutoGen Studio interface shows results when two agents are used to address the task, â€œCreate a 4-page kidsâ€™ .pdf book with details and pictures about weather patterns in Seattleâ€.</span><br><span class="line">AutoGen Studio is the next step forward in enabling developers to advance the multi-agent paradigm. We want to make multi-agent solutions responsibly available to diverse audiences â€“ from academic researchers to professional developers across industries â€“ who want to build multi-agent applications to solve real-world problems. Imagine having access to agents that can automate your vacation planning and grocery shopping, manage your personal finances, help you accomplish your learning goals, or perform any other task you care about. How would you build such agents? What capabilities would you give them? How would you make them work together? How would you ensure they are working as intended?</span><br><span class="line"></span><br><span class="line">DOWNLOAD</span><br><span class="line">AutoGen Studio </span><br><span class="line">These questions motivated us to build AutoGen Studio. With AutoGen Studio, developers can rapidly build, test, deploy, and share agents and agent-teams (workflows), with the community. </span><br><span class="line"></span><br><span class="line">Note: AutoGen is primarily a developer tool to enable rapid prototyping and research. It is not a production ready tool. Please see the GitHub repository(opens in new tab) and documentation(opens in new tab) for instructions on how to get started.</span><br><span class="line"></span><br><span class="line">What can you do with AutoGen Studio right now?</span><br><span class="line">We built AutoGen Studio with the following goals in mind:  </span><br><span class="line"></span><br><span class="line">Lower the barrier to entry in building multi-agent applications  </span><br><span class="line">Facilitate rapid prototyping and testing of multi-agent solutions</span><br><span class="line">Cultivate expertise and community by allowing users to share and re-use this technology </span><br><span class="line">With AutoGen Studioâ€™s early release (v 0.1.0), users can rapidly author agent workflows via a user interface, interactively test and debug agents, reuse artifacts, and deploy workflows.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The video above shows how users can create skills and models, attach them to agents, create agent workflows, test and deploy them in AutoGen Studio. All in a few clicks.</span><br><span class="line">Rapidly author agent workflows</span><br><span class="line">AutoGen Studio provides a â€œBuildâ€ section where users can choose from a library of pre-defined agents and compose them into teams (workflows) that can address tasks in minutes. Furthermore, users can customize agents and agent teams with foundation models, prompts, skills (python functions that accomplish a specific task e.g., fetching the weather from a weather provider), and workflows via a graphical user interface.  Workflows may be sequential (where agents act in a predefined sequential order) or autonomous chat (where the order in which agents act may be driven by a large language model, custom logic, all based on the state of the task).</span><br><span class="line"></span><br><span class="line">AutoGen Studio user interface: agent configuration</span><br><span class="line">In AutoGen Studio, agents can be configured via the user interface. Models and skills can be associated with agents, and agents can be composed into autonomous chat and sequential workflows.</span><br><span class="line">Debug and test agents</span><br><span class="line">AutoGen Studio allows developers to immediately test workflows on a variety of tasks and review resulting artifacts (such as images, code, and documents). Developers can also review the â€œinner monologueâ€ of agent workflows as they address tasks, and view profiling information such as costs associated with the run (such as number of turns and number of tokens), and agent actions (such as whether tools were called and the outcomes of code execution).</span><br><span class="line"></span><br><span class="line">AutoGen Studio user interface: profile sample workflow</span><br><span class="line">AutoGen Studio user interface: sample workflow</span><br><span class="line">In AutoGen Studio, users can test workflows, see results, and view visualizations that profile agent actions (such as how often tools were used or code was executed).</span><br><span class="line">Artifact reuse and deployment</span><br><span class="line">Users can download the skills, agents, and workflow configurations they create as well as share and reuse these artifacts.  AutoGen Studio also offers a seamless process to export workflows and deploy them as application programming interfaces (APIs) that can be consumed in other applications deploying workflows as APIs.</span><br><span class="line"></span><br><span class="line">Specifically, workflows can be exported as JavaScript Object Notation (JSON) files and loaded into any python application, launched as an API endpoint from the command line or wrapped into a Dockerfile that can be deployed on cloud services like Azure Container Apps or Azure Web Apps.</span><br><span class="line"></span><br><span class="line">AutoGen Studio user interface: export workflow</span><br><span class="line">In AutoGen Studio, users can export agent workflows as a JSON configuration file and then reuse them in any python application, launch it as an API from the command line or deploy on a cloud service like Azure Container Apps and Azure Web Apps.</span><br><span class="line">MICROSOFT RESEARCH PODCAST</span><br><span class="line"></span><br><span class="line">Microsoft Research Podcast | What&#x27;s Your Story | Weishung Liu</span><br><span class="line">Whatâ€™s Your Story: Weishung Liu</span><br><span class="line">Principal PM Manager Weishung Liu shares how a career delivering products and customer experiences aligns with her love of people and storytelling and howâ€”despite efforts to defy the expectations that come with growing up in Silicon Valleyâ€”she landed in tech.</span><br><span class="line"></span><br><span class="line">Listen now</span><br><span class="line">Opens in a new tab</span><br><span class="line">What is the community creating with AutoGen Studio?</span><br><span class="line">Over the last few months, we have shared an early version of AutoGen Studio, which has been downloaded more than 154,000 times on pypi (January â€“ May 2024). Our observations of early usage patterns (based on feedback from social platforms like GitHub discussions(opens in new tab) , Discord(opens in new tab) and Youtube(opens in new tab) (opens in new tab)) suggest that AutoGen Studio is driving a new group of users who have basic technical capabilities (that is, they can install the tool) and are interested in rapidly testing out ideas but have limited programming skills.</span><br><span class="line"></span><br><span class="line">We have seen these users prototype examples covering tasks like travel planning, pdf brochure generation, market research, structured data extraction, video generation, and visualization generation among others. Importantly, these tasks are accomplished simply by defining agents, giving them access to large language models and skills, adding agents to a workflow, and running tasks with these workflows.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Users are exploring early use cases such as report/book generation, as seen in the screenshot above. Here, two agents are defined and given access to skills for generating images. The agents are then composed into a workflow where messages and actions are exchanged to solve the task of generating a pdf report.</span><br><span class="line">Open research questions and next steps</span><br><span class="line">Orchestrating teams of agents that can explore plans, reflect on actions, and collaborate offers opportunities to build tools that address challenging tasks. We believe that we are just scratching the surface of what may be possible with the multi-agent paradigm, and much is unknown about how best to harness foundation models, let alone foundation model-based agents and multi-agent solutions.</span><br><span class="line"></span><br><span class="line">This leaves open many opportunities for further research.</span><br><span class="line"></span><br><span class="line">For example, the sophisticated interplay between agents in multi-agent paradigms, particularly for increasingly more complex and dynamic domains, highlights many opportunities for multi-agent evaluation and tooling. Open questions include:</span><br><span class="line"></span><br><span class="line">How can we measure the performance, reliability, and reusability of agents across tasks?</span><br><span class="line">How can we better understand the strengths and limitations of agents?</span><br><span class="line">How can we explore alternative scenarios and outcomes?</span><br><span class="line">How can we compare different agent architectures and collaboration protocols?</span><br><span class="line">These questions require novel methods and metrics that can capture the multi-faceted aspects of multi-agent paradigms and provide actionable insights for developers and users.</span><br><span class="line"></span><br><span class="line">As our understanding of the multi-agent paradigm matures, another opportunity is in distilling design patterns and best practices for building effective agent teams for different types of tasks. For instance:</span><br><span class="line"></span><br><span class="line">What are the optimal number and composition of agents for a given problem?</span><br><span class="line">What is the best way to distribute responsibilities and coordinate actions among agents?</span><br><span class="line">What are the trade-offs between centralized and decentralized control, or between homogeneous and heterogeneous agents?</span><br><span class="line">How can we leverage human oversight and feedback to improve agent reliability and safety?</span><br><span class="line">These questions require systematic studies and empirical evaluations to discover the key dimensions and principles for designing multi-agent solutions.</span><br><span class="line"></span><br><span class="line">Finally, as agents become more long-lived and ubiquitous in our digital world, an open challenge is in automating and optimizing the agent-creation process itself. For example:</span><br><span class="line"></span><br><span class="line"> How can we dynamically spawn agents based on the task requirements and available resources?</span><br><span class="line">How can we tune agent parameter workflow configurations to achieve the best performance?</span><br><span class="line">How can we adapt agent teams to changing environments and user preferences?</span><br><span class="line">Future design improvements</span><br><span class="line">Naturally, we see AutoGen Studio as a potential vehicle to study many of these research questions â€“ from improvements in the user experience of authoring workflows to a gallery of shareable artifacts to advanced tools for making sense of agent behaviors.</span><br><span class="line"></span><br><span class="line">We are currently working on a new drag-and-drop experience in AutoGen Studio, designed to transform how usersâ€™ author multi-agent workflows. Our new visual canvas allows users to easily orchestrate and connect agents, providing an intuitive interface for defining collaboration dynamics.</span><br><span class="line"></span><br><span class="line">AutoGen Studio user interface: visual workflow design</span><br><span class="line">A new visual canvas interface for AutoGen allows users to easily orchestrate and connect agents, providing an intuitive interface for defining collaboration dynamics. Entities such as skills and models can be associated with agents via drag-and-drop interactions.</span><br><span class="line">Visual workflow design: The heart of our enhanced user interface is a visual canvas where you can literally see your workflow come to life. Drag and drop different agents onto the canvas to build complex conversation patterns. This graphical approach not only simplifies the initial setup but also makes the process of modifying agents and workflows more intuitive.</span><br><span class="line"></span><br><span class="line">A new visual canvas interface for AutoGen that allows users to both visualize agent interactions as well as update properties of each agent in the same view pane.</span><br><span class="line">A new visual canvas interface for AutoGen allows users to both visualize agent interactions and update properties of each agent in the same view pane.</span><br><span class="line">Configurable agents, models, and skills: Customize each agentâ€™s role and skills through simple, direct interactions on the canvas. Whether youâ€™re adding new capabilities or tweaking existing ones, the process is straightforward and user-friendly.</span><br><span class="line"></span><br><span class="line">AutoGen Studio user interface: dynamic prototyping and testing</span><br><span class="line">The proposed visual canvas interface for AutoGen will explore updated visualization of agent internal monologues for improved debugging.</span><br><span class="line">Dynamic prototyping and testing: Experimentation is key to perfecting agent workflows. With our new interface, you can prototype various agent configurations and immediately test them in a live environment. This real-time interaction allows you to chat with the workflow, observe all agent messages, and pinpoint areas for improvement on the fly.</span><br><span class="line"></span><br><span class="line">AutoGen Studio community gallery</span><br><span class="line">The new proposed design explores a gallery of curated workflows and entities (such as skills and agents) that can be reused.</span><br><span class="line">Finally, we are developing a community gallery within AutoGen Studio where users can share, discover, and learn from one another. This gallery will allow you to publish your workflows, agents, and skills, fostering a collaborative environment where everyone can benefit from shared knowledge and innovations.</span><br><span class="line"></span><br><span class="line">Note on responsible AI: Promoting safe and ethical multi-agent solutions</span><br><span class="line">AutoGen Studio is designed to provide a low-code environment for rapidly prototyping and testing multi-agent workflows. Our goal is to responsibly advance research and practice in solving problems with multiple agents and to develop tools that contribute to human well-being. Along with AutoGen, AutoGen Studio is committed to implementing features that promote safe and reliable outcomes. For example, AutoGen Studio offers profiling tools to make sense of agent actions and safeguards, such as support for Docker environments for code execution. This feature helps ensure that agents operate within controlled and secure environments, reducing the risk of unintended or harmful actions. For more information on our approach to responsible AI in AutoGen,  please refer to transparency FAQS here: https://github.com/microsoft/autogen/blob/main/TRANSPARENCY_FAQS.md(opens in new tab). Finally, AutoGen Studio is not production ready i.e., it does not focus on implementing authentication and other security measures that are required for production ready deployments.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://research.google/blog/pre-translation-vs-direct-inference-in-multilingual-llm-applications/</span><br><span class="line">google research</span><br><span class="line">Pre-translation vs. direct inference in multilingual LLM applications</span><br><span class="line">June 14, 2024</span><br><span class="line"></span><br><span class="line">Roman Goldenberg, Research Scientist, Verily AI, and Natalie Aizenberg, Research Software Engineer, Google Research &amp; Verily AI</span><br><span class="line"></span><br><span class="line">A comprehensive evaluation comparing pre-translation with direct inference of PaLM2 on multilingual tasks, demonstrating its improved performance using direct inference in the source language, compared to pre-translation to English. PaLM2 models do not need pre-translation to excel in multilingual tasks, as demonstrated in a comprehensive evaluation comparing direct inference with pre-translation.</span><br><span class="line"></span><br><span class="line">Large language models (LLMs) are becoming omnipresent tools for solving a wide range of problems. However, their effectiveness in handling diverse languages has been hampered by inherent limitations in training data, which are often skewed towards English. To address this, pre-translation, where inputs are translated to English before feeding them to the LLM, has become a standard practice.</span><br><span class="line"></span><br><span class="line">Previous research has demonstrated the effectiveness of pre-translation for optimal LLM performance for GPT-3/3.5/4, ChatGPT, PaLM and other models. While pre-translation helps address the language bias issue, it introduces complexities and inefficiencies, and it may lead to information loss. With the introduction of new powerful LLMs trained on massive multilingual datasets, it is time to revisit the assumed necessity of pre-translation.</span><br><span class="line"></span><br><span class="line">In our recent work â€œBreaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?â€, to be presented at NAACLâ€™24, we re-evaluate the need for pre-translation using PaLM2, which has been established as highly performant in multilingual tasks. Our findings challenge the pre-translation paradigm established in prior research and highlight the advantages of direct inference in PaLM2. Specifically, we demonstrate that PaLM2-L consistently outperforms pre-translation in 94 out of 108 languages, offering a more efficient and effective application in multilingual settings while unlocking linguistic authenticity and alleviating the limitations of pre-translation.</span><br><span class="line"></span><br><span class="line">Rethinking multilingual LLM evaluation</span><br><span class="line">Prior research on evaluating the impact of pre-translation mainly focused on discriminative (close-ended) tasks, such as multiple choice question answering (QA), for which the language of the answer is mostly insignificant. For evaluating generative (open-ended) tasks, such as text summarization or attributed QA, the output needs to be in the source language to compare it to the ground truth (GT). This requires adding an extra post-inference translation step. While for source language inference evaluation (a in the figure below), inference is directly compared to GT in the source language, for pre-translation evaluation (b), LLM inference is translated back to source language (c.1).</span><br><span class="line"></span><br><span class="line">BtLB-1-Source</span><br><span class="line">Comparative evaluation of direct inference vs. pre-translation in source language.</span><br><span class="line"></span><br><span class="line">One of the drawbacks of this evaluation scheme is that comparing model output to GT in different languages using standard lexical metrics, such as ROUGE and F1, is language dependent and introduces inconsistencies. Another problem with this approach is that GT answers in open-ended tasks rely primarily on information present within the provided context. Specifically, in reading comprehension Q&amp;A benchmarks, it is common to have the GT be a substring of the original context. This presents a potential disadvantage for pre-translation, which lacks access to the original context from which the GT was extracted.</span><br><span class="line"></span><br><span class="line">To address both these caveats, we perform a complimentary evaluation in English by translating the GT and direct inference results to English. Here, instead of translating the pre-translated inference back to source language, we translate the direct inference output and GT to English (as illustrated below in panels c.2 and c.3, respectively). Then the evaluation against GT is performed in English.</span><br><span class="line"></span><br><span class="line">BtLB-2-English</span><br><span class="line">Comparative evaluation of direct inference vs. pre-translation in English.</span><br><span class="line"></span><br><span class="line">In addition, we found that averaging LLM accuracy metrics across languages, as done in the prior approaches, can be misleading, masking crucial details. To gain a more nuanced understanding, we introduced the Language Ratio metric as an alternative aggregation over commonly used lexical metrics. It is defined as the percentage of languages for which direct inference yields better results than pre-translation.</span><br><span class="line"></span><br><span class="line">The Language Ratio can be computed for any accuracy score of choice (such as F1 or Rouge) over a single inference mode (direct and pre-translation) and language. By inspecting the proportion of languages where one method outperforms the other, rather than averaging language bias scores, a fairer overall comparison and more detailed understanding of relative strengths and weaknesses across languages is possible.</span><br><span class="line"></span><br><span class="line">Direct inference takes the lead</span><br><span class="line">Our analysis encompassed a variety of tasks and languages. We employed six publicly available benchmarks to evaluate PaLM2&#x27;s performance in both discriminative (XCOPA, XStoryCLoze and BeleBele benchmarks) and generative tasks (XLSum, TyDiQA-GP and XQuAD) across 108 languages. Two variants of PaLM2 were evaluated: PaLM2-S (Small - Bison) and PaLM2-L (Large - Unicorn), while using Google Translation API for pre- and post-translation.</span><br><span class="line"></span><br><span class="line">BtLB-4-Results</span><br><span class="line">PaLM2-S (left) and PaLM2-L (right) evaluation results, comparing pre-translation (blue) and direct inference (red). Model performance for generative (open-ended) tasks is evaluated both in the source language and in English. Top: Accuracy metrics (accuracy, Rouge-L, F1) measured on various benchmarks. Bottom: Language Ratio metric.</span><br><span class="line"></span><br><span class="line">The results were strikingly different from those reported in prior literature for other models.</span><br><span class="line"></span><br><span class="line">PaLM2-L consistently achieved better performance with direct inference in 94 out of 108 languages evaluated. The advantage was observed for both close- and open-ended tasks, on all benchmarks. The results were consistent across all evaluations â€” in source language and in English, using standard metrics (Accuracy/F1/Rouge) and the Language Ratio.</span><br><span class="line">PaLM2-S also favors direct inference in all but the XQuAD benchmark, where the result is less conclusive. Better average F1 score is achieved using direct inference (due to significant improvements in Chinese and Thai), but the Language Ratio is better for pre-translation, which emphasizes the complimentary value of this metric.</span><br><span class="line">Direct inference yielded superior results even in low-resource languages (LRL). This is particularly significant for fostering communication and information access in under-represented languages.</span><br><span class="line">Language-focused analysis</span><br><span class="line">While PaLM2-L clearly performs better using direct inference for the majority of languages, pre-translation shows consistent superiority (across benchmarks) for 7 languages: Bambara, Cusco-Collao Quechua, Lingala, Oromo, Punjabi, Tigrinya, and Tsonga. All 7 are LRL, 4 out of 7 are African, with Lingala, the largest, spoken by over 40 million people. Interestingly, the majority (85%) of LRL benefit from direct inference with PaLM2.</span><br><span class="line"></span><br><span class="line">BtLB-5-Performance</span><br><span class="line">PaLM2-L average direct inference Lift over pre-translate inference on LRL. The majority of languages (over 85%) benefit from direct inference with PaLM2, with lifts exceeding 5% (dashed line) in 63% of languages.</span><br><span class="line"></span><br><span class="line">The future of multilingual communication</span><br><span class="line">The comprehensive comparative analysis we performed in this study suggests that the new generation of LLMs, trained on massive multilingual datasets, can better handle information and communication across languages, eliminating the need for pre-translation for certain languages.</span><br><span class="line"></span><br><span class="line">We are committed to ongoing research in this area, focusing on improving LLM performance for all languages and fostering a more inclusive future for multilingual communication.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://runwayml.com/blog/introducing-gen-3-alpha/</span><br><span class="line">Introducing Gen-3 Alpha</span><br><span class="line">A new frontier for high-fidelity, controllable video generation.</span><br><span class="line">Anastasis Germanidis | June 17th, 2024</span><br><span class="line">Gen-3 Alpha is the first of an upcoming series of models trained by Runway on a new infrastructure built for large-scale multimodal training. It is a major improvement in fidelity, consistency, and motion over Gen-2, and a step towards building General World Models.</span><br><span class="line">All of the videos on this page were generated with Gen-3 Alpha with no modifications.</span><br><span class="line"></span><br><span class="line">Prompt: Subtle reflections of a woman on the window of a train moving at hyper-speed in a Japanese city.</span><br><span class="line">Prompt: An astronaut running through an alley in Rio de Janeiro.</span><br><span class="line">Prompt: FPV flying through a colorful coral lined streets of an underwater suburban neighborhood.</span><br><span class="line">Prompt: Handheld tracking shot at night, following a dirty blue ballon floating above the ground in abandon old European street.</span><br><span class="line"></span><br><span class="line">Trained jointly on videos and images, Gen-3 Alpha will power Runway&#x27;s Text to Video, Image to Video and Text to Image tools, existing control modes such as Motion Brush, Advanced Camera Controls, Director Mode as well as upcoming tools for more fine-grained control over structure, style, and motion.</span><br><span class="line"></span><br><span class="line">Gen-3 Alpha will be released with a new set of safeguards, including our new and improved in-house visual moderation system and C2PA provenance standards.</span><br><span class="line">Prompt: An empty warehouse dynamically transformed by flora that explode from the ground.</span><br><span class="line">Prompt: Close up shot of a living flame wisp darting through a bustling fantasy market at night.</span><br><span class="line">Prompt: Handheld tracking shot, following a red ballon floating above the ground in abandon street.</span><br><span class="line">Prompt: A FPV shot zooming through a tunnel into a vibrant underwater space.</span><br><span class="line">Prompt: A wide symmetrical shot of a painting in a museum. The camera zooms in close to the painting.</span><br><span class="line">Prompt: Ultra-fast disorienting hyperlapse racing through a tunnel into a labyrinth of rapidly growing vines.</span><br><span class="line">Prompt: FPV, internal locomotive cab of a train moving at hyper-speed in an old European city.</span><br><span class="line">Prompt: Zooming in hyper-fast to a dandelion to reveal macro dream-like abstract world.</span><br><span class="line">Fine-grained temporal control</span><br><span class="line">Gen-3 Alpha has been trained with highly descriptive, temporally dense captions, enabling imaginative transitions and precise key-framing of elements in the scene.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Prompt: An extreme close-up shot of an ant emerging from its nest. The camera pulls back revealing a neighborhood beyond the hill.</span><br><span class="line">Prompt: A tsunami coming through an alley in Bulgaria, dynamic movement.</span><br><span class="line">Prompt: A FPV drone shot through a castle on a cliff.</span><br><span class="line">Prompt: Internal window of a train moving at hyper-speed in an old European city.</span><br><span class="line">Prompt: Handheld camera moving fast, flashlight light, in a white old wall in a old alley at night a black graffiti that spells â€˜Runwayâ€™.</span><br><span class="line">Prompt: Super fast zoom out from the peak of a frozen mountain where a lonely hiker is arriving to the submit.</span><br><span class="line">Prompt: A first-person POV shot rapidly flies through open doors to reveal a surreal waterfall cascading in the middle of the living room.</span><br><span class="line">Prompt: A first-person POV shot rapidly flies towards a house&#x27;s front door at 10x speed.</span><br><span class="line">Prompt: A pencil drawing an architectural plan.</span><br><span class="line">Photorealistic Humans</span><br><span class="line">Gen-3 Alpha excels at generating expressive human characters with a wide range of actions, gestures, and emotions, unlocking new storytelling opportunities.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Prompt: A cinematic wide portrait of a man with his face lit by the glow of a TV.</span><br><span class="line">Prompt: A close up portrait of a woman lit by the side, the camera pulls back.</span><br><span class="line">Prompt: Zoom in shot to the face of a young woman sitting on a bench in the middle of an empty school gym.</span><br><span class="line">Prompt: A close up of an older man in a warehouse, camera zoom out.</span><br><span class="line">Prompt: An older man playing piano, lit from the side.</span><br><span class="line">Prompt: Macro shot to the face freckles of a young woman trying to look for something.</span><br><span class="line">Prompt: An astronaut walking between stone buildings.</span><br><span class="line">Prompt: A middle-aged sad bald man becomes happy as a wig of curly hair and sunglasses fall suddenly on his head.</span><br><span class="line">For artists, by artists</span><br><span class="line">Training Gen-3 Alpha was a collaborative effort from a cross-disciplinary team of research scientists, engineers, and artists. It was designed to interpret a wide range of styles and cinematic terminology.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Prompt: View out a window of a giant strange creature walking in rundown city at night, one single street lamp dimly lighting the area.</span><br><span class="line">Prompt: A man made of rocks walking in the forest, full-body shot.</span><br><span class="line">Prompt: A slow cinematic push in on an ostrich standing in a 1980s kitchen.</span><br><span class="line">Prompt: A giant humanoid, made of fluffy blue cotton candy, stomping on the ground, and roaring to the sky, clear blue sky behind them.</span><br><span class="line">Prompt: Zooming through a dark forest with neon light flora lighting up.</span><br><span class="line">Prompt: A cyclone of broken glass in an urban alleyway. dynamic movement.</span><br><span class="line">Prompt: A man standing in front of a burning building giving the &#x27;thumbs up&#x27; sign.</span><br><span class="line">Prompt: Highly detailed close up of a bacteria.</span><br><span class="line">Prompt: An ultra-wide shot of a giant stone hand reaching out of a pile of rocks at the base of a mountain.</span><br><span class="line">Prompt: Aerial view shot of a cloaked figure elevating in the sky betweem slyscrapers.</span><br><span class="line">Prompt: An oil painting of a natural forest environment with colorful maple trees and cinematic parallax animation.</span><br><span class="line">Prompt: A Japanese animated film of a young woman standing on a ship and looking back at camera.</span><br><span class="line">Prompt: A close-up shot of a young woman driving a car, looking thoughtful, blurred green forest visible through the rainy car window.</span><br><span class="line">Prompt: Aerial shot of a drone moving fast in a dense green jungle.</span><br><span class="line">Prompt: Hyperlapse shot through a corridor with flashing lights. A silver fabric flies through the entire corridor.</span><br><span class="line">Prompt: Aerial shot of the ocean. a maelstrom forms in the water swirling around until it reveals the fiery depths below.</span><br><span class="line">Prompt: A push through an ocean research outpost.</span><br><span class="line">Prompt: A woman singing and standing in a concert stage with a bright light in the background.</span><br><span class="line">Industry Customization</span><br><span class="line">As part of the family of Gen-3 models, we have been collaborating and partnering with leading entertainment and media organizations to create custom versions of Gen-3.</span><br><span class="line">Customization of Gen-3 models allows for more stylistically controlled and consistent characters, and targets specific artistic and narrative requirements, among other features.</span><br><span class="line">For companies interested in fine-tuning and custom models, reach out to us using the form in the button below:</span><br></pre></td></tr></table></figure>
</details></li>
</ul>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-06-18</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/06/18/2024-6-18-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 6ì›” 18ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/06/21/2024-6-21-AI-NEWS/" title="2024ë…„ 6ì›” 21ì¼ AI ì†Œì‹">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/06/17/2024-6-17-AI-NEWS/" title="2024ë…„ 6ì›” 17ì¼ AI ì†Œì‹">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>