<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 6ì›” 21ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="Summaryì˜¤ëŠ˜ì˜ AI ë‰´ìŠ¤ì—ì„œëŠ” ì—¬ëŸ¬ íšŒì‚¬ë“¤ì˜ ìµœì‹  AI ë°œí‘œì™€ ì—°êµ¬ ê²°ê³¼ê°€ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤. Anthropicì—ì„œëŠ” Claude 3.5 Sonnet ëª¨ë¸ì„ ì¶œì‹œí•˜ì—¬ ì—…ê³„ ê¸°ì¤€ì„ ë†’ì˜€ìœ¼ë©°, OpenAIì˜ ê³µë™ ì°½ë¦½ìì˜€ë˜ Ilya SutskeverëŠ” ìƒˆë¡œìš´ ì•ˆì „ ì¤‘ì‹¬ì˜"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 6ì›” 21ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>ì˜¤ëŠ˜ì˜ AI ë‰´ìŠ¤ì—ì„œëŠ” ì—¬ëŸ¬ íšŒì‚¬ë“¤ì˜ ìµœì‹  AI ë°œí‘œì™€ ì—°êµ¬ ê²°ê³¼ê°€ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤. Anthropicì—ì„œëŠ” Claude 3.5 Sonnet ëª¨ë¸ì„ ì¶œì‹œí•˜ì—¬ ì—…ê³„ ê¸°ì¤€ì„ ë†’ì˜€ìœ¼ë©°, OpenAIì˜ ê³µë™ ì°½ë¦½ìì˜€ë˜ Ilya SutskeverëŠ” ìƒˆë¡œìš´ ì•ˆì „ ì¤‘ì‹¬ì˜ AI ì—°êµ¬ì†Œì¸ Safe Superintelligence Inc.ë¥¼ ì°½ì—…í•˜ì˜€ìŠµë‹ˆë‹¤. BigCodeBenchë¼ëŠ” ìƒˆë¡œìš´ ë ˆë”ë³´ë“œê°€ ë°œí‘œë˜ì—ˆìœ¼ë©° ì‹¤ì œ í”„ë¡œê·¸ë˜ë° ì‘ì—…ì—ì„œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤. Open-SoraëŠ” ì˜¤í”ˆì†ŒìŠ¤ ì˜ìƒ ìƒì„±AI ëª¨ë¸ì˜ 2.1ë²„ì „ì„ ë°œí‘œí–ˆìœ¼ë©°, Character.AIëŠ” íš¨ìœ¨ì ì¸ AI ì¶”ë¡ ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì„ ê³µìœ í–ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ê¸ˆìœµ ì—…ê³„ì˜ AI ìë™í™”ì— ëŒ€í•œ ì „ë§ë„ ë…¼ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
<h2 id="Claude-3-5-Sonnet-ì¶œì‹œ"><a href="#Claude-3-5-Sonnet-ì¶œì‹œ" class="headerlink" title="Claude 3.5 Sonnet ì¶œì‹œ"></a>Claude 3.5 Sonnet ì¶œì‹œ</h2><h3 id="Claude-3-5-Sonnet-ì¶œì‹œ-1"><a href="#Claude-3-5-Sonnet-ì¶œì‹œ-1" class="headerlink" title="Claude 3.5 Sonnet ì¶œì‹œ"></a>Claude 3.5 Sonnet ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://www.anthropic.com/news/claude-3-5-sonnet">ë§í¬</a>, 2024ë…„ 6ì›” 21ì¼, Anthropic</p>
<ul>
<li>Claude 3.5 Sonnet ëª¨ë¸ ì¶œì‹œ</li>
<li>ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ì§€ëŠ¥ê³¼ ì„±ëŠ¥ì´ í–¥ìƒë¨</li>
<li>Claude.aiì™€ Claude iOS ì•±ì—ì„œ ë¬´ë£Œë¡œ ì‚¬ìš© ê°€ëŠ¥</li>
<li>Claude Pro ë° Team í”Œëœ êµ¬ë…ìëŠ” ë” ë†’ì€ ì‚¬ìš© í•œë„ ì œê³µ</li>
<li>Amazon Bedrockê³¼ Google Cloudì˜ Vertex AIë¥¼ í†µí•´ì„œë„ ì œê³µ</li>
<li>ì½”ë“œ ìƒì„± ë° ë²ˆì—­, ê³ ê¸‰ ì½˜í…ì¸  ì‘ì„±ì—ì„œ íƒì›”í•œ ì„±ëŠ¥ ë°œíœ˜</li>
</ul>
<h2 id="Safe-Superintelligence-Inc-ë°œí‘œ"><a href="#Safe-Superintelligence-Inc-ë°œí‘œ" class="headerlink" title="Safe Superintelligence Inc. ë°œí‘œ"></a>Safe Superintelligence Inc. ë°œí‘œ</h2><h3 id="ìƒˆë¡œìš´-AI-ì—°êµ¬ì†Œ-Safe-Superintelligence-Inc-ë°œí‘œ"><a href="#ìƒˆë¡œìš´-AI-ì—°êµ¬ì†Œ-Safe-Superintelligence-Inc-ë°œí‘œ" class="headerlink" title="ìƒˆë¡œìš´ AI ì—°êµ¬ì†Œ Safe Superintelligence Inc. ë°œí‘œ"></a>ìƒˆë¡œìš´ AI ì—°êµ¬ì†Œ Safe Superintelligence Inc. ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://time.com/6990076/safe-superintelligence-inc-announced/">ë§í¬</a>, 2024ë…„ 6ì›” 19ì¼, TIME</p>
<ul>
<li>OpenAIì˜ ê³µë™ ì°½ë¦½ì Ilya Sutskeverê°€ ìƒˆë¡œìš´ AI ì—°êµ¬ì†Œ ë°œí‘œ</li>
<li>ì•ˆì „í•œ â€œìŠˆí¼ì¸í…”ë¦¬ì „ìŠ¤â€ ê°œë°œ ëª©í‘œ</li>
<li>Palo Altoì™€ í…”ì•„ë¹„ë¸Œì— ì‚¬ë¬´ì‹¤ ì„¤ë¦½ ì˜ˆì •</li>
<li>íšŒì‚¬ì˜ ìœ ì¼í•œ ëª©í‘œëŠ” ì•ˆì „í•œ ìŠˆí¼ì¸í…”ë¦¬ì „ìŠ¤ ì‹œìŠ¤í…œ ê°œë°œ</li>
<li>í˜„ì¬ ìê¸ˆ ì¡°ë‹¬ ë°©ì‹ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì€ ë¶ˆí™•ì‹¤</li>
</ul>
<h2 id="BigCodeBench-ë°œí‘œ"><a href="#BigCodeBench-ë°œí‘œ" class="headerlink" title="BigCodeBench ë°œí‘œ"></a>BigCodeBench ë°œí‘œ</h2><h3 id="BigCodeBench-ì‹¤ì§ˆì ì´ê³ -ë„ì „ì ì¸-í”„ë¡œê·¸ë˜ë°-ê³¼ì œ-í‰ê°€"><a href="#BigCodeBench-ì‹¤ì§ˆì ì´ê³ -ë„ì „ì ì¸-í”„ë¡œê·¸ë˜ë°-ê³¼ì œ-í‰ê°€" class="headerlink" title="BigCodeBench: ì‹¤ì§ˆì ì´ê³  ë„ì „ì ì¸ í”„ë¡œê·¸ë˜ë° ê³¼ì œ í‰ê°€"></a>BigCodeBench: ì‹¤ì§ˆì ì´ê³  ë„ì „ì ì¸ í”„ë¡œê·¸ë˜ë° ê³¼ì œ í‰ê°€</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/leaderboard-bigcodebench">ë§í¬</a>, 2024ë…„ 6ì›” 18ì¼, Hugging Face</p>
<ul>
<li>HumanEvalì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ BigCodeBench ë°œí‘œ</li>
<li>1,140ê°œì˜ ê¸°ëŠ¥ ìˆ˜ì¤€ ê³¼ì œë¡œ êµ¬ì„±</li>
<li>ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í•¨ìˆ˜ í˜¸ì¶œì„ í¬í•¨í•˜ì—¬ í˜„ì‹¤ì ì¸ í”„ë¡œê·¸ë˜ë° ê³¼ì œ í‰ê°€</li>
<li>LLMì˜ ì‹¤ì œ í”„ë¡œê·¸ë˜ë° ëŠ¥ë ¥ì„ ì •í™•í•˜ê²Œ í‰ê°€</li>
<li>ê³µê°œ ë° íì‡„ LLM ê°„ì˜ ì„±ëŠ¥ ê²©ì°¨ í™•ì¸</li>
</ul>
<h2 id="Open-Sora-1-2-ë³´ê³ ì„œ-ë°œí‘œ"><a href="#Open-Sora-1-2-ë³´ê³ ì„œ-ë°œí‘œ" class="headerlink" title="Open-Sora 1.2 ë³´ê³ ì„œ ë°œí‘œ"></a>Open-Sora 1.2 ë³´ê³ ì„œ ë°œí‘œ</h2><h3 id="Open-Sora-ì˜¤í”ˆì†ŒìŠ¤-ì˜ìƒ-ìƒì„±-AI"><a href="#Open-Sora-ì˜¤í”ˆì†ŒìŠ¤-ì˜ìƒ-ìƒì„±-AI" class="headerlink" title="Open-Sora: ì˜¤í”ˆì†ŒìŠ¤ ì˜ìƒ ìƒì„± AI"></a>Open-Sora: ì˜¤í”ˆì†ŒìŠ¤ ì˜ìƒ ìƒì„± AI</h3><p><a target="_blank" rel="noopener" href="https://github.com/hpcaitech/Open-Sora/blob/main/docs/report_03.md">ë§í¬</a>, Open-Sora</p>
<ul>
<li>ì˜ìƒ ìƒì„±AI ì¸ Open-Sora 1.2 ë²„ì „ ë°œí‘œ</li>
<li>1.1B ëª¨ë¸ì„ 30M ì´ìƒì˜ ë°ì´í„°ë¡œ í›ˆë ¨</li>
<li>ë¹„ë””ì˜¤ ì••ì¶• ë„¤íŠ¸ì›Œí¬ì™€ ë‹¤ë‹¨ê³„ í›ˆë ¨ ë„ì…</li>
<li>ì´ë¯¸ì§€ì—ì„œ ë¹„ë””ì˜¤ ìƒì„± ë° ë¹„ë””ì˜¤ í™•ì¥ ê¸°ëŠ¥ ì œê³µ</li>
<li>ë‹¤ì–‘í•œ í•´ìƒë„ì™€ ë¹„ë””ì˜¤ ê¸¸ì´ ì§€ì›</li>
</ul>
<h2 id="Character-AI-ì¶”ë¡ -ìµœì í™”"><a href="#Character-AI-ì¶”ë¡ -ìµœì í™”" class="headerlink" title="Character.AI ì¶”ë¡  ìµœì í™”"></a>Character.AI ì¶”ë¡  ìµœì í™”</h2><h3 id="Character-AIì—ì„œ-AI-ì¶”ë¡ -ìµœì í™”"><a href="#Character-AIì—ì„œ-AI-ì¶”ë¡ -ìµœì í™”" class="headerlink" title="Character.AIì—ì„œ AI ì¶”ë¡  ìµœì í™”"></a>Character.AIì—ì„œ AI ì¶”ë¡  ìµœì í™”</h3><p><a target="_blank" rel="noopener" href="https://research.character.ai/optimizing-inference/">ë§í¬</a>, 2024ë…„ 6ì›” 20ì¼, Character.AI</p>
<ul>
<li>íš¨ìœ¨ì ì¸ AI ì¶”ë¡ ì„ ìœ„í•œ ìµœì í™” ë°©ë²• ê³µê°œ</li>
<li>ìºì‹œ í¬ê¸° ì¤„ì´ê¸° ìœ„í•œ Multi-Query Attention ë„ì…</li>
<li>í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜ í˜¸ë¼ì´ì¦Œ ì‚¬ìš©</li>
<li>ë ˆì´ì–´ ê°„ KV ê³µìœ ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¦ê°€</li>
<li>ëŒ€í™” ê¸°ë¡ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìºì‹±í•˜ëŠ” ì‹œìŠ¤í…œ ê°œë°œ</li>
</ul>
<h2 id="ê¸ˆìœµ-ì—…ê³„ì˜-AI-ìë™í™”-ì „ë§"><a href="#ê¸ˆìœµ-ì—…ê³„ì˜-AI-ìë™í™”-ì „ë§" class="headerlink" title="ê¸ˆìœµ ì—…ê³„ì˜ AI ìë™í™” ì „ë§"></a>ê¸ˆìœµ ì—…ê³„ì˜ AI ìë™í™” ì „ë§</h2><h3 id="ê¸ˆìœµì—…-AI-ìë™í™”ë¡œ-ì¼ìë¦¬-ëºê¸¸ë¼â€¦â€ê·¼ë¬´ì¼-3-5ì¼-ë‹¨ì¶•-ê°€ëŠ¥ì„±â†‘â€"><a href="#ê¸ˆìœµì—…-AI-ìë™í™”ë¡œ-ì¼ìë¦¬-ëºê¸¸ë¼â€¦â€ê·¼ë¬´ì¼-3-5ì¼-ë‹¨ì¶•-ê°€ëŠ¥ì„±â†‘â€" class="headerlink" title="ê¸ˆìœµì—…, AI ìë™í™”ë¡œ ì¼ìë¦¬ ëºê¸¸ë¼â€¦â€ê·¼ë¬´ì¼ 3.5ì¼ ë‹¨ì¶• ê°€ëŠ¥ì„±â†‘â€"></a>ê¸ˆìœµì—…, AI ìë™í™”ë¡œ ì¼ìë¦¬ ëºê¸¸ë¼â€¦â€ê·¼ë¬´ì¼ 3.5ì¼ ë‹¨ì¶• ê°€ëŠ¥ì„±â†‘â€</h3><p><a target="_blank" rel="noopener" href="https://n.news.naver.com/article/050/0000076482?cds=news_edit">ë§í¬</a>, 2024ë…„ 6ì›” 20ì¼, ë¸”ë£¸ë²„ê·¸í†µì‹ </p>
<ul>
<li>ê¸ˆìœµ ë¶€ë¬¸ì˜ 54%ê°€ AIë¡œ ìë™í™” ê°€ëŠ¥</li>
<li>ì€í–‰, ë³´í—˜, ì—ë„ˆì§€ ë“± ë‹¤ì–‘í•œ ì—…ì¢…ì—ì„œ ìë™í™” ì˜ˆì¸¡</li>
<li>ê¸€ë¡œë²Œ ì£¼ìš” ì€í–‰ë“¤ì´ AI ë„ì… ì‹¤í—˜ ì¤‘</li>
<li>JPëª¨ê±´ì²´ì´ìŠ¤ CEOëŠ” AI ê¸°ìˆ ë¡œ ì£¼ë‹¹ ê·¼ë¬´ì¼ì„ 3.5ì¼ë¡œ ë‹¨ì¶•í•  ìˆ˜ ìˆë‹¤ê³  ì–¸ê¸‰</li>
<li>ìƒì„±í˜• AIë¡œ ì€í–‰ ê·œì •ì„ ë¹ ë¥´ê²Œ ê²€í† í•˜ê³  ìƒì‚°ì„± í–¥ìƒ</li>
</ul>
<details>
  <summary>Sources</summary>

</details>
This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:

<h1 id="todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹"><a href="#todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹" class="headerlink" title="(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,"></a>(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,</h1><h2 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAIì—ì„œëŠ” ~~~ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.)</p>
<h2 id="Title"><a href="#Title" class="headerlink" title="Title,"></a>Title,</h2><h3 id="í•œê¸€ì œëª©"><a href="#í•œê¸€ì œëª©" class="headerlink" title="í•œê¸€ì œëª©"></a>í•œê¸€ì œëª©</h3><p><a href="link">ë§í¬</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
</ul>
<h2 id="Title-1"><a href="#Title-1" class="headerlink" title="Title,"></a>Title,</h2><h3 id="í•œê¸€ì œëª©-1"><a href="#í•œê¸€ì œëª©-1" class="headerlink" title="í•œê¸€ì œëª©"></a>í•œê¸€ì œëª©</h3><p><a href="link">ë§í¬</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://www.anthropic.com/news/claude-3-5-sonnet</span><br><span class="line">Claude</span><br><span class="line">Research</span><br><span class="line">Company</span><br><span class="line">Careers</span><br><span class="line">News</span><br><span class="line">Announcements</span><br><span class="line">Claude 3.5 Sonnet</span><br><span class="line">2024ë…„ 6ì›” 21ì¼</span><br><span class="line">â—</span><br><span class="line">4 min read</span><br><span class="line">Try on Claude.ai</span><br><span class="line">Claude head illustration</span><br><span class="line">Today, weâ€™re launching Claude 3.5 Sonnetâ€”our first release in the forthcoming Claude 3.5 model family. Claude 3.5 Sonnet raises the industry bar for intelligence, outperforming competitor models and Claude 3 Opus on a wide range of evaluations, with the speed and cost of our mid-tier model, Claude 3 Sonnet.</span><br><span class="line"></span><br><span class="line">Claude 3.5 Sonnet is now available for free on Claude.ai and the Claude iOS app, while Claude Pro and Team plan subscribers can access it with significantly higher rate limits. It is also available via the Anthropic API, Amazon Bedrock, and Google Cloudâ€™s Vertex AI. The model costs $3 per million input tokens and $15 per million output tokens, with a 200K token context window.</span><br><span class="line"></span><br><span class="line">Claude model family</span><br><span class="line">Frontier intelligence at 2x the speed</span><br><span class="line">Claude 3.5 Sonnet sets new industry benchmarks for graduate-level reasoning (GPQA), undergraduate-level knowledge (MMLU), and coding proficiency (HumanEval). It shows marked improvement in grasping nuance, humor, and complex instructions, and is exceptional at writing high-quality content with a natural, relatable tone.</span><br><span class="line"></span><br><span class="line">Claude 3.5 Sonnet operates at twice the speed of Claude 3 Opus. This performance boost, combined with cost-effective pricing, makes Claude 3.5 Sonnet ideal for complex tasks such as context-sensitive customer support and orchestrating multi-step workflows.</span><br><span class="line"></span><br><span class="line">In an internal agentic coding evaluation, Claude 3.5 Sonnet solved 64% of problems, outperforming Claude 3 Opus which solved 38%. Our evaluation tests the modelâ€™s ability to fix a bug or add functionality to an open source codebase, given a natural language description of the desired improvement. When instructed and provided with the relevant tools, Claude 3.5 Sonnet can independently write, edit, and execute code with sophisticated reasoning and troubleshooting capabilities. It handles code translations with ease, making it particularly effective for updating legacy applications and migrating codebases.</span><br><span class="line"></span><br><span class="line">Claude 3.5 Sonnet benchmarks</span><br><span class="line">State-of-the-art vision</span><br><span class="line">Claude 3.5 Sonnet is our strongest vision model yet, surpassing Claude 3 Opus on standard vision benchmarks. These step-change improvements are most noticeable for tasks that require visual reasoning, like interpreting charts and graphs. Claude 3.5 Sonnet can also accurately transcribe text from imperfect imagesâ€”a core capability for retail, logistics, and financial services, where AI may glean more insights from an image, graphic or illustration than from text alone.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Claude 3.5 Sonnet vision evals</span><br><span class="line">Artifactsâ€”a new way to use Claude</span><br><span class="line">Today, weâ€™re also introducing Artifacts on Claude.ai, a new feature that expands how users can interact with Claude. When a user asks Claude to generate content like code snippets, text documents, or website designs, these Artifacts appear in a dedicated window alongside their conversation. This creates a dynamic workspace where they can see, edit, and build upon Claudeâ€™s creations in real-time, seamlessly integrating AI-generated content into their projects and workflows.</span><br><span class="line"></span><br><span class="line">This preview feature marks Claudeâ€™s evolution from a conversational AI to a collaborative work environment. Itâ€™s just the beginning of a broader vision for Claude.ai, which will soon expand to support team collaboration. In the near future, teamsâ€”and eventually entire organizationsâ€”will be able to securely centralize their knowledge, documents, and ongoing work in one shared space, with Claude serving as an on-demand teammate.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Commitment to safety and privacy</span><br><span class="line">Our models are subjected to rigorous testing and have been trained to reduce misuse. Despite Claude 3.5 Sonnetâ€™s leap in intelligence, our red teaming assessments have concluded that Claude 3.5 Sonnet remains at ASL-2. More details can be found in the model card addendum.</span><br><span class="line"></span><br><span class="line">As part of our commitment to safety and transparency, weâ€™ve engaged with external experts to test and refine the safety mechanisms within this latest model. We recently provided Claude 3.5 Sonnet to the UKâ€™s Artificial Intelligence Safety Institute (UK AISI) for pre-deployment safety evaluation. The UK AISI completed tests of 3.5 Sonnet and shared their results with the US AI Safety Institute (US AISI) as part of a Memorandum of Understanding, made possible by the partnership between the US and UK AISIs announced earlier this year.</span><br><span class="line"></span><br><span class="line">We have integrated policy feedback from outside subject matter experts to ensure that our evaluations are robust and take into account new trends in abuse. This engagement has helped our teams scale up our ability to evaluate 3.5 Sonnet against various types of misuse. For example, we used feedback from child safety experts at Thorn to update our classifiers and fine-tune our models.</span><br><span class="line"></span><br><span class="line">One of the core constitutional principles that guides our AI model development is privacy. We do not train our generative models on user-submitted data unless a user gives us explicit permission to do so. To date we have not used any customer or user-submitted data to train our generative models.</span><br><span class="line"></span><br><span class="line">Coming soon</span><br><span class="line">Our aim is to substantially improve the tradeoff curve between intelligence, speed, and cost every few months. To complete the Claude 3.5 model family, weâ€™ll be releasing Claude 3.5 Haiku and Claude 3.5 Opus later this year.</span><br><span class="line"></span><br><span class="line">In addition to working on our next-generation model family, we are developing new modalities and features to support more use cases for businesses, including integrations with enterprise applications. Our team is also exploring features like Memory, which will enable Claude to remember a userâ€™s preferences and interaction history as specified, making their experience even more personalized and efficient.</span><br><span class="line"></span><br><span class="line">Weâ€™re constantly working to improve Claude and love hearing from our users. You can submit feedback on Claude 3.5 Sonnet directly in-product to inform our development roadmap and help our teams to improve your experience. As always, we look forward to seeing what you build, create, and discover with Claude.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://time.com/6990076/safe-superintelligence-inc-announced/</span><br><span class="line">Former OpenAI Chief Scientist Announces New Safety-Focused Company</span><br><span class="line">3 MINUTE READ</span><br><span class="line">Ilya Sutskever</span><br><span class="line">Ilya Sutskever speaks at Tel Aviv University in Tel Aviv on June 5, 2023. Jack Guezâ€”AFP via Getty Images</span><br><span class="line">BY HARRY BOOTHJUNE 19, 2024 5:05 PM EDT</span><br><span class="line">Ilya Sutskever, a co-founder and former chief scientist of OpenAI, announced on Wednesday that heâ€™s launching a new venture dubbed Safe Superintelligence Inc. Sutskever said on X that the new lab will focus solely on building a safe â€œsuperintelligenceâ€â€”an industry term for a hypothetical system thatâ€™s smarter than humans.</span><br><span class="line"></span><br><span class="line">Sutskever is joined at Safe SuperIntelligence Inc. by co-founders Daniel Gross, an investor and engineer who worked on AI at Apple till 2017, and Daniel Levy, another former OpenAI employee. The new American-based firm will have offices in Palo Alto, Calif., and Tel Aviv, according to a description Sutskever shared.</span><br><span class="line"></span><br><span class="line">I am starting a new company: https://t.co/BG3K3SI3A1</span><br><span class="line"></span><br><span class="line">â€” Ilya Sutskever (@ilyasut) June 19, 2024</span><br><span class="line">Sutskever was one of OpenAIâ€™s founding members, and was chief scientist during the companyâ€™s meteoric rise following the release of ChatGPT. In November, Sutskever took part in the infamous attempt to oust OpenAI CEO Sam Altman, only to later change his mind and support Altmanâ€™s return. When Sutskever announced his resignation in May, he said he was â€œconfident that OpenAI will build AGI that is both safe and beneficialâ€ under Altmanâ€™s leadership.</span><br><span class="line"></span><br><span class="line">Safe Superintelligence Inc. says it will only aim to release one product: the system in its name. This model will insulate the company from commercial pressures, its founders wrote. However, itâ€™s currently unclear who will fund the new venture&#x27;s development or what exactly its business model will eventually be.</span><br><span class="line"></span><br><span class="line">â€œOur singular focus means no distraction by management overhead or product cycles,â€ the announcement reads, perhaps subtly taking aim at OpenAI. In May, another senior OpenAI member, Jan Leike, who co-led a safety team with Sutskever, accused the company of prioritizing â€œshiny productsâ€ over safety. Leikeâ€™s accusations came around the time that six other safety-conscious employees left the company. Altman and OpenAIâ€™s President, Greg Brockman, responded to Leikeâ€™s accusations by acknowledging there was more work to be done, saying â€œwe take our role here very seriously and carefully weigh feedback on our actions.â€</span><br><span class="line"></span><br><span class="line">Read more: A Timeline of All the Recent Accusations Leveled at OpenAI and Sam Altman</span><br><span class="line"></span><br><span class="line">In an interview with Bloomberg, Sutskever elaborated on Safe Superintelligence Inc.â€™s approach, saying, â€œBy safe, we mean safe like nuclear safety as opposed to safe as in â€˜trust and safetyâ€™â€; one of OpenAIâ€™s core safety principles is to â€œbe a pioneer in trust and safety.â€</span><br><span class="line"></span><br><span class="line">While many details about the new company remain to be revealed, its founders have one message for those in the industry who are intrigued: Theyâ€™re hiring.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/blog/leaderboard-bigcodebench</span><br><span class="line">BigCodeBench: Benchmarking Large Language Models on Solving Practical and Challenging Programming Tasks</span><br><span class="line">Published June 18, 2024</span><br><span class="line">Terry Yue Zhuo&#x27;s avatar</span><br><span class="line">terryyz</span><br><span class="line">Terry Yue Zhuo</span><br><span class="line">BigCode&#x27;s avatar</span><br><span class="line">bigcode</span><br><span class="line">Jiawei Liu&#x27;s avatar</span><br><span class="line">ganler</span><br><span class="line">Jiawei Liu</span><br><span class="line">BigCode&#x27;s avatar</span><br><span class="line">bigcode</span><br><span class="line">Qian Liu&#x27;s avatar</span><br><span class="line">SivilTaram</span><br><span class="line">Qian Liu</span><br><span class="line">BigCode&#x27;s avatar</span><br><span class="line">bigcode</span><br><span class="line">Binyuan Hui&#x27;s avatar</span><br><span class="line">huybery</span><br><span class="line">Binyuan Hui</span><br><span class="line">BigCode&#x27;s avatar</span><br><span class="line">bigcode</span><br><span class="line">Niklas Muennighoff&#x27;s avatar</span><br><span class="line">Muennighoff</span><br><span class="line">Niklas Muennighoff</span><br><span class="line">BigCode&#x27;s avatar</span><br><span class="line">bigcode</span><br><span class="line">Daniel Fried&#x27;s avatar</span><br><span class="line">dpfried</span><br><span class="line">Daniel Fried</span><br><span class="line">BigCode&#x27;s avatar</span><br><span class="line">bigcode</span><br><span class="line">Harm de Vries&#x27;s avatar</span><br><span class="line">harmdevries</span><br><span class="line">Harm de Vries</span><br><span class="line">BigCode&#x27;s avatar</span><br><span class="line">bigcode</span><br><span class="line">Leandro von Werra&#x27;s avatar</span><br><span class="line">lvwerra</span><br><span class="line">Leandro von Werra</span><br><span class="line">BigCode&#x27;s avatar</span><br><span class="line">bigcode</span><br><span class="line">ClÃ©mentine Fourrier&#x27;s avatar</span><br><span class="line">clefourrier</span><br><span class="line">ClÃ©mentine Fourrier</span><br><span class="line">HumanEval is a reference benchmark for evaluating large language models (LLMs) on code generation tasks, as it makes the evaluation of compact function-level code snippets easy. However, there are growing concerns about its effectiveness in evaluating the programming capabilities of LLMs, and the main concern is that tasks in HumanEval are too simple and may not be representative of real-world programming tasks. Compared to the algorithm-oriented tasks in HumanEval, real-world software development often involves diverse libraries and function calls. Furthermore, LLMs&#x27; performance on HumanEval is subject to contamination and overfitting issues, making it less reliable for evaluating the generalization of LLMs.</span><br><span class="line">While there have been some efforts to address these issues, they are either domain-specific, deterministic, or agent-centric (sorry DS-1000, ODEX, and SWE-bench ğŸ’”). We feel that the community still lacks an easy-to-use benchmark that can broadly evaluate the programming capabilities of LLMs, and that&#x27;s what we focused on.</span><br><span class="line"></span><br><span class="line">We are excited to announce the release of BigCodeBench, which evaluates LLMs on solving practical and challenging programming tasks without contamination. Specifically, BigCodeBench contains 1,140 function-level tasks to challenge LLMs to follow instructions and compose multiple function calls as tools from 139 libraries. To evaluate LLMs rigorously, each programming task encompasses 5.6 test cases with an average branch coverage of 99%.</span><br><span class="line"></span><br><span class="line">Ready to dive into BigCodeBench? Let&#x27;s get started! ğŸš€</span><br><span class="line"></span><br><span class="line">What do the tasks in BigCodeBench look like? ğŸ•µï¸â€â™‚ï¸</span><br><span class="line">task</span><br><span class="line">BigCodeBench features complex, user-oriented instructions for each task, including clear functionality descriptions, input/output formats, error handling, and verified interactive examples. We avoid step-by-step task instructions, believing capable LLMs should understand and solve tasks from the user&#x27;s perspective in an open-ended manner. We verify specific features using test cases.</span><br><span class="line"></span><br><span class="line"># We elaborate the above task with some test cases:</span><br><span class="line"></span><br><span class="line"># Requirements SetUp</span><br><span class="line">import unittest</span><br><span class="line">from unittest.mock import patch</span><br><span class="line">import http.client</span><br><span class="line">import ssl</span><br><span class="line">import socket</span><br><span class="line"></span><br><span class="line"># Start the test</span><br><span class="line">class TestCases(unittest.TestCase):</span><br><span class="line"></span><br><span class="line">    # Mock the successful connection and assess the response content</span><br><span class="line"></span><br><span class="line">    def test_response_content(self, mock_conn):</span><br><span class="line">        &quot;&quot;&quot; Test the content of the response. &quot;&quot;&quot;</span><br><span class="line">        mock_conn.return_value.getresponse.return_value.read.return_value = b&#x27;Expected Content&#x27;</span><br><span class="line">        result = task_func(&#x27;www.example.com&#x27;, 443, &#x27;/content/path&#x27;)</span><br><span class="line">        self.assertEqual(result, &#x27;Expected Content&#x27;)</span><br><span class="line"></span><br><span class="line">    # Mock the failed connection and assess the error handling</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def test_ssl_handshake_error_handling(self, mock_conn, mock_socket):</span><br><span class="line">        &quot;&quot;&quot; Test handling of SSL handshake errors. &quot;&quot;&quot;</span><br><span class="line">        mock_socket.side_effect = ssl.SSLError(&#x27;SSL handshake failed&#x27;)</span><br><span class="line">        with self.assertRaises(ssl.SSLError):</span><br><span class="line">            task_func(&#x27;badssl.com&#x27;, 443, &#x27;/test/path&#x27;)</span><br><span class="line"></span><br><span class="line">    # More test cases...</span><br><span class="line"></span><br><span class="line">Tasks in BigCodeBench utilize diverse function calls from popular libraries. We don&#x27;t restrict the function calls LLMs can use, expecting them to choose appropriate functions and combine them flexibly to solve tasks. Test cases are designed as test harnesses to examine expected program behaviors during runtime.</span><br><span class="line"></span><br><span class="line">To assess LLM performance, we use Pass@1 with greedy decoding, measuring the percentage of tasks correctly solved with the first generated code snippet via curated test cases. This approach aligns with benchmarks like HumanEval and MBPP. We address LLMs&#x27; tendency to skip long code prompts by adding missing setups (e.g., import statements, global constants) during Pass@1 evaluation, referred to as calibrated Pass@1.</span><br><span class="line"></span><br><span class="line">comparison</span><br><span class="line">To better understand implementation complexity and tool-use diversity, we compare the tasks in BigCodeBench with those in representative benchmarks, including APPS, DS-1000, ODEX, APIBench, MBPP, NumpyEval, PandasEval, HumanEval, and TorchDataEval. We find that BigCodeBench requires more complex reasoning and problem-solving skills to implement comprehensive functionalities.</span><br><span class="line"></span><br><span class="line">prompt</span><br><span class="line">As shown in the task figure, the main target scenario is code completion (denoted as BigCodeBench-Complete), where LLMs are required to finish the implementation of a function based on detailed instructions in the docstring. However, considering downstream applications such as multi-turn dialogue, users may describe requirements in a more conversational and less verbose manner. This is where instruction-tuned LLMs are beneficial, as they are trained to follow natural-language instructions and generate code snippets accordingly. To test if models can truly understand human intents and translate them into code, we create BigCodeBench-Instruct, a more challenging variant of BigCodeBench designed to evaluate instruction-tuned LLMs.</span><br><span class="line"></span><br><span class="line">Where do the tasks come from? ğŸ¤”</span><br><span class="line">png</span><br><span class="line">We guarantee the quality of the tasks in BigCodeBench through a systematic &quot;Human-LLM collaboration process.&quot; We start with ODEX as the &quot;seed dataset,&quot; which contains short but realistic human intents and corresponding Python one-liners from Stack Overflow. We use GPT-4 to expand these one-liners into comprehensive function-level tasks.</span><br><span class="line"></span><br><span class="line">Next, 20 human expertsâ€”most with over 5 years of Python programming experienceâ€”voluntarily guide GPT-4 in an execution-based sandbox. They continually instruct it to refine the synthesized tasks and add test cases. The tasks and test cases are then examined in a local environment, pre-evaluated on other LLMs, and cross-checked by 7 additional human experts to ensure their quality.</span><br><span class="line"></span><br><span class="line">To assert overall quality, the authors sample tasks for 11 human experts to solve, achieving an average human performance of 97%.</span><br><span class="line"></span><br><span class="line">How well do LLMs perform on BigCodeBench? ğŸ“Š</span><br><span class="line">We host the BigCodeBench leaderboard on both Hugging Face Space and GitHub Pages. Here, we use the Hugging Face leaderboard as an example.</span><br><span class="line"></span><br><span class="line">Loading...</span><br><span class="line"></span><br><span class="line">bigcode/bigcodebench-leaderboard</span><br><span class="line">built with Gradio.</span><br><span class="line">Hosted on Hugging Face Space Spaces</span><br><span class="line"></span><br><span class="line">Interestingly, we observe that instruction-tuned LLMs like GPT-4 can omit essential import statements in the long prompts of BigCodeBench-Complete, leading to task failures due to missing modules and constants. This behavior, called &quot;model laziness&quot;, is discussed in the community.</span><br><span class="line"></span><br><span class="line">Compared to human performance, LLMs perform significantly lower on BigCodeBench-Complete and even lower on BigCodeBench-Instruct. The best model (GPT-4o) achieves a calibrated Pass@1 of 61.1% on BigCodeBench-Complete and 51.1% on BigCodeBench-Instruct. Additionally, there is a notable performance gap between closed and open LLMs.</span><br><span class="line"></span><br><span class="line">While Pass@1 is a good metric for overall performance, it is not detailed enough to compare models directly. Inspired by Chatbot Arena, we use Elo rating to rank models on BigCodeBench-Complete. This method, originally used in chess, ranks players based on their game performance. We adapt it to programming tasks, treating each task as a game and each model as a player. The Elo rating updates are based on game outcomes and expectations, using task-level calibrated Pass@1 (0% or 100%) and excluding ties. Starting with an initial Elo rating of 1000, we fit it using maximum likelihood estimation and bootstrap with 500 iterations to get final scores. We find that GPT-4o outperforms other models by a large margin, with DeepSeekCoder-V2 in the second tier.</span><br><span class="line"></span><br><span class="line">To help the community understand model performance on each task, we track solve rates, measured by calibrated Pass@1. On BigCodeBench-Complete, 149 tasks remain unsolved by all models, while 6 tasks are completely solved. For BigCodeBench-Instruct, 278 tasks remain unsolved and 14 tasks are fully solved by all models. The significant number of unsolved tasks and the small number of fully solved tasks show that BigCodeBench is a challenging benchmark for LLMs.</span><br><span class="line"></span><br><span class="line">Great! So, how can I evaluate my model on BigCodeBench? ğŸ› ï¸</span><br><span class="line">We make BigCodeBench easily accessible to the community by providing a simple and user-friendly evaluation framework, which can be downloaded via PyPI. The prototype of the evaluation framework is based on EvalPlus for the HumanEval+ and MBPP+ benchmarks. However, as our benchmark has tasks with much more diverse library dependencies than EvalPlus, we build less resource-constrained execution environment, and adapt it for unittest in the test harness of BigCodeBench.</span><br><span class="line"></span><br><span class="line">To facilitate the evaluation, we provide pre-built Docker images for code generation and code execution. Check out our GitHub repository to find more details on how to use the evaluation framework.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/hpcaitech/Open-Sora/blob/main/docs/report_03.md</span><br><span class="line">Open-Sora: Democratizing Efficient Video Production for All</span><br><span class="line">We design and implement Open-Sora, an initiative dedicated to efficiently producing high-quality video. We hope to make the model, tools and all details accessible to all. By embracing open-source principles, Open-Sora not only democratizes access to advanced video generation techniques, but also offers a streamlined and user-friendly platform that simplifies the complexities of video generation. With Open-Sora, our goal is to foster innovation, creativity, and inclusivity within the field of content creation.</span><br><span class="line"></span><br><span class="line">Open-Sora 1.2 Report</span><br><span class="line">Video compression network</span><br><span class="line">Rectified flow and model adaptation</span><br><span class="line">More data and better multi-stage training</span><br><span class="line">Easy and effective model conditioning</span><br><span class="line">Evaluation</span><br><span class="line">Sequence parallelism</span><br><span class="line">In Open-Sora 1.2 release, we train a 1.1B models on &gt;30M data (80k hours), with training cost 35k H100 GPU hours, supporting 0s16s, 144p to 720p, various aspect ratios video generation. Our configurations is listed below. Following our 1.1 version, Open-Sora 1.2 can also do image-to-video generation and video extension.</span><br><span class="line"></span><br><span class="line">image	2s	4s	8s	16s</span><br><span class="line">240p	âœ…	âœ…	âœ…	âœ…	âœ…</span><br><span class="line">360p	âœ…	âœ…	âœ…	âœ…	âœ…</span><br><span class="line">480p	âœ…	âœ…	âœ…	âœ…	ğŸ†—</span><br><span class="line">720p	âœ…	âœ…	âœ…	ğŸ†—	ğŸ†—</span><br><span class="line">Here âœ… means that the data is seen during training, and ğŸ†— means although not trained, the model can inference at that config. Inference for ğŸ†— requires more than one 80G memory GPU and sequence parallelism.</span><br><span class="line"></span><br><span class="line">Besides features introduced in Open-Sora 1.1, Open-Sora 1.2 highlights:</span><br><span class="line"></span><br><span class="line">Video compression network</span><br><span class="line">Rectifie-flow training</span><br><span class="line">More data and better multi-stage training</span><br><span class="line">Easy and effective model conditioning</span><br><span class="line">Better evaluation metrics</span><br><span class="line">All implementations (both training and inference) of the above improvements are available in the Open-Sora 1.2 release. The following sections will introduce the details of the improvements. We also refine our codebase and documentation to make it easier to use and develop, and add a LLM to refine input prompts and support more languages.</span><br><span class="line"></span><br><span class="line">Video compression network</span><br><span class="line">For Open-Sora 1.0 &amp; 1.1, we used stability-ai&#x27;s 83M 2D VAE, which compress the video only in the spatial dimension by 8x8 times. To reduce the temporal dimension, we extracted one frame in every three frames. However, this method led to the low fluency of generated video as the generated fps is sacrificed. Thus, in this release, we introduce the video compression network as OpenAI&#x27;s Sora does. With a 4 times compression in the temporal dimension, we do not need to extract frames and can generate videos with the original fps.</span><br><span class="line"></span><br><span class="line">Considering the high computational cost of training a 3D VAE, we hope to re-use the knowledge learnt in the 2D VAE. We notice that after 2D VAE&#x27;s compression, the features adjacent in the temporal dimension are still highly correlated. Thus, we propose a simple video compression network, which first compress the video in the spatial dimension by 8x8 times, then compress the video in the temporal dimension by 4x times. The network is shown below:</span><br><span class="line"></span><br><span class="line">video_compression_network</span><br><span class="line"></span><br><span class="line">We initialize the 2D VAE with SDXL&#x27;s VAE, which is better than our previously used one. For the 3D VAE, we adopt the structure of VAE in Magvit-v2, which contains 300M parameters. Along with 83M 2D VAE, the total parameters of the video compression network is 384M. We train the 3D VAE for 1.2M steps with local batch size 1. The training data is videos from pixels and pixabay, and the training video size is mainly 17 frames, 256x256 resolution. Causal convolutions are used in the 3D VAE to make the image reconstruction more accurate.</span><br><span class="line"></span><br><span class="line">Our training involves three stages:</span><br><span class="line"></span><br><span class="line">For the first 380k steps, we train on 8 GPUs and freeze the 2D VAE. The training objective includes the reconstruction of the compressed features from 2D VAE (pink one in the figure) and also add a loss to make features from the 3D VAE similar to the features from the 2D VAE (pink one and green one, called identity loss). We find the latter loss can quickly make the whole VAE achieve a good performance for image and much faster to converge in the next stage.</span><br><span class="line">For the next 260k steps, We remove the identity loss and just learn the 3D VAE.</span><br><span class="line">For the last 540k steps , since we find only reconstruction 2D VAE&#x27;s feature cannot lead to further improvement, we remove the loss and train the whole VAE to reconstruct the original videos. This stage is trained on on 24 GPUs.</span><br><span class="line">For both stage 1 and stage 2 training, we adopt 20% images and 80% videos. Following Magvit-v2, we train video using 17 frames, while zero-padding the first 16 frames for image. However, we find that this setting leads to blurring of videos with length different from 17 frames. Thus, in stage 3, we use a random number within 34 frames for mixed video length training (a.k.a., zero-pad the first 43-n frames if we want to train a n frame video), to make our VAE more robust to different video lengths. Our training and inference code is available in the Open-Sora 1.2 release.</span><br><span class="line"></span><br><span class="line">When using the VAE for diffusion model, our stacked VAE requires small memory as the our VAE&#x27;s input is already compressed. We also split the input videos input several 17 frames clips to make the inference more efficient. The performance of our VAE is on par with another open-sourced 3D VAE in Open-Sora-Plan.</span><br><span class="line"></span><br><span class="line">Model	SSIMâ†‘	PSNRâ†‘</span><br><span class="line">Open-Sora-Plan 1.1	0.882	29.890</span><br><span class="line">Open-Sora 1.2	0.880	30.590</span><br><span class="line">Rectified flow and model adaptation</span><br><span class="line">Lastest diffusion model like Stable Diffusion 3 adopts the rectified flow instead of DDPM for better performance. Pitiably, SD3&#x27;s rectified flow training code is not open-sourced. However, Open-Sora 1.2 provides the training code following SD3&#x27;s paper, including:</span><br><span class="line"></span><br><span class="line">Basic rectified flow training (original rectified flow paper)</span><br><span class="line">Logit-norm sampling for training acceleration (SD3 paper Section 3.1, intuitively it is more likely to sample timesteps at middle noise level)</span><br><span class="line">Resolution and video length aware timestep sampling (SD3 paper Section 5.3.2, intuitively it is more likely to sample timesteps with more noise for larger resolution, and we extend it to longer video)</span><br><span class="line">For the resolution-aware timestep sampling, we should use more noise for images with larger resolution. We extend this idea to video generation and use more noise for videos with longer length.</span><br><span class="line"></span><br><span class="line">Open-Sora 1.2 starts from the PixArt-Î£ 2K checkpoint. Note that this model is trained with DDPM and SDXL VAE, also a much higher resolution. We find finetuning on a small dataset can easily adapt the model for our video generation setting. The adaptation process is as follows, all training is done on 8 GPUs (the adaptation for the diffusion model is quite fast and straightforward):</span><br><span class="line"></span><br><span class="line">Multi-resolution image generation ability: we train the model to generate different resolution ranging from 144p to 2K for 20k steps.</span><br><span class="line">QK-norm: we add the QK-norm to the model and train for 18k steps.</span><br><span class="line">Rectified flow: we transform from discrete-time DDPM to continuous-time rectified flow and train for 10k steps.</span><br><span class="line">Rectified flow with logit-norm sampling and resolution-aware timestep sampling: we train for 33k steps.</span><br><span class="line">Smaller AdamW epsilon: following SD3, with QK-norm, we can use a smaller epsilon (1e-15) for AdamW, we train for 8k steps.</span><br><span class="line">New VAE and fps conditioning: we replace the original VAE with ours and add fps conditioning to the timestep conditioning, we train for 25k steps. Note that normalizing each channel is important for rectified flow training.</span><br><span class="line">Temporal attention blocks: we add temporal attention blocks with zero initialized projection layers. We train on images for 3k steps.</span><br><span class="line">Temporal blocks only for video with mask strategy: we train the temporal attention blocks only on videos for 38k steps.</span><br><span class="line">After the above adaptation, we are ready to train the model on videos. The adaptation above maintains the original model&#x27;s ability to generate high-quality images, and brings multiple benefits for video generation:</span><br><span class="line"></span><br><span class="line">With rectified flow, we can accelerate the training and reduce the number of sampling steps for video from 100 to 30, which greatly reduces the waiting time for inference.</span><br><span class="line">With qk-norm, the training is more stablized and an aggressive optimizer can be used.</span><br><span class="line">With new VAE, the temporal dimension is compressed by 4 times, which makes the training more efficient.</span><br><span class="line">With multi-resolution image generation ability, the model can generate videos with different resolutions.</span><br><span class="line">More data and better multi-stage training</span><br><span class="line">Due to a limited computational budget, we carefully arrange the training data from low to high quality and split our training into three stages. Our training involves 12x8 GPUs, and the total training time is about 2 weeks for about 70k steps.</span><br><span class="line"></span><br><span class="line">First stage</span><br><span class="line">We first train the model on Webvid-10M datasets (40k hours) for 30k steps (2 epochs). Since the video is all lower than 360p resolution and contains watermark, we train on this dataset first. The training mainly happens on 240p and 360p, with video length 2s~16s. We use the original caption in the dataset for training. The training config locates in stage1.py.</span><br><span class="line"></span><br><span class="line">Second stage</span><br><span class="line">Then we train the model on Panda-70M datasets. This dataset is large but the quality varies. We use the official 30M subset which clips are more diverse, and filter out videos with aesthetic score lower than 4.5. This leads to a 20M subset with 41k hours. The captions in the dataset are directly used for our training. The training config locates in stage2.py.</span><br><span class="line"></span><br><span class="line">The training mainly happens on 360p and 480p. We train the model for 23k steps, which is 0.5 epoch. The training is not fully done since we hope our new model can meet you earlier.</span><br><span class="line"></span><br><span class="line">Third stage</span><br><span class="line">In this stage, we collect ~2M video clips with a total length of 5K hours from all kinds of sources, including:</span><br><span class="line"></span><br><span class="line">Free-license videos, sourced from Pexels, Pixabay, Mixkit, etc.</span><br><span class="line">MiraData: a high-quality dataset with long videos, mainly from games and city/scenic exploration.</span><br><span class="line">Vript: a densely annotated dataset.</span><br><span class="line">And some other datasets.</span><br><span class="line">While MiraData and Vript have captions from GPT, we use PLLaVA to caption the rest ones. Compared with LLaVA, which is only capable of single frame/image captioning, PLLaVA is specially designed and trained for video captioning. The accelerated PLLaVA is released in our tools/. In practice, we use the pretrained PLLaVA 13B model and select 4 frames from each video for captioning with a spatial pooling shape of 2*2.</span><br><span class="line"></span><br><span class="line">Some statistics of the video data used in this stage are shown below. We present basic statistics of duration and resolution, as well as aesthetic score and optical flow score distribution. We also extract tags for objects and actions from video captions and count their frequencies. stats object_count object_count</span><br><span class="line"></span><br><span class="line">We mainly train 720p and 1080p videos in this stage, aiming to extend the model&#x27;s ability to larger resolutions. We use a mask ratio of 25% during training. The training config locates in stage3.py. We train the model for 15k steps, which is approximately 2 epochs.</span><br><span class="line"></span><br><span class="line">Easy and effective model conditioning</span><br><span class="line">For stage 3, we calculate the aesthetic score and motion score for each video clip. However, since the number of video clips is small, we are not willing to filter out clips with low scores, which leads to a smaller dataset. Instead, we append the scores to the captions and use them as conditioning. We find this method can make model aware of the scores and follows the scores to generate videos with better quality.</span><br><span class="line"></span><br><span class="line">For example, a video with aesthetic score 5.5, motion score 10, and a detected camera motion pan left, the caption will be:</span><br><span class="line"></span><br><span class="line">[Original Caption] aesthetic score: 5.5, motion score: 10, camera motion: pan left.</span><br><span class="line">During inference, we can also use the scores to condition the model. For camera motion, we only label 13k clips with high confidence, and the camera motion detection module is released in our tools.</span><br><span class="line"></span><br><span class="line">Evaluation</span><br><span class="line">Previously, we monitor the training process only by human evaluation, as DDPM traning loss is not well correlated with the quality of generated videos. However, for rectified flow, we find the training loss is well correlated with the quality of generated videos as stated in SD3. Thus, we keep track of rectified flow evaluation loss on 100 images and 1k videos.</span><br><span class="line"></span><br><span class="line">We sampled 1k videos from pixabay as validation dataset. We calculate the evaluation loss for image and different lengths of videos (2s, 4s, 8s, 16s) for different resolution (144p, 240p, 360p, 480p, 720p). For each setting, we equidistantly sample 10 timesteps. Then all the losses are averaged. We also provide a video showing the sampled videos with a fixed prompt for different steps.</span><br><span class="line"></span><br><span class="line">Evaluation Loss Video Evaluation Loss</span><br><span class="line"></span><br><span class="line">In addition, we also keep track of VBench scores during training. VBench is an automatic video evaluation benchmark for short video generation. We calcuate the vbench score with 240p 2s videos. The two metrics verify that our model continues to improve during training.</span><br><span class="line"></span><br><span class="line">VBench</span><br><span class="line"></span><br><span class="line">All the evaluation code is released in eval folder. Check the README for more details.</span><br><span class="line"></span><br><span class="line">Model	Total Score	Quality Score	Semantic Score</span><br><span class="line">Open-Sora V1.0	75.91%	78.81%	64.28%</span><br><span class="line">Open-Sora V1.2	79.23%	80.71%	73.30%</span><br><span class="line">Sequence parallelism</span><br><span class="line">We use sequence parallelism to support long-sequence training and inference. Our implementation is based on Ulysses and the workflow is shown below. When sequence parallelism is enabled, we only need to apply the all-to-all communication to the spatial block in STDiT as only spatial computation is dependent on the sequence dimension.</span><br><span class="line"></span><br><span class="line">SP</span><br><span class="line"></span><br><span class="line">Currently, we have not used sequence parallelism for training as data resolution is small and we plan to do so in the next release. As for inference, we can use sequence parallelism in case your GPU goes out of memory. A simple benchmark shows that sequence parallelism can achieve speedup</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://research.character.ai/optimizing-inference/</span><br><span class="line">JUN 20, 2024 4 MIN READ EFFICIENCY</span><br><span class="line">Optimizing AI Inference at Character.AI</span><br><span class="line">Optimizing AI Inference at Character.AI</span><br><span class="line">At Character.AI, we&#x27;re building toward AGI. In that future state, large language models (LLMs) will enhance daily life, providing business productivity and entertainment and helping people with everything from education to coaching, support, brainstorming, creative writing and more.</span><br><span class="line"></span><br><span class="line">To make that a reality globally, it&#x27;s critical to achieve highly efficient â€œinferenceâ€ â€“ the process by which LLMs generate replies. As a full-stack AI company, Character.AI designs its model architecture, inference stack and product from the ground up, enabling unique opportunities to optimize inference to be more efficient, cost-effective and scalable to a rapidly growing, global audience.</span><br><span class="line"></span><br><span class="line">Today we serve more than 20,000 inference queries per second. To put this in perspective, this is roughly 20% of the request volume served by Google Search, which processes around 105,000 queries per second according to third party estimates (Statista, 2024).</span><br><span class="line"></span><br><span class="line">We can sustainably serve LLMs at this scale because we have developed a number of key innovations across our serving stack. In this blog post, we share some of the techniques and optimizations we have developed over the past two years and recently employed.</span><br><span class="line"></span><br><span class="line">Memory-efficient Architecture Design</span><br><span class="line">The key bottleneck of LLM inference throughput is the size of the cache of attention keys and values (KV). It not only determines the maximum batch size that can fit on a GPU, but also dominates the I/O cost on attention layers. We use the following techniques to reduce KV cache size by more than 20X without regressing quality. With these techniques, GPU memory is no longer a bottleneck for serving large batch sizes.</span><br><span class="line"></span><br><span class="line">1. Multi-Query Attention. We adopt Multi-Query Attention (Shazeer, 2019) in all attention layers. This reduces KV cache size by 8X compared to the Grouped-Query Attention adopted in most open source models.</span><br><span class="line"></span><br><span class="line">2. Hybrid Attention Horizons. We interleave local attention (Beltagy et al., 2020) with global attention layers. Local attention is trained with sliding windows, and reduces the complexity from O(length2) to O(length). We found that reducing attention horizon to 1024 on most attention layers does not have a significant impact on evaluation metrics, including the long context needle-in-haystack benchmark. In our production model, only 1 out of every 6 layers uses global attention.</span><br><span class="line"></span><br><span class="line">3. Cross Layer KV-sharing. We tie the KV cache across neighboring attention layers, which further reduces KV cache size by a factor of 2-3x. For global attention layers, we tie the KV cache of multiple global layers across blocks, since the global attention layers dominate the KV cache size under long context use cases. Similar to a recent publication (Brandon et al., 2024), we find that sharing KV across layers does not regress quality.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Figure 1. Left: Standard transformer design where every attention is global attention. Right: The attention design in our production model. Blue boxes indicate global attention, green boxes indicate local attention, and curves indicate KV-sharing. For global attention layers, we share KV across multiple non-adjacent layers. This illustration depicts only a subset of the layers in the full model.</span><br><span class="line">Stateful Caching</span><br><span class="line">One of our key innovations is an efficient system for caching attention KV on host memory between chat turns. On Character.AI, the majority of chats are long dialogues; the average message has a dialogue history of 180 messages. As dialogues grow longer, continuously refilling KV caches on each turn would be prohibitively expensive.</span><br><span class="line"></span><br><span class="line">To solve this problem, we developed an inter-turn caching system. For every prefilled prefix and generated message, we cache the KV values on host memory and retrieve them for future queries. Similar to RadixAttention (Zheng et al., 2023), we organize cached KV tensors in a LRU cache with a tree structure. The cached KV values are indexed by a rolling hash of prefix tokens. For each new query, a rolling hash is calculated for each prefix of the context, and the cache is retrieved for the longest match. This allows reusing the cache even for partially matched messages.</span><br><span class="line"></span><br><span class="line">At a fleet level, we use sticky sessions to route the queries from the same dialogue to the same server. Since our KV cache size is small, each server can cache thousands of dialogues concurrently. Our system achieves a 95% cache rate, further reducing inference cost.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Figure 2. Blue boxes indicate cached tensors on host memory. Green and yellow boxes indicate KV cache on CUDA memory. When a new query arrives, it retrieves the KV cache for the longest matched prefix. Our rolling hash system allows retrieving cache for partially matched messages.</span><br><span class="line">Quantization for Training and Serving</span><br><span class="line">We use int8 quantization on model weights, activations, and attention KV cache. To support this, we implemented customized int8 kernels for matrix multiplications and attention. Different from commonly adopted &quot;post-training quantization&quot; techniques, we natively train our models in int8 precision, eliminating the risk of training/serving mismatch while also significantly improving training efficiency. Quantized training is a complex topic on its own, and we will address it in future posts.</span><br><span class="line"></span><br><span class="line">Building the Future Together</span><br><span class="line">Efficient inference is crucial for scaling AI systems and integrating them seamlessly into our daily lives. Taken together, the innovations discussed above achieve unprecedented efficiency and reduce inference costs to a level that makes it far easier to serve LLMs at scale. We have reduced serving costs by a factor of 33 compared to when we began in late 2022. Today, if we were to serve our traffic using leading commercial APIs, it would cost at least 13.5X more than with our systems.</span><br><span class="line"></span><br><span class="line">Yet this is just the beginning. At Character.AI, we&#x27;re excited to continue building a future where LLMs are driving innovation and enhancing experiences for everyone worldwide. Join us on this exciting journey as we continue to push the limits of what&#x27;s possible with AI. Together, we are creating a future where efficient and scalable AI systems are at the heart of every interaction.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://n.news.naver.com/article/050/0000076482?cds=news_edit</span><br><span class="line">ê¸ˆìœµì—…, AI ìë™í™”ë¡œ ì¼ìë¦¬ ëºê¸¸ë¼...&quot;ê·¼ë¬´ì¼ 3.5ì¼ ë‹¨ì¶• ê°€ëŠ¥ì„±â†‘&quot;</span><br><span class="line">ì…ë ¥2024.06.20. ì˜¤ì „ 9:26  ìˆ˜ì •2024.06.20. ì˜¤ì „ 10:05 ê¸°ì‚¬ì›ë¬¸</span><br><span class="line">ì •ìœ ì§„ ê¸°ì</span><br><span class="line">ì •ìœ ì§„ ê¸°ì</span><br><span class="line"> 1</span><br><span class="line">1</span><br><span class="line">ë³¸ë¬¸ ìš”ì•½ë´‡</span><br><span class="line">í…ìŠ¤íŠ¸ ìŒì„± ë³€í™˜ ì„œë¹„ìŠ¤ ì‚¬ìš©í•˜ê¸°</span><br><span class="line">ê¸€ì í¬ê¸° ë³€ê²½í•˜ê¸°</span><br><span class="line">SNS ë³´ë‚´ê¸°</span><br><span class="line">ì¸ì‡„í•˜ê¸°</span><br><span class="line"></span><br><span class="line">ì „ì²´ ì¼ìë¦¬ ì¤‘ ê¸ˆìœµ ë¶€ë¬¸ì´ ì¸ê³µì§€ëŠ¥(ì´í•˜ AI)ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ëŠ” ê²°ê³¼ê°€ ë‚˜ì™”ë‹¤.</span><br><span class="line"></span><br><span class="line">19ì¼(í˜„ì§€ì‹œê°„) ë¸”ë£¸ë²„ê·¸í†µì‹ ì— ë”°ë¥´ë©´ ì”¨í‹°ê·¸ë£¹ì€ AIê´€ë ¨ ë³´ê³ ì„œë¥¼ í†µí•´ ì€í–‰ ì—…ë¬´ì˜ 54%ê°€ ìë™í™”ë˜ê³  12%ì˜ ì§ë¬´ì—ì„œ AIì— ì˜í•´ ìƒì‚°ì„± í–¥ìƒ ë“± ê°œì„  íš¨ê³¼ê°€ ë‚˜íƒ€ë‚  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ë¶„ì„í–ˆë‹¤.</span><br><span class="line"></span><br><span class="line">ë³´ê³ ì„œëŠ” ì€í–‰ì—…ì¢…ì— ì´ì–´ ë³´í—˜(48%), ì—ë„ˆì§€(43%), ìë³¸ì‹œì¥(40%), ì—¬í–‰(38%), ì†Œí”„íŠ¸ì›¨ì–´Â·í”Œë«í¼(36%), ì†Œë§¤(34%), ì»¤ë®¤ë‹ˆì¼€ì´ì…˜Â·ë¯¸ë””ì–´(33%), ê³µê³µì„œë¹„ìŠ¤(30%), ìë™ì°¨(30%) ë“± ì—…ì¢… ìˆœìœ¼ë¡œ ì—…ë¬´ ìë™í™” ì •ë„ê°€ í´ ê²ƒìœ¼ë¡œ ë‚´ë‹¤ë´¤ë‹¤.</span><br><span class="line"></span><br><span class="line">ë˜ ë³´ê³ ì„œëŠ” ì‹¤ì œë¡œ ê¸€ë¡œë²Œ ì£¼ìš” ì€í–‰ë“¤ì´ ì§ì›ë“¤ì˜ ìƒì‚°ì„±ì„ ë†’ì´ê³  ë¹„ìš© ì ˆê°ì— ë„ì›€ì„ ë  ê²ƒìœ¼ë¡œ ë³´ê³  ì§€ë‚œí•´ë¶€í„° ì„œì„œíˆ AIë¥¼ ë„ì…í•´ ê°ì¢… ì‹¤í—˜ì„ í•˜ê³  ìˆë‹¤ê³  ì „í–ˆë‹¤.</span><br><span class="line"></span><br><span class="line">ì”¨í‹°ê·¸ë£¹ì˜ ê²½ìš° ê°œë°œìë“¤ì—ê²Œ ë‹¤ì–‘í•œ AIê¸°ìˆ ì„ ì‹¤í—˜í•  ìˆ˜ ìˆëŠ” ì—­ëŸ‰ì„ ê°–ì¶”ë„ë¡ í–ˆìœ¼ë©°, ê°„ë‹¨í•œ ì§ˆë¬¸ì´ë‚˜ ëª…ë ¹ì— ë”°ë¼ ë¬¸ì¥ì´ë‚˜ ì—ì„¸ì´ ë“±ì„ ìƒì‚°í•  ìˆ˜ ìˆëŠ” ìƒì„±í˜• AIë¥¼ í™œìš©í•´ ìˆ˜ë°± ìª½ì— ë‹¬í•˜ëŠ” ê·œì •ì„ ë¹ ë¥´ê²Œ ê²€í† í•˜ê³  ìˆë‹¤ê³  ì†Œê°œí–ˆë‹¤.</span><br><span class="line"></span><br><span class="line">JPëª¨ê±´ì²´ì´ìŠ¤ëŠ” â€œAI ê¸°ìˆ ê³¼ ê´€ë ¨í•œ ì¸ì¬ ì˜ì…ì— ë‚˜ì„°ë‹¤â€ë©° â€œì´ íšŒì‚¬ì˜ ì œì´ë¯¸ ë‹¤ì´ë¨¼ ìµœê³ ê²½ì˜ì(CEO)ëŠ” ì´ ê¸°ìˆ ì„ í™œìš©í•˜ë©´ ê³ ìš©ì£¼ë“¤ì´ ì£¼ë‹¹ ê·¼ë¬´ì¼ì„ 3.5ì¼ë¡œ ë‹¨ì¶•í•  ìˆ˜ ìˆì„ ê²ƒâ€ì´ë¼ê³  ë§í–ˆë‹¤.</span><br><span class="line"></span><br><span class="line">ì”¨í‹°ê·¸ë£¹ ìµœê³ ê¸°ìˆ ì±…ì„ì(CTO) ë°ì´ë¹„ë“œ ê·¸ë¦¬í”¼ìŠ¤ëŠ” â€œìƒì„±í˜• AIê°€ ì€í–‰ ì‚°ì—…ì„ í˜ì‹ í•˜ê³  ìˆ˜ìµì„±ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆë‹¤â€ ë©° â€œì”¨í‹°ì—ì„œëŠ” íšŒì‚¬ì™€ ì§ì› ì—­ëŸ‰ ê°•í™”ë¥¼ ìœ„í•´ ì•ˆì „í•˜ê³  ì±…ì„ ìˆëŠ” ë°©ì‹ìœ¼ë¡œ ìƒì„±í˜• AIë¥¼ êµ¬í˜„í•˜ëŠ” ë° ì§‘ì¤‘í•˜ê³  ìˆë‹¤â€ê³  ì „í–ˆë‹¤.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-06-21</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/06/21/2024-6-21-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 6ì›” 21ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/06/25/2024-6-25-AI-NEWS/" title="2024ë…„ 6ì›” 25ì¼ AI ì†Œì‹">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/06/18/2024-6-18-AI-NEWS/" title="2024ë…„ 6ì›” 18ì¼ AI ì†Œì‹">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>