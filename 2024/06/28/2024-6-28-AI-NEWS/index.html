<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 6ì›” 28ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="SummaryGoogle Researchì—ì„œëŠ” Gemma 2ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤. Gemma 2ëŠ” 9B ë° 27B íŒŒë¼ë¯¸í„° í¬ê¸°ë¡œ ì œê³µë˜ë©°, ê°ê° 13ì¡° ë° 8ì¡° í† í°ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ Meta Llama 3 70Bì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Metaì—ì„œëŠ” "><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 6ì›” 28ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Google Researchì—ì„œëŠ” Gemma 2ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤. Gemma 2ëŠ” 9B ë° 27B íŒŒë¼ë¯¸í„° í¬ê¸°ë¡œ ì œê³µë˜ë©°, ê°ê° 13ì¡° ë° 8ì¡° í† í°ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ Meta Llama 3 70Bì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Metaì—ì„œëŠ” Meta LLM Compilerë¥¼ ê³µê°œí•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” Code Llama ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ì ì¸ ì½”ë“œ ìµœì í™”ì™€ ì»´íŒŒì¼ëŸ¬ ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤. OpenAIëŠ” TIMEê³¼ì˜ ì „ëµì  ì½˜í…ì¸  íŒŒíŠ¸ë„ˆì‹­ì„ ë°œí‘œí•˜ì˜€ìœ¼ë©°, Anthropicì—ì„œëŠ” Claude.aiì— í”„ë¡œì íŠ¸ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, Hugging FaceëŠ” ìƒˆë¡œìš´ ì˜¤í”ˆ LLM ë¦¬ë”ë³´ë“œë¥¼ ê³µê°œí•˜ì˜€ê³ , FineWeb ë°ì´í„°ì…‹ì— ê´€í•œ ë…¼ë¬¸ì„ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.</p>
<h2 id="Google"><a href="#Google" class="headerlink" title="Google,"></a>Google,</h2><h3 id="Gemma-2-ë°œí‘œ"><a href="#Gemma-2-ë°œí‘œ" class="headerlink" title="Gemma 2 ë°œí‘œ"></a>Gemma 2 ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315">ë§í¬</a>, 2024ë…„ 6ì›” 28ì¼,<br>Google Research</p>
<ul>
<li>Gemma 2 ëª¨ë¸ ë°œí‘œ, 9B ë° 27B íŒŒë¼ë¯¸í„° í¬ê¸°ë¡œ ì œê³µ</li>
<li>ê°ê° 13ì¡° ë° 8ì¡° í† í°ìœ¼ë¡œ í›ˆë ¨</li>
<li>27B ëª¨ë¸ì€ Meta Llama 3 70Bì™€ ì„±ëŠ¥ ê²½ìŸ ê°€ëŠ¥</li>
<li>ì²« Chatbot Arena í‰ê°€ì—ì„œ Gemma2 27BëŠ” Anthropic Claude 3 Sonnet, Llama 3 70B, OpenAI GPT-4ì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ ê¸°ë¡</li>
<li>9B ëª¨ë¸ì€ 71.3 MMLU, 52.8 AGIEval, 40.2 HumanEval ì ìˆ˜ ê¸°ë¡</li>
<li>27B ëª¨ë¸ì€ 75.2 MMLU, 55.1 AGIEval, 51.8 HumanEval ì ìˆ˜ ê¸°ë¡</li>
<li>ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥, Hugging Faceì—ì„œ ì œê³µ</li>
<li>Google TPUv5eì—ì„œ í›ˆë ¨, íš¨ìœ¨ì ì¸ ì¶”ë¡  ì„±ëŠ¥ ì œê³µ</li>
<li>ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì–´í…ì…˜, ë¡œì§“ ì†Œí”„íŠ¸ìº¡í•‘ ë° ê·¸ë£¹í™”ëœ ì¿¼ë¦¬ ì–´í…ì…˜(GQA) ê¸°ëŠ¥ í¬í•¨</li>
<li>Google Cloudì—ì„œ ê°„í¸í•œ 1í´ë¦­ ë°°í¬ ì§€ì›</li>
</ul>
<h2 id="META"><a href="#META" class="headerlink" title="META,"></a>META,</h2><h3 id="Meta-LLM-Compiler-ë°œí‘œ"><a href="#Meta-LLM-Compiler-ë°œí‘œ" class="headerlink" title="Meta LLM Compiler ë°œí‘œ"></a>Meta LLM Compiler ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/collections/facebook/llm-compiler-667c5b05557fe99a9edd25cb">ë§í¬</a>, 2024ë…„ 6ì›” 28ì¼,<br>META</p>
<ul>
<li>Meta LLM Compiler ëª¨ë¸ ë°œí‘œ, ì½”ë“œ í¬ê¸° ìµœì í™” ë° ë””ìŠ¤ì–´ì…ˆë¸”ë¦¬ ì‘ì—…ì—ì„œ ìµœì²¨ë‹¨ ê²°ê³¼ ë‹¬ì„±</li>
<li>GPT-4ë³´ë‹¤ ì½”ë“œ í¬ê¸° ê°œì„  ë° ë””ìŠ¤ì–´ì…ˆë¸”ë¦¬ ì„±ëŠ¥ ìš°ìˆ˜</li>
<li>ë‘ ê°€ì§€ ëª¨ë¸ ì œê³µ: LLM Compiler, LLM Compiler FTD</li>
<li>LLM Compiler: LLVM-IR, x86_84, ARM, CUDA ì–´ì…ˆë¸”ë¦¬ ì½”ë“œ 5000ì–µ í† í°ìœ¼ë¡œ ì‚¬ì „ í›ˆë ¨</li>
<li>LLM Compiler FTD: LLVM ì–´ì…ˆë¸”ë¦¬ ì½”ë“œ ìµœì í™” ë° ë””ìŠ¤ì–´ì…ˆë¸”ë¦¬ ì˜ˆì¸¡ì„ ìœ„í•´ ì¶”ê°€ í›ˆë ¨</li>
<li>ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥, ì—°êµ¬ ë° ìƒì—…ì  ìš©ë„ë¡œ ì œê³µ</li>
<li>ì»´íŒŒì¼ëŸ¬ ì™„ë²½í•˜ê²Œ ì—ë®¬ë ˆì´ì…˜í•˜ëŠ” ë¹„ìœ¨ 20%</li>
</ul>
<h2 id="OpenAI"><a href="#OpenAI" class="headerlink" title="OpenAI,"></a>OpenAI,</h2><h3 id="TIMEê³¼ì˜-ì „ëµì -ì½˜í…ì¸ -íŒŒíŠ¸ë„ˆì‹­"><a href="#TIMEê³¼ì˜-ì „ëµì -ì½˜í…ì¸ -íŒŒíŠ¸ë„ˆì‹­" class="headerlink" title="TIMEê³¼ì˜ ì „ëµì  ì½˜í…ì¸  íŒŒíŠ¸ë„ˆì‹­"></a>TIMEê³¼ì˜ ì „ëµì  ì½˜í…ì¸  íŒŒíŠ¸ë„ˆì‹­</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/strategic-content-partnership-with-time/">ë§í¬</a>, 2024ë…„ 6ì›” 27ì¼,<br>OpenAI</p>
<ul>
<li>TIMEì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì €ë„ë¦¬ì¦˜ ì½˜í…ì¸ ì— ëŒ€í•œ ì ‘ê·¼ ì œê³µ</li>
<li>101ë…„ê°„ì˜ ì•„ì¹´ì´ë¸Œ ì½˜í…ì¸  í¬í•¨</li>
<li>OpenAI ì œí’ˆì—ì„œ ì½˜í…ì¸  ì¸ìš© ë° ì›ë³¸ ë§í¬ ì œê³µ</li>
<li>TIMEì€ OpenAI ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ìƒˆë¡œìš´ ì œí’ˆ ê°œë°œ</li>
<li>OpenAIëŠ” TIMEì˜ í”¼ë“œë°±ì„ í†µí•´ ì €ë„ë¦¬ì¦˜ ì œê³µ ë°©ì‹ ê°œì„ </li>
</ul>
<h2 id="Anthropic"><a href="#Anthropic" class="headerlink" title="Anthropic,"></a>Anthropic,</h2><h3 id="Claude-ai-í”„ë¡œì íŠ¸-ê¸°ëŠ¥-ì¶”ê°€"><a href="#Claude-ai-í”„ë¡œì íŠ¸-ê¸°ëŠ¥-ì¶”ê°€" class="headerlink" title="Claude.ai í”„ë¡œì íŠ¸ ê¸°ëŠ¥ ì¶”ê°€"></a>Claude.ai í”„ë¡œì íŠ¸ ê¸°ëŠ¥ ì¶”ê°€</h3><p><a target="_blank" rel="noopener" href="https://www.anthropic.com/news/projects">ë§í¬</a>, 2024ë…„ 6ì›” 26ì¼,<br>Anthropic</p>
<ul>
<li>Claude.ai Pro ë° Team ì‚¬ìš©ìì—ê²Œ í”„ë¡œì íŠ¸ ê¸°ëŠ¥ ì œê³µ</li>
<li>200K ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ë¡œ ê´€ë ¨ ë¬¸ì„œ, ì½”ë“œ, ì¸ì‚¬ì´íŠ¸ ì¶”ê°€ ê°€ëŠ¥</li>
<li>ì‚¬ìš©ì ì •ì˜ ì§€ì¹¨ ì„¤ì • ê°€ëŠ¥, ì˜ˆ: ë” ê³µì‹ì ì¸ ì–´ì¡° ì‚¬ìš©</li>
<li>Artifacts ê¸°ëŠ¥ìœ¼ë¡œ ì½˜í…ì¸  ìƒì„± ë° ì‹¤ì‹œê°„ ë¯¸ë¦¬ë³´ê¸° ì œê³µ</li>
<li>íŒ€ ë‚´ ê³µìœ  í™œë™ í”¼ë“œë¡œ í˜‘ì—… ê°•í™”</li>
<li>í”„ë¡œì íŠ¸ ê¸°ëŠ¥ì€ íŒ€ì˜ ë‚´ë¶€ ì§€ì‹ì„ í™œìš©í•˜ì—¬ Claudeì˜ ì¶œë ¥ì„ ê°•í™”</li>
</ul>
<h2 id="Hugging-Face"><a href="#Hugging-Face" class="headerlink" title="Hugging Face,"></a>Hugging Face,</h2><h3 id="ì˜¤í”ˆ-LLM-ë¦¬ë”ë³´ë“œ-2-ê³µê°œ"><a href="#ì˜¤í”ˆ-LLM-ë¦¬ë”ë³´ë“œ-2-ê³µê°œ" class="headerlink" title="ì˜¤í”ˆ LLM ë¦¬ë”ë³´ë“œ 2 ê³µê°œ"></a>ì˜¤í”ˆ LLM ë¦¬ë”ë³´ë“œ 2 ê³µê°œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard">ë§í¬</a>, 2024ë…„ 6ì›” 28ì¼,<br>Hugging Face</p>
<ul>
<li>ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ ë„ì…: MMLU-Pro, GPQA, MuSR, MATH, IFEval, BBH</li>
<li>ì„±ëŠ¥ ìˆœìœ„ ê°œì„  ë° ìƒˆë¡œìš´ Gradio ì»´í¬ë„ŒíŠ¸ ì œê³µ</li>
<li>Qwen2 72B Instruct ëª¨ë¸ì´ ìƒìœ„ ì°¨ì§€</li>
<li>ì»¤ë®¤ë‹ˆí‹° íˆ¬í‘œ ì‹œìŠ¤í…œ ë„ì…</li>
<li>í–¥ìƒëœ ì¬í˜„ì„±, ë¸íƒ€ ì›¨ì´íŠ¸ ë° ì±— í…œí”Œë¦¿ ì§€ì›</li>
</ul>
<h2 id="Hugging-Face-1"><a href="#Hugging-Face-1" class="headerlink" title="Hugging Face,"></a>Hugging Face,</h2><h3 id="FineWeb-ë°ì´í„°ì…‹-ë°œí‘œ"><a href="#FineWeb-ë°ì´í„°ì…‹-ë°œí‘œ" class="headerlink" title="FineWeb ë°ì´í„°ì…‹ ë°œí‘œ"></a>FineWeb ë°ì´í„°ì…‹ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2406.17557">ë§í¬</a>, 2024ë…„ 6ì›” 25ì¼,<br>Hugging Face</p>
<ul>
<li>FineWeb ë°ì´í„°ì…‹: 96ê°œì˜ Common Crawl ìŠ¤ëƒ…ìƒ·ì—ì„œ 15ì¡° í† í°ìœ¼ë¡œ êµ¬ì„±</li>
<li>FineWeb-Edu: êµìœ¡ìš© í…ìŠ¤íŠ¸ í•„í„°ë§ëœ 1.3ì¡° í† í° ë°ì´í„°ì…‹</li>
<li>LLM ì„±ëŠ¥ ê°œì„ , ë‹¤ì–‘í•œ ê³µê°œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ ê¸°ë¡</li>
<li>ë°ì´í„°ì…‹ ë° ë°ì´í„° íë ˆì´ì…˜ ì½”ë“œë² ì´ìŠ¤ ê³µê°œ</li>
<li>ì¤‘ë³µ ì œê±° ë° í•„í„°ë§ ì „ëµì— ëŒ€í•œ ì‹¬ë„ ìˆëŠ” ì—°êµ¬ í¬í•¨</li>
</ul>
<h2 id="Infiniflow"><a href="#Infiniflow" class="headerlink" title="Infiniflow,"></a>Infiniflow,</h2><h3 id="RAGFlow-ê³µê°œ"><a href="#RAGFlow-ê³µê°œ" class="headerlink" title="RAGFlow ê³µê°œ"></a>RAGFlow ê³µê°œ</h3><p><a target="_blank" rel="noopener" href="https://github.com/infiniflow/ragflow">ë§í¬</a>, 2024ë…„ 6ì›” 25ì¼,<br>Infiniflow</p>
<ul>
<li>ì˜¤í”ˆì†ŒìŠ¤ RAG ì—”ì§„, ê¹Šì€ ë¬¸ì„œ ì´í•´ ê¸°ë°˜ ì§€ì‹ ì¶”ì¶œ</li>
<li>ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¹„êµ¬ì¡°í™” ë°ì´í„° ì§€ì›</li>
<li>í’ˆì§ˆ ê¸°ë°˜ ì§ˆë¬¸ ì‘ë‹µ ê¸°ëŠ¥ ì œê³µ</li>
<li>ê°„í¸í•œ RAG ì›Œí¬í”Œë¡œìš°, ê°œì¸ ë° ëŒ€ê¸°ì—…ì— ì í•©</li>
<li>ë‹¤ì¤‘ ë¦¬ì½œ ë° ì¬ìˆœìœ„ ë§¤ê¸°ê¸° ê¸°ëŠ¥ ì œê³µ</li>
<li>í…œí”Œë¦¿ ê¸°ë°˜ ì²­í‚¹, ì¸ê°„ì˜ ê°œì… í—ˆìš©</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹"><a href="#todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹" class="headerlink" title="(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,"></a>(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,</h1><h2 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAIì—ì„œëŠ” ~~~ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.)</p>
<h2 id="Title"><a href="#Title" class="headerlink" title="Title,"></a>Title,</h2><h3 id="í•œê¸€ì œëª©"><a href="#í•œê¸€ì œëª©" class="headerlink" title="í•œê¸€ì œëª©"></a>í•œê¸€ì œëª©</h3><p><a href="link">ë§í¬</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
</ul>
<h2 id="Title-1"><a href="#Title-1" class="headerlink" title="Title,"></a>Title,</h2><h3 id="í•œê¸€ì œëª©-1"><a href="#í•œê¸€ì œëª©-1" class="headerlink" title="í•œê¸€ì œëª©"></a>í•œê¸€ì œëª©</h3><p><a href="link">ë§í¬</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315</span><br><span class="line">June 28, 2024</span><br><span class="line">Google Research</span><br><span class="line">Gemma 2 released! Google just released the next iteration of its open LLM! Gemma 2 comes in two sizes, 9B &amp; 27B, trained on 13T tokens. Gemma 2 27B approaches Meta Llama 3 70B performance! First Chatbot Arena evals place Gemma2 27B around Anthropic Claude 3 Sonnet, Llama 3 70B, and OpenAI GPT-4. ğŸ¤¯</span><br><span class="line">What&#x27;s new with Gemma 2:</span><br><span class="line">ğŸ§® 9B &amp; 27B Instruction and base version with 8192 context window</span><br><span class="line">ğŸ”  Trained on 13T tokens (27B) and 8T tokens (9B)</span><br><span class="line">ğŸ†• Sliding window attention, logit soft-capping and Grouped-Query Attention (GQA)</span><br><span class="line">ğŸ¥‡ 9B scores 71.3 MMLU; 52.8 AGIEval; 40.2 HumanEval</span><br><span class="line">ğŸ† 27B scores 75.2 MMLU; 55.1 AGIEval; 51.8 HumanEval</span><br><span class="line">âœ… Commercial use allowed</span><br><span class="line">ğŸ§¬ Used SFT, Distillation, RLHF &amp; Model Merging.</span><br><span class="line">ğŸ§  Trained on Google TPUv5e</span><br><span class="line">ğŸ¤— Available on Hugging Face</span><br><span class="line">ğŸ”œ 1-click deployment to Google Cloud from Hugging Face</span><br><span class="line"></span><br><span class="line">DEVELOPERS</span><br><span class="line"></span><br><span class="line">Gemma 2 is now available to researchers and developers</span><br><span class="line">Jun 27, 2024</span><br><span class="line"></span><br><span class="line">4 min read</span><br><span class="line"></span><br><span class="line">Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.</span><br><span class="line"></span><br><span class="line">C</span><br><span class="line">Clement Farabet</span><br><span class="line">VP of Research, Google DeepMind</span><br><span class="line">T</span><br><span class="line">Tris Warkentin</span><br><span class="line">Director, Google DeepMind</span><br><span class="line">Share</span><br><span class="line">AI has the potential to address some of humanity&#x27;s most pressing problems â€” but only if everyone has the tools to build with it. That&#x27;s why earlier this year we introduced Gemma, a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models. Weâ€™ve continued to grow the Gemma family with CodeGemma, RecurrentGemma and PaliGemma â€” each offering unique capabilities for different AI tasks and easily accessible through integrations with partners like Hugging Face, NVIDIA and Ollama.</span><br><span class="line"></span><br><span class="line">Now weâ€™re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And thatâ€™s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.</span><br><span class="line"></span><br><span class="line">A new open model standard for efficiency and performance</span><br><span class="line">We built Gemma 2 on a redesigned architecture, engineered for both exceptional performance and inference efficiency. Hereâ€™s what makes it stand out:</span><br><span class="line"></span><br><span class="line">Outsized performance: At 27B, Gemma 2 delivers the best performance for its size class, and even offers competitive alternatives to models more than twice its size. The 9B Gemma 2 model also delivers class-leading performance, outperforming Llama 3 8B and other open models in its size category. For detailed performance breakdowns, check out the technical report.</span><br><span class="line">Unmatched efficiency and cost savings: The 27B Gemma 2 model is designed to run inference efficiently at full precision on a single Google Cloud TPU host, NVIDIA A100 80GB Tensor Core GPU, or NVIDIA H100 Tensor Core GPU, significantly reducing costs while maintaining high performance. This allows for more accessible and budget-friendly AI deployments.</span><br><span class="line">Blazing fast inference across hardware: Gemma 2 is optimized to run at incredible speed across a range of hardware, from powerful gaming laptops and high-end desktops, to cloud-based setups. Try Gemma 2 at full precision in Google AI Studio, unlock local performance with the quantized version with Gemma.cpp on your CPU, or try it on your home computer with an NVIDIA RTX or GeForce RTX via Hugging Face Transformers.</span><br><span class="line">A chart showing Gemma 2 performance benchmarks</span><br><span class="line">Built for developers and researchers</span><br><span class="line">Gemma 2 is not only more powerful, it&#x27;s designed to more easily integrate into your workflows:</span><br><span class="line"></span><br><span class="line">Open and accessible: Just like the original Gemma models, Gemma 2 is available under our commercially-friendly Gemma license, giving developers and researchers the ability to share and commercialize their innovations.</span><br><span class="line">Broad framework compatibility: Easily use Gemma 2 with your preferred tools and workflows thanks to its compatibility with major AI frameworks like Hugging Face Transformers, and JAX, PyTorch and TensorFlow via native Keras 3.0, vLLM, Gemma.cpp, Llama.cpp and Ollama. In addition, Gemma is optimized with NVIDIA TensorRT-LLM to run on NVIDIA-accelerated infrastructure or as an NVIDIA NIM inference microservice, with optimization for NVIDIAâ€™s NeMo to come. You can fine-tune today with Keras and Hugging Face. We are actively working to enable additional parameter-efficient fine-tuning options.1</span><br><span class="line">Effortless deployment: Starting next month, Google Cloud customers will be able to easily deploy and manage Gemma 2 on Vertex AI.</span><br><span class="line">Explore the new Gemma Cookbook, a collection of practical examples and recipes to guide you through building your own applications and fine-tuning Gemma 2 models for specific tasks. Discover how to easily use Gemma with your tooling of choice, including for common tasks like retrieval-augmented generation.</span><br><span class="line"></span><br><span class="line">Responsible AI development</span><br><span class="line">We&#x27;re committed to providing developers and researchers with the resources they need to build and deploy AI responsibly, including through our Responsible Generative AI Toolkit. The recently open-sourced LLM Comparator helps developers and researchers with in-depth evaluation of language models. Starting today, you can use the companion Python library to run comparative evaluations with your model and data, and visualize the results in the app. Additionally, weâ€™re actively working on open sourcing our text watermarking technology, SynthID, for Gemma models.</span><br><span class="line"></span><br><span class="line">When training Gemma 2, we followed our robust internal safety processes, filtering pre-training data and performing rigorous testing and evaluation against a comprehensive set of metrics to identify and mitigate potential biases and risks. We publish our results on a large set of public benchmarks related to safety and representational harms.</span><br><span class="line"></span><br><span class="line">A chart showing Gemma 2 safety evaluations</span><br><span class="line">Projects built with Gemma</span><br><span class="line">Our first Gemma launch led to more than 10 million downloads and countless inspiring projects. Navarasa, for instance, used Gemma to create a model rooted in Indiaâ€™s linguistic diversity.</span><br><span class="line"></span><br><span class="line">Developing for Indic languages: Gemma and Navarasa</span><br><span class="line">3:14</span><br><span class="line">Now, Gemma 2 will help developers get even more ambitious projects off the ground, unlocking new levels of performance and potential in their AI creations. We&#x27;ll continue to explore new architectures and develop specialized Gemma variants to tackle a wider range of AI tasks and challenges. This includes an upcoming 2.6B parameter Gemma 2 model, designed to further bridge the gap between lightweight accessibility and powerful performance. You can learn more about this upcoming release in the technical report.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/collections/facebook/llm-compiler-667c5b05557fe99a9edd25cb</span><br><span class="line">June 28, 2024</span><br><span class="line">META</span><br><span class="line">Today weâ€™re releasing Meta LLM Compiler, a family of models built on Meta Code Llama with additional code optimization and compiler capabilities. The models achieve state-of-the-art results on optimization of code size and disassembly tasks.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">LLM Compiler can emulate the compiler, predict optimal passes for code size, and disassemble code. It can be fine-tuned for new optimizations and compiler tasks. This work shows that AI is learning to optimize code and can assist compiler experts in identifying opportunities to apply optimizations. We believe this work could have an impact ranging from use in optimization for individual developer environments to inclusion in a compiler such as LLVM.</span><br><span class="line">Weâ€™re releasing LLM Compiler 7B &amp; 13B models under a permissive license for both research and commercial use in the hopes of making it easier for developers and researchers alike to leverage this in their work and carry forward new research in this highly impactful space.</span><br><span class="line"></span><br><span class="line">WAIT, it&#x27;s not over; Meta just dropped the LLM Compiler! ğŸ§‘â€ğŸ’»</span><br><span class="line">&gt; Beats GPT-4 on code size improvement and disassembly</span><br><span class="line">&gt; Achieves 77% of the optimising potential of an autotuning search and 45% disassembly round trip ğŸ”¥</span><br><span class="line">&gt; Built on top of CodeLLaMa with improved code optimisation and compiler reasoning.</span><br><span class="line">&gt; Allows commercial use</span><br><span class="line">Two model types:</span><br><span class="line">&gt; LLM Compiler: the foundational models, pre-trained on over 500B tokens of LLVM-IR, x86_84, ARM, and CUDA assembly codes and trained to predict the effect of LLVM optimisations</span><br><span class="line">&gt;LLM Compiler FTD, which is further fine-tuned to predict the best optimisations for code in LLVM assembly to reduce code size and disassemble assembly code to LLVM-IR</span><br><span class="line">&gt; Perfectly emulating the compiler 20% of the time âš¡</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/strategic-content-partnership-with-time/</span><br><span class="line">June 27, 2024</span><br><span class="line">OpenAI</span><br><span class="line">Strategic Content Partnership with TIME</span><br><span class="line">New access to current and historic content from TIME&#x27;s extensive archives from the last 101 years to enhance OpenAI products and display in response to user inquiries.</span><br><span class="line"></span><br><span class="line">Time &gt; Hero &gt; Media &gt; Asset</span><br><span class="line">Today, TIME and OpenAI announced a multi-year content deal and strategic partnership to bring TIME&#x27;s trusted journalism to OpenAIâ€™s products, including ChatGPT.</span><br><span class="line"></span><br><span class="line">Through this collaboration, OpenAI will gain access to current and historic content from TIME&#x27;s extensive archives from the last 101 years to enhance its products and display in response to user inquiriesâ€”featuring a citation and link back to the original source on Time.com. The new partnership furthers TIMEâ€™s commitment to expanding global access to accurate and trusted information.</span><br><span class="line"></span><br><span class="line">&quot;Throughout our 101-year history, TIME has embraced innovation to ensure that the delivery of our trusted journalism evolves alongside technology,&quot; said TIME Chief Operating Officer Mark Howard.  &quot;This partnership with OpenAI advances our mission to expand access to trusted information globally as we continue to embrace innovative new ways of bringing TIMEâ€™s journalism to audiences globally.â€</span><br><span class="line"></span><br><span class="line">â€œWeâ€™re partnering with TIME to make it easier for people to access news content through our AI tools, and to support reputable journalism by providing proper attribution to original sources,â€ said Brad Lightcap, Chief Operating Officer of OpenAI.</span><br><span class="line"></span><br><span class="line">The partnership will also enable TIME to gain access to OpenAI&#x27;s technology to develop new products for its audiences, along with the opportunity to provide vital feedback and share practical applications to refine and enhance the delivery of journalism in ChatGPT and other OpenAI products and shape the future of news experiences.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">OpenAI</span><br><span class="line">June 26, 2024</span><br><span class="line">We&#x27;re sharing an update on the advanced Voice Mode we demoed during our Spring Update, which we remain very excited about:</span><br><span class="line">We had planned to start rolling this out in alpha to a small group of ChatGPT Plus users in late June, but need one more month to reach our bar to launch. For example, weâ€™re improving the modelâ€™s ability to detect and refuse certain content. Weâ€™re also working on improving the user experience and preparing our infrastructure to scale to millions while maintaining real-time responses.</span><br><span class="line">As part of our iterative deployment strategy, we&#x27;ll start the alpha with a small group of users to gather feedback and expand based on what we learn. We are planning for all Plus users to have access in the fall. Exact timelines depend on meeting our high safety and reliability bar. We are also working on rolling out the new video and screen sharing capabilities we demoed separately, and will keep you posted on that timeline.</span><br><span class="line">ChatGPTâ€™s advanced Voice Mode can understand and respond with emotions and non-verbal cues, moving us closer to real-time, natural conversations with AI. Our mission is to bring these new experiences to you thoughtfully.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.anthropic.com/news/projects</span><br><span class="line">Collaborate with Claude on Projects</span><br><span class="line">2024ë…„ 6ì›” 26ì¼</span><br><span class="line">Anthorphic</span><br><span class="line">â—</span><br><span class="line">3 min read</span><br><span class="line">Illustration of individuals collaborating around Claude logo</span><br><span class="line">Our vision for Claude has always been to create AI systems that work alongside people and meaningfully enhance their workflows. As a step in this direction, Claude.ai Pro and Team users can now organize their chats into Projects, bringing together curated sets of knowledge and chat activity in one placeâ€”with the ability to make their best chats with Claude viewable by teammates. With this new functionality, Claude can enable idea generation, more strategic decision-making, and exceptional results.</span><br><span class="line"></span><br><span class="line">Projects are available on Claude.ai for all Pro and Team customers, and can be powered by Claude 3.5 Sonnet, our latest release which outperforms its peers on a wide variety of benchmarks. Each project includes a 200K context window, the equivalent of a 500-page book, so users can add all of the relevant documents, code, and insights to enhance Claudeâ€™s effectiveness.</span><br><span class="line"></span><br><span class="line">Avoid the cold start problem</span><br><span class="line">Projects allow you to ground Claudeâ€™s outputs in your internal knowledgeâ€”be it style guides, codebases, interview transcripts, or past work. This added context enables Claude to provide expert assistance across tasks, from writing emails like your marketing team to writing SQL queries like a data analyst.</span><br><span class="line"></span><br><span class="line">App screen showing a user uploading docs to Claude.ai</span><br><span class="line">In addition, you can define custom instructions for each Project to further tailor Claudeâ€™s responses, including instructing Claude to use a more formal tone or answer questions from the perspective of a specific role or industry. With Projects, you can get started much faster and extend your skills further for any task.</span><br><span class="line"></span><br><span class="line">App screen showing custom instructions</span><br><span class="line">Create side-by-side with Claude</span><br><span class="line">Artifacts help you better work with Claude by helping you see, edit, and build with Claude. Simply ask Claude to generate content like code snippets, text documents, graphics, diagrams, or website designs, and Artifacts appear in a dedicated window alongside your conversation.</span><br><span class="line"></span><br><span class="line">Artifacts especially enhance Claudeâ€™s coding capabilities for developers, offering a larger code window and live previews for frontends that streamline reviews. Join the feature preview for Artifacts in Claude.ai via the account menu on the left-side panel.</span><br><span class="line"></span><br><span class="line">App screen that shows the Artifacts panel alongside the user chat</span><br><span class="line">Spark inspiration through sharing</span><br><span class="line">Claude Team users can also share snapshots of their best conversations with Claude into your teamâ€™s shared project activity feed. Activity feeds help each teammate get inspired around different ways to work with Claude, and helps the entire team uplevel their skills working with AI.</span><br><span class="line"></span><br><span class="line">App screen showing shared chats within a Project</span><br><span class="line">Sharing work products that were co-created with Claude can improve innovation in areas like product development and research, where bringing together organizational knowledge from across the company can produce higher-quality outputs.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Customer spotlight: North Highland</span><br><span class="line">At North Highland, a leading change and transformation consultancy, hundreds of employees across consulting, business development, and marketing teams use Claude to work better. From writing proposals to analyzing complex documents like 10-Ks, teams use Claude to enhance and scale their expert services.</span><br><span class="line"></span><br><span class="line">The Claude Team plan is transforming our way of working at North Highland. Claude is a truly exceptional writer that has helped our team complete content creation and analysis tasks up to 5x faster than beforeâ€”turning what was once two weeks of writing and research into minutes of work. With Claude, weâ€™re future-proofing our workforce, finding more excitement in daily challenges, and leaping into the future of AI-assisted collaboration and creativity.</span><br><span class="line">Luka Anic, Senior Director of Technical AI Program and Product Manager at North Highland</span><br><span class="line"></span><br><span class="line">The future of work with Claude</span><br><span class="line">These latest features around shared knowledge and collaboration integrate Claude into your existing team processes, enabling you to save time and elevate your work. By harnessing Claudeâ€™s accuracy and advanced coding and writing capabilities, Projects can amplify your teamâ€™s potential. Additionally, as part of our commitment to user privacy, any data or chats shared within Projects will not be used to train our generative models without a userâ€™s explicit consent.</span><br><span class="line"></span><br><span class="line">In the coming months, weâ€™ll continue making Claude easier to use while expanding the types of project knowledge you can bring to Claude via native integrations with popular applications and tools. Weâ€™re excited to see how your team works with Claude.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard</span><br><span class="line">Open LLM Leaderboard 2 released! Evaluating LLMs is not easy. Finding new ways to compare LLM fairly, transparently, and reproducibly is important! Benchmarks are not perfect, but they give us a first understanding of how well models perform and where their strengths are.</span><br><span class="line">What&#x27;s new?!</span><br><span class="line">ğŸ“ˆ New benchmarks with MMLU-Pro, GPQA, MuSR, MATH, IFEval and BBH.</span><br><span class="line">ğŸ“Š Improved ranking with normalized scores adjusted to baselines</span><br><span class="line">ğŸ† Qwen2 72B Instruct &gt; Meta Llama 3 70B Instruct &gt; Cohere Command R+</span><br><span class="line">âš¡ Faster, simpler Interface with a new Gradio component.</span><br><span class="line">ğŸ› ï¸ Enhanced reproducibility with support for delta weights and chat templates</span><br><span class="line">â­ Introduction of &quot;maintainer&#x27;s highlight&quot; and â€œcommunity voting systemâ€</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2406.17557</span><br><span class="line">The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale</span><br><span class="line">Published on Jun 25</span><br><span class="line">Â·</span><br><span class="line">Submitted by</span><br><span class="line">philschmid</span><br><span class="line">on Jun 26</span><br><span class="line">#1 Paper of the day</span><br><span class="line">Authors:</span><br><span class="line"></span><br><span class="line">Guilherme Penedo</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Hynek KydlÃ­Äek</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Loubna Ben allal</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Anton Lozhkov</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Margaret Mitchell</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Colin Raffel</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Leandro Von Werra</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Thomas Wolf</span><br><span class="line">Abstract</span><br><span class="line">The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. In this work, we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, we carefully document and ablate all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we publicly release our data curation codebase and all of the models trained during our ablation experiments.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/infiniflow/ragflow</span><br><span class="line">ğŸ’¡ What is RAGFlow?</span><br><span class="line">RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding. It offers a streamlined RAG workflow for businesses of any scale, combining LLM (Large Language Models) to provide truthful question-answering capabilities, backed by well-founded citations from various complex formatted data.</span><br><span class="line"></span><br><span class="line">ğŸŒŸ Key Features</span><br><span class="line">ğŸ­ &quot;Quality in, quality out&quot;</span><br><span class="line">Deep document understanding-based knowledge extraction from unstructured data with complicated formats.</span><br><span class="line">Finds &quot;needle in a data haystack&quot; of literally unlimited tokens.</span><br><span class="line">ğŸ± Template-based chunking</span><br><span class="line">Intelligent and explainable.</span><br><span class="line">Plenty of template options to choose from.</span><br><span class="line">ğŸŒ± Grounded citations with reduced hallucinations</span><br><span class="line">Visualization of text chunking to allow human intervention.</span><br><span class="line">Quick view of the key references and traceable citations to support grounded answers.</span><br><span class="line">ğŸ” Compatibility with heterogeneous data sources</span><br><span class="line">Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.</span><br><span class="line">ğŸ›€ Automated and effortless RAG workflow</span><br><span class="line">Streamlined RAG orchestration catered to both personal and large businesses.</span><br><span class="line">Configurable LLMs as well as embedding models.</span><br><span class="line">Multiple recall paired with fused re-ranking.</span><br><span class="line">Intuitive APIs for seamless integration with business.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-06-28</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/06/28/2024-6-28-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 6ì›” 28ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/06/25/2024-6-25-AI-NEWS/" title="2024ë…„ 6ì›” 25ì¼ AI ì†Œì‹">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>