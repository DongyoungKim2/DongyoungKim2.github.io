<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 7ì›” 2ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="SummaryOpenAIì—ì„œëŠ” Critic ëª¨ë¸ì„ ë„ì…í•˜ì—¬ AI ì½”ë“œ í‰ê°€ ì‹ ë¢°ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. Critic ëª¨ë¸ì€ ì¸ê°„ë³´ë‹¤ ì½”ë“œì˜ ì˜¤ë¥˜ë¥¼ ë” ì˜ ì¡ì•„ë‚´ë©°, ì¸ê°„ í‰ê°€ìì™€ í˜‘ë ¥í•˜ì—¬ ì„±ê³¼ë¥¼ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤. NVIDIAëŠ” AI í´ë¼ìš°ë“œ ì œê³µì—…ì²´ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ì°¸ì¡° ì•„í‚¤í…ì²˜ë¥¼"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 7ì›” 2ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>OpenAIì—ì„œëŠ” Critic ëª¨ë¸ì„ ë„ì…í•˜ì—¬ AI ì½”ë“œ í‰ê°€ ì‹ ë¢°ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. Critic ëª¨ë¸ì€ ì¸ê°„ë³´ë‹¤ ì½”ë“œì˜ ì˜¤ë¥˜ë¥¼ ë” ì˜ ì¡ì•„ë‚´ë©°, ì¸ê°„ í‰ê°€ìì™€ í˜‘ë ¥í•˜ì—¬ ì„±ê³¼ë¥¼ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤. NVIDIAëŠ” AI í´ë¼ìš°ë“œ ì œê³µì—…ì²´ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ì°¸ì¡° ì•„í‚¤í…ì²˜ë¥¼ ë°œí‘œí•˜ì—¬ AI ì†”ë£¨ì…˜ ë°°í¬ ì‹œê°„ì„ ë‹¨ì¶•í•˜ê³  ë¹„ìš©ì„ ì ˆê°í•˜ëŠ” ë™ì‹œì— ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤. ìƒˆë¡œìš´ ì—°êµ¬ì—ì„œëŠ” 10ì–µ ê°œì˜ í˜ë¥´ì†Œë‚˜ë¥¼ í™œìš©í•œ ë°ì´í„° ìƒì„± ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ê³¼ í™•ì¥ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤. FigmaëŠ” AI ê¸°ëŠ¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ë‹¤ì–‘í•œ ë””ìì¸ ë„êµ¬ë¥¼ ì—…ë°ì´íŠ¸í•˜ì˜€ìœ¼ë©°, GroqëŠ” Whisper Large V3ì˜ ì„±ëŠ¥ì„ ëŒ€í­ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. SKê·¸ë£¹ì€ AIì™€ ë°˜ë„ì²´ ë¶„ì•¼ì— 2026ë…„ê¹Œì§€ 80ì¡° ì›ì„ íˆ¬ìí•  ê³„íšì„ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.</p>
<h2 id="OpenAI-Critic-ëª¨ë¸ë¡œ-AI-ì½”ë“œ-í‰ê°€-ì‹ ë¢°ì„±-í–¥ìƒ"><a href="#OpenAI-Critic-ëª¨ë¸ë¡œ-AI-ì½”ë“œ-í‰ê°€-ì‹ ë¢°ì„±-í–¥ìƒ" class="headerlink" title="OpenAI, Critic ëª¨ë¸ë¡œ AI ì½”ë“œ í‰ê°€ ì‹ ë¢°ì„± í–¥ìƒ"></a>OpenAI, Critic ëª¨ë¸ë¡œ AI ì½”ë“œ í‰ê°€ ì‹ ë¢°ì„± í–¥ìƒ</h2><p><a target="_blank" rel="noopener" href="https://cdn.openai.com/llm-critics-help-catch-llm-bugs-paper.pdf">ë§í¬</a>, 2024ë…„ 6ì›” 28ì¼,<br>OpenAI</p>
<ul>
<li>Critic ëª¨ë¸ì€ ì¸ê°„ë³´ë‹¤ ì½”ë“œì˜ ì˜¤ë¥˜ë¥¼ ë” ì˜ ì¡ì•„ë‚´ë©°, ì½”ë“œ í‰ê°€ì˜ ì •í™•ì„±ì„ ë†’ì„</li>
<li>CriticGPTëŠ” RLHFë¥¼ ì‚¬ìš©í•˜ì—¬ ìì—°ì–´ í”¼ë“œë°±ì„ ìƒì„±í•˜ê³  ì½”ë“œì˜ ë¬¸ì œë¥¼ ê°•ì¡°</li>
<li>Critic ëª¨ë¸ì€ ë•Œë•Œë¡œ í—ˆêµ¬ì˜ ì˜¤ë¥˜ë¥¼ ìƒì„±í•˜ì—¬ ì¸ê°„ì„ í˜¼ë€ìŠ¤ëŸ½ê²Œ í•  ìˆ˜ ìˆìŒ</li>
<li>Critic ëª¨ë¸ì˜ ë„ì…ìœ¼ë¡œ AIì™€ ì¸ê°„ í‰ê°€ìì˜ íŒ€ì´ ìœ ì‚¬í•œ ìˆ˜ì˜ ì˜¤ë¥˜ë¥¼ ì¡ì•„ë‚´ë©°, ì¸ê°„ í‰ê°€ìì˜ ì˜¤ë¥˜ ìˆ˜ë¥¼ ì¤„ì„</li>
<li>Critic ëª¨ë¸ì€ ChatGPT í›ˆë ¨ ë°ì´í„°ì˜ ìˆ˜ë°± ê°€ì§€ ì˜¤ë¥˜ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì‹ë³„</li>
<li>Critic ëª¨ë¸ì€ ì½”ë“œ ì´ì™¸ì˜ ì‘ì—…ì—ì„œë„ íš¨ê³¼ì ì„</li>
<li>Force Sampling Beam Search ê¸°ë²•ì„ ë„ì…í•˜ì—¬ ì‹¤ì œ ì˜¤ë¥˜ì™€ í—ˆêµ¬ì˜ ì˜¤ë¥˜ë¥¼ ê· í˜• ìˆê²Œ ê°ì§€</li>
</ul>
<h2 id="NVIDIA-AI-í´ë¼ìš°ë“œ-ì œê³µì—…ì²´ë¥¼-ìœ„í•œ-ìƒˆë¡œìš´-ë ˆí¼ëŸ°ìŠ¤-ì•„í‚¤í…ì²˜-ë°œí‘œ"><a href="#NVIDIA-AI-í´ë¼ìš°ë“œ-ì œê³µì—…ì²´ë¥¼-ìœ„í•œ-ìƒˆë¡œìš´-ë ˆí¼ëŸ°ìŠ¤-ì•„í‚¤í…ì²˜-ë°œí‘œ" class="headerlink" title="NVIDIA, AI í´ë¼ìš°ë“œ ì œê³µì—…ì²´ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ë ˆí¼ëŸ°ìŠ¤ ì•„í‚¤í…ì²˜ ë°œí‘œ"></a>NVIDIA, AI í´ë¼ìš°ë“œ ì œê³µì—…ì²´ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ë ˆí¼ëŸ°ìŠ¤ ì•„í‚¤í…ì²˜ ë°œí‘œ</h2><p><a target="_blank" rel="noopener" href="https://blogs.nvidia.com/blog/ai-cloud-providers-reference-architecture/?ncid=so-link-519834">ë§í¬</a>, 2024ë…„ 6ì›” 26ì¼,<br>NVIDIA</p>
<ul>
<li>NVIDIA í´ë¼ìš°ë“œ íŒŒíŠ¸ë„ˆ ì°¸ì¡° ì•„í‚¤í…ì²˜ëŠ” ê³ ì„±ëŠ¥, í™•ì¥ì„±, ë³´ì•ˆì„ ê°–ì¶˜ ë°ì´í„° ì„¼í„° êµ¬ì¶•ì„ ìœ„í•œ ì²­ì‚¬ì§„ì„ ì œê³µ</li>
<li>GPU ì„œë²„, ìŠ¤í† ë¦¬ì§€, ë„¤íŠ¸ì›Œí‚¹, ê´€ë¦¬ ì†”ë£¨ì…˜, AI ì†Œí”„íŠ¸ì›¨ì–´ í¬í•¨</li>
<li>AI ì†”ë£¨ì…˜ ë°°í¬ ì‹œê°„ì„ ë‹¨ì¶•í•˜ê³  ë¹„ìš© ì ˆê° íš¨ê³¼ë¥¼ ì œê³µ</li>
<li>ë‹¤ì–‘í•œ AI ë° LLM ì›Œí¬ë¡œë“œë¥¼ ì§€ì›í•˜ì—¬ í´ë¼ìš°ë“œ ì œê³µì—…ì²´ê°€ AI ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆë„ë¡ ì§€ì›</li>
<li>NVIDIA Quantum-2 InfiniBand ë° Spectrum-X Ethernet ë„¤íŠ¸ì›Œí‚¹ì„ í†µí•´ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ í†µì‹ ì„ ì œê³µ</li>
<li>NVIDIA BlueField-3 DPUsëŠ” ê³ ì„±ëŠ¥ ë¶ë‚¨ ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ ì œê³µí•˜ê³ , ë°ì´í„° ì €ì¥ ê°€ì†, íƒ„ë ¥ì  GPU ì»´í“¨íŒ… ë° ì œë¡œ íŠ¸ëŸ¬ìŠ¤íŠ¸ ë³´ì•ˆì„ ê°€ëŠ¥í•˜ê²Œ í•¨</li>
<li>NVIDIA AI Enterprise ì†Œí”„íŠ¸ì›¨ì–´ëŠ” í´ë¼ìš°ë“œ ì œê³µì—…ì²´ê°€ ì„œë²„ë¥¼ í”„ë¡œë¹„ì €ë‹í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì§€ì›</li>
<li>NVIDIA NeMo í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ í´ë¼ìš°ë“œ ì œê³µì—…ì²´ê°€ ìƒì„± AI ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë¯¸ì„¸ ì¡°ì •í•  ìˆ˜ ìˆë„ë¡ í•¨</li>
<li>NVIDIA RivaëŠ” ìŒì„± ì„œë¹„ìŠ¤ë¥¼ ì œê³µ</li>
<li>NVIDIA RAPIDSëŠ” Spark ì›Œí¬ë¡œë“œë¥¼ ê°€ì†í™”</li>
</ul>
<h2 id="10ì–µ-ê°œì˜-í˜ë¥´ì†Œë‚˜ë¥¼-í™œìš©í•œ-ë°ì´í„°-ìƒì„±-ë°©ë²•-ì œì•ˆ"><a href="#10ì–µ-ê°œì˜-í˜ë¥´ì†Œë‚˜ë¥¼-í™œìš©í•œ-ë°ì´í„°-ìƒì„±-ë°©ë²•-ì œì•ˆ" class="headerlink" title="10ì–µ ê°œì˜ í˜ë¥´ì†Œë‚˜ë¥¼ í™œìš©í•œ ë°ì´í„° ìƒì„± ë°©ë²• ì œì•ˆ"></a>10ì–µ ê°œì˜ í˜ë¥´ì†Œë‚˜ë¥¼ í™œìš©í•œ ë°ì´í„° ìƒì„± ë°©ë²• ì œì•ˆ</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.20094">ë§í¬</a>, 2024ë…„ 6ì›” 28ì¼,<br>Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, Dong Yu</p>
<ul>
<li>Persona Hubë¼ëŠ” 10ì–µ ê°œì˜ í˜ë¥´ì†Œë‚˜ë¥¼ ìë™ìœ¼ë¡œ ì›¹ ë°ì´í„°ì—ì„œ ìˆ˜ì§‘í•˜ì—¬ ë°ì´í„° ìƒì„±</li>
<li>ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê³ í’ˆì§ˆì˜ ìˆ˜í•™ ë° ë…¼ë¦¬ì  ì¶”ë¡  ë¬¸ì œ, ì§€ì‹ í’ë¶€í•œ í…ìŠ¤íŠ¸ ë“±ì„ ìƒì„±</li>
<li>MATH í‰ê°€ì—ì„œ ë†’ì€ ì„±ê³¼ë¥¼ ë³´ì´ë©° GPT-4 ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±</li>
<li>ë°ì´í„° ìƒì„±ì˜ ë‹¤ì–‘ì„±ê³¼ í™•ì¥ì„±ì„ ê·¹ëŒ€í™”í•˜ì—¬ LLM ì—°êµ¬ ë° ê°œë°œì— ê¸°ì—¬</li>
<li>ê¸°ì¡´ì˜ ì¸ìŠ¤í„´ìŠ¤ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì´ë‚˜ í•µì‹¬ í¬ì¸íŠ¸ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ë³´ë‹¤ ì»¤ë²„ë¦¬ì§€, í’ˆì§ˆ ë° ê´€ì ì„ í™•ì¥í•˜ì—¬ ë°ì´í„° ìƒì„± ê³¼ì •ì˜ ê²¬ê³ ì„±ì„ ê°•í™”</li>
<li>ë‹¤ì–‘í•œ ìš©ë„ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ MATH, ë…¼ë¦¬ì  ì¶”ë¡  ë¬¸ì œ, ì‚¬ìš©ì ì§€ì‹œë¬¸, ê²Œì„ NPC, ë„êµ¬ ê°œë°œ ë“±ì— í™œìš©í•  ìˆ˜ ìˆìŒ</li>
</ul>
<h2 id="Figma-AI-ê¸°ëŠ¥ì„-ì¤‘ì‹¬ìœ¼ë¡œ-í•œ-ë‹¤ì–‘í•œ-ì—…ë°ì´íŠ¸-ë°œí‘œ"><a href="#Figma-AI-ê¸°ëŠ¥ì„-ì¤‘ì‹¬ìœ¼ë¡œ-í•œ-ë‹¤ì–‘í•œ-ì—…ë°ì´íŠ¸-ë°œí‘œ" class="headerlink" title="Figma, AI ê¸°ëŠ¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ë‹¤ì–‘í•œ ì—…ë°ì´íŠ¸ ë°œí‘œ"></a>Figma, AI ê¸°ëŠ¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ë‹¤ì–‘í•œ ì—…ë°ì´íŠ¸ ë°œí‘œ</h2><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=n5gJgkO2Dg0&ab_channel=Figma">ë§í¬</a>, 2024ë…„ 6ì›” 30ì¼,<br>Figma</p>
<ul>
<li>â€˜Make Designâ€™ ê¸°ëŠ¥ì„ í†µí•´ í…ìŠ¤íŠ¸ ì„¤ëª…ìœ¼ë¡œ ë””ìì¸ì„ ìƒì„±í•  ìˆ˜ ìˆìŒ</li>
<li>â€˜Search for Similarâ€™ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬í•œ ìš”ì†Œë¥¼ ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆìŒ</li>
<li>ì´ë¯¸ì§€ ë°°ê²½ ì œê±°, ë‹¤êµ­ì–´ ë²ˆì—­, ë ˆì´ì–´ ëª… ìë™ ì •ë¦¬, í”„ë¡œí† íƒ€ì… ìë™ ìƒì„± ë“± ë‹¤ì–‘í•œ AI ê¸°ëŠ¥ í¬í•¨</li>
<li>ë””ìì¸ ê³¼ì •ì˜ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ê³  ì‚¬ìš©ì ê²½í—˜ì„ í¬ê²Œ ê°œì„ </li>
<li>AI ìë™ ìƒì„± ê¸°ëŠ¥ì„ í†µí•´ ì‘ì—… íë¦„ì„ ìœ ì§€í•  ìˆ˜ ìˆë„ë¡ ë„ì›€</li>
<li>ì´ë²ˆ ì—…ë°ì´íŠ¸ëŠ” AIë¥¼ í†µí•´ ë””ìì¸ ì‘ì—…ì„ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ë„ì›€</li>
<li>ì‚¬ìš©ì ê²½í—˜ì„ ê°œì„ í•˜ì—¬ ë””ìì¸ê³¼ ê°œë°œ ê°„ì˜ í˜‘ì—…ì„ ë”ìš± ì›í™œí•˜ê²Œ ë§Œë“¦</li>
</ul>
<h2 id="Groq-Whisper-Large-V3-ì„±ëŠ¥-ëŒ€í­-í–¥ìƒ"><a href="#Groq-Whisper-Large-V3-ì„±ëŠ¥-ëŒ€í­-í–¥ìƒ" class="headerlink" title="Groq, Whisper Large V3 ì„±ëŠ¥ ëŒ€í­ í–¥ìƒ"></a>Groq, Whisper Large V3 ì„±ëŠ¥ ëŒ€í­ í–¥ìƒ</h2><p><a target="_blank" rel="noopener" href="https://wow.groq.com/groq-runs-whisper-large-v3-at-a-164x-speed-factor-according-to-new-artificial-analysis-benchmark/">ë§í¬</a>, 2024ë…„ 6ì›” 28ì¼,<br>Groq</p>
<ul>
<li>Whisper Large V3ë¥¼ GroqCloudâ„¢ë¥¼ í†µí•´ ê°œë°œì ì»¤ë®¤ë‹ˆí‹°ì— ì œê³µ</li>
<li>10ë¶„ ê¸¸ì´ì˜ ì˜¤ë””ì˜¤ íŒŒì¼ì„ 3.7ì´ˆ ë§Œì— ì „ì‚¬í•˜ëŠ” 164ë°° ì†ë„ ë‹¬ì„±</li>
<li>Word Error Rate (WER)ë¥¼ 10.3%ë¡œ ìµœì†Œí™”í•˜ì—¬ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±</li>
<li>AI ìŒì„± ê²½í—˜ì„ ìœ„í•œ ì €ì§€ì—° ì „ì‚¬ ì„±ëŠ¥ ì œê³µ</li>
<li>Whisper Large V3ëŠ” AI ìŒì„± ì¸ì‹ ë° ìŒì„± ë²ˆì—­ì„ ìœ„í•œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸</li>
<li>Groqì˜ LPUâ„¢ ì¶”ë¡  ì—”ì§„ì„ í†µí•´ ì €ì§€ì—° AI ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•¨</li>
<li>GroqCloudâ„¢ì—ì„œ ì œê³µë˜ì–´ ê°œë°œìë“¤ì´ Whisperë¥¼ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ</li>
<li>í”„ë¡œì íŠ¸ Media QAì—ì„œ Whisper ì„±ëŠ¥ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ</li>
</ul>
<h2 id="SKê·¸ë£¹-AIì™€-ë°˜ë„ì²´-ë¶„ì•¼ì—-2026ë…„ê¹Œì§€-80ì¡°-ì›-íˆ¬ì"><a href="#SKê·¸ë£¹-AIì™€-ë°˜ë„ì²´-ë¶„ì•¼ì—-2026ë…„ê¹Œì§€-80ì¡°-ì›-íˆ¬ì" class="headerlink" title="SKê·¸ë£¹, AIì™€ ë°˜ë„ì²´ ë¶„ì•¼ì— 2026ë…„ê¹Œì§€ 80ì¡° ì› íˆ¬ì"></a>SKê·¸ë£¹, AIì™€ ë°˜ë„ì²´ ë¶„ì•¼ì— 2026ë…„ê¹Œì§€ 80ì¡° ì› íˆ¬ì</h2><p><a target="_blank" rel="noopener" href="https://n.news.naver.com/article/032/0003305572?cds=news_my">ë§í¬</a>, 2024ë…„ 6ì›” 30ì¼,<br>SKê·¸ë£¹</p>
<ul>
<li>SKê·¸ë£¹ì€ AIì™€ ë°˜ë„ì²´ë¥¼ ë¹„ë¡¯í•œ ë¯¸ë˜ ì„±ì¥ ë¶„ì•¼ì— 80ì¡° ì›ì„ íˆ¬ìí•  ê³„íš</li>
<li>ê¸‰ë³€í•˜ëŠ” ì‹œì¥ì— ëŒ€ì‘í•˜ê³  ì„ íƒê³¼ ì§‘ì¤‘ì„ í†µí•´ ì§ˆì  ì„±ì¥ ì¶”êµ¬</li>
<li>2026ë…„ê¹Œì§€ ìˆ˜ìµì„± ê°œì„ , ì‚¬ì—…êµ¬ì¡° ìµœì í™”, ì‹œë„ˆì§€ ì œê³  ë“±ì„ í†µí•´ 80ì¡° ì›ì˜ ì¬ì› í™•ë³´</li>
<li>SKí•˜ì´ë‹‰ìŠ¤ëŠ” 5ë…„ê°„ 103ì¡° ì›ì„ íˆ¬ìí•˜ì—¬ ë°˜ë„ì²´ ì‚¬ì—… ê²½ìŸë ¥ ê°•í™”</li>
<li>7ì›” 1ì¼ë¶€ë¡œ ìˆ˜í™ìŠ¤ì¶”êµ¬í˜‘ì˜íšŒì— ë°˜ë„ì²´ìœ„ì›íšŒë¥¼ ì‹ ì„¤</li>
<li>CEOë“¤ì€ ì „ì²´ ê³„ì—´ì‚¬ ìˆ˜ë¥¼ â€˜ê´€ë¦¬ ê°€ëŠ¥í•œ ë²”ìœ„â€™ë¡œ ì¡°ì •í•  í•„ìš”ì„±ì— ê³µê°í•˜ê³ , ì´ë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ ì¶”ì§„</li>
<li>í˜„ì¬ SKì˜ ê³„ì—´ì‚¬ëŠ” ì´ 219ê³³ìœ¼ë¡œ, ì´ë¥¼ ìµœì í™”í•˜ì—¬ ê´€ë¦¬ ë²”ìœ„ë¥¼ ì¡°ì •í•  ê³„íš</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹"><a href="#todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹" class="headerlink" title="(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,"></a>(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,</h1><h2 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAIì—ì„œëŠ” ~~~ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.)</p>
<h2 id="Title"><a href="#Title" class="headerlink" title="Title,"></a>Title,</h2><h3 id="í•œê¸€ì œëª©"><a href="#í•œê¸€ì œëª©" class="headerlink" title="í•œê¸€ì œëª©"></a>í•œê¸€ì œëª©</h3><p><a href="link">ë§í¬</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
</ul>
<h2 id="Title-1"><a href="#Title-1" class="headerlink" title="Title,"></a>Title,</h2><h3 id="í•œê¸€ì œëª©-1"><a href="#í•œê¸€ì œëª©-1" class="headerlink" title="í•œê¸€ì œëª©"></a>í•œê¸€ì œëª©</h3><p><a href="link">ë§í¬</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://cdn.openai.com/llm-critics-help-catch-llm-bugs-paper.pdf</span><br><span class="line">OpenAI</span><br><span class="line">Jun 28, 2024</span><br><span class="line">Abstract:</span><br><span class="line">Reinforcement learning from human feedback (RLHF) is fundamentally limited</span><br><span class="line">by the capacity of humans to correctly evaluate model output. To improve human</span><br><span class="line">evaluation ability and overcome that limitation this work trains â€œcriticâ€ models</span><br><span class="line">that help humans to more accurately evaluate model-written code. These critics</span><br><span class="line">are themselves LLMs trained with RLHF to write natural language feedback</span><br><span class="line">highlighting problems in code from real-world assistant tasks. On code containing</span><br><span class="line">naturally occurring LLM errors model-written critiques are preferred over human</span><br><span class="line">critiques in 63% of cases, and human evaluation finds that models catch more bugs</span><br><span class="line">than human contractors paid for code review. We further confirm that our fine-tuned</span><br><span class="line">LLM critics can successfully identify hundreds of errors in ChatGPT training data</span><br><span class="line">rated as â€œflawlessâ€, even though the majority of those tasks are non-code tasks</span><br><span class="line">and thus out-of-distribution for the critic model. Critics can have limitations of</span><br><span class="line">their own, including hallucinated bugs that could mislead humans into making</span><br><span class="line">mistakes they might have otherwise avoided, but human-machine teams of critics</span><br><span class="line">and contractors catch similar numbers of bugs to LLM critics while hallucinating</span><br><span class="line">less than LLMs alone.</span><br><span class="line"></span><br><span class="line">Summary</span><br><span class="line">Blog Post: Improving AI Reliability with Critic Models for Better Code Evaluation</span><br><span class="line">In the swiftly changing world of artificial intelligence (AI), guaranteeing the reliability of AI-generated outputs is increasingly crucial. This is particularly true for AI models that generate or evaluate code, which can occasionally contain subtle bugs or errors not immediately noticeable. These errors are risky in enterprise environments where accuracy is essential. Introducing critic models, which assess and critique model outputs, offers a promising solution to enhance AI reliability, especially in code evaluation.</span><br><span class="line">Understanding Critic Models</span><br><span class="line">Critic models, such as CriticGPT, are a new development designed to improve the evaluation of AI-generated outputs, including code. Unlike traditional methods that rely on human feedback, critic models use a sophisticated training process to identify errors that humans might miss. However, they also face challenges, such as mistakenly identifying errors that donâ€™t exist.</span><br><span class="line">notion image</span><br><span class="line"></span><br><span class="line">How Critic Models Are Trained and Evaluated</span><br><span class="line">The training and evaluation of critic models involve several key steps and criteria:</span><br><span class="line">Comprehensiveness: They must cover all significant issues in the code.</span><br><span class="line">Critique-Bug Inclusion (CBI): They should pinpoint specific, known bugs.</span><br><span class="line">Minimizing false positives: Avoiding the identification of non-existent issues.</span><br><span class="line">Helpfulness and style: The critiques should be constructive and clear.</span><br><span class="line">These models are assessed through blind tests and compared using Elo scores, offering a detailed analysis of their performance.</span><br><span class="line">Training Process</span><br><span class="line">Training critic models involves generating critiques for code, which are then rated by human evaluators. These ratings help train a reward model that further refines the critic models&#x27; accuracy.</span><br><span class="line">Breakthrough Results with Critic Models</span><br><span class="line">Critic models have shown promising results. For instance, CriticGPT has surpassed human evaluators in identifying bugs, indicating a significant advancement in AI-assisted code evaluation. Combining these models with human evaluators leads to even better performance. Additionally, techniques like Force Sampling Beam Search have improved the balance between detecting real and imagined issues, enhancing evaluation reliability.</span><br><span class="line"></span><br><span class="line">notion image</span><br><span class="line">notion image</span><br><span class="line">Expanding the Use of Critic Models</span><br><span class="line">The application of critic models in code evaluation is just the beginning. These models are part of broader research into making AI more self-corrective and reliable across various coding tasks. Understanding their role helps us see their potential to revolutionize the field.</span><br><span class="line">Future Directions and Challenges</span><br><span class="line">Critic models are paving the way for AI that is not only more reliable but also capable of self-assessment. However, challenges such as potential biases and distinguishing between different types of errors need to be addressed.</span><br><span class="line">Conclusion</span><br><span class="line">Critic models offer a significant improvement in ensuring the reliability of AI-generated code. By critiquing and evaluating code more accurately, they enhance human evaluators&#x27; ability to spot and fix errors. As we refine these models, we edge closer to AI systems that are not just effective but also inherently safe. For AI engineers in enterprise settings, this represents an exciting opportunity to lead in the application of critic models, contributing to the development of AI that is both powerful and dependable. This journey marks a step towards a future where AI and humans collaborate more seamlessly, unlocking new possibilities.</span><br><span class="line"></span><br><span class="line">Is OpenAI following Anthropic? LLM Critics Help Catch LLM Bugs is the latest paper from OpenAI describing how LLM Critiques and AI Feedback help to improve RLHF and data quality and outscale human experts. ğŸ‘€</span><br><span class="line">CriticGPT is an autoregressive language model trained with RLHF (InstructGPT and ChatGPT) to accept a question-answer pair as input and output a structured critique that highlights potential problems in the answer. ğŸ’¡ - Pretty similar to Anthropics Constitutional AI method.</span><br><span class="line">RLHF pipeline to train CritiqueGPT, similar to ChatGPT:</span><br><span class="line">1ï¸âƒ£ Step 1: Generate several critiques for each (question, answer) pair in the dataset by AI &amp; Contractors.</span><br><span class="line">2ï¸âƒ£ Step 2: Contractors rated the attributes of the sampled critiques, including overall quality.</span><br><span class="line">3ï¸âƒ£ Step 3: Train a reward model to predict the human overall quality rankings.</span><br><span class="line">4ï¸âƒ£ Step 4: Train CritiqueGPT using PPO and Reward Model</span><br><span class="line">Insights</span><br><span class="line">ğŸ› Used â€œTamperingâ€ Humans added bugs in code and wrote a critique about it</span><br><span class="line">ğŸ” CriticGPT identified hundreds of errors in ChatGPT data</span><br><span class="line">ğŸ“Š Used Preference scores (B&gt;A&gt;D&gt;C) on a 1-7 ordinal scale for RLHF</span><br><span class="line">â±ï¸ Humans needed 50 minutes per example to write critiques.</span><br><span class="line">ğŸ¤– The reward model was trained on a mix of ChatGPT and CriticGPT</span><br><span class="line">ğŸš€ Introduce Force Sampling Beam Search (FSBS) which uses Reward Model to improve outputs</span><br><span class="line">ğŸ–¥ï¸ CriticGPT was fine-tuned with less computing than ChatGPT.</span><br><span class="line">ğŸ“ Used Prompts from Reward Modelling dataset for PPO</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://blogs.nvidia.com/blog/ai-cloud-providers-reference-architecture/?ncid=so-link-519834</span><br><span class="line">NVIDIA Unveils Reference Architecture for AI Cloud Providers</span><br><span class="line">June 26, 2024 by Marc Hamilton</span><br><span class="line"> Share</span><br><span class="line"></span><br><span class="line">NVIDIA has announced a new reference architecture for cloud providers that want to offer generative AI services to their customers.</span><br><span class="line"></span><br><span class="line">The NVIDIA Cloud Partner reference architecture is a blueprint for building high-performance, scalable and secure data centers that can handle generative AI and large language models (LLMs).</span><br><span class="line"></span><br><span class="line">The reference architecture enables NVIDIA Cloud Partners within the NVIDIA Partner Network to reduce the time and cost of deploying AI solutions, while ensuring compatibility and interoperability among various hardware and software components.</span><br><span class="line"></span><br><span class="line">The architecture will also help cloud providers meet the growing demand for AI services from organizations â€” of all sizes and industries â€” that want to leverage the power of generative AI and LLMs without investing in their own infrastructure.</span><br><span class="line"></span><br><span class="line">Generative AI and LLMs are transforming the way organizations solve complex problems and create new value. These technologies use deep neural networks to generate realistic and novel outputs, such as text, images, audio and video, based on a given input or context. Generative AI and LLMs can be used for a variety of applications, such as copilots, chatbots and other content creation.</span><br><span class="line"></span><br><span class="line">However, generative AI and LLMs also pose significant challenges for cloud providers, which need to provide the infrastructure and software to support these workloads. The technologies require massive amounts of computing power, storage and network bandwidth, as well as specialized hardware and software to optimize performance and efficiency.</span><br><span class="line"></span><br><span class="line">For example, LLM training involves many GPU servers working together, communicating constantly among themselves and with storage systems. This translates to east-west and north-south traffic in data centers, which requires high-performance networks for fast and efficient communication.</span><br><span class="line"></span><br><span class="line">Similarly, generative AI inference with larger models needs multiple GPUs to work together to process a single query.</span><br><span class="line"></span><br><span class="line">Moreover, cloud providers need to ensure that their infrastructure is secure, reliable and scalable, as they serve multiple customers with different needs and expectations. Cloud providers also need to comply with industry standards and best practices, as well as provide support and maintenance for their services.</span><br><span class="line"></span><br><span class="line">The NVIDIA Cloud Partner reference architecture addresses these challenges by providing a comprehensive, full-stack hardware and software solution for cloud providers to offer AI services and workflows for different use cases. Based on the years of experience NVIDIA has in designing and building large-scale deployments both internally and for customers, the reference architecture includes:</span><br><span class="line"></span><br><span class="line">GPU servers from NVIDIA and its manufacturing partners, featuring NVIDIAâ€™s latest GPU architectures, such as Hopper and Blackwell, which deliver unparalleled compute power and performance for AI workloads.</span><br><span class="line">Storage offerings from certified partners, which provide high-performance storage optimized for AI and LLM workloads. The offerings also include those tested and validated for NVIDIA DGX SuperPOD and NVIDIA DGX Cloud. They are proven to be reliable, efficient and scalable.</span><br><span class="line">NVIDIA Quantum-2 InfiniBand and Spectrum-X Ethernet networking, which provide a high-performance east-west network for fast and efficient communication between GPU servers.</span><br><span class="line">NVIDIA BlueField-3 DPUs, which deliver high-performance north-south network connectivity and enable data storage acceleration, elastic GPU computing and zero-trust security.</span><br><span class="line">In/out-of-band management solutions from NVIDIA and management partners, which provide tools and services for provisioning, monitoring and managing AI data center infrastructure.</span><br><span class="line">NVIDIA AI Enterprise software, including:</span><br><span class="line">NVIDIA Base Command Manager Essentials, which helps cloud providers provision and manage their servers.</span><br><span class="line">NVIDIA NeMo framework, which helps cloud providers train and fine-tune generative AI models.</span><br><span class="line">NVIDIA NIM, a set of easy-to-use microservices designed to accelerate deployment of generative AI across enterprises.</span><br><span class="line">NVIDIA Riva, for speech services.</span><br><span class="line">NVIDIA RAPIDS accelerator for Spark, to accelerate Spark workloads.</span><br><span class="line">The NVIDIA Cloud Partner reference architecture offers the following key benefits to cloud providers:</span><br><span class="line"></span><br><span class="line">Build, Train and Go: NVIDIA infrastructure specialists use the architecture to physically install and provision the cluster for faster rollouts for cloud providers.</span><br><span class="line">Speed: By incorporating the expertise and best practices of NVIDIA and partner vendors, the architecture can help cloud providers accelerate the deployment of AI solutions and gain a competitive edge in the market.</span><br><span class="line">High Performance: The architecture is tuned and benchmarked with industry-standard benchmarks, ensuring optimal performance for AI workloads.</span><br><span class="line">Scalability: The architecture is designed for cloud-native environments, facilitating the development of scalable AI systems that offer flexibility and can seamlessly expand to meet increasing demand of end users.</span><br><span class="line">Interoperability: The architecture ensures compatibility among various components of the architecture, making integration and communication between components seamless.</span><br><span class="line">Maintenance and Support: NVIDIA Cloud Partners have access to NVIDIA subject-matter experts, who can help address any unexpected challenges that may arise during and after deployment.</span><br><span class="line">The NVIDIA Cloud Partner reference architecture provides a proven blueprint for cloud providers to stand up and manage high-performance scalable infrastructure for AI data centers.</span><br><span class="line"></span><br><span class="line">See notice regarding software product information.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2406.20094</span><br><span class="line">[Submitted on 28 Jun 2024]</span><br><span class="line">Scaling Synthetic Data Creation with 1,000,000,000 Personas</span><br><span class="line">Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, Dong Yu</span><br><span class="line">We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce Persona Hub -- a collection of 1 billion diverse personas automatically curated from web data. These 1 billion personas (~13% of the world&#x27;s total population), acting as distributed carriers of world knowledge, can tap into almost every perspective encapsulated within the LLM, thereby facilitating the creation of diverse synthetic data at scale for various scenarios. By showcasing Persona Hub&#x27;s use cases in synthesizing high-quality mathematical and logical reasoning problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs and tools (functions) at scale, we demonstrate persona-driven data synthesis is versatile, scalable, flexible, and easy to use, potentially driving a paradigm shift in synthetic data creation and applications in practice, which may have a profound impact on LLM research and development.</span><br><span class="line">This is one of the coolest ideas for scaling synthetic data that I&#x27;ve come across.</span><br><span class="line">Proposes 1 billion diverse personas to facilitate the creation of diverse synthetic data for different scenarios.</span><br><span class="line">It&#x27;s easy to generate synthetic data but hard to scale up its diversity which is essential for its application.</span><br><span class="line">This paper proposes a novel persona-driven data synthesis methodology to generate diverse and distinct data covering a wide range of perspectives.</span><br><span class="line">Previous works synthesize data using either instance-driven approaches (e.g., using seed corpus) or key-point-driven methods (e.g., using topic/subject). Both of these approaches lack the desired coverage, quality, and perspectives needed to robustly scale the data synthesis process.</span><br><span class="line">To measure the quality of the synthetic datasets, they performed an out-of-distribution evaluation on MATH. A fine-tuned model on their synthesized 1.07M math problems achieves 64.9% on MATH, matching the performance of gpt-4-turbo-preview at only a 7B scale.</span><br><span class="line">Their method is not only effective for MATH problems, but it can also be used to generate logical reasoning problems, instructions, game NPCs, tool development, knowledge-rich text, and many more use cases.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.youtube.com/watch?v=n5gJgkO2Dg0&amp;ab_channel=Figma</span><br><span class="line">Config 2024ì—ì„œ FigmaëŠ” í˜ì‹ ì ì¸ AI ê¸°ëŠ¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹¤ì–‘í•œ ì—…ë°ì´íŠ¸ë¥¼ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. Figma AIëŠ” ì‚¬ìš©ìê°€ í…ìŠ¤íŠ¸ ì„¤ëª…ìœ¼ë¡œ ë””ìì¸ì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” &#x27;Make Design&#x27; ê¸°ëŠ¥, ìœ ì‚¬í•œ ìš”ì†Œë¥¼ ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆëŠ” &#x27;Search for Similar&#x27; ê¸°ëŠ¥, ê·¸ë¦¬ê³  ì‘ì—… íë¦„ì„ ìœ ì§€í•˜ë„ë¡ ë•ëŠ” AI ìë™ ìƒì„± ê¸°ëŠ¥ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì´ë¯¸ì§€ ë°°ê²½ ì œê±°, ë‹¤êµ­ì–´ ë²ˆì—­, ë ˆì´ì–´ ëª… ìë™ ì •ë¦¬, í”„ë¡œí† íƒ€ì… ìë™ ìƒì„± ë“± AIë¥¼ í™œìš©í•œ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ í†µí•´ ë””ìì¸ ê³¼ì •ì„ í˜ì‹ ì ìœ¼ë¡œ ë³€í™”ì‹œí‚µë‹ˆë‹¤. Figma AIëŠ” ì‘ì—… íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ê³ , ë””ìì¸ê³¼ ê°œë°œ ê°„ì˜ í˜‘ì—…ì„ ë”ìš± ì›í™œí•˜ê²Œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤. ì´ë²ˆ ì—…ë°ì´íŠ¸ëŠ” AIë¥¼ í†µí•´ ì‚¬ìš©ì ê²½í—˜ì„ í¬ê²Œ ê°œì„ í•˜ê³ , ë””ìì¸ ì‘ì—…ì„ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://wow.groq.com/groq-runs-whisper-large-v3-at-a-164x-speed-factor-according-to-new-artificial-analysis-benchmark/</span><br><span class="line">Groq Runs Whisper Large V3 at a 164x Speed Factor According to New Artificial Analysis Benchmark</span><br><span class="line">Written by:</span><br><span class="line">Groq</span><br><span class="line">Whisper Large V3 Is Now Available to the Developer Community via GroqCloudâ„¢</span><br><span class="line">Weâ€™re excited to announce Groq is officially running Whisper Large V3 on the LPUâ„¢ Inference Engine, available to our developer community via GroqCloudâ„¢ through our Developer Playground. Whisper is a pre-trained model for automatic speech recognition and speech translation, trained on 680k hours of labeled data. Whisper and models like it are paving the way for accurate and seamless GenAI voice experiences while broadening the possibilities on developer application and use cases, both of which require low-latency AI inference.</span><br><span class="line"></span><br><span class="line">This also marks an addition to the expanding GenAI model portfolio hosted by Groq. Large Language Models (LLMs) continue to run on the Groq LPU, the addition of Whisper Large V3 is another step on our way to multi-modal.</span><br><span class="line"></span><br><span class="line">Artificial Analysis has included our Whisper performance in their latest independent speech-to-text benchmark.</span><br><span class="line"></span><br><span class="line">Dive into the results below. To see see this model in action, check out Project Media QA on GroqLabs. If you are a developer interested in Whisper running on Groq, sign up for access via GroqCloud at console.groq.com.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Artificial Analysis has independently benchmarked Whisper Large V3 on Groq as achieving a Speed Factor of 164. This means Groq can transcribe our 10-minute audio test file in just 3.7 seconds. Low latency transcription is a critical component for seamless voice experiences. AI voice experiences require low latency inference on transcription, language, and voice models to enable immediate responses that keep users engaged.</span><br><span class="line"></span><br><span class="line">- Micah Hill-Smith, Co-founder &amp; CEO, ArtificialAnalysis.ai</span><br><span class="line">Repost</span><br><span class="line">Speed Factor</span><br><span class="line"></span><br><span class="line">Measured as input audio seconds transcribed per second, Groq clocks in at a speed factor rate of 164x real-time, the fastest implementation of the base Whisper Large V3 model.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Quality</span><br><span class="line"></span><br><span class="line">Artificial Analysis defines Word Error Rate (WER) as the percentage of of words transcribed incorrectly. Groq minimized its Word Error rate to 10.3% for Whisper Large V3, matching the lowest WER from other providers on the leaderboard</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Price</span><br><span class="line"></span><br><span class="line">Artificial Analysis defines price as USD per 1000 minutes of audio, bringing the Groq price to $0.5 based on offering Whisper Large V3 at a price of $0.03 per hour transcribed.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://n.news.naver.com/article/032/0003305572?cds=news_my</span><br><span class="line">ìµœíƒœì› íšŒì¥ â€œAI ë¶„ì•¼ ì„ ì œì  ëŒ€ì‘â€â€¦SK, 2026ë…„ê¹Œì§€ 80ì¡°ì› ì§‘ì¤‘ íˆ¬ì…</span><br><span class="line">ì…ë ¥2024.06.30. ì˜¤í›„ 8:40 ê¸°ì‚¬ì›ë¬¸</span><br><span class="line">ì´ì§„ì£¼ ê¸°ì</span><br><span class="line">  32</span><br><span class="line">84</span><br><span class="line">ë³¸ë¬¸ ìš”ì•½ë´‡</span><br><span class="line">í…ìŠ¤íŠ¸ ìŒì„± ë³€í™˜ ì„œë¹„ìŠ¤ ì‚¬ìš©í•˜ê¸°</span><br><span class="line">ê¸€ì í¬ê¸° ë³€ê²½í•˜ê¸°</span><br><span class="line">SNS ë³´ë‚´ê¸°</span><br><span class="line">ì¸ì‡„í•˜ê¸°</span><br><span class="line">â€˜ê³„ì—´ì‚¬ ì¬ì¡°ì •â€™ ë‹¨ê³„ì  ì¶”ì§„</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SKê·¸ë£¹ì´ 2026ë…„ê¹Œì§€ 80ì¡°ì›ì˜ ì¬ì›ì„ í™•ë³´í•´ ì¸ê³µì§€ëŠ¥(AI)ê³¼ ë°˜ë„ì²´ë¥¼ ë¹„ë¡¯í•œ ë¯¸ë˜ ì„±ì¥ ë¶„ì•¼ì— íˆ¬ìí•œë‹¤.</span><br><span class="line"></span><br><span class="line">ê¸‰ë³€í•˜ëŠ” ì‹œì¥ì— ì„ ì œì ìœ¼ë¡œ ëŒ€ì‘í•˜ê³  â€˜ì„ íƒê³¼ ì§‘ì¤‘â€™ì„ í†µí•´ ì§ˆì  ì„±ì¥ì„ ê¾€í•œë‹¤ëŠ” ì „ëµì´ë‹¤.</span><br><span class="line"></span><br><span class="line">SKëŠ” ì§€ë‚œ 28~29ì¼ ê²½ê¸° ì´ì²œ SKMSì—°êµ¬ì†Œì—ì„œ ìµœíƒœì› íšŒì¥(ì‚¬ì§„), ìµœì¬ì› ìˆ˜ì„ë¶€íšŒì¥, ìµœì°½ì› ìˆ˜í™ìŠ¤ì¶”êµ¬í˜‘ì˜íšŒ ì˜ì¥, ì£¼ìš” ê³„ì—´ì‚¬ ìµœê³ ê²½ì˜ì(CEO) 20ì—¬ëª… ë“±ì´ ì°¸ì„í•œ ê°€ìš´ë° ê²½ì˜ì „ëµíšŒì˜ë¥¼ ì—´ê³  ì´ ê°™ì€ ì „ëµ ë°©í–¥ì— ëœ»ì„ ëª¨ì•˜ë‹¤ê³  30ì¼ ë°í˜”ë‹¤.</span><br><span class="line"></span><br><span class="line">ì´ë²ˆ íšŒì˜ì—ëŠ” ìµœ íšŒì¥ì˜ ì¥ë…€ì¸ ìµœìœ¤ì • SKë°”ì´ì˜¤íŒœ ì‚¬ì—…ê°œë°œë³¸ë¶€ì¥(ë¶€ì‚¬ì¥)ì´ ì²˜ìŒ ì°¸ì„í•œ ê²ƒìœ¼ë¡œ ì•Œë ¤ì¡Œë‹¤. ë¯¸êµ­ ì¶œì¥ ì¤‘ì¸ ìµœ íšŒì¥ì€ í™”ìƒìœ¼ë¡œ íšŒì˜ì— ì°¸ì„í•´ â€œâ€˜ìƒˆë¡œìš´ íŠ¸ëœì§€ì…˜(ì „í™˜) ì‹œëŒ€â€™ë¥¼ ë§ì•„ ë¯¸ë˜ ì¤€ë¹„ ë“±ì„ ìœ„í•œ ì„ ì œì ì´ê³  ê·¼ë³¸ì ì¸ ë³€í™”ê°€ í•„ìš”í•˜ë‹¤â€ê³  ê°•ì¡°í–ˆë‹¤.</span><br><span class="line"></span><br><span class="line">ìµœ íšŒì¥ì€ â€œì§€ê¸ˆ ë¯¸êµ­ì—ì„œëŠ” AI ë§ê³ ëŠ” í•  ì–˜ê¸°ê°€ ì—†ë‹¤ê³  í•  ì •ë„ë¡œ AI ê´€ë ¨ ë³€í™”ì˜ ë°”ëŒì´ ê±°ì„¸ë‹¤â€ë©° â€œê·¸ë£¹ ë³´ìœ  ì—­ëŸ‰ì„ í™œìš©í•´ AI ì„œë¹„ìŠ¤ë¶€í„° ì¸í”„ë¼ê¹Œì§€ â€˜AI ë°¸ë¥˜ì²´ì¸(ê°€ì¹˜ì‚¬ìŠ¬) ë¦¬ë”ì‹­â€™ì„ ê°•í™”í•´ì•¼ í•œë‹¤â€ê³  ì£¼ë¬¸í–ˆë‹¤.</span><br><span class="line"></span><br><span class="line">ìµœ íšŒì¥ì€ SKê°€ ê°•ì ì„ ê°€ì§„ ì—ë„ˆì§€ ì†”ë£¨ì…˜ ë¶„ì•¼ë„ ê¸€ë¡œë²Œ ì‹œì¥ì—ì„œ AI ëª»ì§€ì•Šì€ ì„±ì¥ ê¸°íšŒë¥¼ í™•ë³´í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ì „ë§í–ˆë‹¤.</span><br><span class="line"></span><br><span class="line">SK ê²½ì˜ì§„ì€ ì´ë²ˆ íšŒì˜ì—ì„œ ìˆ˜ìµì„± ê°œì„ ê³¼ ì‚¬ì—…êµ¬ì¡° ìµœì í™”, ì‹œë„ˆì§€ ì œê³  ë“±ìœ¼ë¡œ 2026ë…„ê¹Œì§€ 80ì¡°ì›ì˜ ì¬ì›ì„ í™•ë³´í•˜ê³ , ì´ë¥¼ AIì™€ ë°˜ë„ì²´ ë“± ë¯¸ë˜ ì„±ì¥ ë¶„ì•¼ íˆ¬ìì™€ ì£¼ì£¼ í™˜ì› ë“±ì— í™œìš©í•˜ê¸°ë¡œ ì˜ê²¬ì„ ëª¨ì•˜ë‹¤.</span><br><span class="line"></span><br><span class="line">ë˜ ìš´ì˜ ê°œì„ ì„ í†µí•´ 3ë…„ ë‚´ 30ì¡°ì›ì˜ ì‰ì—¬í˜„ê¸ˆíë¦„(FCF)ì„ ë§Œë“¤ì–´ ë¶€ì±„ë¹„ìœ¨ì„ 100% ì´í•˜ë¡œ ê´€ë¦¬í•œë‹¤ëŠ” ëª©í‘œë„ ì„¸ì› ë‹¤. SKëŠ” ì§€ë‚œí•´ 10ì¡°ì› ì ìë¥¼ ê¸°ë¡í•œ ì„¸ì „ì´ìµì´ ì˜¬í•´ëŠ” í‘ìë¡œ ì „í™˜í•´ 22ì¡°ì› ì•ˆíŒì— ì´ë¥¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒí–ˆë‹¤.</span><br><span class="line"></span><br><span class="line">SKí•˜ì´ë‹‰ìŠ¤ëŠ” 2028ë…„ê¹Œì§€ í–¥í›„ 5ë…„ê°„ ì´ 103ì¡°ì›ì„ íˆ¬ìí•´ ë°˜ë„ì²´ ì‚¬ì—… ê²½ìŸë ¥ì„ ê°•í™”í•˜ê¸°ë¡œ í–ˆë‹¤. ì´ ì¤‘ ì•½ 80%ì— í•´ë‹¹í•˜ëŠ” 82ì¡°ì›ì€ HBM ë“± AI ê´€ë ¨ ì‚¬ì—…ì— íˆ¬ìí•œë‹¤.</span><br><span class="line"></span><br><span class="line">7ì›”1ì¼ë¶€ë¡œ ìˆ˜í™ìŠ¤ì¶”êµ¬í˜‘ì˜íšŒì— â€˜ë°˜ë„ì²´ìœ„ì›íšŒâ€™ë„ ì‹ ì„¤í•œë‹¤. ìœ„ì›ì¥ì€ ê³½ë…¸ì • SKí•˜ì´ë‹‰ìŠ¤ ì‚¬ì¥ì´ ë§¡ëŠ”ë‹¤.</span><br><span class="line"></span><br><span class="line">CEOë“¤ì€ ì „ì²´ ê³„ì—´ì‚¬ ìˆ˜ë¥¼ â€˜ê´€ë¦¬ ê°€ëŠ¥í•œ ë²”ìœ„â€™ë¡œ ì¡°ì •í•  í•„ìš”ì„±ì´ ìˆë‹¤ëŠ” ë° ê³µê°í•˜ê³ , ê° ì‚¬ë³„ ë‚´ë¶€ ì ˆì°¨ë¥¼ ê±°ì³ ì´ë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ ì¶”ì§„í•˜ê¸°ë¡œ í–ˆë‹¤. í˜„ì¬ SKì˜ ê³„ì—´ì‚¬ëŠ” ì´ 219ê³³ìœ¼ë¡œ, ì‚¼ì„±(63ê³³) ë“± ì£¼ìš” ê·¸ë£¹ê³¼ ë¹„êµí•´ë„ ë§ë‹¤ëŠ” ì§€ì ì´ ë‚˜ì˜¨ë‹¤.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-07-02</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/07/02/2024-7-2-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 7ì›” 2ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/07/02/Config-2024-Figma-product-launch-keynote/" title="Config 2024: Figma product launch keynote">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/06/28/2024-6-28-AI-NEWS/" title="2024ë…„ 6ì›” 28ì¼ AI ì†Œì‹">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>