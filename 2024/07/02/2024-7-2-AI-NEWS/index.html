<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 7월 2일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="SummaryOpenAI에서는 Critic 모델을 도입하여 AI 코드 평가 신뢰성을 높였습니다. Critic 모델은 인간보다 코드의 오류를 더 잘 잡아내며, 인간 평가자와 협력하여 성과를 극대화합니다. NVIDIA는 AI 클라우드 제공업체를 위한 새로운 참조 아키텍처를"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 7월 2일 AI 소식</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>OpenAI에서는 Critic 모델을 도입하여 AI 코드 평가 신뢰성을 높였습니다. Critic 모델은 인간보다 코드의 오류를 더 잘 잡아내며, 인간 평가자와 협력하여 성과를 극대화합니다. NVIDIA는 AI 클라우드 제공업체를 위한 새로운 참조 아키텍처를 발표하여 AI 솔루션 배포 시간을 단축하고 비용을 절감하는 동시에 성능을 최적화합니다. 새로운 연구에서는 10억 개의 페르소나를 활용한 데이터 생성 방법을 제안하여 데이터의 다양성과 확장성을 극대화합니다. Figma는 AI 기능을 중심으로 한 다양한 디자인 도구를 업데이트하였으며, Groq는 Whisper Large V3의 성능을 대폭 향상시켰습니다. SK그룹은 AI와 반도체 분야에 2026년까지 80조 원을 투자할 계획을 발표하였습니다.</p>
<h2 id="OpenAI-Critic-모델로-AI-코드-평가-신뢰성-향상"><a href="#OpenAI-Critic-모델로-AI-코드-평가-신뢰성-향상" class="headerlink" title="OpenAI, Critic 모델로 AI 코드 평가 신뢰성 향상"></a>OpenAI, Critic 모델로 AI 코드 평가 신뢰성 향상</h2><p><a target="_blank" rel="noopener" href="https://cdn.openai.com/llm-critics-help-catch-llm-bugs-paper.pdf">링크</a>, 2024년 6월 28일,<br>OpenAI</p>
<ul>
<li>Critic 모델은 인간보다 코드의 오류를 더 잘 잡아내며, 코드 평가의 정확성을 높임</li>
<li>CriticGPT는 RLHF를 사용하여 자연어 피드백을 생성하고 코드의 문제를 강조</li>
<li>Critic 모델은 때때로 허구의 오류를 생성하여 인간을 혼란스럽게 할 수 있음</li>
<li>Critic 모델의 도입으로 AI와 인간 평가자의 팀이 유사한 수의 오류를 잡아내며, 인간 평가자의 오류 수를 줄임</li>
<li>Critic 모델은 ChatGPT 훈련 데이터의 수백 가지 오류를 성공적으로 식별</li>
<li>Critic 모델은 코드 이외의 작업에서도 효과적임</li>
<li>Force Sampling Beam Search 기법을 도입하여 실제 오류와 허구의 오류를 균형 있게 감지</li>
</ul>
<h2 id="NVIDIA-AI-클라우드-제공업체를-위한-새로운-레퍼런스-아키텍처-발표"><a href="#NVIDIA-AI-클라우드-제공업체를-위한-새로운-레퍼런스-아키텍처-발표" class="headerlink" title="NVIDIA, AI 클라우드 제공업체를 위한 새로운 레퍼런스 아키텍처 발표"></a>NVIDIA, AI 클라우드 제공업체를 위한 새로운 레퍼런스 아키텍처 발표</h2><p><a target="_blank" rel="noopener" href="https://blogs.nvidia.com/blog/ai-cloud-providers-reference-architecture/?ncid=so-link-519834">링크</a>, 2024년 6월 26일,<br>NVIDIA</p>
<ul>
<li>NVIDIA 클라우드 파트너 참조 아키텍처는 고성능, 확장성, 보안을 갖춘 데이터 센터 구축을 위한 청사진을 제공</li>
<li>GPU 서버, 스토리지, 네트워킹, 관리 솔루션, AI 소프트웨어 포함</li>
<li>AI 솔루션 배포 시간을 단축하고 비용 절감 효과를 제공</li>
<li>다양한 AI 및 LLM 워크로드를 지원하여 클라우드 제공업체가 AI 서비스를 제공할 수 있도록 지원</li>
<li>NVIDIA Quantum-2 InfiniBand 및 Spectrum-X Ethernet 네트워킹을 통해 빠르고 효율적인 통신을 제공</li>
<li>NVIDIA BlueField-3 DPUs는 고성능 북남 네트워크 연결을 제공하고, 데이터 저장 가속, 탄력적 GPU 컴퓨팅 및 제로 트러스트 보안을 가능하게 함</li>
<li>NVIDIA AI Enterprise 소프트웨어는 클라우드 제공업체가 서버를 프로비저닝하고 관리할 수 있도록 지원</li>
<li>NVIDIA NeMo 프레임워크를 통해 클라우드 제공업체가 생성 AI 모델을 훈련하고 미세 조정할 수 있도록 함</li>
<li>NVIDIA Riva는 음성 서비스를 제공</li>
<li>NVIDIA RAPIDS는 Spark 워크로드를 가속화</li>
</ul>
<h2 id="10억-개의-페르소나를-활용한-데이터-생성-방법-제안"><a href="#10억-개의-페르소나를-활용한-데이터-생성-방법-제안" class="headerlink" title="10억 개의 페르소나를 활용한 데이터 생성 방법 제안"></a>10억 개의 페르소나를 활용한 데이터 생성 방법 제안</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.20094">링크</a>, 2024년 6월 28일,<br>Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, Dong Yu</p>
<ul>
<li>Persona Hub라는 10억 개의 페르소나를 자동으로 웹 데이터에서 수집하여 데이터 생성</li>
<li>다양한 시나리오에서 사용 가능한 고품질의 수학 및 논리적 추론 문제, 지식 풍부한 텍스트 등을 생성</li>
<li>MATH 평가에서 높은 성과를 보이며 GPT-4 수준의 성능을 달성</li>
<li>데이터 생성의 다양성과 확장성을 극대화하여 LLM 연구 및 개발에 기여</li>
<li>기존의 인스턴스 기반 접근 방식이나 핵심 포인트 기반 접근 방식보다 커버리지, 품질 및 관점을 확장하여 데이터 생성 과정의 견고성을 강화</li>
<li>다양한 용도로 사용 가능한 데이터 세트를 생성하여 MATH, 논리적 추론 문제, 사용자 지시문, 게임 NPC, 도구 개발 등에 활용할 수 있음</li>
</ul>
<h2 id="Figma-AI-기능을-중심으로-한-다양한-업데이트-발표"><a href="#Figma-AI-기능을-중심으로-한-다양한-업데이트-발표" class="headerlink" title="Figma, AI 기능을 중심으로 한 다양한 업데이트 발표"></a>Figma, AI 기능을 중심으로 한 다양한 업데이트 발표</h2><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=n5gJgkO2Dg0&ab_channel=Figma">링크</a>, 2024년 6월 30일,<br>Figma</p>
<ul>
<li>‘Make Design’ 기능을 통해 텍스트 설명으로 디자인을 생성할 수 있음</li>
<li>‘Search for Similar’ 기능을 사용하여 유사한 요소를 빠르게 찾을 수 있음</li>
<li>이미지 배경 제거, 다국어 번역, 레이어 명 자동 정리, 프로토타입 자동 생성 등 다양한 AI 기능 포함</li>
<li>디자인 과정의 효율성을 극대화하고 사용자 경험을 크게 개선</li>
<li>AI 자동 생성 기능을 통해 작업 흐름을 유지할 수 있도록 도움</li>
<li>이번 업데이트는 AI를 통해 디자인 작업을 보다 효율적으로 수행할 수 있도록 도움</li>
<li>사용자 경험을 개선하여 디자인과 개발 간의 협업을 더욱 원활하게 만듦</li>
</ul>
<h2 id="Groq-Whisper-Large-V3-성능-대폭-향상"><a href="#Groq-Whisper-Large-V3-성능-대폭-향상" class="headerlink" title="Groq, Whisper Large V3 성능 대폭 향상"></a>Groq, Whisper Large V3 성능 대폭 향상</h2><p><a target="_blank" rel="noopener" href="https://wow.groq.com/groq-runs-whisper-large-v3-at-a-164x-speed-factor-according-to-new-artificial-analysis-benchmark/">링크</a>, 2024년 6월 28일,<br>Groq</p>
<ul>
<li>Whisper Large V3를 GroqCloud™를 통해 개발자 커뮤니티에 제공</li>
<li>10분 길이의 오디오 파일을 3.7초 만에 전사하는 164배 속도 달성</li>
<li>Word Error Rate (WER)를 10.3%로 최소화하여 최고 성능 달성</li>
<li>AI 음성 경험을 위한 저지연 전사 성능 제공</li>
<li>Whisper Large V3는 AI 음성 인식 및 음성 번역을 위한 사전 훈련된 모델</li>
<li>Groq의 LPU™ 추론 엔진을 통해 저지연 AI 추론을 가능하게 함</li>
<li>GroqCloud™에서 제공되어 개발자들이 Whisper를 쉽게 사용할 수 있음</li>
<li>프로젝트 Media QA에서 Whisper 성능을 확인할 수 있음</li>
</ul>
<h2 id="SK그룹-AI와-반도체-분야에-2026년까지-80조-원-투자"><a href="#SK그룹-AI와-반도체-분야에-2026년까지-80조-원-투자" class="headerlink" title="SK그룹, AI와 반도체 분야에 2026년까지 80조 원 투자"></a>SK그룹, AI와 반도체 분야에 2026년까지 80조 원 투자</h2><p><a target="_blank" rel="noopener" href="https://n.news.naver.com/article/032/0003305572?cds=news_my">링크</a>, 2024년 6월 30일,<br>SK그룹</p>
<ul>
<li>SK그룹은 AI와 반도체를 비롯한 미래 성장 분야에 80조 원을 투자할 계획</li>
<li>급변하는 시장에 대응하고 선택과 집중을 통해 질적 성장 추구</li>
<li>2026년까지 수익성 개선, 사업구조 최적화, 시너지 제고 등을 통해 80조 원의 재원 확보</li>
<li>SK하이닉스는 5년간 103조 원을 투자하여 반도체 사업 경쟁력 강화</li>
<li>7월 1일부로 수펙스추구협의회에 반도체위원회를 신설</li>
<li>CEO들은 전체 계열사 수를 ‘관리 가능한 범위’로 조정할 필요성에 공감하고, 이를 단계적으로 추진</li>
<li>현재 SK의 계열사는 총 219곳으로, 이를 최적화하여 관리 범위를 조정할 계획</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h2 id="Title"><a href="#Title" class="headerlink" title="Title,"></a>Title,</h2><h3 id="한글제목"><a href="#한글제목" class="headerlink" title="한글제목"></a>한글제목</h3><p><a href="link">링크</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h2 id="Title-1"><a href="#Title-1" class="headerlink" title="Title,"></a>Title,</h2><h3 id="한글제목-1"><a href="#한글제목-1" class="headerlink" title="한글제목"></a>한글제목</h3><p><a href="link">링크</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://cdn.openai.com/llm-critics-help-catch-llm-bugs-paper.pdf</span><br><span class="line">OpenAI</span><br><span class="line">Jun 28, 2024</span><br><span class="line">Abstract:</span><br><span class="line">Reinforcement learning from human feedback (RLHF) is fundamentally limited</span><br><span class="line">by the capacity of humans to correctly evaluate model output. To improve human</span><br><span class="line">evaluation ability and overcome that limitation this work trains “critic” models</span><br><span class="line">that help humans to more accurately evaluate model-written code. These critics</span><br><span class="line">are themselves LLMs trained with RLHF to write natural language feedback</span><br><span class="line">highlighting problems in code from real-world assistant tasks. On code containing</span><br><span class="line">naturally occurring LLM errors model-written critiques are preferred over human</span><br><span class="line">critiques in 63% of cases, and human evaluation finds that models catch more bugs</span><br><span class="line">than human contractors paid for code review. We further confirm that our fine-tuned</span><br><span class="line">LLM critics can successfully identify hundreds of errors in ChatGPT training data</span><br><span class="line">rated as “flawless”, even though the majority of those tasks are non-code tasks</span><br><span class="line">and thus out-of-distribution for the critic model. Critics can have limitations of</span><br><span class="line">their own, including hallucinated bugs that could mislead humans into making</span><br><span class="line">mistakes they might have otherwise avoided, but human-machine teams of critics</span><br><span class="line">and contractors catch similar numbers of bugs to LLM critics while hallucinating</span><br><span class="line">less than LLMs alone.</span><br><span class="line"></span><br><span class="line">Summary</span><br><span class="line">Blog Post: Improving AI Reliability with Critic Models for Better Code Evaluation</span><br><span class="line">In the swiftly changing world of artificial intelligence (AI), guaranteeing the reliability of AI-generated outputs is increasingly crucial. This is particularly true for AI models that generate or evaluate code, which can occasionally contain subtle bugs or errors not immediately noticeable. These errors are risky in enterprise environments where accuracy is essential. Introducing critic models, which assess and critique model outputs, offers a promising solution to enhance AI reliability, especially in code evaluation.</span><br><span class="line">Understanding Critic Models</span><br><span class="line">Critic models, such as CriticGPT, are a new development designed to improve the evaluation of AI-generated outputs, including code. Unlike traditional methods that rely on human feedback, critic models use a sophisticated training process to identify errors that humans might miss. However, they also face challenges, such as mistakenly identifying errors that don’t exist.</span><br><span class="line">notion image</span><br><span class="line"></span><br><span class="line">How Critic Models Are Trained and Evaluated</span><br><span class="line">The training and evaluation of critic models involve several key steps and criteria:</span><br><span class="line">Comprehensiveness: They must cover all significant issues in the code.</span><br><span class="line">Critique-Bug Inclusion (CBI): They should pinpoint specific, known bugs.</span><br><span class="line">Minimizing false positives: Avoiding the identification of non-existent issues.</span><br><span class="line">Helpfulness and style: The critiques should be constructive and clear.</span><br><span class="line">These models are assessed through blind tests and compared using Elo scores, offering a detailed analysis of their performance.</span><br><span class="line">Training Process</span><br><span class="line">Training critic models involves generating critiques for code, which are then rated by human evaluators. These ratings help train a reward model that further refines the critic models&#x27; accuracy.</span><br><span class="line">Breakthrough Results with Critic Models</span><br><span class="line">Critic models have shown promising results. For instance, CriticGPT has surpassed human evaluators in identifying bugs, indicating a significant advancement in AI-assisted code evaluation. Combining these models with human evaluators leads to even better performance. Additionally, techniques like Force Sampling Beam Search have improved the balance between detecting real and imagined issues, enhancing evaluation reliability.</span><br><span class="line"></span><br><span class="line">notion image</span><br><span class="line">notion image</span><br><span class="line">Expanding the Use of Critic Models</span><br><span class="line">The application of critic models in code evaluation is just the beginning. These models are part of broader research into making AI more self-corrective and reliable across various coding tasks. Understanding their role helps us see their potential to revolutionize the field.</span><br><span class="line">Future Directions and Challenges</span><br><span class="line">Critic models are paving the way for AI that is not only more reliable but also capable of self-assessment. However, challenges such as potential biases and distinguishing between different types of errors need to be addressed.</span><br><span class="line">Conclusion</span><br><span class="line">Critic models offer a significant improvement in ensuring the reliability of AI-generated code. By critiquing and evaluating code more accurately, they enhance human evaluators&#x27; ability to spot and fix errors. As we refine these models, we edge closer to AI systems that are not just effective but also inherently safe. For AI engineers in enterprise settings, this represents an exciting opportunity to lead in the application of critic models, contributing to the development of AI that is both powerful and dependable. This journey marks a step towards a future where AI and humans collaborate more seamlessly, unlocking new possibilities.</span><br><span class="line"></span><br><span class="line">Is OpenAI following Anthropic? LLM Critics Help Catch LLM Bugs is the latest paper from OpenAI describing how LLM Critiques and AI Feedback help to improve RLHF and data quality and outscale human experts. 👀</span><br><span class="line">CriticGPT is an autoregressive language model trained with RLHF (InstructGPT and ChatGPT) to accept a question-answer pair as input and output a structured critique that highlights potential problems in the answer. 💡 - Pretty similar to Anthropics Constitutional AI method.</span><br><span class="line">RLHF pipeline to train CritiqueGPT, similar to ChatGPT:</span><br><span class="line">1️⃣ Step 1: Generate several critiques for each (question, answer) pair in the dataset by AI &amp; Contractors.</span><br><span class="line">2️⃣ Step 2: Contractors rated the attributes of the sampled critiques, including overall quality.</span><br><span class="line">3️⃣ Step 3: Train a reward model to predict the human overall quality rankings.</span><br><span class="line">4️⃣ Step 4: Train CritiqueGPT using PPO and Reward Model</span><br><span class="line">Insights</span><br><span class="line">🐛 Used “Tampering” Humans added bugs in code and wrote a critique about it</span><br><span class="line">🔍 CriticGPT identified hundreds of errors in ChatGPT data</span><br><span class="line">📊 Used Preference scores (B&gt;A&gt;D&gt;C) on a 1-7 ordinal scale for RLHF</span><br><span class="line">⏱️ Humans needed 50 minutes per example to write critiques.</span><br><span class="line">🤖 The reward model was trained on a mix of ChatGPT and CriticGPT</span><br><span class="line">🚀 Introduce Force Sampling Beam Search (FSBS) which uses Reward Model to improve outputs</span><br><span class="line">🖥️ CriticGPT was fine-tuned with less computing than ChatGPT.</span><br><span class="line">📝 Used Prompts from Reward Modelling dataset for PPO</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://blogs.nvidia.com/blog/ai-cloud-providers-reference-architecture/?ncid=so-link-519834</span><br><span class="line">NVIDIA Unveils Reference Architecture for AI Cloud Providers</span><br><span class="line">June 26, 2024 by Marc Hamilton</span><br><span class="line"> Share</span><br><span class="line"></span><br><span class="line">NVIDIA has announced a new reference architecture for cloud providers that want to offer generative AI services to their customers.</span><br><span class="line"></span><br><span class="line">The NVIDIA Cloud Partner reference architecture is a blueprint for building high-performance, scalable and secure data centers that can handle generative AI and large language models (LLMs).</span><br><span class="line"></span><br><span class="line">The reference architecture enables NVIDIA Cloud Partners within the NVIDIA Partner Network to reduce the time and cost of deploying AI solutions, while ensuring compatibility and interoperability among various hardware and software components.</span><br><span class="line"></span><br><span class="line">The architecture will also help cloud providers meet the growing demand for AI services from organizations — of all sizes and industries — that want to leverage the power of generative AI and LLMs without investing in their own infrastructure.</span><br><span class="line"></span><br><span class="line">Generative AI and LLMs are transforming the way organizations solve complex problems and create new value. These technologies use deep neural networks to generate realistic and novel outputs, such as text, images, audio and video, based on a given input or context. Generative AI and LLMs can be used for a variety of applications, such as copilots, chatbots and other content creation.</span><br><span class="line"></span><br><span class="line">However, generative AI and LLMs also pose significant challenges for cloud providers, which need to provide the infrastructure and software to support these workloads. The technologies require massive amounts of computing power, storage and network bandwidth, as well as specialized hardware and software to optimize performance and efficiency.</span><br><span class="line"></span><br><span class="line">For example, LLM training involves many GPU servers working together, communicating constantly among themselves and with storage systems. This translates to east-west and north-south traffic in data centers, which requires high-performance networks for fast and efficient communication.</span><br><span class="line"></span><br><span class="line">Similarly, generative AI inference with larger models needs multiple GPUs to work together to process a single query.</span><br><span class="line"></span><br><span class="line">Moreover, cloud providers need to ensure that their infrastructure is secure, reliable and scalable, as they serve multiple customers with different needs and expectations. Cloud providers also need to comply with industry standards and best practices, as well as provide support and maintenance for their services.</span><br><span class="line"></span><br><span class="line">The NVIDIA Cloud Partner reference architecture addresses these challenges by providing a comprehensive, full-stack hardware and software solution for cloud providers to offer AI services and workflows for different use cases. Based on the years of experience NVIDIA has in designing and building large-scale deployments both internally and for customers, the reference architecture includes:</span><br><span class="line"></span><br><span class="line">GPU servers from NVIDIA and its manufacturing partners, featuring NVIDIA’s latest GPU architectures, such as Hopper and Blackwell, which deliver unparalleled compute power and performance for AI workloads.</span><br><span class="line">Storage offerings from certified partners, which provide high-performance storage optimized for AI and LLM workloads. The offerings also include those tested and validated for NVIDIA DGX SuperPOD and NVIDIA DGX Cloud. They are proven to be reliable, efficient and scalable.</span><br><span class="line">NVIDIA Quantum-2 InfiniBand and Spectrum-X Ethernet networking, which provide a high-performance east-west network for fast and efficient communication between GPU servers.</span><br><span class="line">NVIDIA BlueField-3 DPUs, which deliver high-performance north-south network connectivity and enable data storage acceleration, elastic GPU computing and zero-trust security.</span><br><span class="line">In/out-of-band management solutions from NVIDIA and management partners, which provide tools and services for provisioning, monitoring and managing AI data center infrastructure.</span><br><span class="line">NVIDIA AI Enterprise software, including:</span><br><span class="line">NVIDIA Base Command Manager Essentials, which helps cloud providers provision and manage their servers.</span><br><span class="line">NVIDIA NeMo framework, which helps cloud providers train and fine-tune generative AI models.</span><br><span class="line">NVIDIA NIM, a set of easy-to-use microservices designed to accelerate deployment of generative AI across enterprises.</span><br><span class="line">NVIDIA Riva, for speech services.</span><br><span class="line">NVIDIA RAPIDS accelerator for Spark, to accelerate Spark workloads.</span><br><span class="line">The NVIDIA Cloud Partner reference architecture offers the following key benefits to cloud providers:</span><br><span class="line"></span><br><span class="line">Build, Train and Go: NVIDIA infrastructure specialists use the architecture to physically install and provision the cluster for faster rollouts for cloud providers.</span><br><span class="line">Speed: By incorporating the expertise and best practices of NVIDIA and partner vendors, the architecture can help cloud providers accelerate the deployment of AI solutions and gain a competitive edge in the market.</span><br><span class="line">High Performance: The architecture is tuned and benchmarked with industry-standard benchmarks, ensuring optimal performance for AI workloads.</span><br><span class="line">Scalability: The architecture is designed for cloud-native environments, facilitating the development of scalable AI systems that offer flexibility and can seamlessly expand to meet increasing demand of end users.</span><br><span class="line">Interoperability: The architecture ensures compatibility among various components of the architecture, making integration and communication between components seamless.</span><br><span class="line">Maintenance and Support: NVIDIA Cloud Partners have access to NVIDIA subject-matter experts, who can help address any unexpected challenges that may arise during and after deployment.</span><br><span class="line">The NVIDIA Cloud Partner reference architecture provides a proven blueprint for cloud providers to stand up and manage high-performance scalable infrastructure for AI data centers.</span><br><span class="line"></span><br><span class="line">See notice regarding software product information.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2406.20094</span><br><span class="line">[Submitted on 28 Jun 2024]</span><br><span class="line">Scaling Synthetic Data Creation with 1,000,000,000 Personas</span><br><span class="line">Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, Dong Yu</span><br><span class="line">We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce Persona Hub -- a collection of 1 billion diverse personas automatically curated from web data. These 1 billion personas (~13% of the world&#x27;s total population), acting as distributed carriers of world knowledge, can tap into almost every perspective encapsulated within the LLM, thereby facilitating the creation of diverse synthetic data at scale for various scenarios. By showcasing Persona Hub&#x27;s use cases in synthesizing high-quality mathematical and logical reasoning problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs and tools (functions) at scale, we demonstrate persona-driven data synthesis is versatile, scalable, flexible, and easy to use, potentially driving a paradigm shift in synthetic data creation and applications in practice, which may have a profound impact on LLM research and development.</span><br><span class="line">This is one of the coolest ideas for scaling synthetic data that I&#x27;ve come across.</span><br><span class="line">Proposes 1 billion diverse personas to facilitate the creation of diverse synthetic data for different scenarios.</span><br><span class="line">It&#x27;s easy to generate synthetic data but hard to scale up its diversity which is essential for its application.</span><br><span class="line">This paper proposes a novel persona-driven data synthesis methodology to generate diverse and distinct data covering a wide range of perspectives.</span><br><span class="line">Previous works synthesize data using either instance-driven approaches (e.g., using seed corpus) or key-point-driven methods (e.g., using topic/subject). Both of these approaches lack the desired coverage, quality, and perspectives needed to robustly scale the data synthesis process.</span><br><span class="line">To measure the quality of the synthetic datasets, they performed an out-of-distribution evaluation on MATH. A fine-tuned model on their synthesized 1.07M math problems achieves 64.9% on MATH, matching the performance of gpt-4-turbo-preview at only a 7B scale.</span><br><span class="line">Their method is not only effective for MATH problems, but it can also be used to generate logical reasoning problems, instructions, game NPCs, tool development, knowledge-rich text, and many more use cases.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.youtube.com/watch?v=n5gJgkO2Dg0&amp;ab_channel=Figma</span><br><span class="line">Config 2024에서 Figma는 혁신적인 AI 기능을 중심으로 다양한 업데이트를 발표했습니다. Figma AI는 사용자가 텍스트 설명으로 디자인을 생성할 수 있는 &#x27;Make Design&#x27; 기능, 유사한 요소를 빠르게 찾을 수 있는 &#x27;Search for Similar&#x27; 기능, 그리고 작업 흐름을 유지하도록 돕는 AI 자동 생성 기능을 포함하고 있습니다. 또한, 이미지 배경 제거, 다국어 번역, 레이어 명 자동 정리, 프로토타입 자동 생성 등 AI를 활용한 다양한 기능을 통해 디자인 과정을 혁신적으로 변화시킵니다. Figma AI는 작업 효율성을 극대화하고, 디자인과 개발 간의 협업을 더욱 원활하게 만들어 줍니다. 이번 업데이트는 AI를 통해 사용자 경험을 크게 개선하고, 디자인 작업을 보다 효율적으로 수행할 수 있도록 돕습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://wow.groq.com/groq-runs-whisper-large-v3-at-a-164x-speed-factor-according-to-new-artificial-analysis-benchmark/</span><br><span class="line">Groq Runs Whisper Large V3 at a 164x Speed Factor According to New Artificial Analysis Benchmark</span><br><span class="line">Written by:</span><br><span class="line">Groq</span><br><span class="line">Whisper Large V3 Is Now Available to the Developer Community via GroqCloud™</span><br><span class="line">We’re excited to announce Groq is officially running Whisper Large V3 on the LPU™ Inference Engine, available to our developer community via GroqCloud™ through our Developer Playground. Whisper is a pre-trained model for automatic speech recognition and speech translation, trained on 680k hours of labeled data. Whisper and models like it are paving the way for accurate and seamless GenAI voice experiences while broadening the possibilities on developer application and use cases, both of which require low-latency AI inference.</span><br><span class="line"></span><br><span class="line">This also marks an addition to the expanding GenAI model portfolio hosted by Groq. Large Language Models (LLMs) continue to run on the Groq LPU, the addition of Whisper Large V3 is another step on our way to multi-modal.</span><br><span class="line"></span><br><span class="line">Artificial Analysis has included our Whisper performance in their latest independent speech-to-text benchmark.</span><br><span class="line"></span><br><span class="line">Dive into the results below. To see see this model in action, check out Project Media QA on GroqLabs. If you are a developer interested in Whisper running on Groq, sign up for access via GroqCloud at console.groq.com.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Artificial Analysis has independently benchmarked Whisper Large V3 on Groq as achieving a Speed Factor of 164. This means Groq can transcribe our 10-minute audio test file in just 3.7 seconds. Low latency transcription is a critical component for seamless voice experiences. AI voice experiences require low latency inference on transcription, language, and voice models to enable immediate responses that keep users engaged.</span><br><span class="line"></span><br><span class="line">- Micah Hill-Smith, Co-founder &amp; CEO, ArtificialAnalysis.ai</span><br><span class="line">Repost</span><br><span class="line">Speed Factor</span><br><span class="line"></span><br><span class="line">Measured as input audio seconds transcribed per second, Groq clocks in at a speed factor rate of 164x real-time, the fastest implementation of the base Whisper Large V3 model.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Quality</span><br><span class="line"></span><br><span class="line">Artificial Analysis defines Word Error Rate (WER) as the percentage of of words transcribed incorrectly. Groq minimized its Word Error rate to 10.3% for Whisper Large V3, matching the lowest WER from other providers on the leaderboard</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Price</span><br><span class="line"></span><br><span class="line">Artificial Analysis defines price as USD per 1000 minutes of audio, bringing the Groq price to $0.5 based on offering Whisper Large V3 at a price of $0.03 per hour transcribed.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://n.news.naver.com/article/032/0003305572?cds=news_my</span><br><span class="line">최태원 회장 “AI 분야 선제적 대응”…SK, 2026년까지 80조원 집중 투입</span><br><span class="line">입력2024.06.30. 오후 8:40 기사원문</span><br><span class="line">이진주 기자</span><br><span class="line">  32</span><br><span class="line">84</span><br><span class="line">본문 요약봇</span><br><span class="line">텍스트 음성 변환 서비스 사용하기</span><br><span class="line">글자 크기 변경하기</span><br><span class="line">SNS 보내기</span><br><span class="line">인쇄하기</span><br><span class="line">‘계열사 재조정’ 단계적 추진</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SK그룹이 2026년까지 80조원의 재원을 확보해 인공지능(AI)과 반도체를 비롯한 미래 성장 분야에 투자한다.</span><br><span class="line"></span><br><span class="line">급변하는 시장에 선제적으로 대응하고 ‘선택과 집중’을 통해 질적 성장을 꾀한다는 전략이다.</span><br><span class="line"></span><br><span class="line">SK는 지난 28~29일 경기 이천 SKMS연구소에서 최태원 회장(사진), 최재원 수석부회장, 최창원 수펙스추구협의회 의장, 주요 계열사 최고경영자(CEO) 20여명 등이 참석한 가운데 경영전략회의를 열고 이 같은 전략 방향에 뜻을 모았다고 30일 밝혔다.</span><br><span class="line"></span><br><span class="line">이번 회의에는 최 회장의 장녀인 최윤정 SK바이오팜 사업개발본부장(부사장)이 처음 참석한 것으로 알려졌다. 미국 출장 중인 최 회장은 화상으로 회의에 참석해 “‘새로운 트랜지션(전환) 시대’를 맞아 미래 준비 등을 위한 선제적이고 근본적인 변화가 필요하다”고 강조했다.</span><br><span class="line"></span><br><span class="line">최 회장은 “지금 미국에서는 AI 말고는 할 얘기가 없다고 할 정도로 AI 관련 변화의 바람이 거세다”며 “그룹 보유 역량을 활용해 AI 서비스부터 인프라까지 ‘AI 밸류체인(가치사슬) 리더십’을 강화해야 한다”고 주문했다.</span><br><span class="line"></span><br><span class="line">최 회장은 SK가 강점을 가진 에너지 솔루션 분야도 글로벌 시장에서 AI 못지않은 성장 기회를 확보할 수 있을 것으로 전망했다.</span><br><span class="line"></span><br><span class="line">SK 경영진은 이번 회의에서 수익성 개선과 사업구조 최적화, 시너지 제고 등으로 2026년까지 80조원의 재원을 확보하고, 이를 AI와 반도체 등 미래 성장 분야 투자와 주주 환원 등에 활용하기로 의견을 모았다.</span><br><span class="line"></span><br><span class="line">또 운영 개선을 통해 3년 내 30조원의 잉여현금흐름(FCF)을 만들어 부채비율을 100% 이하로 관리한다는 목표도 세웠다. SK는 지난해 10조원 적자를 기록한 세전이익이 올해는 흑자로 전환해 22조원 안팎에 이를 것으로 예상했다.</span><br><span class="line"></span><br><span class="line">SK하이닉스는 2028년까지 향후 5년간 총 103조원을 투자해 반도체 사업 경쟁력을 강화하기로 했다. 이 중 약 80%에 해당하는 82조원은 HBM 등 AI 관련 사업에 투자한다.</span><br><span class="line"></span><br><span class="line">7월1일부로 수펙스추구협의회에 ‘반도체위원회’도 신설한다. 위원장은 곽노정 SK하이닉스 사장이 맡는다.</span><br><span class="line"></span><br><span class="line">CEO들은 전체 계열사 수를 ‘관리 가능한 범위’로 조정할 필요성이 있다는 데 공감하고, 각 사별 내부 절차를 거쳐 이를 단계적으로 추진하기로 했다. 현재 SK의 계열사는 총 219곳으로, 삼성(63곳) 등 주요 그룹과 비교해도 많다는 지적이 나온다.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-07-02</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/07/02/2024-7-2-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 7월 2일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/07/02/Config-2024-Figma-product-launch-keynote/" title="Config 2024: Figma product launch keynote">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/06/28/2024-6-28-AI-NEWS/" title="2024년 6월 28일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>