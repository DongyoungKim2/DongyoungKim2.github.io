<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 7ì›” 5ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="SummaryKyutaiì—ì„œëŠ” Moshië¼ëŠ” ì‹¤ì‹œê°„ ë„¤ì´í‹°ë¸Œ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤. MoshiëŠ” ê°ì •ì„ í‘œí˜„í•˜ê³  ì´í•´í•˜ë©°, ìŒì„±ì„ ìƒì„±í•˜ê³  ë“¤ì„ ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤. InternLMì—ì„œëŠ” IXC-2.5ë¼ëŠ” ìƒˆë¡œìš´ ë¹„ì „ ì–¸ì–´ ëª¨ë¸ì„ ê³µê°œí•˜ì˜€ìŠµë‹ˆë‹¤. "><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 7ì›” 5ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Kyutaiì—ì„œëŠ” Moshië¼ëŠ” ì‹¤ì‹œê°„ ë„¤ì´í‹°ë¸Œ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤. MoshiëŠ” ê°ì •ì„ í‘œí˜„í•˜ê³  ì´í•´í•˜ë©°, ìŒì„±ì„ ìƒì„±í•˜ê³  ë“¤ì„ ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤. InternLMì—ì„œëŠ” IXC-2.5ë¼ëŠ” ìƒˆë¡œìš´ ë¹„ì „ ì–¸ì–´ ëª¨ë¸ì„ ê³µê°œí•˜ì˜€ìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì´í•´ì™€ ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì§€ì›í•˜ë©°, ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. NVIDIAëŠ” DoRAë¼ëŠ” ìƒˆë¡œìš´ íŒŒì¸íŠœë‹ ë°©ë²•ì„ ì†Œê°œí•˜ì˜€ê³ , Metaì—ì„œëŠ” ë‹¤ì¤‘ í† í° ì˜ˆì¸¡ì„ ì‚¬ìš©í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ Hugging Faceì—ì„œëŠ” RT-DETRì´ë¼ëŠ” ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ ëª¨ë¸ì„ ì§€ì›í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
<h2 id="Kyutai-Moshi-ë°œí‘œ"><a href="#Kyutai-Moshi-ë°œí‘œ" class="headerlink" title="Kyutai, Moshi ë°œí‘œ"></a>Kyutai, Moshi ë°œí‘œ</h2><p><a target="_blank" rel="noopener" href="https://kyutai.org/">Kyutai</a>, 2024ë…„ 7ì›” 3ì¼,<br>Kyutai</p>
<ul>
<li>KyutaiëŠ” ì‹¤ì‹œê°„ ë„¤ì´í‹°ë¸Œ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ Moshië¥¼ ë°œí‘œ</li>
<li>MoshiëŠ” ê°ì •ì„ í‘œí˜„í•˜ê³  ì´í•´í•˜ëŠ” ëŠ¥ë ¥ ë³´ìœ </li>
<li>ê°ì •ì„ ë‹´ì•„ â€œí”„ë‘ìŠ¤ ì–µì–‘â€ìœ¼ë¡œ ë§í•˜ê¸° ê°€ëŠ¥</li>
<li>ìŒì„± ìƒì„± ë° ì²­ì·¨ ê¸°ëŠ¥ ì œê³µ</li>
<li>í…ìŠ¤íŠ¸ì™€ ì˜¤ë””ì˜¤ í˜¼í•© ë°ì´í„°ë¥¼ ê³µë™ í›ˆë ¨</li>
<li>Kyutaiê°€ ì œì‘í•œ Helium 7B LLMì˜ í•©ì„± í…ìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©</li>
<li>100k â€œoral-styleâ€ í•©ì„± ëŒ€í™” ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì •</li>
<li>ë³„ë„ì˜ TTS ëª¨ë¸ë¡œ ìƒì„±ëœ í•©ì„± ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„± í•™ìŠµ</li>
<li>200msì˜ ì¢…ë‹¨ê°„ ì§€ì—° ì‹œê°„ ë‹¬ì„±</li>
<li>MacBookì´ë‚˜ ì¼ë°˜ GPUì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥í•œ ì†Œí˜• ë²„ì „ ì¡´ì¬</li>
<li>AI ìƒì„± ì˜¤ë””ì˜¤ë¥¼ ê°ì§€í•˜ëŠ” ì›Œí„°ë§ˆí‚¹ ê¸°ëŠ¥ í¬í•¨</li>
<li>ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ê³µê°œ ì˜ˆì •</li>
<li>MoshiëŠ” ì˜¤í”ˆ ì—°êµ¬ì™€ AI ìƒíƒœê³„ ë°œì „ì— ê¸°ì—¬í•  ê²ƒ</li>
</ul>
<h2 id="InternLM-InternLM-XComposer-2-5-ê³µê°œ"><a href="#InternLM-InternLM-XComposer-2-5-ê³µê°œ" class="headerlink" title="InternLM, InternLM-XComposer-2.5 ê³µê°œ"></a>InternLM, InternLM-XComposer-2.5 ê³µê°œ</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.03320">arXiv</a>, 2024ë…„ 7ì›” 3ì¼,<br>InternLM</p>
<ul>
<li>IXC-2.5ëŠ” ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì´í•´ì™€ ì‘ë¬¸ ì‘ìš©ì— íƒì›”</li>
<li>7B íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë¹„ì „ ì–¸ì–´ ëª¨ë¸</li>
<li>24Kì˜ êµì°¨ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë¬¸ë§¥ìœ¼ë¡œ í›ˆë ¨</li>
<li>RoPE ë³´ê°„ ê¸°ìˆ ë¡œ 96K ê¸´ ë¬¸ë§¥ ì§€ì›</li>
<li>ê³ í•´ìƒë„ ì´ë¯¸ì§€ì™€ ë™ì˜ìƒ ì´í•´ ëŠ¥ë ¥ ì œê³µ</li>
<li>ë©€í‹°í„´ ë©€í‹°ì´ë¯¸ì§€ ëŒ€í™” ì§€ì›</li>
<li>í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì‘ë¬¸ ë° ì›¹í˜ì´ì§€ ì œì‘ì— ì‚¬ìš©</li>
<li>IXC-2.5ëŠ” ì›¹í˜ì´ì§€ ì œì‘ê³¼ ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ê¸°ì‚¬ ì‘ì„±ì— í™œìš© ê°€ëŠ¥</li>
<li>28ê°œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ ì…ì¦</li>
<li>GPT-4V ë° Gemini Proì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ ë°œíœ˜</li>
<li>ì›¹í˜ì´ì§€ ì‘ì„±ê³¼ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ê¸°ì‚¬ ì‘ì„±ì—ì„œ íŠ¹ë³„íˆ ì„¤ê³„ëœ Chain-of-Thought (CoT)ì™€ Direct Preference Optimization (DPO) ê¸°ë²• ì‚¬ìš©</li>
</ul>
<h2 id="NVIDIA-DoRA-ê³ ì„±ëŠ¥-íŒŒì¸íŠœë‹-ëŒ€ì•ˆ"><a href="#NVIDIA-DoRA-ê³ ì„±ëŠ¥-íŒŒì¸íŠœë‹-ëŒ€ì•ˆ" class="headerlink" title="NVIDIA, DoRA: ê³ ì„±ëŠ¥ íŒŒì¸íŠœë‹ ëŒ€ì•ˆ"></a>NVIDIA, DoRA: ê³ ì„±ëŠ¥ íŒŒì¸íŠœë‹ ëŒ€ì•ˆ</h2><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/introducing-dora-a-high-performing-alternative-to-lora-for-fine-tuning/">NVIDIA</a>, 2024ë…„ 6ì›” 28ì¼,<br>NVIDIA</p>
<ul>
<li>DoRAëŠ” LoRAì˜ ëŒ€ì•ˆìœ¼ë¡œ ì œì•ˆëœ íŒŒì¸íŠœë‹ ë°©ë²•</li>
<li>LoRAë³´ë‹¤ í•™ìŠµ ìš©ëŸ‰ê³¼ ì•ˆì •ì„± í–¥ìƒ</li>
<li>ì¶”ê°€ ì¶”ë¡  ë¹„ìš© ì—†ì´ ì„±ëŠ¥ ê°œì„ </li>
<li>ë‹¤ì–‘í•œ ì–¸ì–´ ë° ë¹„ì „ ëª¨ë¸ ì‘ì—…ì—ì„œ LoRAë¥¼ ëŠ¥ê°€</li>
<li>LLM ë° VLM ì‘ì—…ì—ì„œ ê³µí†µì ì¸ ì„±ëŠ¥ í–¥ìƒ</li>
<li>ê° íŒŒë¼ë¯¸í„°ì˜ ë°©í–¥ê³¼ í¬ê¸°ë¥¼ ë¶„í•´í•˜ì—¬ í•™ìŠµ</li>
<li>ICML 2024ì—ì„œ êµ¬ìˆ  ë…¼ë¬¸ìœ¼ë¡œ ë°œí‘œ</li>
<li>DoRAëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ì— ì ìš© ê°€ëŠ¥</li>
<li>LoRAë³´ë‹¤ FT í•™ìŠµ íŒ¨í„´ê³¼ ìœ ì‚¬í•œ í•™ìŠµ í–‰ë™ì„ ë³´ì„</li>
<li>QLoRAì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ìˆ˜ìš” ê°ì†Œ ê°€ëŠ¥</li>
<li>Hugging Faceì˜ DreamBoothë¡œ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ê°œì¸í™”ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ ë°œíœ˜</li>
</ul>
<h2 id="Meta-ë‹¤ì¤‘-í† í°-ì˜ˆì¸¡-ì ‘ê·¼ë²•-ë°œí‘œ"><a href="#Meta-ë‹¤ì¤‘-í† í°-ì˜ˆì¸¡-ì ‘ê·¼ë²•-ë°œí‘œ" class="headerlink" title="Meta, ë‹¤ì¤‘ í† í° ì˜ˆì¸¡ ì ‘ê·¼ë²• ë°œí‘œ"></a>Meta, ë‹¤ì¤‘ í† í° ì˜ˆì¸¡ ì ‘ê·¼ë²• ë°œí‘œ</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/facebook/multi-token-prediction">Meta</a>, 2024ë…„ 7ì›” 4ì¼,<br>Meta</p>
<ul>
<li>ë‹¤ì¤‘ í† í° ì˜ˆì¸¡ì„ ì‚¬ìš©í•œ ìƒˆë¡œìš´ LLM í›ˆë ¨ ì ‘ê·¼ë²• ë°œí‘œ</li>
<li>ëª¨ë¸ ì„±ëŠ¥ ë° í›ˆë ¨ íš¨ìœ¨ì„± í–¥ìƒ</li>
<li>ì½”ë“œ ì™„ì„±ì„ ìœ„í•œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ê³µê°œ</li>
<li>Hugging Faceì—ì„œ ëª¨ë¸ ì´ìš© ê°€ëŠ¥</li>
<li>200B í† í°ê³¼ 1T í† í°ì˜ ì½”ë“œ ë°ì´í„°ë¡œ í›ˆë ¨ëœ ëª¨ë¸ í¬í•¨</li>
<li>í‘œì¤€ Llama 2 SentencePiece í† í¬ë‚˜ì´ì € ì‚¬ìš©</li>
</ul>
<h2 id="Hugging-Face-RT-DETR-ì‹¤ì‹œê°„-ê°ì²´-íƒì§€-ëª¨ë¸-ì§€ì›"><a href="#Hugging-Face-RT-DETR-ì‹¤ì‹œê°„-ê°ì²´-íƒì§€-ëª¨ë¸-ì§€ì›" class="headerlink" title="Hugging Face, RT-DETR ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ ëª¨ë¸ ì§€ì›"></a>Hugging Face, RT-DETR ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ ëª¨ë¸ ì§€ì›</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/merve/RT-DETR-tracking-coco">Hugging Face</a>, 2024ë…„ 7ì›” 5ì¼,<br>Hugging Face</p>
<ul>
<li>RT-DETR ëª¨ë¸ì€ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ ê¸°ëŠ¥ ì œê³µ</li>
<li>YOLO ëª¨ë¸ë³´ë‹¤ ì†ë„ì™€ ì •í™•ì„±ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ ë°œíœ˜</li>
<li>Apache 2.0 ë¼ì´ì„ ìŠ¤ë¡œ ìƒì—…ì  ìš©ë„ë¡œ ììœ ë¡­ê²Œ ì‚¬ìš© ê°€ëŠ¥</li>
<li>Metaì˜ Transformer ê¸°ë°˜ íƒì§€ ëª¨ë¸ì¸ DETRì˜ í›„ì†ì‘</li>
<li>í•˜ì´ë¸Œë¦¬ë“œ ì¸ì½”ë” ì„¤ê³„ë¥¼ í†µí•´ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§•ì„ ì‹ ì†íˆ ì²˜ë¦¬</li>
<li>ê³ í’ˆì§ˆ ì´ˆê¸° ì¿¼ë¦¬ë¥¼ ì œê³µí•˜ì—¬ ì •í™•ë„ í–¥ìƒ</li>
<li>ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì— ì ì‘í•  ìˆ˜ ìˆëŠ” ìœ ì—°í•œ ì†ë„ ì¡°ì ˆ ì§€ì›</li>
<li>T4 GPUì—ì„œ 108&#x2F;74 FPS ì„±ëŠ¥ ë°œíœ˜</li>
<li>Objects365ë¡œ ì‚¬ì „ í›ˆë ¨ í›„ 55.3%&#x2F;56.2% AP ë‹¬ì„±</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹"><a href="#todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹" class="headerlink" title="(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,"></a>(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,</h1><h2 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAIì—ì„œëŠ” ~~~ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.)</p>
<h2 id="Title"><a href="#Title" class="headerlink" title="Title,"></a>Title,</h2><h3 id="í•œê¸€ì œëª©"><a href="#í•œê¸€ì œëª©" class="headerlink" title="í•œê¸€ì œëª©"></a>í•œê¸€ì œëª©</h3><p><a href="link">ë§í¬</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
</ul>
<h2 id="Title-1"><a href="#Title-1" class="headerlink" title="Title,"></a>Title,</h2><h3 id="í•œê¸€ì œëª©-1"><a href="#í•œê¸€ì œëª©-1" class="headerlink" title="í•œê¸€ì œëª©"></a>í•œê¸€ì œëª©</h3><p><a href="link">ë§í¬</a>, date,<br>company name</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://kyutai.org/</span><br><span class="line">kyutai</span><br><span class="line">July 3, 2024</span><br><span class="line">Did Open Science just beat OpenAI? ğŸ¤¯ Kyutai just announced Moshi, a real-time native multimodal foundation model that can listen and speak, similar to what OpenAI demoed GPT-4o in May. ğŸ‘€</span><br><span class="line">Moshi:</span><br><span class="line">&gt; Expresses and understands emotions, e.g. speak with â€œfrench accessâ€</span><br><span class="line">&gt; Listens and generates Audio/Speech</span><br><span class="line">&gt; thinks as it speaks (textual thoughts)</span><br><span class="line">&gt; Supports 2 streams of audio to listen and speak at the same time</span><br><span class="line">&gt; Used Joint pre-training on mix of text and audio</span><br><span class="line">&gt; Used synthetic data text data from Helium a 7B LLM (Kyutai created)</span><br><span class="line">&gt; Is fine-tuned on 100k â€œoral-styleâ€ synthetic (conversations) converted with TTS</span><br><span class="line">&gt; Learned its voice from synthetic data generated by a separate TTS model</span><br><span class="line">&gt; Achieves a end-to-end latency of 200ms</span><br><span class="line">&gt; Has a smaller variant that runs on a MacBook or consumer-size GPU.</span><br><span class="line">&gt; Uses watermarking to detect AI-generated audio (WIP)</span><br><span class="line">&gt; Will be released open source!</span><br><span class="line">1. Itâ€™s small, 7B model (14GB VRAM in bf16/ fp16, 7GB in fp8/ int8) - Can be quantised further to run in even constrained environments. Massive win for accessibility.</span><br><span class="line">2. 160-200ms latency speech-in, speech-out - you can iterate quickly and prototype.</span><br><span class="line">3. Upcoming technical report + code + model weights release - just by code and report alone we can learn so much about scaling such models for further use-cases.</span><br><span class="line">4. Remember that while the model itself is important thereâ€™s a lot more artefacts behind it - The LLM (Helium 7B), Audio Codec (Mimi), Inference stack (based on Rust, possibly candle), Watermarking (possibly audioseal or a variant) and lots more.</span><br><span class="line">5. Itâ€™s a v1 - this is the worst this tech will ever be! This team is less than 6 months old and theyâ€™ve managed to ship a world class open demo.</span><br><span class="line">Itâ€™s easy to dunk on those which build in public and be open about the shortcomings/ tidbits about the model. Tell me when your fav ClosedAI company does something similar.</span><br><span class="line">Congratulations again to the Kyutai team! ğŸ¤—</span><br><span class="line">1</span><br><span class="line">PRESS RELEASE</span><br><span class="line">Paris, July 3, 2024</span><br><span class="line">Kyutai unveils today the very first voice-enabled AI openly accessible to all</span><br><span class="line">In just 6 months, with a team of 8, the Kyutai research lab developed from scratch an</span><br><span class="line">artificial intelligence (AI) model with unprecedented vocal capabilities called Moshi.</span><br><span class="line">The team publicly unveiled its experimental prototype today in Paris. At the end of the</span><br><span class="line">presentation, the participants â€“ researchers, developers, entrepreneurs, investors and journalists</span><br><span class="line">â€“ were themselves able to interact with Moshi. The interactive demo of the AI will be accessible</span><br><span class="line">from the Kyutai website at the end of the day. It can therefore be freely tested online as from</span><br><span class="line">today, which constitutes a world first for a generative voice AI.</span><br><span class="line">This new type of technology makes it possible for the first time to communicate in a smooth,</span><br><span class="line">natural and expressive way with an AI. During the presentation, the Kyutai team interacted with</span><br><span class="line">Moshi to illustrate its potential as a coach or companion for example, and its creativity through the</span><br><span class="line">incarnation of characters in roleplays.</span><br><span class="line">More broadly, Moshi has the potential to revolutionize the use of speech in the digital world.</span><br><span class="line">For instance, its text-to-speech capabilities are exceptional in terms of emotion and interaction</span><br><span class="line">between multiple voices.</span><br><span class="line">2</span><br><span class="line">Compact, Moshi can also be installed locally and therefore run safely on an unconnected</span><br><span class="line">device.</span><br><span class="line">With Moshi, Kyutai intends to contribute to open research in AI and to the development of the</span><br><span class="line">entire ecosystem. The code and weights of the models will soon be freely shared, which is also</span><br><span class="line">unprecedented for such technology. They will be useful both to researchers in the field and to</span><br><span class="line">developers working on voice-based products and services. This technology can therefore be</span><br><span class="line">studied in depth, modified, extended or specialized according to needs. The community will in</span><br><span class="line">particular be able to extend Moshi&#x27;s knowledge base and factuality, which are currently deliberately</span><br><span class="line">limited in such a lightweight model, while exploiting its unparalleled voice interaction capabilities.</span><br><span class="line">-----------------------------</span><br><span class="line">About Kyutai</span><br><span class="line">Kyutai is a non-profit laboratory dedicated to open research in AI, founded in November 2023 by the iliad Group,</span><br><span class="line">CMA CGM and Schmidt Sciences. Launched with an initial team of six leading scientists, who have all worked with</span><br><span class="line">Big Tech labs in the USA, Kyutai continues to recruit at the highest level, and also offers internships to research</span><br><span class="line">Masterâ€™s degree students. Now comprising a dozen members, the team will launch its first PhD theses at the end</span><br><span class="line">of the year. The research undertaken explores new general-purpose models with high capabilities. The lab is</span><br><span class="line">currently working in particular on multimodality, i.e., the possibility for a model to exploit different types of content</span><br><span class="line">(text, sound, images, etc.) both for learning and for inference. All the models developed are intended to be freely</span><br><span class="line">shared, as are the software and know-how that enabled their creation. To carry out its work and train its models,</span><br><span class="line">Kyutai relies in particular for its compute on the Nabu 23 superpod made available by Scaleway, a subsidiary of the</span><br><span class="line">iliad Group.</span><br><span class="line">Follow us on:</span><br><span class="line">www.kyutai.org</span><br><span class="line">X: @kyutai_labs</span><br><span class="line">Contacts</span><br><span class="line">For any requests for interviews and/or photos of the Kyutai team, please send an email to presse@kyutai.org</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2407.03320</span><br><span class="line">[Submitted on 3 Jul 2024]</span><br><span class="line">InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output</span><br><span class="line">Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, Jiaqi Wang</span><br><span class="line">We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output. IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output contexts. Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In addition to comprehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks. The InternLM-XComposer-2.5 is publicly available at this https URL.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">InternLM2.5 has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:</span><br><span class="line"></span><br><span class="line">Outstanding reasoning capability: State-of-the-art performance on Math reasoning, surpassing models like Llama3 and Gemma2-9B.</span><br><span class="line"></span><br><span class="line">1M Context window: Nearly perfect at finding needles in the haystack with 1M-long context, with leading performance on long-context tasks like LongBench. Try it with LMDeploy for 1M-context inference.</span><br><span class="line"></span><br><span class="line">Stronger tool use: InternLM2.5 supports gathering information from more than 100 web pages, corresponding implementation will be released in Lagent soon. InternLM2.5 has better tool utilization-related capabilities in instruction following, tool selection and reflection. See examples.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">InternLM-XComposer-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. IXC-2.5 is trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation. This long-context capability allows IXC-2.5 to perform exceptionally well in tasks requiring extensive input and output contexts.</span><br><span class="line"></span><br><span class="line">Ultra-High Resolution Understanding: IXC-2.5 enhances the dynamic resolution solution proposed in IXC2-4KHD with a native 560 Ã— 560 ViT vision encoder, supporting high-resolution images with any aspect ratio.</span><br><span class="line"></span><br><span class="line">Fine-Grained Video Understanding: IXC-2.5 treats videos as a ultra-high-resolution composite picture consisting of tens to hundreds of frames, allowing it to capture fine details through dense sampling and higher resolution for each frame.</span><br><span class="line"></span><br><span class="line">Multi-Turn Multi-Image Dialogue: IXC-2.5 supports free-form multi-turn multi-image dialogue, allowing it to naturally interact with humans in multi-round conversations.</span><br><span class="line"></span><br><span class="line">Webpage Crafting: IXC-2.5 can be readily applied to create webpages by composing source code (HTML, CSS, and JavaScript) following text-image instructions.</span><br><span class="line"></span><br><span class="line">Composing High-Quality Text-Image Articles: IXC-2.5 leverages specially designed Chain-of-Thought (CoT) and Direct Preference Optimization (DPO) techniques to significantly enhance the quality of its written content.</span><br><span class="line"></span><br><span class="line">Awesome performance: IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://developer.nvidia.com/blog/introducing-dora-a-high-performing-alternative-to-lora-for-fine-tuning/</span><br><span class="line">Generative AI</span><br><span class="line"></span><br><span class="line">English</span><br><span class="line">Introducing DoRA, a High-Performing Alternative to LoRA for Fine-Tuning</span><br><span class="line">Jun 28, 2024</span><br><span class="line">By Min-Hung Chen</span><br><span class="line"></span><br><span class="line">+10</span><br><span class="line">Like</span><br><span class="line"> Discuss (0)</span><br><span class="line"></span><br><span class="line">LTFRE</span><br><span class="line">Full fine-tuning (FT) is commonly employed to tailor general pretrained models for specific downstream tasks. To reduce the training cost, parameter-efficient fine-tuning (PEFT) methods have been introduced to fine-tune pretrained models with a minimal number of parameters. Among these, Low-Rank Adaptation (LoRA) and its variants have gained considerable popularity because they avoid additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning.</span><br><span class="line"></span><br><span class="line">NVIDIA Research Taiwan and the NVIDIA Learning and Perception Research Group developed Weight-Decomposed Low-Rank Adaptation (DoRA), which could be the default replacement for LoRA. DoRA improves both the learning capacity and stability of LoRA, without introducing any additional inference overhead.</span><br><span class="line"></span><br><span class="line">DoRA consistently outperforms LoRA across a wide variety of large language model (LLM) and vision language model (VLM) tasks, such as common-sense reasoning (+3.7/+1.0 on Llama 7B/13B, +2.9 on Llama 2 7B, and +4.4 on Llama 3 8B), Multi-Turn (MT) Benchmark (+0.4/+0.3 for Llama/Llama 2 7B), image/video-text understanding (+0.9/+1.9 on VL-BART), and visual instruction tuning (+0.6 on LLaVA 7B). DoRA has also been demonstrated in other tasks, including compression-aware LLM and text-to-image generation. This work has been accepted to ICML 2024 as an oral paper (1.5% acceptance rate).</span><br><span class="line"></span><br><span class="line">Diagram showing that DoRA consistently outperforms LoRA on various tasks (LLM, VLM, LVLM) and backbones (Llama 2 and 3).</span><br><span class="line">Figure 1. Comparison of DoRA and LoRA on various tasks and backbones</span><br><span class="line">How does DoRA work?</span><br><span class="line">DoRA begins by decomposing the pretrained weight into its magnitude and directional components and then fine-tunes both. Given the substantial size of the directional component in terms of parameters, DoRA exploits LoRA for â€Œdirectional adaptation to enable efficient fine-tuning, as illustrated in Figure 2. Finally, DoRA can be merged with the pretrained weight before inference, thereby avoiding the introduction of additional latency.</span><br><span class="line"></span><br><span class="line">Diagram of proposed DoRA, which decomposes the pretrained weight into magnitude and direction components for fine-tuning, especially with LoRA to efficiently update the direction component.</span><br><span class="line">Figure 2. An overview of DoRA</span><br><span class="line">How does DoRA affect model training?</span><br><span class="line">To investigate how DoRA affects model training, the magnitude and directional differences (âˆ†D, âˆ†M) between the DoRA weight Wâ€™ and the pretrained weight W0 are visualized in Figure 3 (so as FT and LoRA). From the regression line for (âˆ†D, âˆ†M) of both DoRA and FT, a distinct negative slope characterizes DoRA and FT, instead of a clear positive correlation shown by LoRA. Different markers represent matrices of different training steps and different colors represent the matrices of each layer.</span><br><span class="line"></span><br><span class="line">Figure shows magnitude and direction updates of FT, LoRA, and DoRA of the query matrices across different layers and intermediate steps. DoRA and FT show a distinct negative slope while LoRA shows a clear positive correlation, indicating that DoRA has a learning capacity closely resembling FT.</span><br><span class="line">Figure 3. Magnitude and direction updates of FT, LoRA, and DoRA</span><br><span class="line">DoRA demonstrates the ability to make only substantial directional adjustments with relatively minimal changes in magnitude or the reverse, while showing learning patterns closer to FT. This signifies its superior learning capacity over LoRA. For more qualitative and mathematical analyses, see DoRA: Weight-Decomposed Low-Rank Adaptation.</span><br><span class="line"></span><br><span class="line">Performance</span><br><span class="line">DoRA outperforms LoRA across a wide variety of models, including LLM, VLM, compressed LLM, and diffusion models.</span><br><span class="line"></span><br><span class="line">Large language models</span><br><span class="line">DoRA significantly outperforms LoRA in terms of the overall commonsense reasoning ability, as shown in Table 1. Moreover, DoRA can provide better conversation and instruction-following capabilities than LoRA, as demonstrated by the MT Benchmark in Table 2.</span><br><span class="line"></span><br><span class="line">Model	# Params (%)	BoolQ 	PIQA	SIQA	HellaSwag 	WinoGrande 	ARC-e 	ARC-c 	OBQA 	Avg.</span><br><span class="line">ChatGPT-3.5	â€“	73.1	85.4	68.5	78.5	66.1	89.8	79.9	74.8	77.0</span><br><span class="line">Llama-LoRA	0.83	68.9	80.7	77.4	78.1	78.8	77.8	61.3	74.8	74.7</span><br><span class="line">Llama-DoRA (Ours)	0.84	69.7	83.4	78.6	87.2	81.0	81.9	66.2	79.2	78.4</span><br><span class="line">Llama 2-LoRA	0.83	69.8	79.9	79.5	83.6	82.6	79.8	64.7	81.0	77.6</span><br><span class="line">Llama 2-DoRA (Ours)	0.84	72.0	83.1	79.9	89.1	83.0	84.5	71.0	81.2	80.5</span><br><span class="line">Llama 3-LoRA	0.83	70.8	85.2	79.9	91.7	84.3	84.2	71.2	79.0	80.8</span><br><span class="line">Llama 3-DoRA (Ours)	0.84	74.6	89.3	79.9	95.5	85.6	90.5	80.4	85.8	85.2</span><br><span class="line">Table 1. Comparison of LoRA and DoRA on the commonsense reasoning benchmark</span><br><span class="line">Model	# Params (%)	Score</span><br><span class="line">Llama-LoRA	2.31	5.1</span><br><span class="line">Llama-DoRA (Ours)	2.33	5.5</span><br><span class="line">Llama-VeRA	0.02	4.3</span><br><span class="line">Llama-DVoRA (Ours)	0.04	5.0</span><br><span class="line">Llama 2-LoRA	2.31	5.7</span><br><span class="line">Llama 2-DoRA (Ours)	2.33	6.0</span><br><span class="line">Llama 2-VeRA	0.02	5.5</span><br><span class="line">Llama 2-DVoRA (Ours)	0.04	6.0</span><br><span class="line">Table 2. Comparison of LoRA and DoRA on MT-Bench (scored by GPT-4). DVoRA is obtained by integrating DoRA on VeRA</span><br><span class="line">Vision language models</span><br><span class="line">In addition to pure natural language processing (NLP), DoRA also outperforms LoRA in terms of image-text understanding (Table 3), video-text understanding (Table 4), and visual instruction tuning (Table 5) abilities.</span><br><span class="line"></span><br><span class="line">Model	# Params (%)	VQAv2	GQA	NVLR2	COCO Cap.	Avg.</span><br><span class="line">VLBART-LoRA	5.93	65.2	53.6	71.9	115.3	76.5</span><br><span class="line">VLBART-DoRA (Ours)	5.96	65.8	54.7	73.1	115.9	77.4</span><br><span class="line">Table 3. Comparison of LoRA and DoRA on image-text understanding tasks</span><br><span class="line">Model	# Params (%)	TVQA 	How2QA 	TVC 	YC2C	Avg.</span><br><span class="line">VLBART-LoRA	5.17	75.5	72.9	44.6	140.9	83.5</span><br><span class="line">VLBART-DoRA (Ours)	5.19	76.3	74.1	45.8	145.4	85.4</span><br><span class="line">Table 4. Comparison of LoRA and DoRA on video-text understanding tasks</span><br><span class="line">Model	# Params (%)	VQAv2 	GQA 	Vis-Wiz</span><br><span class="line">SQA 	VQAT 	POPE 	MMBench 	Avg.</span><br><span class="line">LLaVA-LoRA	4.61	79.1	62.9	47.8	68.4	58.2	86.4	66.1	66.9</span><br><span class="line">LLaVA-DoRA (Ours)	4.63	78.6	62.9	52.2	69.9	57.0	87.2	66.1	67.6</span><br><span class="line">Table 5. Comparison of LoRA and DoRA on visual instruction tuning tasks</span><br><span class="line">Compression-aware LLMs</span><br><span class="line">To further decrease the memory demands of PEFT fine-tuning, QLoRA suggests quantizing the pretrained model to 4-bit and fine-tuning LoRA on top of the frozen low-bit backbone. With DoRA, which narrows the gap between LoRA and FT, it is natural to also explore whether DoRA can enhance the accuracy of LoRA within the QLoRA framework.</span><br><span class="line"></span><br><span class="line">Recently, our team collaborated with several researchers in Answer.AI on their QDoRA project, which substitutes the LoRA component in QLoRA with DoRA. The results show that QDoRA outperforms FT, QLoRA on both Llama 2 and Llama 3, respectively (Figure 4).</span><br><span class="line"></span><br><span class="line">Graph showing that QDoRA significantly outperforms QLoRA on the Math Problem Benchmark, Orca-Math, with either Llama2 or Llama3 backbone. QDoRA+Llama2 has comparable results with QLoRA+Llama3. Moreover, QDoRA outperforms FT, which requires much larger memory.</span><br><span class="line">Figure 4. Accuracy comparison of QDoRA and other methods on the Orca-Math dataset including 100K training samples</span><br><span class="line">Text-to-image generation</span><br><span class="line">DoRA can also be applied on DreamBooth for text-to-image personalization with the advanced training scripts developed by Hugging Face. Testing results on the challenging 3d_icon and lego_set datasets show that DoRA can obtain significantly better personalization results than LoRA under the same training configurations (Figure 5).</span><br><span class="line"></span><br><span class="line">Two sets of images showing that on the challenging 3d_icon and lego_set datasets, DoRA can obtain significantly better personalization results than LoRA under the same DreamBooth training configurations.</span><br><span class="line">Figure 5. Personalization results using DreamBooth plus DoRA on the challenging 3D Icon (top) and Lego (bottom) datasets</span><br><span class="line">Summary</span><br><span class="line">DoRA is a generally efficient and effective training technique and will be supported soon by various NVIDIA services, platforms, and frameworks. DoRA is a fine-tuning method that is compatible with LoRA and its variants and exhibits a closer resemblance to FT learning behavior. DoRA consistently outperforms LoRA across various fine-tuning tasks and model architectures. Moreover, DoRA can be considered a costless replacement for LoRA, as its decomposed magnitude and direction components can be merged back into the pretrained weight after the training, ensuring that there is no extra inference overhead. We hope DoRA can help NVIDIA effectively adapt various foundation models to diverse applications in NVIDIA Metropolis, NVIDIA NeMo, NVIDIA NIM, NVIDIA TensorRT, audiovisual, robotics, generative AI, and more.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/facebook/multi-token-prediction</span><br><span class="line">META</span><br><span class="line">July 4, 2024</span><br><span class="line"></span><br><span class="line">In April, we published a research paper on a new approach for building better and faster LLMs by using multi-token prediction. Using this approach, we can train language models to predict multiple future words at once, improving model capabilities and training efficiency while allowing for faster inference.</span><br><span class="line">In the spirit of responsible open science, weâ€™ve released pre-trained models for code completion using this approach to enable further exploration in the research community.</span><br><span class="line">Get the model on Hugging Face â¡ï¸</span><br><span class="line">https://go.fb.me/dm1giu</span><br><span class="line">More on this approach â¡ï¸</span><br><span class="line">https://go.fb.me/x1zhdq</span><br><span class="line">Multi-token prediction models and baselines</span><br><span class="line">Models accompanying the research paper &quot;Better &amp; Faster Large Language Models via Multi-token Prediction&quot; (https://arxiv.org/abs/2404.19737).</span><br><span class="line"></span><br><span class="line">Included are the following four 7B parameter models trained on code:</span><br><span class="line"></span><br><span class="line">baseline model (n=1) trained on 200B tokens of code: 7B_200B_1/</span><br><span class="line">multi-token prediction model (n=4) trained on 200B tokens of code: 7B_200B_4/</span><br><span class="line">baseline model (n=1) trained on 1T tokens of code: 7B_1T_1/</span><br><span class="line">multi-token prediction model (n=4) trained on 1T tokens of code: 7B_1T_4/</span><br><span class="line">Tokenizer: standard Llama 2 SentencePiece tokenizer in tokenizer.model.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2407.01370</span><br><span class="line">Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems</span><br><span class="line">Published on Jul 2</span><br><span class="line">Â·</span><br><span class="line">Submitted by</span><br><span class="line">philschmid</span><br><span class="line">on Jul 3</span><br><span class="line">#1 Paper of the day</span><br><span class="line">Authors:</span><br><span class="line"></span><br><span class="line">Philippe Laban</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Alexander R. Fabbri</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Caiming Xiong</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Chien-Sheng Wu</span><br><span class="line">Abstract</span><br><span class="line">LLMs and RAG systems are now capable of handling millions of input tokens or more. However, evaluating the output quality of such systems on long-context tasks remains challenging, as tasks like Needle-in-a-Haystack lack complexity. In this work, we argue that summarization can play a central role in such evaluation. We design a procedure to synthesize Haystacks of documents, ensuring that specific insights repeat across documents. The &quot;Summary of a Haystack&quot; (SummHay) task then requires a system to process the Haystack and generate, given a query, a summary that identifies the relevant insights and precisely cites the source documents. Since we have precise knowledge of what insights should appear in a haystack summary and what documents should be cited, we implement a highly reproducible automatic evaluation that can score summaries on two aspects - Coverage and Citation. We generate Haystacks in two domains (conversation, news), and perform a large-scale evaluation of 10 LLMs and corresponding 50 RAG systems. Our findings indicate that SummHay is an open challenge for current systems, as even systems provided with an Oracle signal of document relevance lag our estimate of human performance (56\%) by 10+ points on a Joint Score. Without a retriever, long-context LLMs like GPT-4o and Claude 3 Opus score below 20% on SummHay. We show SummHay can also be used to study enterprise RAG systems and position bias in long-context models. We hope future systems can equal and surpass human performance on SummHay.</span><br><span class="line">How good are LLMs in a long context, and do we need RAG? ğŸ¤” Summary of a Haystack (SummHay) tries to solve the limitations of â€œNeedle in a Haystackâ€ by focusing on challenging information extraction. Google DeepMind Gemini 1.5 pro performs the best with and without RAG (37-44%), while OpenAI GPT-4o and Anthropic Claude 3 Opus are below 20%. ğŸ‘€</span><br><span class="line">SummHay includes 92 subtopics for evaluating long-context LLMs and RAG. It was curated by synthesizing &quot;Haystacks&quot; with specific insights repeated across documents. LLMs need to generate summaries that identify relevant insights and accurately cite source documents. Performance is measured using Coverage (how well the summary captures the important insights) and Citation (how accurately the summary cites the source documents).</span><br><span class="line">Insights</span><br><span class="line">ğŸ’¡ RAG always improves the performance of LLMs if correct information is retrieved</span><br><span class="line">ğŸ“Š Evaluated 10 LLMs and 50 RAG systems, including GPT-4o, Claude 3 Opus, and Gemini-1.5-pro</span><br><span class="line">ğŸ† Claude 3 Opus achieved the highest Coverage; Gemini-1.5-pro highest citation</span><br><span class="line">ğŸ¯ Gemini-1.5-pro is the best LLM without RAG with 37.8; Claude 3 Sonnet 18.3; GPT-4o 11.4;</span><br><span class="line">âš™ï¸ Gemini-1.5-pro + Oracle RAG achieves 44.6, whereas humans achieved 56.1.</span><br><span class="line">ğŸ”¢ Full input is around 100,000 tokens, while Oracle RAG is reduced to 15,000 tokens</span><br><span class="line">ğŸ“ˆ Smaller Models like Claude 3 Haiku or Gemini 1.5 Flash outperform bigger LLMs (GPT-4o, Claude 3 Opus) with RAG</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2407.01219</span><br><span class="line">[Submitted on 1 Jul 2024]</span><br><span class="line">Searching for Best Practices in Retrieval-Augmented Generation</span><br><span class="line">Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, Xuanjing Huang</span><br><span class="line">Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a &quot;retrieval as generation&quot; strategy.</span><br><span class="line">Subjects:	Computation and Language (cs.CL)</span><br><span class="line">Cite as:	arXiv:2407.01219 [cs.CL]</span><br><span class="line"> 	(or arXiv:2407.01219v1 [cs.CL] for this version)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/spaces/merve/RT-DETR-tracking-coco</span><br><span class="line">RT-DETR is now supported in Hugging Face Transformers! ğŸ™Œ</span><br><span class="line">RT-DETR, short for â€œReal-Time DEtection TRansformerâ€, is a computer vision model developed at Peking University and Baidu, Inc. capable of real-time object detection. The authors claim better performance than YOLO models in both speed and accuracy. The model comes with an Apache 2.0 license, meaning people can freely use it for commercial applications. ğŸ”¥</span><br><span class="line">RT-DETR is a follow-up work of DETR, a model developed by AI at Meta that successfully used Transformers for the first time for object detection. The latter has been in the Transformers library since 2020. After this, lots of improvements have been made to enable faster convergence and inference speed. RT-DETR is an important example of that as it unlocks real-time inference at high accuracy!</span><br><span class="line">https://huggingface.co/papers/2304.08069</span><br><span class="line">DETRs Beat YOLOs on Real-time Object Detection</span><br><span class="line">Published on Apr 17, 2023</span><br><span class="line">Authors:</span><br><span class="line">Yian Zhao</span><br><span class="line">,</span><br><span class="line">Wenyu Lv</span><br><span class="line">,</span><br><span class="line">Shangliang Xu</span><br><span class="line">,</span><br><span class="line">Jinman Wei</span><br><span class="line">,</span><br><span class="line">Guanzhong Wang</span><br><span class="line">,</span><br><span class="line">Qingqing Dang</span><br><span class="line">,</span><br><span class="line">Yi Liu</span><br><span class="line">,</span><br><span class="line">Jie Chen</span><br><span class="line">Abstract</span><br><span class="line">The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1% / 54.3% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3% / 56.2% AP. The project page: https://zhao-yian.github.io/RTDETR.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/andrewyng/translation-agent</span><br><span class="line">Translation Agent: Agentic translation using reflection workflow</span><br><span class="line">This is a Python demonstration of a reflection agentic workflow for machine translation. The main steps are:</span><br><span class="line"></span><br><span class="line">Prompt an LLM to translate a text from source_language to target_language;</span><br><span class="line">Have the LLM reflect on the translation to come up with constructive suggestions for improving it;</span><br><span class="line">Use the suggestions to improve the translation.</span><br><span class="line">Customizability</span><br><span class="line">By using an LLM as the heart of the translation engine, this system is highly steerable. For example, by changing the prompts, it is easier using this workflow than a traditional machine translation (MT) system to:</span><br><span class="line"></span><br><span class="line">Modify the output&#x27;s style, such as formal/informal.</span><br><span class="line">Specify how to handle idioms and special terms like names, technical terms, and acronyms. For example, including a glossary in the prompt lets you make sure particular terms (such as open source, H100 or GPU) are translated consistently.</span><br><span class="line">Specify specific regional use of the language, or specific dialects, to serve a target audience. For example, Spanish spoken in Latin America is different from Spanish spoken in Spain; French spoken in Canada is different from how it is spoken in France.</span><br><span class="line">This is not mature software, and is the result of Andrew playing around with translations on weekends the past few months, plus collaborators (Joaquin Dominguez, Nedelina Teneva, John Santerre) helping refactor the code.</span><br><span class="line"></span><br><span class="line">According to our evaluations using BLEU score on traditional translation datasets, this workflow is sometimes competitive with, but also sometimes worse than, leading commercial offerings. However, weâ€™ve also occasionally gotten fantastic results (superior to commercial offerings) with this approach. We think this is just a starting point for agentic translations, and that this is a promising direction for translation, with significant headroom for further improvement, which is why weâ€™re releasing this demonstration to encourage more discussion, experimentation, research and open-source contributions.</span><br><span class="line"></span><br><span class="line">If agentic translations can generate better results than traditional architectures (such as an end-to-end transformer that inputs a text and directly outputs a translation) -- which are often faster/cheaper to run than our approach here -- this also provides a mechanism to automatically generate training data (parallel text corpora) that can be used to further train and improve traditional algorithms. (See also this article in The Batch on using LLMs to generate training data.)</span><br><span class="line"></span><br><span class="line">Comments and suggestions for how to improve this are very welcome!</span><br><span class="line"></span><br><span class="line">Getting Started</span><br><span class="line">To get started with translation-agent, follow these steps:</span><br><span class="line"></span><br><span class="line">Installation:</span><br><span class="line">The Poetry package manager is required for installation. Poetry Installation Depending on your environment, this might work:</span><br><span class="line">pip install poetry</span><br><span class="line">A .env file with a OPENAI_API_KEY is required to run the workflow. See the .env.sample file as an example.</span><br><span class="line">git clone https://github.com/andrewyng/translation-agent.git</span><br><span class="line">cd translation-agent</span><br><span class="line">poetry install</span><br><span class="line">poetry shell # activates virtual environment</span><br><span class="line">Usage:</span><br><span class="line">import translation_agent as ta</span><br><span class="line">source_lang, target_lang, country = &quot;English&quot;, &quot;Spanish&quot;, &quot;Mexico&quot;</span><br><span class="line">translation = ta.translate(source_lang, target_lang, source_text, country)</span><br><span class="line">See examples/example_script.py for an example script to try out.</span><br><span class="line"></span><br><span class="line">License</span><br><span class="line">Translation Agent is released under the MIT License. You are free to use, modify, and distribute the code for both commercial and non-commercial purposes.</span><br><span class="line"></span><br><span class="line">Ideas for extensions</span><br><span class="line">Here are ideas we havenâ€™t had time to experiment with but that we hope the open-source community will:</span><br><span class="line"></span><br><span class="line">Try other LLMs. We prototyped this primarily using gpt-4-turbo. We would love for others to experiment with other LLMs as well as other hyperparameter choices and see if some do better than others for particular language pairs.</span><br><span class="line">Glossary Creation. Whatâ€™s the best way to efficiently build a glossary -- perhaps using an LLM -- of the most important terms that we want translated consistently? For example, many businesses use specialized terms that are not widely used on the internet and that LLMs thus donâ€™t know about, and there are also many terms that can be translated in multiple ways. For example, â€open sourceâ€ in Spanish can be â€œCÃ³digo abiertoâ€ or â€œFuente abiertaâ€; both are fine, but itâ€™d better to pick one and stick with it for a single document.</span><br><span class="line">Glossary Usage and Implementation. Given a glossary, whatâ€™s the best way to include it in the prompt?</span><br><span class="line">Evaluations on different languages. How does its performance vary in different languages? Are there changes that make it work better for particular source or target languages? (Note that for very high levels of performance, which MT systems are approaching, weâ€™re not sure if BLEU is a great metric.) Also, its performance on lower resource languages needs further study.</span><br><span class="line">Error analysis. Weâ€™ve found that specifying a language and a country/region (e.g., â€œSpanish as colloquially spoken in Mexicoâ€) does a pretty good job for our applications. Where does the current approach fall short? Weâ€™re also particularly interested in understanding its performance on specialized topics (like law, medicine) or special types of text (like movie subtitles) to understand its limitations.</span><br><span class="line">Better evals. Finally, we think better evaluations (evals) is a huge and important research topic. As with other LLM applications that generate free text, current evaluation metrics appear to fall short. For example, we found that even on documents where our agentic workflow captures context and terminology better, resulting in translations that our human raters prefer over current commercial offerings, evaluation at the sentence level (using the FLORES dataset) resulted in the agentic system scoring lower on BLEU. Can we design better metrics (perhaps using an LLM to evaluate translations?) that capture translation quality at a document level that correlates better with human preferences?</span><br><span class="line">Related work</span><br><span class="line">A few academic research groups are also starting to look at LLM-based and agentic translation. We think itâ€™s early days for this field!</span><br><span class="line"></span><br><span class="line">ChatGPT MT: Competitive for High- (but not Low-) Resource Languages, Robinson et al. (2023), https://arxiv.org/pdf/2309.07423</span><br><span class="line">How to Design Translation Prompts for ChatGPT: An Empirical Study, Gao et al. (2023), https://arxiv.org/pdf/2304.02182v2</span><br><span class="line">Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts, Wu et al. (2024), https://arxiv.org/pdf/2405.11804</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-07-05</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/07/05/2024-7-5-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 7ì›” 5ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/07/10/2024-7-10-AI-NEWS/" title="2024ë…„ 7ì›” 10ì¼ AI ì†Œì‹">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/07/02/Config-2024-Figma-product-launch-keynote/" title="Config 2024: Figma product launch keynote">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>