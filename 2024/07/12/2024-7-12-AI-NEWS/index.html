<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 7ì›” 12ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="Summaryì˜¤ëŠ˜ì˜ AI ì†Œì‹ì—ì„œëŠ” PyTorch, Microsoft, OpenAI, Amazon, Apple, ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ ì—°êµ¬ ê¸°ê´€ì—ì„œ ë°œí‘œëœ ìµœì‹  ê¸°ìˆ ê³¼ ì—°êµ¬ ê²°ê³¼ë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤. PyTorchì—ì„œëŠ” FlashAttention-3ë¥¼ ë°œí‘œí•˜ë©° ì£¼ëª©í•  ë§Œí•œ ì„±ëŠ¥ í–¥ìƒ"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 7ì›” 12ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>ì˜¤ëŠ˜ì˜ AI ì†Œì‹ì—ì„œëŠ” PyTorch, Microsoft, OpenAI, Amazon, Apple, ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ ì—°êµ¬ ê¸°ê´€ì—ì„œ ë°œí‘œëœ ìµœì‹  ê¸°ìˆ ê³¼ ì—°êµ¬ ê²°ê³¼ë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤. PyTorchì—ì„œëŠ” FlashAttention-3ë¥¼ ë°œí‘œí•˜ë©° ì£¼ëª©í•  ë§Œí•œ ì„±ëŠ¥ í–¥ìƒì„ ì´ë£¨ì—ˆê³ , MicrosoftëŠ” AgentInstructë¥¼ í†µí•´ LLMì˜ ìƒˆë¡œìš´ í•™ìŠµ ë°©ë²•ì„ ì†Œê°œí–ˆìŠµë‹ˆë‹¤. OpenAIì™€ Los Alamos National LaboratoryëŠ” AIì˜ ì•ˆì „í•œ ì‹¤í—˜ì‹¤ ì‚¬ìš©ì„ ìœ„í•œ í˜‘ë ¥ì„ ë°œí‘œí–ˆìœ¼ë©°, Amazonì€ Anthropicì˜ Claude 3 Haiku ëª¨ë¸ì˜ ë¯¸ì„¸ ì¡°ì • ë°©ë²•ì„ ê³µìœ í–ˆìŠµë‹ˆë‹¤. Appleì€ ì½”ë“œ ìƒì„± ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” RLAIF í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œí–ˆê³ , ë‹¤ì–‘í•œ ì—°êµ¬íŒ€ë“¤ì€ VLMì˜ ì‹œê°ì  í•œê³„ì™€ ìˆ˜í•™ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.</p>
<h2 id="ì£¼ìš”-ë‰´ìŠ¤"><a href="#ì£¼ìš”-ë‰´ìŠ¤" class="headerlink" title="ì£¼ìš” ë‰´ìŠ¤"></a>ì£¼ìš” ë‰´ìŠ¤</h2><h3 id="PyTorch-FlashAttention-3-ë°œí‘œ"><a href="#PyTorch-FlashAttention-3-ë°œí‘œ" class="headerlink" title="PyTorch, FlashAttention-3 ë°œí‘œ"></a>PyTorch, FlashAttention-3 ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://pytorch.org/blog/flashattention-3/">ë§í¬</a>, 2024ë…„ 7ì›” 11ì¼,</p>
<ul>
<li>FlashAttention-3ëŠ” ë¹„ë™ê¸°ì„±ê³¼ ì €ì •ë°€ë„ë¥¼ í™œìš©í•˜ì—¬ ì£¼ëª©ì„ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” ìƒˆë¡œìš´ ê¸°ìˆ ì„ ë„ì….</li>
<li>Tensor Coresì™€ TMAë¥¼ ì´ìš©í•´ ë°ì´í„° ì´ë™ê³¼ ê³„ì‚°ì„ ê²¹ì¹˜ë„ë¡ ì„¤ê³„.</li>
<li>FP16ì—ì„œ FlashAttention-2ë³´ë‹¤ 1.5-2.0ë°° ë¹ ë¥´ë©° H100 GPUì˜ ì´ë¡ ì  ìµœëŒ€ FLOPSì˜ 75%ë¥¼ ë‹¬ì„±.</li>
<li>FP8ì—ì„œëŠ” ìµœëŒ€ 1.2 PFLOPSì— ë„ë‹¬í•˜ë©° ê¸°ë³¸ FP8 ì£¼ëª©ë³´ë‹¤ 2.6ë°° ì‘ì€ ì˜¤ë¥˜ë¥¼ ë‚˜íƒ€ëƒ„.</li>
<li>FlashAttention-3ì€ <a target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention">GitHub</a>ì—ì„œ ì´ìš© ê°€ëŠ¥.</li>
</ul>
<h3 id="Microsoft-AgentInstruct-ë°œí‘œ"><a href="#Microsoft-AgentInstruct-ë°œí‘œ" class="headerlink" title="Microsoft, AgentInstruct ë°œí‘œ"></a>Microsoft, AgentInstruct ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2407.03502">ë§í¬</a>, 2024ë…„ 7ì›” 4ì¼,</p>
<ul>
<li>AgentInstructëŠ” LLM ì—ì´ì „íŠ¸ê°€ ìƒì„±í•œ í•©ì„± ë°ì´í„°ë¥¼ í†µí•´ ìƒˆë¡œìš´ ê¸°ìˆ ì´ë‚˜ í–‰ë™ì„ ê°€ë¥´ì¹˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•.</li>
<li>Orca-3 ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì•½ 20% í–¥ìƒì‹œí‚¤ê³  GPT-4ì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì„.</li>
<li>ì—¬ëŸ¬ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¥¼ í†µí•´ ì›ì‹œ ë°ì´í„°ë¥¼ ê³ í’ˆì§ˆì˜ í•™ìŠµ ë°ì´í„°ë¡œ ë³€í™˜.</li>
<li>AgentInstructëŠ” ì›ì‹œ ë¹„êµ¬ì¡°í™” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±.</li>
</ul>
<h3 id="OpenAI-Los-Alamos-National-Laboratoryì™€-í˜‘ë ¥"><a href="#OpenAI-Los-Alamos-National-Laboratoryì™€-í˜‘ë ¥" class="headerlink" title="OpenAI, Los Alamos National Laboratoryì™€ í˜‘ë ¥"></a>OpenAI, Los Alamos National Laboratoryì™€ í˜‘ë ¥</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/openai-and-los-alamos-national-laboratory-work-together/">ë§í¬</a>, 2024ë…„ 7ì›” 10ì¼,</p>
<ul>
<li>OpenAIì™€ Los Alamos National LaboratoryëŠ” ì‹¤í—˜ì‹¤ í™˜ê²½ì—ì„œ AI ëª¨ë¸ì„ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í‰ê°€ ì—°êµ¬ë¥¼ ê³µë™ìœ¼ë¡œ ì§„í–‰.</li>
<li>GPT-4o ëª¨ë¸ì´ ìƒë¬¼í•™ì  ì—°êµ¬ë¥¼ ì§€ì›í•  ìˆ˜ ìˆëŠ”ì§€ í‰ê°€í•˜ëŠ” ìµœì´ˆì˜ ì‹¤í—˜ì„ ìˆ˜í–‰í•  ì˜ˆì •.</li>
<li>AIì˜ ë‹¤ì¤‘ ëª¨ë“œ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ì „ë¬¸ê°€ì™€ ì´ˆë³´ì ëª¨ë‘ë¥¼ ì§€ì›í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ íƒêµ¬.</li>
<li>ì´ë²ˆ í˜‘ë ¥ì€ AI ìƒë¬¼ë³´ì•ˆ í‰ê°€ ì—°êµ¬ì˜ ìµœì „ì„ ì— ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë¨.</li>
</ul>
<h3 id="Amazon-Anthropic-Claude-3-Haiku-ë¯¸ì„¸-ì¡°ì •-ë°©ë²•-ê³µìœ "><a href="#Amazon-Anthropic-Claude-3-Haiku-ë¯¸ì„¸-ì¡°ì •-ë°©ë²•-ê³µìœ " class="headerlink" title="Amazon, Anthropic Claude 3 Haiku ë¯¸ì„¸ ì¡°ì • ë°©ë²• ê³µìœ "></a>Amazon, Anthropic Claude 3 Haiku ë¯¸ì„¸ ì¡°ì • ë°©ë²• ê³µìœ </h3><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/machine-learning/fine-tune-anthropics-claude-3-haiku-in-amazon-bedrock-to-boost-model-accuracy-and-quality/">ë§í¬</a>, 2024ë…„ 7ì›” 10ì¼,</p>
<ul>
<li>Amazon Bedrockì—ì„œ Anthropic Claude 3 Haiku ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ íŠ¹ì • ë„ë©”ì¸ì´ë‚˜ ì‘ì—…ì—ì„œ ìµœì ì˜ ì„±ëŠ¥ì„ ì œê³µ.</li>
<li>ë¯¸ì„¸ ì¡°ì •ì€ ë¶„ë¥˜, êµ¬ì¡°í™”ëœ ì¶œë ¥, ì‚°ì—… ì§€ì‹, ë„êµ¬ ë° API ì‚¬ìš© ë“± ë‹¤ì–‘í•œ ìš©ë„ì— í™œìš© ê°€ëŠ¥.</li>
<li>ì´ˆê¸° í…ŒìŠ¤íŠ¸ì—ì„œ ë¶„ë¥˜ ì •í™•ë„ê°€ 81.5%ì—ì„œ 99.6%ë¡œ í–¥ìƒë˜ê³  ì¿¼ë¦¬ë‹¹ í† í° ìˆ˜ê°€ 89% ê°ì†Œ.</li>
<li>Amazon Bedrockì€ Claude 3 Haiku ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•  ìˆ˜ ìˆëŠ” ìœ ì¼í•œ ê´€ë¦¬í˜• ì„œë¹„ìŠ¤ ì œê³µ.</li>
</ul>
<h3 id="Apple-RLAIF-í”„ë ˆì„ì›Œí¬-ë°œí‘œ"><a href="#Apple-RLAIF-í”„ë ˆì„ì›Œí¬-ë°œí‘œ" class="headerlink" title="Apple, RLAIF í”„ë ˆì„ì›Œí¬ ë°œí‘œ"></a>Apple, RLAIF í”„ë ˆì„ì›Œí¬ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://machinelearning.apple.com/research/applying-rlaif">ë§í¬</a>, 2024ë…„ 7ì›”,</p>
<ul>
<li>Appleì€ ê²½ëŸ‰ LLMì˜ ì½”ë“œ ìƒì„± ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ RLAIF í”„ë ˆì„ì›Œí¬ë¥¼ ë„ì….</li>
<li>í° LLMì˜ í”¼ë“œë°±ì„ í™œìš©í•˜ì—¬ ë³´ìƒ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ì‘ì€ LLMì˜ ì„±ëŠ¥ì„ ê°œì„ .</li>
<li>ì½”ë“œ ì‹¤í–‰ ê°€ëŠ¥ì„±ì—ì„œ 4.5% í–¥ìƒ, 780M íŒŒë¼ë¯¸í„° ëª¨ë¸ì´ 7B íŒŒë¼ë¯¸í„° ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„.</li>
<li>Gorilla ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ì‹¤í—˜ì—ì„œ ì½”ë“œ í’ˆì§ˆì„ ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ì„ í†µí•´ í‰ê°€.</li>
</ul>
<h2 id="ì—°êµ¬-ë‰´ìŠ¤"><a href="#ì—°êµ¬-ë‰´ìŠ¤" class="headerlink" title="ì—°êµ¬ ë‰´ìŠ¤"></a>ì—°êµ¬ ë‰´ìŠ¤</h2><h3 id="ì‹œê°-ì–¸ì–´-ëª¨ë¸ì˜-í•œê³„"><a href="#ì‹œê°-ì–¸ì–´-ëª¨ë¸ì˜-í•œê³„" class="headerlink" title="ì‹œê° ì–¸ì–´ ëª¨ë¸ì˜ í•œê³„"></a>ì‹œê° ì–¸ì–´ ëª¨ë¸ì˜ í•œê³„</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.06581">ë§í¬</a>, 2024ë…„ 7ì›” 11ì¼,</p>
<ul>
<li>Vision language models (VLMs)ëŠ” ê°„ë‹¨í•œ ì‹œê°ì  ì‘ì—…ì—ì„œ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìœ¼ë©°, ì •í™•ë„ê°€ ë‚®ë‹¤ëŠ” ì—°êµ¬ ê²°ê³¼ ë°œí‘œ.</li>
<li>BlindTestë¼ëŠ” ê°„ë‹¨í•œ ì‹œê°ì  ì‘ì—… ì„¸íŠ¸ë¥¼ í†µí•´ VLMì˜ í•œê³„ë¥¼ í‰ê°€.</li>
<li>í…ŒìŠ¤íŠ¸ ê²°ê³¼, ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë„ í‰ê·  56.20%ì˜ ì •í™•ë„ë¥¼ ê¸°ë¡.</li>
<li>ì´ ì—°êµ¬ëŠ” VLMì´ ì •í™•í•œ ê³µê°„ ì •ë³´ì™€ ìˆ˜ ê³„ì‚° ì‘ì—…ì—ì„œ ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤ëŠ” ê²ƒì„ ê°•ì¡°.</li>
</ul>
<h3 id="NuminaMath-7B-TIR-ë°œí‘œ"><a href="#NuminaMath-7B-TIR-ë°œí‘œ" class="headerlink" title="NuminaMath 7B TIR ë°œí‘œ"></a>NuminaMath 7B TIR ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/AI-MO/NuminaMath-7B-TIR">ë§í¬</a>, 2024ë…„ 7ì›” 11ì¼,</p>
<ul>
<li>NuminaMath 7B TIRëŠ” ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì‹œí•˜ë©° AI Math Olympiadì—ì„œ ìš°ìˆ˜í•œ ì„±ì ì„ ê¸°ë¡.</li>
<li>Chain-of-Thought ì¶”ë¡ ê³¼ Python REPLì„ í™œìš©í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°.</li>
<li>ë‘ ë‹¨ê³„ì˜ ê°ë… í•™ìŠµì„ í†µí•´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ìˆ˜í•™ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í–¥ìƒ.</li>
<li>AMC 12 ìˆ˜ì¤€ì˜ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ ë³´ìœ .</li>
</ul>
<h3 id="ìƒˆë¡œìš´-ë””ì½”ë”©-ê¸°ìˆ -DoLa-ë°œí‘œ"><a href="#ìƒˆë¡œìš´-ë””ì½”ë”©-ê¸°ìˆ -DoLa-ë°œí‘œ" class="headerlink" title="ìƒˆë¡œìš´ ë””ì½”ë”© ê¸°ìˆ  DoLa ë°œí‘œ"></a>ìƒˆë¡œìš´ ë””ì½”ë”© ê¸°ìˆ  DoLa ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2309.03883">ë§í¬</a>, 2024ë…„ 7ì›” 11ì¼,</p>
<ul>
<li>DoLa ë””ì½”ë”©ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ í™˜ê° í˜„ìƒì„ ì¤„ì´ëŠ” ë° í¬ê²Œ ê¸°ì—¬.</li>
<li>íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ì €ì¸µê³¼ ê³ ì¸µ ì‚¬ì´ì˜ ë¡œì§“ ë³€í™”ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ìŒ í† í°ì„ ì„ íƒ.</li>
<li>ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ 5% - 20%ì˜ ì„±ëŠ¥ í–¥ìƒ.</li>
<li>ì‹¤í–‰ ì‹œê°„ ì¦ê°€ê°€ ë¯¸ë¯¸í•˜ì—¬ ì‹¤ìš©ì„±ì´ ë†’ìŒ.</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹"><a href="#todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹" class="headerlink" title="(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,"></a>(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,</h1><h2 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAIì—ì„œëŠ” ~~~ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.)</p>
<h2 id="Title"><a href="#Title" class="headerlink" title="Title,"></a>Title,</h2><h3 id="company-name-ì œëª©"><a href="#company-name-ì œëª©" class="headerlink" title="company name, ì œëª©"></a>company name, ì œëª©</h3><p><a href="link">ë§í¬</a>, date,</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
</ul>
<h2 id="Title-1"><a href="#Title-1" class="headerlink" title="Title,"></a>Title,</h2><h3 id="company-name-ì œëª©-1"><a href="#company-name-ì œëª©-1" class="headerlink" title="company name, ì œëª©"></a>company name, ì œëª©</h3><p><a href="link">ë§í¬</a>, date,</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://pytorch.org/blog/flashattention-3/</span><br><span class="line">July 11, 2024</span><br><span class="line">Pytorch</span><br><span class="line"></span><br><span class="line">FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision</span><br><span class="line"></span><br><span class="line">by Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, Tri Dao</span><br><span class="line"></span><br><span class="line">Attention, as a core layer of the ubiquitous Transformer architecture, is a bottleneck for large language models and long-context applications. FlashAttention (and FlashAttention-2) pioneered an approach to speed up attention on GPUs by minimizing memory reads/writes, and is now used by most libraries to accelerate Transformer training and inference. This has contributed to a massive increase in LLM context length in the last two years, from 2-4K (GPT-3, OPT) to 128K (GPT-4), or even 1M (Llama 3). However, despite its success, FlashAttention has yet to take advantage of new capabilities in modern hardware, with FlashAttention-2 achieving only 35% utilization of theoretical max FLOPs on the H100 GPU. In this blogpost, we describe three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) incoherent processing that leverages hardware support for FP8 low-precision.</span><br><span class="line"></span><br><span class="line">Weâ€™re excited to release FlashAttention-3 that incorporates these techniques. Itâ€™s 1.5-2.0x faster than FlashAttention-2 with FP16, up to 740 TFLOPS, i.e., 75% utilization of H100 theoretical max FLOPS. With FP8, FlashAttention-3 reaches close to 1.2 PFLOPS, with 2.6x smaller error than baseline FP8 attention.</span><br><span class="line"></span><br><span class="line">FlashAttention-3 is available at: https://github.com/Dao-AILab/flash-attention</span><br><span class="line">Paper</span><br><span class="line"></span><br><span class="line">FLASHATTENTION RECAP</span><br><span class="line">FlashAttention is an algorithm that reorders the attention computation and leverages tiling and recomputation to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. We use tiling to load blocks of inputs from HBM (GPU memory) to SRAM (fast cache), perform attention with respect to that block, and update the output in HBM. By not writing the large intermediate attention matrices to HBM, we reduce the amount of memory reads/writes, which brings 2-4x wallclock time speedup.</span><br><span class="line"></span><br><span class="line">Here we show a diagram of FlashAttention forward pass: with tiling and softmax rescaling, we operate by blocks and avoid having to read/write from HBM, while obtaining the correct output with no approximation.</span><br><span class="line"></span><br><span class="line">math equations</span><br><span class="line"></span><br><span class="line">NEW HARDWARE FEATURES ON HOPPER GPUS - WGMMA, TMA, FP8</span><br><span class="line">While FlashAttention-2 can achieve up to 70% theoretical max FLOPS on Ampere (A100) GPUs, it does not yet take advantage of new features on Hopper GPUs to maximize performance. We describe some of the new Hopper-specific features here, and why they are important.</span><br><span class="line"></span><br><span class="line">1. WGMMA (Warpgroup Matrix Multiply-Accumulate). This new feature makes use of the new Tensor Cores on Hopper, with much higher throughput1 than the older mma.sync instruction in Ampere (image from the H100 white paper).</span><br><span class="line"></span><br><span class="line">image from the H100 white paper</span><br><span class="line"></span><br><span class="line">2. TMA (Tensor Memory Accelerator). This is a special hardware unit that accelerates the transfer of data between global memory and shared memory, taking care of all index calculation and out-of-bound predication. This frees up registers, which is a valuable resource to increase tile size and efficiency.</span><br><span class="line"></span><br><span class="line">block diagram</span><br><span class="line"></span><br><span class="line">3. Low-precision with FP8. This doubles the Tensor Core throughput (e.g. 989 TFLOPS with FP16 and 1978 TFLOPS with FP8), but trades off accuracy by using fewer bits to represent floating point numbers.</span><br><span class="line"></span><br><span class="line">6x throughput</span><br><span class="line"></span><br><span class="line">FlashAttention-3 makes use of all of these new features of Hopper, using powerful abstractions from NVIDIAâ€™s CUTLASS library.</span><br><span class="line"></span><br><span class="line">By rewriting FlashAttention to use these new features, we can already significantly speed it up (e.g., from 350 TFLOPS in FlashAttention-2 FP16 forward pass to around 540-570 TFLOPS). However, the asynchronous nature of the new instructions on Hopper (WGMMA and TMA) opens up additional algorithmic opportunities to overlap operations and thereby extract even greater performance. For this blogpost, weâ€™ll explain two such techniques specific to attention. The generic technique of warp specialization, with separate producer and consumer warps doing TMA and WGMMA, is well-covered elsewhere in the context of GEMM and works the same here.</span><br><span class="line"></span><br><span class="line">ASYNCHRONY: OVERLAPPING GEMM AND SOFTMAX</span><br><span class="line">Why overlap?</span><br><span class="line"></span><br><span class="line">Attention has GEMMs (those matmuls between Q and K and between attention probability P and V) and softmax as its two main operations. Why do we need to overlap them? Isnâ€™t most of the FLOPS in the GEMMs anyway? As long as the GEMMs are fast (e.g., computed using WGMMA instructions), shouldnâ€™t the GPU be going brrrr?</span><br><span class="line"></span><br><span class="line">The problem is that non-matmul operations are much slower than matmul operations on modern accelerators. Special functions such as exponential (for the softmax) have even lower throughput than floating point multiply-add; they are evaluated by the multi-function unit, a unit separate from floating point multiply-add or matrix multiply-add. As an example, the H100 GPU SXM5 has 989 TFLOPS of FP16 matrix multiply, but only 3.9 TFLOPS (256x less throughput) for special functions2! For head dimension 128, there are 512x more matmul FLOPS than exponential, which means that exponential can take 50% of the time compared to matmul. The situation is even worse for FP8, where the matmul FLOPS are twice as fast yet exponential FLOPS stay the same speed. Ideally we want matmul and softmax to operate in parallel. While the Tensor Cores are busy with matmul, the multi-function units should be calculating exponential!</span><br><span class="line"></span><br><span class="line">Inter-warpgroup overlapping with pingpong scheduling</span><br><span class="line">The first and easiest way to overlap GEMM and softmax is to do nothing at all! The warp schedulers already try to schedule warps so that if some warps are blocked (e.g., waiting for GEMM results), other warps can run. That is, the warp schedulers do some of this overlapping for us, for free.</span><br><span class="line"></span><br><span class="line">However, we can improve on this by doing some of the scheduling manually. As an example, if we have 2 warpgroups (labeled 1 and 2 â€“ each warpgroup is a group of 4 warps), we can use synchronization barriers (bar.sync) so that warpgroup 1 first does its GEMMs (e.g., GEMM1 of one iteration and GEMM0 of the next iteration), and then warpgroup 2 does its GEMMs while warpgroup 1 does its softmax, and so on. This â€œpingpongâ€ schedule is illustrated in the figure below, where the same color denotes the same iteration.</span><br><span class="line"></span><br><span class="line">block chart</span><br><span class="line"></span><br><span class="line">This would allow us to perform the softmax in the shadow of the GEMMs of the other warpgroup. Of course, this figure is just a caricature; in practice the scheduling is not really this clean. Nevertheless, pingpong scheduling can improve FP16 attention forward pass from around 570 TFLOPS to 620 TFLOPS (head dim 128, seqlen 8K).</span><br><span class="line"></span><br><span class="line">Intra-warpgroup overlapping of GEMM and Softmax</span><br><span class="line">Even within one warpgroup, we can have some part of softmax running while the GEMMs of that warpgroup is running. This is illustrated in this figure, where the same color denotes the same iteration.</span><br><span class="line"></span><br><span class="line">block chart</span><br><span class="line"></span><br><span class="line">This pipelining increases throughput from around 620 TFLOPS to around 640-660 TFLOPS for FP16 attention forward, at the cost of higher register pressure. We need more registers to hold both accumulators of the GEMMs, and the input/output of softmax. Overall, we find this technique to offer a favorable tradeoff.</span><br><span class="line"></span><br><span class="line">LOW-PRECISION: REDUCE QUANTIZATION ERROR WITH INCOHERENT PROCESSING</span><br><span class="line">LLM activation can have outliers with much larger magnitude than the rest of the features. These outliers make it difficult to quantize, producing much larger quantization errors. We leverage incoherent processing, a technique used in the quantization literature (e.g. from QuIP) that multiplies the query and key with a random orthogonal matrix to â€œspread outâ€ the outliers and reduce quantization error. In particular, we use the Hadamard transform (with random signs), which can be done per attention head in O(d log d) instead of O(d^2) time, where d is the head dimension. Since the Hadamard transform is memory-bandwidth bound, it can be fused with previous operations such as rotary embedding (also memory-bandwidth bound) â€œfor freeâ€.</span><br><span class="line"></span><br><span class="line">In our experiment where Q, K, V are generated from a standard normal distribution but 0.1% of the entries have large magnitudes (to simulate outliers), we found that incoherent processing can reduce the quantization error by 2.6x. We show numerical error comparison in the table below. Please see the paper for details.</span><br><span class="line"></span><br><span class="line">text diagram</span><br><span class="line"></span><br><span class="line">ATTENTION BENCHMARK</span><br><span class="line">We show some results with FlashAttention-3, and compare it to FlashAttention-2, as well as the implementation in Triton and cuDNN (both of which already use new hardware features of Hopper GPUs).</span><br><span class="line"></span><br><span class="line">For FP16, we see about 1.6x-1.8x speedup over FlashAttention-2</span><br><span class="line"></span><br><span class="line">speed charts</span><br><span class="line"></span><br><span class="line">speed charts</span><br><span class="line"></span><br><span class="line">For FP8, we can reach close to 1.2 PFLOPS!</span><br><span class="line"></span><br><span class="line">speed charts</span><br><span class="line"></span><br><span class="line">DISCUSSION</span><br><span class="line">This blogpost highlights some of the optimizations for FlashAttention available on Hopper GPUs. Other optimizations (e.g., variable length sequences, persistent kernel, and in-kernel transpose for FP8) are covered in the paper.</span><br><span class="line"></span><br><span class="line">We have seen that designing algorithms that take advantage of the hardware they run on can bring significant efficiency gains and unlock new model capabilities such as long context. We look forward to future work on optimization for LLM inference, as well as generalizing our techniques to other hardware architectures.</span><br><span class="line"></span><br><span class="line">We also look forward to FlashAttention-3 being integrated in a future release of PyTorch.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2407.03502</span><br><span class="line">A recipe for Synthetic Data 2.0? Microsoft introduced â€œAgentInstructâ€ a new way to teach an LLM a new skill or behavior from synthetic data generated by LLM Agents. AgentInstruct improved a 7B (Orca-3) model by ~20% across all benchmarks and matched GPT-4 on RAG.  ğŸš€</span><br><span class="line">AgentInstruct employs a multi-agent workflow by LLMs and tools, to transform raw data into high-quality instructional data:</span><br><span class="line">1ï¸âƒ£ Data Collection: Gather raw unstructured text documents and source code files from various sources.</span><br><span class="line">2ï¸âƒ£ Content Transformation Flow: Transform and improve formatting and quality of raw data for generating instructional content using specialized agents, e.g. convert raw text into a meeting text or technical document.</span><br><span class="line">3ï¸âƒ£ Seed Instruction Generation Flow: Generate diverse instructional tasks from the transformed text, leveraging a comprehensive taxonomy with 100+ subcategories, e.g. coding, reading comprehension.</span><br><span class="line">4ï¸âƒ£ Instruction Refinement Flow: Evolve the quality and complexity of generated instructions through iterative refinement by suggester-editor pairs.</span><br><span class="line">Insights:</span><br><span class="line">ğŸ‹ Orca-3 is a trained mistral 7B on 22M data pairs for 17 different capabilities</span><br><span class="line">ğŸ“ˆ Orca-3 +40% on AGIEval, +19% on MMLU; +54% on GSM8K; +38% on BBH; +45% AlpacaEval</span><br><span class="line">ğŸ“‰ Orca-3 achieves 31.34% reduction in hallucinations for summarization tasks</span><br><span class="line">ğŸ“ AgentInstruct uses raw unstructured text as inputs</span><br><span class="line">ğŸ§® AgentInstruct can be used to teach Math, Reasoning, RAG</span><br><span class="line">ğŸš€ Agents can generate data that surpasses the capabilities of the underlying LLMs</span><br><span class="line">AgentInstruct: Toward Generative Teaching with Agentic Flows</span><br><span class="line">Published on Jul 4</span><br><span class="line">Â·</span><br><span class="line">Submitted by</span><br><span class="line">ari9dam</span><br><span class="line">on Jul 10</span><br><span class="line">Authors:</span><br><span class="line"></span><br><span class="line">Arindam Mitra</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Luciano Del Corro</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Guoqing Zheng</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Shweti Mahajan</span><br><span class="line">,</span><br><span class="line">Dany Rouhana</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Andres Codas</span><br><span class="line">,</span><br><span class="line">Yadong Lu</span><br><span class="line">,</span><br><span class="line">Wei-ge Chen</span><br><span class="line">,</span><br><span class="line">Olga Vrousgos</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Corby Rosset</span><br><span class="line">,</span><br><span class="line">Fillipe Silva</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Hamed Khanpour</span><br><span class="line">,</span><br><span class="line">Yash Lara</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Ahmed Awadallah</span><br><span class="line">Abstract</span><br><span class="line">Synthetic data is becoming increasingly important for accelerating the development of language models, both large and small. Despite several successful use cases, researchers also raised concerns around model collapse and drawbacks of imitating other models. This discrepancy can be attributed to the fact that synthetic data varies in quality and diversity. Effective use of synthetic data usually requires significant human effort in curating the data. We focus on using synthetic data for post-training, specifically creating data by powerful models to teach a new skill or behavior to another model, we refer to this setting as Generative Teaching. We introduce AgentInstruct, an extensible agentic framework for automatically creating large amounts of diverse and high-quality synthetic data. AgentInstruct can create both the prompts and responses, using only raw data sources like text documents and code files as seeds. We demonstrate the utility of AgentInstruct by creating a post training dataset of 25M pairs to teach language models different skills, such as text editing, creative writing, tool usage, coding, reading comprehension, etc. The dataset can be used for instruction tuning of any base model. We post-train Mistral-7b with the data. When comparing the resulting model Orca-3 to Mistral-7b-Instruct (which uses the same base model), we observe significant improvements across many benchmarks. For example, 40% improvement on AGIEval, 19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement on BBH and 45% improvement on AlpacaEval. Additionally, it consistently outperforms other models such as LLAMA-8B-instruct and GPT-3.5-turbo.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/openai-and-los-alamos-national-laboratory-work-together/</span><br><span class="line">July 10, 2024</span><br><span class="line"></span><br><span class="line">OpenAI and Los Alamos National Laboratory announce bioscience research partnership</span><br><span class="line">OpenAI and Los Alamos National Laboratory are developing evaluations to understand how multimodal AI models can be used safely by scientists in laboratory settings.</span><br><span class="line"></span><br><span class="line">LosAlamos OpenAI</span><br><span class="line">OpenAI and Los Alamos National Laboratory (LANL) â€“ one of the United Statesâ€™ leading national laboratories â€“ are working together to study how artificial intelligence can be used safely by scientists in laboratory settings to advance bioscientific research. This partnership follows a long tradition of the U.S. public sector, and in particular the national labs, working with the U.S. private sector to ensure advances in innovation translate to advancements in essential areas like health care and bioscience.</span><br><span class="line"></span><br><span class="line">The recent White House Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence(opens in a new window) tasks the U.S. Department of Energyâ€™s national labs to help evaluate the capabilities of frontier AI models, including biological capabilities. This is important to OpenAI because we believe AI has the potential to multiply the speed and impact of science for good. Already, Moderna is leveraging OpenAIâ€™s technology to augment clinical trial development by building a data-analysis assistant designed to help analyze large data sets. Color Health built a new copilot using GPT-4o to assist healthcare providers to make evidence-based decisions about cancer screening and treatment.</span><br><span class="line"></span><br><span class="line">â€œAs a private company dedicated to serving the public interest, weâ€™re thrilled to announce a first-of-its-kind partnership with Los Alamos National Laboratory to study bioscience capabilities,â€ said Mira Murati, OpenAIâ€™s Chief Technology Officer. â€œThis partnership marks a natural progression in our mission, advancing scientific research, while also understanding and mitigating risks.â€</span><br><span class="line"></span><br><span class="line">â€œAI is a powerful tool that has the potential for great benefits in the field of science, but, as with any new technology, comes with risks,â€ said Nick Generous, deputy group leader for Information Systems and Modeling.  &quot;At Los Alamos this work will be led by the laboratory&#x27;s new AI Risks Technical Assessment Group, which will help assess and better understand those risks.â€</span><br><span class="line"></span><br><span class="line">OpenAI and Los Alamos National Laboratoryâ€™s Bioscience Division are working on an evaluation study to assess how frontier models like GPT-4o can assist humans with performing tasks in a physical laboratory setting through multimodal capabilities like vision and voice. This includes biological safety evaluations for GPT-4o and its currently unreleased real-time voice systems to understand how they could be used to support research in bioscience. We believe our upcoming evaluation will be the first of its kind and contribute to state-of-the-art research on AI biosecurity evaluations. It will build upon our existing work on biothreat risks and follow our Preparedness Framework, which outlines our approach to tracking, evaluating, forecasting, and protecting against model risks, and is consistent with our commitments to Frontier AI Safety agreed at the 2024 AI Seoul Summit.</span><br><span class="line"></span><br><span class="line">Our upcoming evaluation with Los Alamos will be the first experiment to test multimodal frontier models in a lab setting by assessing the abilities of both experts and novices to perform and troubleshoot a safe protocol consisting of standard laboratory experimental tasks. These tasks are intended to serve as a proxy for more complex tasks that pose a dual use concern. Tasks may include transformation (e.g., introducing foreign genetic material into a host organism; cell culture (e.g., maintaining and propagating cells in vitro), and cell separation (e.g., through centrifugation). By examining the uplift in task completion and accuracy enabled by GPT-4o, we aim to quantify and assess how frontier models can upskill both existing professionals / PhDs as well as novices in real-world biological tasks.</span><br><span class="line"></span><br><span class="line">These new evaluations extend our previous work in several new dimensions:</span><br><span class="line"></span><br><span class="line">Incorporating wet lab techniques. Written tasks and responses for synthesizing and disseminating compounds were indicative, but do not fully capture the skills required to actually conduct biological benchwork. For example, it may be easy to know one must conduct mass spectrometry or even detail the steps in writing; it is much harder to perform correctly, with real samples.</span><br><span class="line"></span><br><span class="line">Incorporating multiple modalities. Our previous work focused on GPT-4, which involved written outputs. GPT-4oâ€™s ability to reason across modalities and take voice and visual inputs can potentially expedite learning. For example, a user less familiar with all the components of a wet lab setup can simply show their setup to GPT-4o and prompt it with questions, and troubleshoot scenarios visually through the camera instead of needing to convey the situation as a written question.</span><br><span class="line"></span><br><span class="line">Los Alamos National Laboratory has been a pioneer in safety research and we look forward to working together on novel and robust safety evaluations for frontier AI models as capabilities continue to rapidly improve. This cooperative effort not only underscores the potential of multimodal AI models like GPT-4o to support scientific research, but also emphasizes the critical importance of private and public sector collaboration in both leveraging innovation and ensuring safety. As we look forward to the results of these evaluations, we hope that this partnership will help set new standards for AI safety and efficacy in the sciences, paving the way for future innovations that benefit humanity.</span><br><span class="line"></span><br><span class="line">Voice</span><br><span class="line">Speed</span><br><span class="line"></span><br><span class="line">Ember</span><br><span class="line"></span><br><span class="line">Cove</span><br><span class="line"></span><br><span class="line">Juniper</span><br><span class="line"></span><br><span class="line">Breeze</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://aws.amazon.com/blogs/machine-learning/fine-tune-anthropics-claude-3-haiku-in-amazon-bedrock-to-boost-model-accuracy-and-quality/</span><br><span class="line">Fine-tune Anthropicâ€™s Claude 3 Haiku in Amazon Bedrock to boost model accuracy and quality</span><br><span class="line">by Yanyan Zhang, Fang Liu, Sovik Nath, and Carrie Wu | on 10 JUL 2024 | in Amazon Bedrock, Artificial Intelligence, Generative AI, Intermediate (200) | Permalink |  Comments |  Share</span><br><span class="line">Frontier large language models (LLMs) like Anthropic Claude on Amazon Bedrock are trained on vast amounts of data, allowing Anthropic Claude to understand and generate human-like text. Fine-tuning Anthropic Claude 3 Haiku on proprietary datasets can provide optimal performance on specific domains or tasks. The fine-tuning as a deep level of customization represents a key differentiating factor by using your own unique data.</span><br><span class="line"></span><br><span class="line">Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) along with a broad set of capabilities to build generative artificial intelligence (AI) applications, simplifying development with security, privacy, and responsible AI. With Amazon Bedrock custom models, you can customize FMs securely with your data. According to Anthropic, Claude 3 Haiku is the fastest and most cost-effective model on the market for its intelligence category. You can now fine-tune Anthropic Claude 3 Haiku in Amazon Bedrock in a preview capacity in the US West (Oregon) AWS Region. Amazon Bedrock is the only fully managed service that provides you with the ability to fine-tune Anthropic Claude models.</span><br><span class="line"></span><br><span class="line">This post introduces the workflow of fine-tuning Anthropic Claude 3 Haiku in Amazon Bedrock. We first introduce the general concept of fine-tuning and then focus on the important steps in fining-tuning the model, including setting up permissions, preparing for data, commencing the fine-tuning jobs, and conducting evaluation and deployment of the fine-tuned models.</span><br><span class="line"></span><br><span class="line">Solution overview</span><br><span class="line">Fine-tuning is a technique in natural language processing (NLP) where a pre-trained language model is customized for a specific task. During fine-tuning, the weights of the pre-trained Anthropic Claude 3 Haiku model will get updated to enhance its performance on a specific target task. Fine-tuning allows the model to adapt its knowledge to the task-specific data distribution and vocabulary. Hyperparameters like learning rate and batch size need to be tuned for optimal fine-tuning.</span><br><span class="line"></span><br><span class="line">Fine-tuning Anthropic Claude 3 Haiku in Amazon Bedrock offers significant advantages for enterprises. This process enhances task-specific model performance, allowing the model to handle custom use cases with task-specific performance metrics that meet or surpass more powerful models like Anthropic Claude 3 Sonnet or Anthropic Claude 3 Opus. As a result, businesses can achieve improved performance with reduced costs and latency. Essentially, fine-tuning Anthropic Claude 3 Haiku provides you with a versatile tool to customize Anthropic Claude, enabling you to meet specific performance and latency goals efficiently.</span><br><span class="line"></span><br><span class="line">You can benefit from fine-tuning Anthropic Claude 3 Haiku in different use cases, using your own data. The following use cases are well-suited for fine-tuning the Anthropic Claude 3 Haiku model:</span><br><span class="line"></span><br><span class="line">Classification â€“ For example, when you have 10,000 labeled examples and want Anthropic Claude to do really well at this task</span><br><span class="line">Structured outputs â€“ For example, when you need Anthropic Claudeâ€™s response to always conform to a given structure</span><br><span class="line">Industry knowledge â€“ For example, when you need to teach Anthropic Claude how to answer questions about your company or industry</span><br><span class="line">Tools and APIs â€“ For example, when you need to teach Anthropic Claude how to use your APIs really well</span><br><span class="line">In the following sections, we go through the steps of fine-tuning and deploying Anthropic Claude 3 Haiku in Amazon Bedrock using the Amazon Bedrock console and the Amazon Bedrock API.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In testing, we fine-tuned Haiku to moderate comments on internet forums. Fine-tuning improved classification accuracy from 81.5% to 99.6% and reduced tokens per query by 89%.</span><br><span class="line">Early customers, like SK Telecom, have used fine-tuning to create custom Claude 3 models. These models deliver more effective responses across a range</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://machinelearning.apple.com/research/applying-rlaif</span><br><span class="line">Apple</span><br><span class="line">Machine Learning Research</span><br><span class="line">OverviewResearchEventsWork with us</span><br><span class="line">research areaMethods and Algorithms, research areaSpeech and Natural Language Processing | conference ACL</span><br><span class="line">content type Paper | published July 2024</span><br><span class="line">Applying RLAIF for Code Generation with API-usage in Lightweight LLMs</span><br><span class="line">AuthorsSujan Dutta, Sayantan Mahinder, Raviteja Anantha, Bortik Bandyopadhyay</span><br><span class="line"></span><br><span class="line">View publication</span><br><span class="line"></span><br><span class="line">Copy Bibtex</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">This paper was accepted at the Natural Language Reasoning and Structured Explanations workshop at ACL 2024.</span><br><span class="line"></span><br><span class="line">Reinforcement Learning from AI Feedback (RLAIF) has demonstrated significant potential across various domains, including mitigating harm in LLM outputs, enhancing text summarization, and mathematical reasoning. This paper introduces an RLAIF framework for improving the code generation abilities of lightweight (&lt;1B parameters) LLMs. We specifically focus on code generation tasks that require writing appropriate API calls, which is challenging due to the well-known issue of hallucination in LLMs. Our framework extracts AI feedback from a larger LLM (e.g., GPT-3.5) through a specialized prompting strategy and uses this data to train a reward model towards better alignment from smaller LLMs. We run our experiments on the Gorilla dataset and meticulously assess the quality of the model-generated code across various metrics, including AST, ROUGE, and Code-BLEU, and develop a pipeline to compute its executability rate accurately. Our approach significantly enhances the fine-tuned LLM baseline&#x27;s performance, achieving a 4.5% improvement in executability rate. Notably, a smaller LLM model (780M parameters) trained with RLAIF surpasses a much larger fine-tuned baseline with 7B parameters, achieving a 1.0% higher code executability rate.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/pdf/2407.06581</span><br><span class="line">Vision language models are blind</span><br><span class="line">Pooyan Rahmanzadehgervi1â‹† Logan Bolton1â‹†</span><br><span class="line">pooyan.rmz@gmail.com logan.bolton@auburn.edu</span><br><span class="line">Mohammad Reza Taesiri2â‹† Anh Totti Nguyen1</span><br><span class="line">mtaesiri@gmail.com anh.ng8@gmail.com</span><br><span class="line">1 Auburn University, AL, USA</span><br><span class="line">2 University of Alberta, Canada</span><br><span class="line">Abstract. Large language models with vision capabilities (VLMs), e.g.,</span><br><span class="line">GPT-4o and Gemini-1.5 Pro are powering countless image-text applications and scoring high on many vision-understanding benchmarks. We</span><br><span class="line">propose BlindTest, a suite of 7 visual tasks absurdly easy to humans</span><br><span class="line">such as identifying (a) whether two circles overlap; (b) whether two lines</span><br><span class="line">intersect; (c) which letter is being circled in a word; and (d) counting the</span><br><span class="line">number of circles in a Olympic-like logo. Surprisingly, four state-of-theart VLMs are, on average, only 56.20% accurate on our benchmark, with</span><br><span class="line">Sonnet-3.5 being the best (73.77% accuracy). On BlindTest, VLMs</span><br><span class="line">struggle with tasks that requires precise spatial information and counting (from 0 to 10), sometimes providing an impression of a person with</span><br><span class="line">myopia seeing fine details as blurry and making educated guesses. Code</span><br><span class="line">is available at: https://vlmsareblind.github.io/</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/AI-MO/NuminaMath-7B-TIR</span><br><span class="line">7/11/24</span><br><span class="line">NuminaMath 7B TIR released! A 7B task-specific LLM that can solve complex math problems better than most high school students! It uses tool-integrated reasoning to solve problems by applying Chain-of-Thought reasoning and Python REPLs in an agentic flow with self-healing.ğŸ¤¯</span><br><span class="line">NuminaMath 7B TIR solves math problems by:</span><br><span class="line">1ï¸âƒ£ Generating a Chain of Thought reasoning on how to approach the problem.</span><br><span class="line">2ï¸âƒ£ Translating the CoT into Python Code.</span><br><span class="line">3ï¸âƒ£ Executes the Python Code in a REPL.</span><br><span class="line">4ï¸âƒ£ If it fails, it tries to self-heal, repeating steps 1ï¸âƒ£-3ï¸âƒ£ using the wrong output.</span><br><span class="line">    If it succeeds, it generates a nice response with the result.</span><br><span class="line">Model TL;DR:</span><br><span class="line">ğŸ”¬ Fine-tuned from deepseek-math-7b-base</span><br><span class="line">ğŸ† Won the first progress prize in the AI Math Olympiad (AIMO)</span><br><span class="line">ğŸ§¬ Built a large synthetic dataset following ToRA paper</span><br><span class="line">ğŸ§  Trained in two-stage using Supervised Fine-Tuning on the Hugging Face cluster</span><br><span class="line">ğŸ Utilizes tool-integrated reasoning with Python REPL</span><br><span class="line">ğŸ¤— Available on Hugging Face under Apache 2.0 license</span><br><span class="line">ğŸ“Š Capable of solving problems at AMC 12 level</span><br><span class="line">Numina Logo</span><br><span class="line">Model Card for NuminaMath 7B TIR</span><br><span class="line">NuminaMath is a series of language models that are trained to solve math problems using tool-integrated reasoning (TIR). NuminaMath 7B TIR won the first progress prize of the AI Math Olympiad (AIMO), with a score of 29/50 on the public and private tests sets.</span><br><span class="line"></span><br><span class="line">image/png</span><br><span class="line"></span><br><span class="line">This model is a fine-tuned version of deepseek-ai/deepseek-math-7b-base with two stages of supervised fine-tuning:</span><br><span class="line"></span><br><span class="line">Stage 1: fine-tune the base model on a large, diverse dataset of natural language math problems and solutions, where each solution is templated with Chain of Thought (CoT) to facilitate reasoning.</span><br><span class="line">Stage 2: fine-tune the model from Stage 1 on a synthetic dataset of tool-integrated reasoning, where each math problem is decomposed into a sequence of rationales, Python programs, and their outputs. Here we followed Microsoftâ€™s ToRA paper and prompted GPT-4 to produce solutions in the ToRA format with code execution feedback. Fine-tuning on this data produces a reasoning agent that can solve mathematical problems via a mix of natural language reasoning and use of the Python REPL to compute intermediate results.</span><br><span class="line">Model description</span><br><span class="line">Model type: A 7B parameter math LLM fine-tuned in two stages of supervised fine-tuning, first on a dataset with math problem-solution pairs and then on a synthetic dataset with examples of multi-step generations using tool-integrated reasoning.</span><br><span class="line">Language(s) (NLP): Primarily English</span><br><span class="line">License: Apache 2.0</span><br><span class="line">Finetuned from model: deepseek-ai/deepseek-math-7b-base</span><br><span class="line">Model Sources</span><br><span class="line">Repository: Coming soon!</span><br><span class="line">Demo: https://huggingface.co/spaces/AI-MO/math-olympiad-solver</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2309.03883</span><br><span class="line">ğğğ° ğğğœğ¨ğğ¢ğ§ğ  ğ­ğğœğ¡ğ§ğ¢ğªğ®ğ ğ¢ğ§ ğ­ğ«ğšğ§ğ¬ğŸğ¨ğ«ğ¦ğğ«ğ¬ ğ¬ğ¢ğ ğ§ğ¢ğŸğ¢ğœğšğ§ğ­ğ¥ğ² ğ«ğğğ®ğœğğ¬ ğ¡ğšğ¥ğ¥ğ®ğœğ¢ğ§ğšğ­ğ¢ğ¨ğ§ğ¬ ğŸ‘</span><br><span class="line">DoLa decoding, which made a conference paper at ICLR &#x27;24, has just been merged in Transformers by JoÃ£o Gante and Yung-Sung Chuang.</span><br><span class="line">This new decoding method is simple yet extremely impressive!</span><br><span class="line">Reminder: Decoder LLMs (the GPT kind of LLM, the most common one) generate their outputs one token at a time: at each step, given a current text, they compute, for each token in their vocabulary, a &quot;logit&quot; that should represent the probability that this token is coming next.</span><br><span class="line">Then the decoder either picks the highest logit token (greedy decoding) or samples one with a probability defined by the logits (sampling). The token gets appended to the current text, and the decoder compute logits again, and the cycle continues.</span><br><span class="line">The authors of DoLa wanted to improve that simple method.</span><br><span class="line">They knew this established fact that transformer LMs encode low-level info (like base syntax) in early layers and more high-level info like knowledge in the later layers.</span><br><span class="line">ğŸ’¡ This gave them their key idea: During decoding, rather than picking the token with the highest logit, ğ˜„ğ—µğ˜† ğ—»ğ—¼ğ˜ ğ—½ğ—¶ğ—°ğ—¸ ğ˜ğ—µğ—² ğ˜ğ—¼ğ—¸ğ—²ğ—» ğ˜„ğ—¶ğ˜ğ—µ ğ˜ğ—µğ—² ğ—ºğ—¼ğ˜€ğ˜ ğ—¶ğ—ºğ—½ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ˜ƒğ—² ğ—¶ğ—»ğ—°ğ—¿ğ—²ğ—®ğ˜€ğ—² ğ—¶ğ—» ğ—¹ğ—¼ğ—´ğ—¶ğ˜ ğ—®ğ—°ğ—¿ğ—¼ğ˜€ğ˜€ ğ—¹ğ—®ğ˜†ğ—²ğ—¿ğ˜€?</span><br><span class="line">Implementation is actually quite simple: at each step, you get the layer for which the logits diverge most from your final layer, and this chosen layer becomes the premature layer. Then you subtract the logits from the premature layer to your final layer, in order to reward tokens for which the logits progressed most. And this lets you pick your next token.</span><br><span class="line">Their test settings:</span><br><span class="line">â¤ Test 4 sizes of Llama-1 models (7b to 65B)</span><br><span class="line">â¤ Benchmarks on multiple choice QA (TruthfulQA, FACTOR) and open-ended QA (TruthfulQA-open-ended, GSM8K)</span><br><span class="line">âœ¨ ğ—¥ğ—²ğ˜€ğ˜‚ğ—¹ğ˜ğ˜€ ğ—®ğ—¿ğ—² ğ—²ğ˜…ğ˜ğ—¿ğ—²ğ—ºğ—²ğ—¹ğ˜† ğ—¶ğ—ºğ—½ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ˜ƒğ—²!</span><br><span class="line">ğŸš€ ğŸ±% - ğŸ®ğŸ¬% ğ—¯ğ—®ğ˜€ğ—² ğ—½ğ—¼ğ—¶ğ—»ğ˜ğ˜€ ğ—¶ğ—»ğ—°ğ—¿ğ—²ğ—®ğ˜€ğ—² ğ—®ğ—°ğ—¿ğ—¼ğ˜€ğ˜€ ğ˜ğ—µğ—² ğ—¯ğ—²ğ—»ğ—°ğ—µğ—ºğ—®ğ—¿ğ—¸ğ˜€</span><br><span class="line">ğŸš€ For instance on TruthfulQA / Open-ended, across all model sizes the increase in truthfulness is 14 base points, which is ğ—®ğ—¿ğ—¼ğ˜‚ğ—»ğ—± ğŸ°ğŸ¬% ğ—¶ğ—ºğ—½ğ—¿ğ—¼ğ˜ƒğ—²ğ—ºğ—²ğ—»ğ˜ ğ—°ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—²ğ—± ğ˜ğ—¼ ğ˜€ğ˜ğ—®ğ—»ğ—±ğ—®ğ—¿ğ—± ğ—±ğ—²ğ—°ğ—¼ğ—±ğ—¶ğ—»ğ—´!</span><br><span class="line">ğŸ¤” Wouldn&#x27;t decoding take longer because of this added contrasting step? ğŸ‘‰ ğ—§ğ—µğ—² ğ—¿ğ˜‚ğ—»ğ˜ğ—¶ğ—ºğ—² ğ—¶ğ—»ğ—°ğ—¿ğ—²ğ—®ğ˜€ğ—² ğ—¶ğ˜€ ğ—»ğ—²ğ—´ğ—¹ğ—¶ğ—´ğ—¶ğ—¯ğ—¹ğ—², ğŸ­ ğ˜ğ—¼ ğŸ´% ğ—¼ğ—»ğ—¹ğ˜†.</span><br><span class="line">The paper has additional insights such as how token confidence evolves across layers for different types of tokens: I recommend to read it!</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-07-12</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/07/12/2024-7-12-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 7ì›” 12ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/07/10/2024-7-10-AI-NEWS/" title="2024ë…„ 7ì›” 10ì¼ AI ì†Œì‹">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>