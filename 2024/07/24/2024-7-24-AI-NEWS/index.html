<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 7월 24일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="Meta에서는 Llama 3.1을 출시하였으며, 8B, 70B 및 405B 크기의 모델로 제공됩니다. 이 모델은 다국어 지원과 상업적 사용이 가능하며, 효율적인 추론을 위해 양자화된 버전을 제공합니다. DeepSeek에서는 DeepSeek-V2-Chat-0628 모델을"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 7월 24일 AI 소식</a></h3></div><div class="post-content"><p>Meta에서는 Llama 3.1을 출시하였으며, 8B, 70B 및 405B 크기의 모델로 제공됩니다. 이 모델은 다국어 지원과 상업적 사용이 가능하며, 효율적인 추론을 위해 양자화된 버전을 제공합니다. DeepSeek에서는 DeepSeek-V2-Chat-0628 모델을 개선하여 LMSYS Chatbot Arena에서 높은 순위를 기록했습니다. OpenAI는 비용 효율적인 GPT-4o mini 모델을 발표하였으며, Apple은 7B 오픈소스 LLM을 출시하였습니다. Mistral은 12B 모델을 출시하였고, Salesforce는 xLAM 모델을 공개했습니다. NVIDIA는 Minitron 모델을 발표하여 교육 비용을 줄이면서 성능을 향상시켰습니다. Google은 새로운 RLHF 방법을 발표하였고, Apple은 LazyLLM 방법을 소개하였습니다. 최근 AI 연구 논문들도 다양하게 발표되었습니다.</p>
<h2 id="Meta-Llama-3-1-출시"><a href="#Meta-Llama-3-1-출시" class="headerlink" title="Meta, Llama 3.1 출시"></a>Meta, Llama 3.1 출시</h2><p><a target="_blank" rel="noopener" href="https://llama.meta.com/">링크</a>, 2024년 7월 24일</p>
<ul>
<li>8B, 70B 및 405B 크기의 모델 제공</li>
<li>8개 언어 지원</li>
<li>15T 이상의 토큰으로 훈련, 25M 이상의 인간 및 합성 샘플로 미세 조정</li>
<li>상업적 사용이 가능한 라이선스 제공</li>
<li>효율적인 추론을 위한 FP8, AWQ 및 GPTQ 버전 제공</li>
<li>Hugging Face Inference API 및 HuggingChat에서 사용 가능</li>
<li>128K 토큰의 컨텍스트 윈도우 지원</li>
<li>다양한 벤치마크에서 GPT-4o 수준의 성능</li>
</ul>
<h2 id="DeepSeek-DeepSeek-V2-Chat-0628-출시"><a href="#DeepSeek-DeepSeek-V2-Chat-0628-출시" class="headerlink" title="DeepSeek, DeepSeek-V2-Chat-0628 출시"></a>DeepSeek, DeepSeek-V2-Chat-0628 출시</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat-0628">링크</a>, 2024년 7월 19일</p>
<ul>
<li>LMSYS Chatbot Arena에서 #11 순위 기록</li>
<li>코딩 능력에서 #3 순위, 어려운 문제 해결에서 #3 순위</li>
<li>이전 버전 대비 HumanEval에서 3.7% 향상</li>
<li>MATH 벤치마크에서 17.1% 향상</li>
<li>IFEval에서 13.8% 향상</li>
<li>Arena-Hard에서 26.7% 향상</li>
<li>JSON 출력 성능 7% 향상</li>
<li>시스템 영역에서 명령어 따르기 능력 최적화, 사용자 경험 향상</li>
</ul>
<h2 id="OpenAI-GPT-4o-mini-발표"><a href="#OpenAI-GPT-4o-mini-발표" class="headerlink" title="OpenAI, GPT-4o mini 발표"></a>OpenAI, GPT-4o mini 발표</h2><p><a target="_blank" rel="noopener" href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/">링크</a>, 2024년 7월 18일</p>
<ul>
<li>비용 효율적인 소형 모델, MMLU에서 82% 기록</li>
<li>GPT-3.5 Turbo 대비 60% 저렴한 가격</li>
<li>128K 토큰의 컨텍스트 윈도우, 16K 출력 토큰 지원</li>
<li>텍스트와 비전에서 우수한 성능, 다중모드 추론 지원</li>
<li>안전 조치 내장, 포괄적인 안전성 평가 실시</li>
<li>Assistants API, Chat Completions API, Batch API에서 사용 가능</li>
<li>개발자들이 더 효율적이고 저렴하게 AI 애플리케이션을 구축하고 확장할 수 있도록 지원</li>
</ul>
<h2 id="Apple-7B-오픈소스-LLM-출시"><a href="#Apple-7B-오픈소스-LLM-출시" class="headerlink" title="Apple, 7B 오픈소스 LLM 출시"></a>Apple, 7B 오픈소스 LLM 출시</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/apple/DCLM-7B">링크</a>, 2024년 7월 16일</p>
<ul>
<li>2.5T 토큰으로 훈련된 7B 기본 모델</li>
<li>주로 영어 데이터를 사용, 2048 컨텍스트 윈도우 지원</li>
<li>MMLU에서 0.6372 점수 기록, Mistral보다 우수</li>
<li>PyTorch 및 OpenLM 프레임워크 사용</li>
<li>Hugging Face 및 Transformers에서 사용 가능</li>
</ul>
<h2 id="Mistral-Nemo-12B-모델-출시"><a href="#Mistral-Nemo-12B-모델-출시" class="headerlink" title="Mistral, Nemo 12B 모델 출시"></a>Mistral, Nemo 12B 모델 출시</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/mistralai?search_models=nemo">링크</a>, 2024년 7월 24일</p>
<ul>
<li>128K 컨텍스트 윈도우 지원, 새로운 토크나이저 Tekken 사용</li>
<li>9개 언어 지원, Apache 2.0 라이선스 제공</li>
<li>Instruct 버전은 함수 호출 지원</li>
<li>NVIDIA와 협력하여 개발, 3,072 H100 80GB로 훈련</li>
<li>Hugging Face에서 사용 가능</li>
</ul>
<h2 id="Salesforce-xLAM-모델-발표"><a href="#Salesforce-xLAM-모델-발표" class="headerlink" title="Salesforce, xLAM 모델 발표"></a>Salesforce, xLAM 모델 발표</h2><p><a target="_blank" rel="noopener" href="https://github.com/SalesforceAIResearch/xLAM">링크</a>, 2024년 7월</p>
<ul>
<li>1.35B 및 7B 모델 제공, 최대 16K 컨텍스트 길이 지원</li>
<li>자율적으로 작업을 계획하고 실행하는 기능</li>
<li>GPT-4 및 Claude 3.5 수준의 성능</li>
<li>DeepSeek Coder로 생성된 60K 함수 호출 데이터셋 공개</li>
<li>Transformers와 호환, GGUF 지원</li>
</ul>
<h2 id="NVIDIA-Minitron-4B-및-8B-모델-출시"><a href="#NVIDIA-Minitron-4B-및-8B-모델-출시" class="headerlink" title="NVIDIA, Minitron 4B 및 8B 모델 출시"></a>NVIDIA, Minitron 4B 및 8B 모델 출시</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/collections/nvidia/minitron-669ac727dc9c86e6ab7f0f3e">링크</a>, 2024년 7월 24일</p>
<ul>
<li>큰 LLM에서 2-4배 작은 모델로 가지치기 및 증류</li>
<li>40배 적은 교육 토큰 사용, MMLU에서 16% 향상</li>
<li>94B 교육 토큰, 256K 어휘</li>
<li>Iterative pruning + distillation 방법 사용</li>
<li>Hugging Face와 통합</li>
</ul>
<h2 id="Google-J-BOND-RLHF-방법-발표"><a href="#Google-J-BOND-RLHF-방법-발표" class="headerlink" title="Google, J-BOND RLHF 방법 발표"></a>Google, J-BOND RLHF 방법 발표</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2407.14622">링크</a>, 2024년 7월 20일</p>
<ul>
<li>Best-of-N Distillation 알고리즘 도입</li>
<li>Monte Carlo 샘플링을 사용하여 보상 백분위수를 추정</li>
<li>제프리 다이버전스를 사용하여 모드 커버링과 모드 시킹 행동 균형</li>
<li>여러 벤치마크에서 효과 입증</li>
</ul>
<h2 id="Apple-LazyLLM-방법-발표"><a href="#Apple-LazyLLM-방법-발표" class="headerlink" title="Apple, LazyLLM 방법 발표"></a>Apple, LazyLLM 방법 발표</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2407.14057">링크</a>, 2024년 7월 19일</p>
<ul>
<li>긴 컨텍스트 LLM 추론을 위한 동적 토큰 가지치기</li>
<li>LLama 2 7B 모델에서 2.34배 속도 향상</li>
<li>정확도를 유지하면서 생성 시간 단축</li>
</ul>
<h2 id="AI-연구-논문"><a href="#AI-연구-논문" class="headerlink" title="AI 연구 논문"></a>AI 연구 논문</h2><h3 id="텍스트-TO-SQL-작업에-LLM을-사용하는-방법에-대한-조사"><a href="#텍스트-TO-SQL-작업에-LLM을-사용하는-방법에-대한-조사" class="headerlink" title="텍스트-TO-SQL 작업에 LLM을 사용하는 방법에 대한 조사"></a>텍스트-TO-SQL 작업에 LLM을 사용하는 방법에 대한 조사</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.15186">링크</a>, 2024년 7월 21일</p>
<ul>
<li>데이터베이스 액세스를 용이하게 하기 위한 텍스트-TO-SQL 변환의 중요성 강조</li>
<li>LLM을 활용한 새로운 방법들 소개</li>
<li>프롬프트 엔지니어링 및 파인튜닝 방법 논의</li>
</ul>
<h3 id="LLM의-프롬프트-엔지니어링-방법에-대한-조사"><a href="#LLM의-프롬프트-엔지니어링-방법에-대한-조사" class="headerlink" title="LLM의 프롬프트 엔지니어링 방법에 대한 조사"></a>LLM의 프롬프트 엔지니어링 방법에 대한 조사</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.12994">링크</a>, 2024년 7월 17일</p>
<ul>
<li>LLM을 위한 프롬프트 엔지니어링 기술의 발전 논의</li>
<li>다양한 NLP 작업에서의 프롬프트 방법들 정리</li>
<li>44개의 연구 논문 요약, 39개의 프롬프트 방법과 29개의 NLP 작업 소개</li>
</ul>
<h3 id="오픈-인공지능-지식-데이터셋-발표"><a href="#오픈-인공지능-지식-데이터셋-발표" class="headerlink" title="오픈 인공지능 지식 데이터셋 발표"></a>오픈 인공지능 지식 데이터셋 발표</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.14371">링크</a>, 2024년 7월 19일</p>
<ul>
<li>고품질, 다양하고 윤리적으로 소싱된 데이터셋의 필요성 강조</li>
<li>Wikipedia의 주요 카테고리를 기반으로 한 5억 개 이상의 토큰 데이터셋 제공</li>
<li>다양한 LLM을 사용하여 높은 지식 범위와 일관성, 정확성을 유지</li>
</ul>
<p>이번 AI 소식에서는 주요 AI 모델 출시 및 기술적인 세부 사항과 더불어 최신 연구 논문까지 다양하게 소개되었습니다. AI 기술의 빠른 발전과 함께 이러한 정보들이 더욱 널리 활용될 수 있기를 기대합니다.</p>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h2 id="Title"><a href="#Title" class="headerlink" title="Title,"></a>Title,</h2><h3 id="company-name-제목"><a href="#company-name-제목" class="headerlink" title="company name, 제목"></a>company name, 제목</h3><p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h2 id="Title-1"><a href="#Title-1" class="headerlink" title="Title,"></a>Title,</h2><h3 id="company-name-제목-1"><a href="#company-name-제목-1" class="headerlink" title="company name, 제목"></a>company name, 제목</h3><p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://llama.meta.com/</span><br><span class="line">7/24/24</span><br><span class="line">META</span><br><span class="line">Llama 405B is here, and it comes with more than expected! 🚨 Meta Llama 3.1 comes in 3 sizes, 8B, 70B, and 405B, and speaks 8 languages! 🌍 Llama 3.1 405B matches or beats the Openai GPT-4o across many text benchmarks.</span><br><span class="line">New and improvements of 3.1✨:</span><br><span class="line">🧮 8B, 70B &amp; 405B versions as Instruct and Base with 128k context</span><br><span class="line">🌐 Multilingual, supports 8 languages, including English, German, French, and more.</span><br><span class="line">🔠 Trained on &gt;15T Tokens &amp; fine-tuned on 25M human and synthetic samples</span><br><span class="line">📃 Commercial friendly license with allowance to use model outputs to improve other LLMs</span><br><span class="line">⚖️ Quantized versions in FP8, AWQ, and GPTQ for efficient inference.</span><br><span class="line">🚀 Llama 3 405B matches and beast GPT-4o on many benchmarks</span><br><span class="line">🧑🏻‍💻 8B &amp; 70B improved Coding and instruction, following up to 12%</span><br><span class="line">⚒️ Supports Tool use and Function Calling</span><br><span class="line">🤖 Llama 3.1 405B available on Hugging Face Inference API and in HuggingChat</span><br><span class="line">🤗 Available on @huggingface</span><br><span class="line">🔜  1-click deployments on Hugging Face, Amazon SageMaker, Google Cloud</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Big Kudos to Meta for releasing Llama 3.1, including 405B. This will help everyone accelerate and adopt AI more easily and faster. ❤️</span><br><span class="line">Llama 3.1 is here!</span><br><span class="line">8B, 70B, and 405B versions are available.</span><br><span class="line">Results on common benchmarks suggest that Llama 3.1 405B is a GPT-4o level model. Closes the gap on both GPT-4o and Claude 3.5 Sonnet.</span><br><span class="line">Here is my full video with an overview of Llama 3.1, takeaways, first impressions, and test cases:</span><br><span class="line"></span><br><span class="line">More results:</span><br><span class="line">128K tokens context window supported. &quot;Pre-trains a model with 405B parameters on 15.6T tokens using a context window of 8K tokens. This standard pre-training stage is followed by a continued pre-training stage that increases the supported context window to 128K tokens.&quot;</span><br><span class="line">Llama 3.1 405B shows strong performance on a variety of proficiency exams. &quot;We observe that the performance of our Llama 3 405B model is very similar to Claude 3.5 Sonnet and GPT-4 4o. Our 70B model has an even more impressive performance. It is significantly better than GPT-3.5 Turbo and beats Nemotron 4 340B on many tests.&quot;</span><br><span class="line">The 405B results are comparable to Claude 3.5 Sonnet and GPT-4o on common code generation benchmarks.</span><br><span class="line">Uses a five-stage compositional training approach to add multimodal capabilities. That&#x27;s right, this model has strong vision and video recognition capabilities too.</span><br><span class="line">The 405B model was quantized from 16-bit (BF16) to 8-bit (FP8) which helps to reduce the compute requirements.</span><br><span class="line">Llama 3.1 405B is trained on up to 16K H100 GPUs!</span><br><span class="line"></span><br><span class="line">Model Information</span><br><span class="line">The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.</span><br><span class="line"></span><br><span class="line">Model developer: Meta</span><br><span class="line"></span><br><span class="line">Model Architecture: Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.</span><br><span class="line"></span><br><span class="line">Training Data	Params	Input modalities	Output modalities	Context length	GQA	Token count	Knowledge cutoff</span><br><span class="line">Llama 3.1 (text only)	A new mix of publicly available online data.	8B	Multilingual Text	Multilingual Text and code	128k	Yes	15T+	December 2023</span><br><span class="line">70B	Multilingual Text	Multilingual Text and code	128k	Yes</span><br><span class="line">405B	Multilingual Text	Multilingual Text and code	128k	Yes</span><br><span class="line">Supported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.</span><br><span class="line"></span><br><span class="line">Llama 3.1 family of models. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.</span><br><span class="line"></span><br><span class="line">Model Release Date: July 23, 2024.</span><br><span class="line"></span><br><span class="line">Status: This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.</span><br><span class="line"></span><br><span class="line">License: A custom commercial license, the Llama 3.1 Community License, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE</span><br><span class="line"></span><br><span class="line">Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go here.</span><br><span class="line"></span><br><span class="line">Intended Use</span><br><span class="line">Intended Use Cases Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases.</span><br><span class="line"></span><br><span class="line">Out-of-scope Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card.</span><br><span class="line"></span><br><span class="line">Note: Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.</span><br><span class="line"></span><br><span class="line">Hardware and Software</span><br><span class="line">Training Factors We used custom training libraries, Meta&#x27;s custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.</span><br><span class="line"></span><br><span class="line">Training Energy Use Training utilized a cumulative of 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.</span><br><span class="line"></span><br><span class="line">Training Greenhouse Gas Emissions Estimated total location-based greenhouse gas emissions were 11,390 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.</span><br><span class="line"></span><br><span class="line">Training Time (GPU hours)	Training Power Consumption (W)	Training Location-Based Greenhouse Gas Emissions</span><br><span class="line">(tons CO2eq)</span><br><span class="line"></span><br><span class="line">Training Market-Based Greenhouse Gas Emissions</span><br><span class="line">(tons CO2eq)</span><br><span class="line"></span><br><span class="line">Llama 3.1 8B	1.46M	700	420	0</span><br><span class="line">Llama 3.1 70B	7.0M	700	2,040	0</span><br><span class="line">Llama 3.1 405B	30.84M	700	8,930	0</span><br><span class="line">Total	39.3M</span><br><span class="line">11,390	0</span><br><span class="line">The methodology used to determine training energy use and greenhouse gas emissions can be found here. Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.</span><br><span class="line"></span><br><span class="line">Training Data</span><br><span class="line">Overview: Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.</span><br><span class="line"></span><br><span class="line">Data Freshness: The pretraining data has a cutoff of December 2023.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat-0628</span><br><span class="line">Deepseek changed the game with  v2 chat 0628 - The best open LLM on LYMSYS arena right now - 236B parameter model with 21B active parameters. It also excels at coding (rank #3) and arena hard problems (rank #3)</span><br><span class="line">7/19/24</span><br><span class="line">DeepSeek-V2-Chat-0628</span><br><span class="line">1. Introduction</span><br><span class="line">DeepSeek-V2-Chat-0628 is an improved version of DeepSeek-V2-Chat. For model details, please visit DeepSeek-V2 page for more information.</span><br><span class="line"></span><br><span class="line">DeepSeek-V2-Chat-0628 has achieved remarkable performance on the LMSYS Chatbot Arena Leaderboard:</span><br><span class="line"></span><br><span class="line">Overall Ranking: #11, outperforming all other open-source models.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Coding Arena Ranking: #3, showcasing exceptional capabilities in coding tasks.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Hard Prompts Arena Ranking: #3, demonstrating strong performance on challenging prompts.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2. Improvement</span><br><span class="line">Compared to the previous version DeepSeek-V2-Chat, the new version has made the following improvements:</span><br><span class="line"></span><br><span class="line">Benchmark	DeepSeek-V2-Chat	DeepSeek-V2-Chat-0628	Improvement</span><br><span class="line">HumanEval	81.1	84.8	+3.7</span><br><span class="line">MATH	53.9	71.0	+17.1</span><br><span class="line">BBH	79.7	83.4	+3.7</span><br><span class="line">IFEval	63.8	77.6	+13.8</span><br><span class="line">Arena-Hard	41.6	68.3	+26.7</span><br><span class="line">JSON Output (Internal)	78	85	+7</span><br><span class="line">Furthermore, the instruction following capability in the &quot;system&quot; area has been optimized, significantly enhancing the user experience for immersive translation, RAG, and other tasks.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/</span><br><span class="line">July 18, 2024</span><br><span class="line"></span><br><span class="line">GPT-4o mini: advancing cost-efficient intelligence</span><br><span class="line">Introducing our most cost-efficient small model</span><br><span class="line"></span><br><span class="line">Introducing GPT-4o mini &gt; Hero &gt; Media Item</span><br><span class="line">OpenAI is committed to making intelligence as broadly accessible as possible. Today, we&#x27;re announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.</span><br><span class="line"></span><br><span class="line">GPT-4o mini enables a broad range of tasks with its low cost and latency, such as applications that chain or parallelize multiple model calls (e.g., calling multiple APIs), pass a large volume of context to the model (e.g., full code base or conversation history), or interact with customers through fast, real-time text responses (e.g., customer support chatbots).</span><br><span class="line"></span><br><span class="line">Today, GPT-4o mini supports text and vision in the API, with support for text, image, video and audio inputs and outputs coming in the future. The model has a context window of 128K tokens, supports up to 16K output tokens per request, and has knowledge up to October 2023. Thanks to the improved tokenizer shared with GPT-4o, handling non-English text is now even more cost effective.</span><br><span class="line"></span><br><span class="line">A small model with superior textual intelligence and multimodal reasoning</span><br><span class="line">GPT-4o mini surpasses GPT-3.5 Turbo and other small models on academic benchmarks across both textual intelligence and multimodal reasoning, and supports the same range of languages as GPT-4o. It also demonstrates strong performance in function calling, which can enable developers to build applications that fetch data or take actions with external systems, and improved long-context performance compared to GPT-3.5 Turbo.</span><br><span class="line"></span><br><span class="line">GPT-4o mini has been evaluated across several key benchmarks2.</span><br><span class="line"></span><br><span class="line">Reasoning tasks: GPT-4o mini is better than other small models at reasoning tasks involving both text and vision, scoring 82.0% on MMLU, a textual intelligence and reasoning benchmark, as compared to 77.9% for Gemini Flash and 73.8% for Claude Haiku.</span><br><span class="line"></span><br><span class="line">Math and coding proficiency: GPT-4o mini excels in mathematical reasoning and coding tasks, outperforming previous small models on the market. On MGSM, measuring math reasoning, GPT-4o mini scored 87.0%, compared to 75.5% for Gemini Flash and 71.7% for Claude Haiku. GPT-4o mini scored 87.2% on HumanEval, which measures coding performance, compared to 71.5% for Gemini Flash and 75.9% for Claude Haiku.</span><br><span class="line"></span><br><span class="line">Multimodal reasoning: GPT-4o mini also shows strong performance on MMMU, a multimodal reasoning eval, scoring 59.4% compared to 56.1% for Gemini Flash and 50.2% for Claude Haiku.</span><br><span class="line"></span><br><span class="line">Model Evaluation Scores</span><br><span class="line">GPT-4o mini</span><br><span class="line">Gemini Flash</span><br><span class="line">Claude Haiku</span><br><span class="line">GPT-3.5 Turbo</span><br><span class="line">GPT-4o</span><br><span class="line">Accuracy (%)</span><br><span class="line">100</span><br><span class="line">75</span><br><span class="line">50</span><br><span class="line">25</span><br><span class="line">0</span><br><span class="line">82.0</span><br><span class="line">77.9</span><br><span class="line">73.8</span><br><span class="line">69.8</span><br><span class="line">88.7</span><br><span class="line">40.2</span><br><span class="line">38.6</span><br><span class="line">35.7</span><br><span class="line">30.8</span><br><span class="line">53.6</span><br><span class="line">79.7</span><br><span class="line">78.4</span><br><span class="line">78.4</span><br><span class="line">70.2</span><br><span class="line">83.4</span><br><span class="line">87.0</span><br><span class="line">75.5</span><br><span class="line">71.7</span><br><span class="line">56.3</span><br><span class="line">90.5</span><br><span class="line">70.2</span><br><span class="line">40.9</span><br><span class="line">40.9</span><br><span class="line">43.1</span><br><span class="line">76.6</span><br><span class="line">87.2</span><br><span class="line">71.5</span><br><span class="line">75.9</span><br><span class="line">68.0</span><br><span class="line">90.2</span><br><span class="line">59.4</span><br><span class="line">56.1</span><br><span class="line">50.2</span><br><span class="line">0.0</span><br><span class="line">69.1</span><br><span class="line">56.7</span><br><span class="line">58.4</span><br><span class="line">46.4</span><br><span class="line">0.0</span><br><span class="line">63.8</span><br><span class="line">MMLU</span><br><span class="line">GPQA</span><br><span class="line">DROP</span><br><span class="line">MGSM</span><br><span class="line">MATH</span><br><span class="line">HumanEval</span><br><span class="line">MMMU</span><br><span class="line">MathVista</span><br><span class="line">Eval Benchmark</span><br><span class="line">As part of our model development process, we worked with a handful of trusted partners to better understand the use cases and limitations of GPT-4o mini. We partnered with companies like Ramp(opens in a new window) and Superhuman(opens in a new window) who found GPT-4o mini to perform significantly better than GPT-3.5 Turbo for tasks such as extracting structured data from receipt files or generating high quality email responses when provided with thread history.</span><br><span class="line"></span><br><span class="line">Built-in safety measures</span><br><span class="line">Safety is built into our models from the beginning, and reinforced at every step of our development process. In pre-training, we filter out(opens in a new window) information that we do not want our models to learn from or output, such as hate speech, adult content, sites that primarily aggregate personal information, and spam. In post-training, we align the model’s behavior to our policies using techniques such as reinforcement learning with human feedback (RLHF) to improve the accuracy and reliability of the models’ responses.</span><br><span class="line"></span><br><span class="line">GPT-4o mini has the same safety mitigations built-in as GPT-4o, which we carefully assessed using both automated and human evaluations according to our Preparedness Framework and in line with our voluntary commitments. More than 70 external experts in fields like social psychology and misinformation tested GPT-4o to identify potential risks, which we have addressed and plan to share the details of in the forthcoming GPT-4o system card and Preparedness scorecard. Insights from these expert evaluations have helped improve the safety of both GPT-4o and GPT-4o mini.</span><br><span class="line"></span><br><span class="line">Building on these learnings, our teams also worked to improve the safety of GPT-4o mini using new techniques informed by our research. GPT-4o mini in the API is the first model to apply our instruction hierarchy(opens in a new window) method, which helps to improve the model’s ability to resist jailbreaks, prompt injections, and system prompt extractions. This makes the model’s responses more reliable and helps make it safer to use in applications at scale.</span><br><span class="line"></span><br><span class="line">We’ll continue to monitor how GPT-4o mini is being used and improve the model’s safety as we identify new risks.</span><br><span class="line"></span><br><span class="line">Availability and pricing</span><br><span class="line">GPT-4o mini is now available as a text and vision model in the Assistants API, Chat Completions API, and Batch API. Developers pay 15 cents per 1M input tokens and 60 cents per 1M output tokens (roughly the equivalent of 2500 pages in a standard book). We plan to roll out fine-tuning for GPT-4o mini in the coming days.</span><br><span class="line"></span><br><span class="line">In ChatGPT, Free, Plus and Team users will be able to access GPT-4o mini starting today, in place of GPT-3.5. Enterprise users will also have access starting next week, in line with our mission to make the benefits of AI accessible to all.</span><br><span class="line"></span><br><span class="line">What’s Next</span><br><span class="line">Over the past few years, we’ve witnessed remarkable advancements in AI intelligence paired with substantial reductions in cost. For example, the cost per token of GPT-4o mini has dropped by 99% since text-davinci-003, a less capable model introduced in 2022. We’re committed to continuing this trajectory of driving down costs while enhancing model capabilities.</span><br><span class="line"></span><br><span class="line">We envision a future where models become seamlessly integrated in every app and on every website. GPT-4o mini is paving the way for developers to build and scale powerful AI applications more efficiently and affordably. The future of AI is becoming more accessible, reliable, and embedded in our daily digital experiences, and we’re excited to continue to lead the way.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/apple/DCLM-7B</span><br><span class="line">Apple</span><br><span class="line">7/16/24</span><br><span class="line">Apple has entered the game! Apple just released a 7B open-source LLM, weights, training code, and dataset! 👀</span><br><span class="line">TL;DR:</span><br><span class="line">🧠 7B base model, trained on 2.5T tokens on an open datasets</span><br><span class="line">🌐 Primarily English data and a 2048 context window</span><br><span class="line">📈 Combined DCLM-BASELINE, StarCoder, and ProofPile2 data</span><br><span class="line">🏆 MMLU 0.6372 &gt; Mistral &amp; &lt; Llama3</span><br><span class="line">🔓 Open License with Apple Sample Code License</span><br><span class="line">📊 Matches closed-dataset models like Mistral</span><br><span class="line">🔬 Trained using PyTorch with OpenLM framework</span><br><span class="line">🤗 Available on Hugging Face and in Transformers</span><br><span class="line">Paper: DataComp-LM: In search of the next generation of training sets for language models</span><br><span class="line">Model Card for DCLM-Baseline-7B</span><br><span class="line">DCLM-Baseline-7B is a 7 billion parameter language model trained on the DCLM-Baseline dataset, which was curated as part of the DataComp for Language Models (DCLM) benchmark. This model is designed to showcase the effectiveness of systematic data curation techniques for improving language model performance.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/mistralai?search_models=nemo</span><br><span class="line">Mistral releases 12B open LLM! 🤯 Mistral Nemo comes as a base and instruct version with a 128k Context window and is multilingual in 9 Language with a new tokenizer. 👀</span><br><span class="line">TL;DR:</span><br><span class="line">🧠 12B Base and Instruct drop-in-replacement for Mistral 7B</span><br><span class="line">🪟 Supports 128k context window and new tokenizer, Tekken, based on Tiktoken</span><br><span class="line">🌍 Base Model multilingual in English, French, German, Spanish, Italian and more</span><br><span class="line">🔓 Released under Apache 2.0</span><br><span class="line">🏆 Base MMLU 68.0%; Instruct 53.4% MixEval Hard;</span><br><span class="line">⚒️ Instruct versions support function calling</span><br><span class="line">🤯 quantized aware-training for FP8 inference without any performance loss</span><br><span class="line">🤝 Created as a Collaboration between NVIDIA and Mistral AI</span><br><span class="line">🚀 Trained on 3,072 H100 80GB on DGX Cloud</span><br><span class="line">🤗 Available on Hugging Face</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/SalesforceAIResearch/xLAM</span><br><span class="line">Salseforce</span><br><span class="line">07.2024</span><br><span class="line">Missed it, Salesforce released xLAM - 1.35B &amp; 7B Large Action Models, (upto 16K context length) ⚡️</span><br><span class="line">LAMs autonomously plan and execute tasks to achieve specific goals!</span><br><span class="line">&gt; Competitive with GPT4 &amp; Claude 3.5 on  BFCL (function calling leaderboard)</span><br><span class="line">Beats pretty much all open access models (command r plus, Mixtral 8x22B etc)</span><br><span class="line">&gt; 7B scores 88.24% whilst the 2B scores 78.94% on BFCL</span><br><span class="line">&gt; They release a function calling dataset with 60K entries created with DeepSeek Coder</span><br><span class="line">&gt; Each datapoint is verified through three hierarchical stages: format checking, actual function executions, and semantic verification</span><br><span class="line">&gt; Works out of the box with Transformers 🤗</span><br><span class="line">&gt; They also ship GGUFs compatible with llama.cpp 🦙</span><br><span class="line">Kudos to Salesforce, the LAMs look quite powerful and more so thanks for releasing the dataset too! 🤗</span><br><span class="line">[07.2024]: We are excited to announce the release of our two function-calling models: xLAM-1b-fc-r and xLAM-7b-fc-r. These models have achieved impressive rankings, placing #3 and #25 on the Berkeley Function-Calling Leaderboard, outperforming many significantly larger models. Stay tuned for more powerful models coming soon.</span><br><span class="line">Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories.</span><br><span class="line"></span><br><span class="line">This repo introduces xLAM that aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/collections/nvidia/minitron-669ac727dc9c86e6ab7f0f3e</span><br><span class="line">NVIDIA</span><br><span class="line">7/24/24</span><br><span class="line">Nvidia releases Minitron 4B &amp; 8B - iteratively pruning and distilling 2-4x smaller models from large LLMs, requiring 40x fewer training tokens and with 16% improvement on MMLU! 🔥</span><br><span class="line">Distilled model (w/ pruning + retraining) beats teacher!</span><br><span class="line">&gt; Competitive with L3 8B/ Mistral 7B with fractional compute + training tokens.</span><br><span class="line">&gt; 94B training tokens only.</span><br><span class="line">&gt; 256K vocab.</span><br><span class="line">&gt; Integrated with transformers.</span><br><span class="line">Best practices:</span><br><span class="line">1. Train a big LLM, iteratively prune + distil + retrain.</span><br><span class="line">2. Use KL Divergence as the loss function for distillation.</span><br><span class="line">3. Logit loss is sufficient for retraining/ distilling, so there is no need for CLM loss.</span><br><span class="line">4. Iterative (instead of one-shot) pruning results in the student model outperforming the teacher.</span><br><span class="line">5. Depth + Width pruning results in the best performance.</span><br><span class="line">6. Lightweight Neural Architecture Search for distilled checkpoints.</span><br><span class="line">And many more in the paper..</span><br><span class="line">They released the base checkpoints for 8B and 4B; it would be cool to see the instruct checkpoints, too!</span><br><span class="line">Kudos Nvidia! 🤗</span><br><span class="line">Now.. how&#x27;s going to do this L3.1 405B? ;)</span><br><span class="line">Minitron is a family of small language models (SLMs) obtained by pruning NVIDIA&#x27;s Nemotron-4 15B model. We prune model embedding size, attention heads, and MLP intermediate dimension, following which, we perform continued training with distillation to arrive at the final models.</span><br><span class="line"></span><br><span class="line">Deriving the Minitron 8B and 4B models from the base 15B model using our approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. Please refer to our arXiv paper for more details.</span><br><span class="line"></span><br><span class="line">Minitron models are for research and development only.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2407.14622</span><br><span class="line">Google</span><br><span class="line">RLHF method behind Google Gemma 1.1 released. 👀J-BOND a new RLHF method from Google DeepMind was used to fine-tune models Gemma 1.1 2B and 7B. J-BOND introduces a Best-of-N Distillation (BOND) algorithm that emulates Best-of-N sampling using Monte Carlo sampling to estimate reward quantiles.</span><br><span class="line">Implementation:</span><br><span class="line">1️⃣ Collect datasets of prompts and a Reward Model</span><br><span class="line">2️⃣ Generate 1 sample from current policy and 2 samples from anchor (ref model) for each prompt</span><br><span class="line">3️⃣ Compute forward KL gradient using the best of the 2 anchor samples</span><br><span class="line">4️⃣ Compute backward KL gradient using the policy sample and a reward function rJ-BOND</span><br><span class="line">5️⃣ Update policy weights using a combined gradient (Jeffreys divergence + KL regularization)</span><br><span class="line">6️⃣ Update anchor model using Exponential Moving Average (EMA)</span><br><span class="line">Insights:</span><br><span class="line">📌 The Anchor Model in J-BOND is the “reference” model initialized from the SFT model</span><br><span class="line">🏆 J-BOND outperforms REINFORCE baselines in terms of reward/KL trade-off</span><br><span class="line">🤖 Was used to RLHF Gemma 1.1 2B and 7B, no mention of Gemma2</span><br><span class="line">🐢 Slower updates of the anchor model improve the stability of the training</span><br><span class="line">🎯 The anchor model serves as a moving target for the policy to improve upon</span><br><span class="line">📉 No mention of common benchmarks like MT Bench, Alpaca Eval, or Arena Hard</span><br><span class="line">BOND: Aligning LLMs with Best-of-N Distillation</span><br><span class="line">Published on Jul 20</span><br><span class="line">·</span><br><span class="line">Submitted by</span><br><span class="line">piergs</span><br><span class="line">on Jul 23</span><br><span class="line">Authors:</span><br><span class="line"></span><br><span class="line">Abstract</span><br><span class="line">Reinforcement learning from human feedback (RLHF) is a key driver of quality and safety in state-of-the-art large language models. Yet, a surprisingly simple and strong inference-time strategy is Best-of-N sampling that selects the best generation among N candidates. In this paper, we propose Best-of-N Distillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but without its significant computational overhead at inference time. Specifically, BOND is a distribution matching algorithm that forces the distribution of generations from the policy to get closer to the Best-of-N distribution. We use the Jeffreys divergence (a linear combination of forward and backward KL) to balance between mode-covering and mode-seeking behavior, and derive an iterative formulation that utilizes a moving anchor for efficiency. We demonstrate the effectiveness of our approach and several design choices through experiments on abstractive summarization and Gemma models. Aligning Gemma policies with BOND outperforms other RLHF algorithms by improving results on several benchmarks.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2407.14057</span><br><span class="line">Published on Jul 19</span><br><span class="line">Apple</span><br><span class="line">Apple presents LazyLLM</span><br><span class="line">Dynamic Token Pruning for Efficient Long Context LLM Inference</span><br><span class="line"></span><br><span class="line">The inference of transformer-based large language models consists of two sequential stages: 1) a prefilling stage to compute the KV cache of prompts and generate the first token, and 2) a decoding stage to generate subsequent tokens. For long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase the time needed to generate the first token. Consequently, the prefilling stage may become a bottleneck in the generation process. An open question remains whether all prompt tokens are essential for generating the first token. To answer this, we introduce a novel method, LazyLLM, that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps. Extensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34x while maintaining accuracy.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2407.15186</span><br><span class="line">[Submitted on 21 Jul 2024]</span><br><span class="line">A Survey on Employing Large Language Models for Text-to-SQL Tasks</span><br><span class="line">Liang Shi, Zhengju Tang, Zhi Yang</span><br><span class="line">The increasing volume of data stored in relational databases has led to the need for efficient querying and utilization of this data in various sectors. However, writing SQL queries requires specialized knowledge, which poses a challenge for non-professional users trying to access and query databases. Text-to-SQL parsing solves this issue by converting natural language queries into SQL queries, thus making database access more accessible for non-expert users. To take advantage of the recent developments in Large Language Models (LLMs), a range of new methods have emerged, with a primary focus on prompt engineering and fine-tuning. This survey provides a comprehensive overview of LLMs in text-to-SQL tasks, discussing benchmark datasets, prompt engineering, fine-tuning methods, and future research directions. We hope this review will enable readers to gain a broader understanding of the recent advances in this field and offer some insights into its future trajectory.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2407.14371</span><br><span class="line">[Submitted on 19 Jul 2024]</span><br><span class="line">Open Artificial Knowledge</span><br><span class="line">Vadim Borisov, Richard H. Schreiber</span><br><span class="line">The tremendous success of chat-based AI systems like ChatGPT, Claude, and Gemini stems from Large Language Models (LLMs) trained on vast amount of datasets. However, acquiring high-quality, diverse, and ethically sourced training data remains a significant challenge. We introduce the Open Artificial Knowledge (OAK) dataset, a large-scale resource of over 500 million tokens (at the moment of writing) designed to address this issue. OAK leverages an ensemble of state-of-the-art LLMs, including GPT4o, LLaMa3-70B, LLaMa3-8B, Mixtral-8x7B, Gemma-7B, and Gemma-2-9B , to generate high-quality text across diverse domains, guided by Wikipedia&#x27;s main categories. Our methodology ensures broad knowledge coverage while maintaining coherence and factual accuracy. The OAK dataset aims to foster the development of more capable and aligned language models while addressing critical issues of data scarcity and privacy in LLM training, and it is freely available on this http URL.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2407.12994</span><br><span class="line">[Submitted on 17 Jul 2024]</span><br><span class="line">A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks</span><br><span class="line">Shubham Vatsal, Harsh Dubey</span><br><span class="line">Large language models (LLMs) have shown remarkable performance on many different Natural Language Processing (NLP) tasks. Prompt engineering plays a key role in adding more to the already existing abilities of LLMs to achieve significant performance gains on various NLP tasks. Prompt engineering requires composing natural language instructions called prompts to elicit knowledge from LLMs in a structured way. Unlike previous state-of-the-art (SoTA) models, prompt engineering does not require extensive parameter re-training or fine-tuning based on the given NLP task and thus solely operates on the embedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently extract LLMs&#x27; knowledge through a basic natural language conversational exchange or prompt engineering, allowing more and more people even without deep mathematical machine learning background to experiment with LLMs. With prompt engineering gaining popularity in the last two years, researchers have come up with numerous engineering techniques around designing prompts to improve accuracy of information extraction from the LLMs. In this paper, we summarize different prompting techniques and club them together based on different NLP tasks that they have been used for. We further granularly highlight the performance of these prompting strategies on various datasets belonging to that NLP task, talk about the corresponding LLMs used, present a taxonomy diagram and discuss the possible SoTA for specific datasets. In total, we read and present a survey of 44 research papers which talk about 39 different prompting methods on 29 different NLP tasks of which most of them have been published in the last two years.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-07-24</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/07/24/2024-7-24-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 7월 24일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/07/26/2024-7-26-AI-NEWS/" title="2024년 7월 26일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/07/17/2024-7-17-AI-NEWS/" title="2024년 7월 17일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>