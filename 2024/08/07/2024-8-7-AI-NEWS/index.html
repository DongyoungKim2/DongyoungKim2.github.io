<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 8ì›” 7ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="LG AI ResearchëŠ” EXAONE 3.0ì„ ê³µê°œí•˜ë©°, OpenAIëŠ” Structured Outputs ê¸°ëŠ¥ì„ ë„ì…í•˜ì˜€ìŠµë‹ˆë‹¤. MetaëŠ” Self-Taught Evaluators ì ‘ê·¼ë²•ì„ ì†Œê°œí•˜ë©°, Hugging FaceëŠ” Idefics3-8Bë¥¼ ì¶œì‹œí–ˆìŠµë‹ˆë‹¤. B"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 8ì›” 7ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><p>LG AI ResearchëŠ” EXAONE 3.0ì„ ê³µê°œí•˜ë©°, OpenAIëŠ” Structured Outputs ê¸°ëŠ¥ì„ ë„ì…í•˜ì˜€ìŠµë‹ˆë‹¤. MetaëŠ” Self-Taught Evaluators ì ‘ê·¼ë²•ì„ ì†Œê°œí•˜ë©°, Hugging FaceëŠ” Idefics3-8Bë¥¼ ì¶œì‹œí–ˆìŠµë‹ˆë‹¤. Black Forest LabsëŠ” FLUX.1 ëª¨ë¸ì„ ë°œí‘œí•˜ì˜€ê³ , BCì¹´ë“œëŠ” K-ê¸ˆìœµ íŠ¹í™” AIë¥¼ ë¬´ìƒ ê³µê°œí–ˆìŠµë‹ˆë‹¤.</p>
<h3 id="LG-AI-Research-EXAONE-3-0-ë°œí‘œ"><a href="#LG-AI-Research-EXAONE-3-0-ë°œí‘œ" class="headerlink" title="LG AI Research, EXAONE 3.0 ë°œí‘œ"></a>LG AI Research, EXAONE 3.0 ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct">ë§í¬</a>, 2024ë…„ 8ì›” 7ì¼</p>
<ul>
<li>EXAONE 3.0 7.8B Instruction Tuned ëª¨ë¸ ê³µê°œ<ul>
<li>7.8B íŒŒë¼ë¯¸í„°ì™€ 8ì¡° ê°œì˜ í† í° ë°ì´í„°ë¡œ í›ˆë ¨ëœ ë””ì½”ë” ì „ìš© íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ ê¸°ë°˜</li>
</ul>
</li>
<li>ì˜ì–´ì™€ í•œêµ­ì–´ì—ì„œ ê¸€ë¡œë²Œ ìµœìƒìœ„ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ ë‹¬ì„±<ul>
<li>ì˜ì–´: ì‹¤ì„¸ê³„ ì‚¬ìš© ì‚¬ë¡€ì™€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ í‰ê·  1ìœ„ ê¸°ë¡</li>
<li>í•œêµ­ì–´: ì‹¤ì„¸ê³„ ì‚¬ìš© ì‚¬ë¡€ì™€ ì¼ë°˜ ì„±ëŠ¥ì—ì„œ ëª¨ë‘ ìµœìƒìœ„ ê²°ê³¼</li>
</ul>
</li>
<li>ê²½ì œì„± í™•ë³´: 3ë…„ê°„ì˜ ì—°êµ¬ê°œë°œë¡œ ë¹„ìš© 6% ì ˆê°<ul>
<li>EXAONE 1.0 ëŒ€ë¹„ ì¶”ë¡  ì²˜ë¦¬ ì‹œê°„ 56% ë‹¨ì¶•, ë¹„ìš© 72% ì ˆê°</li>
</ul>
</li>
<li>AI ìœ¤ë¦¬ì™€ íˆ¬ëª…ì„± ê°•ì¡°<ul>
<li>Red Teaming ê³¼ì •ì„ ê±°ì³ ìœ¤ë¦¬ì„±ê³¼ ë³´ì•ˆ í‰ê°€ ìˆ˜í–‰</li>
<li>ë¹„ì°¨ë³„ì ì´ê³  ë²•ì  ë¬¸ì œ ì—†ëŠ” ë‹µë³€ ì œê³µ, ê°œì„  í•„ìš” ì˜ì—­ íˆ¬ëª…í•˜ê²Œ ê³µê°œ</li>
</ul>
</li>
</ul>
<h3 id="OpenAI-Structured-Outputs-ê¸°ëŠ¥-ë„ì…"><a href="#OpenAI-Structured-Outputs-ê¸°ëŠ¥-ë„ì…" class="headerlink" title="OpenAI, Structured Outputs ê¸°ëŠ¥ ë„ì…"></a>OpenAI, Structured Outputs ê¸°ëŠ¥ ë„ì…</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/introducing-structured-outputs-in-the-api/">ë§í¬</a>, 2024ë…„ 8ì›” 6ì¼</p>
<ul>
<li>APIì— Structured Outputs ê¸°ëŠ¥ ì¶”ê°€<ul>
<li>ê°œë°œìê°€ ì œê³µí•œ JSON ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ëª¨ë¸ ì¶œë ¥ ë³´ì¥</li>
<li>ë³µì¡í•œ JSON ìŠ¤í‚¤ë§ˆ ë”°ë¥´ê¸°ì—ì„œ 100% ì‹ ë¢°ì„± ë‹¬ì„±</li>
</ul>
</li>
<li>ìƒˆë¡œìš´ ëª¨ë¸ gpt-4o-2024-08-06 ì¶œì‹œ<ul>
<li>ë³µì¡í•œ JSON ìŠ¤í‚¤ë§ˆ ì¶”ì¢…ì—ì„œ ê¸°ì¡´ ëª¨ë¸(gpt-4-0613)ë³´ë‹¤ ë†’ì€ ì ìˆ˜ ê¸°ë¡</li>
</ul>
</li>
</ul>
<h3 id="OpenAI-ì£¼ìš”-ì¸ì‚¬-ë³€ë™"><a href="#OpenAI-ì£¼ìš”-ì¸ì‚¬-ë³€ë™" class="headerlink" title="OpenAI, ì£¼ìš” ì¸ì‚¬ ë³€ë™"></a>OpenAI, ì£¼ìš” ì¸ì‚¬ ë³€ë™</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/introducing-structured-outputs-in-the-api/">ë§í¬</a>, 2024ë…„ 8ì›” 6ì¼</p>
<ul>
<li>ê³µë™ ì°½ì—…ì John Schulman, Greg Brockman, Peter Deng ë“± ì£¼ìš” ì¸ì‚¬ ì´íƒˆ<ul>
<li>John Schulmanì€ ê²½ìŸì‚¬ Anthropicìœ¼ë¡œ ì´ë™</li>
<li>Greg Brockmanì€ ì•ˆì‹ë…„ ê³„íš</li>
<li>Peter Dengì€ í‡´ì‚¬</li>
</ul>
</li>
<li>ì˜¬í•´ ì´ˆì—ë„ ì£¼ìš” ì¸ì‚¬ ì´íƒˆ<ul>
<li>ê³µë™ ì°½ì—…ì Andrej Karpathy, Jan Leike, Ilya Sutskever í‡´ì‚¬</li>
</ul>
</li>
<li>OpenAIì˜ ìƒˆë¡œìš´ ìŒì„± ê¸°ëŠ¥ì— ëŒ€í•œ ê¸ì •ì ì¸ ì´ˆê¸° í‰ê°€</li>
</ul>
<h3 id="Meta-Self-Taught-Evaluators-ë°œí‘œ"><a href="#Meta-Self-Taught-Evaluators-ë°œí‘œ" class="headerlink" title="Meta, Self-Taught Evaluators ë°œí‘œ"></a>Meta, Self-Taught Evaluators ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.02666">ë§í¬</a>, 2024ë…„ 8ì›” 5ì¼</p>
<ul>
<li>ì¸ê°„ì˜ ì„ í˜¸ ë°ì´í„° ì—†ì´ ëª¨ë¸ í‰ê°€ìë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ì ‘ê·¼ë²• ì†Œê°œ<ul>
<li>ëŒ€ì¡°ì  ëª¨ë¸ ì¶œë ¥ì„ ìƒì„±í•˜ê³  LLM-as-a-Judgeë¥¼ í›ˆë ¨í•˜ì—¬ ìµœì¢… íŒë‹¨ ìƒì„±</li>
<li>ê°œì„ ëœ ì˜ˆì¸¡ì„ ì‚¬ìš©í•˜ì—¬ ë°˜ë³µì ìœ¼ë¡œ í›ˆë ¨ ìˆ˜í–‰</li>
</ul>
</li>
<li>Llama3-70B-Instruct ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ<ul>
<li>RewardBenchì—ì„œ 75.4ì—ì„œ 88.3ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ (ë‹¤ìˆ˜ê²°ë¡œ 88.7)</li>
<li>GPT-4ì™€ ê°™ì€ ê¸°ì¡´ í‰ê°€ìë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ ë‹¬ì„±</li>
</ul>
</li>
</ul>
<h3 id="Hugging-Face-Idefics-3-8B-ë°œí‘œ"><a href="#Hugging-Face-Idefics-3-8B-ë°œí‘œ" class="headerlink" title="Hugging Face, Idefics 3-8B ë°œí‘œ"></a>Hugging Face, Idefics 3-8B ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3">ë§í¬</a>, 2024ë…„ 8ì›” 4ì¼</p>
<ul>
<li>í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸<ul>
<li>SigLip ë¹„ì „ ë°±ë³¸ê³¼ Llama 3.1 8B í…ìŠ¤íŠ¸ ë°±ë³¸ í†µí•©</li>
<li>ë¬¸ì„œ ì§ˆë¬¸ ì‘ë‹µ ì„±ëŠ¥(DocVQA) 87.7, MMStar 55.9 ë‹¬ì„±</li>
<li>ìµœëŒ€ 10K ì»¨í…ìŠ¤íŠ¸ ì§€ì›</li>
</ul>
</li>
<li>OCR, ë¬¸ì„œ ì´í•´ ë° ì‹œê°ì  ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒ</li>
<li>Apache 2.0 ë¼ì´ì„ ìŠ¤ë¡œ ê³µê°œ</li>
<li>Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í†µí•©</li>
</ul>
<h3 id="Black-Forest-Labs-FLUX-1-ëª¨ë¸-ë°œí‘œ"><a href="#Black-Forest-Labs-FLUX-1-ëª¨ë¸-ë°œí‘œ" class="headerlink" title="Black Forest Labs, FLUX.1 ëª¨ë¸ ë°œí‘œ"></a>Black Forest Labs, FLUX.1 ëª¨ë¸ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/black-forest-labs/FLUX.1-schnell">ë§í¬</a>, 2024ë…„ 8ì›” 1ì¼</p>
<ul>
<li>í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ FLUX.1 ì‹œë¦¬ì¦ˆ ë°œí‘œ<ul>
<li>FLUX.1 [pro], FLUX.1 [dev], FLUX.1 [schnell] ì„¸ ê°€ì§€ ë³€í˜• ì œê³µ</li>
<li>ê°ê¸° ë‹¤ë¥¸ í•´ìƒë„ì™€ ë¹„ìœ¨ ì§€ì›</li>
<li>12B íŒŒë¼ë¯¸í„° í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜ ì‚¬ìš©</li>
<li>Latent adversarial diffusion distillation ê¸°ë²• ì ìš©</li>
</ul>
</li>
<li>ì‹œë“œ í€ë”©ìœ¼ë¡œ 3100ë§Œ ë‹¬ëŸ¬ í™•ë³´<ul>
<li>ì£¼ìš” íˆ¬ìì: Andreessen Horowitz, Brendan Iribe, Michael Ovitz ë“±</li>
</ul>
</li>
<li>ë†’ì€ í’ˆì§ˆì˜ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± ëŠ¥ë ¥<ul>
<li>Midjourney v6.0, DALLÂ·E 3 (HD) ë“±ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥</li>
</ul>
</li>
</ul>
<h3 id="BCì¹´ë“œ-K-ê¸ˆìœµ-íŠ¹í™”-AI-ë¬´ìƒ-ê³µê°œ"><a href="#BCì¹´ë“œ-K-ê¸ˆìœµ-íŠ¹í™”-AI-ë¬´ìƒ-ê³µê°œ" class="headerlink" title="BCì¹´ë“œ, K-ê¸ˆìœµ íŠ¹í™” AI ë¬´ìƒ ê³µê°œ"></a>BCì¹´ë“œ, K-ê¸ˆìœµ íŠ¹í™” AI ë¬´ìƒ ê³µê°œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/BCCard">ë§í¬</a>, 2024ë…„ 7ì›” 25ì¼</p>
<ul>
<li>í•œêµ­ ê¸ˆìœµê¶Œì— ìµœì í™”ëœ ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ (LLM) ê³µê°œ<ul>
<li>Llama 3 ê¸°ë°˜, 200ì–µ ê°œì˜ íŒŒë¼ë¯¸í„° ì‚¬ìš©</li>
<li>í•œêµ­ì–´ í•™ìŠµ ëŠ¥ë ¥ ë° ë‹¤ì–‘í•œ ê¸ˆìœµ ì§€ì‹ ì •ë³´ íƒ‘ì¬</li>
</ul>
</li>
<li>2ë§Œì—¬ ê°œì˜ ê¸ˆìœµ ì§€ì‹ í•™ìŠµ ë°ì´í„°ì™€ í•¨ê»˜ ê³µê°œ</li>
<li>ê¸ˆìœµ AX ë¶„ì•¼ ë°œì „ì— ê¸°ì—¬</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹"><a href="#todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹" class="headerlink" title="(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,"></a>(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAIì—ì„œëŠ” ~~~ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<p><a href="link">ë§í¬</a>, date,</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct</span><br><span class="line">LG AI Research</span><br><span class="line">8/7/24</span><br><span class="line"></span><br><span class="line">EXAONE 3.0 : Showcasing Our First Open-Source LLM with Global Top-Level Performance</span><br><span class="line">The three-year journey from version 1.0 to 3.0 of LG AI Researchâ€™s EXAONE has not been an easy one, as we have continued our research and released upgraded models and external commercialization results every year. From proving the development potential of AI technology in various industries one by one, weâ€™ve created models that users can better utilize between the two pillars of performance and cost and developed expert-level AI that can be applied to real-world industrial fields.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">EXAONE Milestone</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Release of the EXAONE 3.0 7.8B Instruction Tuned Language Model</span><br><span class="line"></span><br><span class="line">August 2024. Finally, we are excited to announce EXAONE 3.0. Among various EXAONE 3.0 language model lineups, we are releasing the 7.8B Instruction Tuned model as an open source for research. We hope that this model will help AI researchers in Korea and abroad to conduct more meaningful research and help the AI ecosystem move forward.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The 7.8B model released this time is based on the Decoder-only Transformer Architecture in line with recent trends, with 7.8B parameters and 8T training data (tokens). This post will introduce the main features, performance evaluation results, and insights of EXAONE 3.0 7.8B Instruction Tuned language model. For our performance evaluation, we utilized a combination of publicly available datasets and our own benchmark datasets to compare the performance of the 7.8B model with the latest AI models that support English and Korean, which are similar in size to the 7.8B model.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Key Takeaways</span><br><span class="line"></span><br><span class="line">â–  Achieved global top level in English : Ranked 1st average in real-world use cases and excellent performance in benchmarks</span><br><span class="line"></span><br><span class="line">The English performance of the 7.8B model is at the Global Top-level compared to other models. EXAONE is aiming to be a high-level Expert AI that can be utilized in specialized industries. In order for AI models to be utilized in specialized industries and fields of expertise, they must perform well in real-world use cases, i.e., in a complex manner so humans can trust and use them. To evaluate this aspect, the Chatbot Arena method has recently been widely used, which is a method of directly using and evaluating models based on features that humans often use. While this evaluation is time-consuming, an accurate assessment of the real-world utility of the model is an advantage it provides. To confirm the English performance of the 7.8B model, we selected four key benchmarks that are similar to how Chatbot Arena is evaluated and evaluated the model on items with high human utilization. The results showed that EXAONE 7.8B model ranked first in most benchmarks, with the highest average score.</span><br><span class="line"></span><br><span class="line">It also demonstrated superior performance on benchmarks. It ranked first in average scores for math and coding, demonstrating superiority over other models. And it also achieved strong performance results in reasoning.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Evaluation Results of Real-world Use Cases (English)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Benchmark â€“ Math, Coding, Reasoning (English)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">â–  Clearly outstanding Korean language performance : Ranked first in average scores for both real-world use cases and benchmarks</span><br><span class="line"></span><br><span class="line">EXAONE 7.8B model is a bilingual model that targets both English and Korean languages. For the Korean performance evaluation, we used two benchmarks to check the performance for real-world use cases, and configured multiple benchmarks to check general performance. As a result, we were able to see top overall results in both real-world use cases and general performance.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Evaluation Results of Real-world Use Cases (Korean)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Benchmark (Korean)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">â–  Securing economic feasibility : Reduced to 6% of the cost of the initially released model through three years of research and development</span><br><span class="line"></span><br><span class="line">In order for AI to be applied to our lives, it is essential to improve performance as well as enhance economic feasibility. Since the release of EXAONE 1.0 in 2021, we have spent the past three years focusing on research and development in AI model compression technologies to achieve cost efficiency. As a result, the 7.8B model released shows a 56% reduction in inference processing time and a 72% reduction in cost compared to EXAONE 2.0. In particular, it is a significant reduction in cost, bringing it down to just 6% of the cost of the initially released EXAONE 1.0.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">EXAONE 3.0 Performance Improvement</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">â–  Ethical transparency : In addtion to excellent results, disclosure of areas requiring improvement</span><br><span class="line"></span><br><span class="line">LG AI Research always considers AI ethics in the research and development process of AI models. EXAONE 3.0 7.8B Instruction Tuned language model also underwent a Red Teaming process to assess its ethics and security and was evaluated using both internal and external third-party datasets.</span><br><span class="line"></span><br><span class="line">While the model released this time is excellent at providing non-sexually discriminatory and legal answers, there are areas that need to be improved. We disclosed the evaluation results as they are because we believe that transparent disclosure of information is a prerequisite for the development of AI ethics. We hope that researchers will conduct more active research on AI ethics based on this disclosure, and LG AI Research will also continue to research AI ethics.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Evaluation Results of Harmlessness (Korean Large Language Model Trustworthiness Benchmark Data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">You can view the detailed information, including the model&#x27;s performance evaluation results, through the link below and directly download and use the 7.8B model. We hope that the release of this model will contribute to assisting various research and development by AI researchers and enhancing technological competitiveness.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/introducing-structured-outputs-in-the-api/</span><br><span class="line">OpenAI</span><br><span class="line">August 6, 2024</span><br><span class="line"></span><br><span class="line">Introducing Structured Outputs in the API</span><br><span class="line">We are introducing Structured Outputs in the APIâ€”model outputs now reliably adhere to developer-supplied JSON Schemas.</span><br><span class="line"></span><br><span class="line">Structured Output in the API &gt; Hero Image &gt; Media Item</span><br><span class="line">Last year at DevDay, we introduced JSON modeâ€”a useful building block for developers looking to build reliable applications with our models. While JSON mode improves model reliability for generating valid JSON outputs, it does not guarantee that the modelâ€™s response will conform to a particular schema. Today weâ€™re introducing Structured Outputs in the API, a new feature designed to ensure model-generated outputs will exactly match JSON Schemas provided by developers.</span><br><span class="line"></span><br><span class="line">Generating structured data from unstructured inputs is one of the core use cases for AI in todayâ€™s applications. Developers use the OpenAI API to build powerful assistants that have the ability to fetch data and answer questions via function calling(opens in a new window), extract structured data for data entry, and build multi-step agentic workflows that allow LLMs to take actions. Developers have long been working around the limitations of LLMs in this area via open source tooling, prompting, and retrying requests repeatedly to ensure that model outputs match the formats needed to interoperate with their systems. Structured Outputs solves this problem by constraining OpenAI models to match developer-supplied schemas and by training our models to better understand complicated schemas.</span><br><span class="line"></span><br><span class="line">On our evals of complex JSON schema following, our new model gpt-4o-2024-08-06 with Structured Outputs scores a perfect 100%. In comparison, gpt-4-0613 scores less than 40%.</span><br><span class="line"></span><br><span class="line">With Structured Outputs, gpt-4o-2024-08-06 achieves 100% reliability in our evals, perfectly matching the output schemas.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">OpenAI</span><br><span class="line">August 6, 2024</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">â€¢ Cofounder John Schulman is heading to rival Anthropic</span><br><span class="line">â€¢ Cofounder Greg Brockman is taking a sabbatical</span><br><span class="line">â€¢ Product leader Peter Deng is also departing</span><br><span class="line"></span><br><span class="line">This is after other key members left earlier this year:</span><br><span class="line"></span><br><span class="line">â€¢ Cofounder Andrej Karpathy left in Feb</span><br><span class="line">â€¢ Jan Leike, who led OpenAI safety team, left in May</span><br><span class="line">â€¢ Chief Scientist and co-founder Ilya Sutskever also left in May</span><br><span class="line"></span><br><span class="line">It seems like the company is in free fall as many key employees are leaving the company â€“ some going directly to rivals like Anthropic and Google.</span><br><span class="line"></span><br><span class="line">This is happening as Google&#x27;s Gemini overtook GPT-4o last week. OpenAI is also finding its business model under attack from Meta&#x27;s open source AI model strategy.</span><br><span class="line"></span><br><span class="line">Tough time lie ahead but there may be some light at the end of the tunnel. Early testers of OpenAI&#x27;s new voice feature are sharing rave reviews, and it may just be the next big thing in AI</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2408.02666</span><br><span class="line">META</span><br><span class="line">[Submitted on 5 Aug 2024]</span><br><span class="line">Self-Taught Evaluators</span><br><span class="line">Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, Xian Li</span><br><span class="line">Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples.</span><br><span class="line">Meta presents Self-Taught Evaluators, an approach to improve model-based evaluators using synthetic training data only.</span><br><span class="line">It first generates contrasting outputs (good and bad model responses) and trains an LLM-as-a-Judge to produce reasoning traces and final judgments.</span><br><span class="line">The self-improvement scheme repeats the training process in an iterative way using its improved predictions.</span><br><span class="line">Keep in mind that this doesn&#x27;t use any labeled preference data so no human preference judgements are required.</span><br><span class="line">They claim to outperform LLM-judges such as GPT-4 and match top-performing reward models trained on labeled examples.</span><br><span class="line">&quot;Self-Taught Evaluator can improve a strong LLM (Llama3-70BInstruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench.&quot;</span><br><span class="line">This is another interesting way to use synthetic data to iteratively improve the evaluation capabilities of the LLM.</span><br><span class="line">This sounds like a good application for small language models but I read in the paper that these were not tried. The seed model needs to have the capability to generate reasonable evaluations (i.e., already instruction-tuned to human preferences).</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/datasets/argilla/magpie-ultra-v0.1</span><br><span class="line">META</span><br><span class="line">8/1/24</span><br><span class="line"></span><br><span class="line">The first Synthetic dataset created with Meta Llama 3.1 405B released. ğŸ MagPie-Ultra is the first open dataset using Llama 3.1 405B-Instruct FP8 to generate 50,000 synthetic instruction pairs using the MagPie recipe and Argilla distilabel. It includes challenging instructions for coding math, data analysis, creative writing, advice seeking, or Brainstorming. âš—ï¸</span><br><span class="line">MagPie datasets are created by prompting LLMs with &quot;empty&quot; prompts that consist only of starting special tokens, allowing the model to auto-regressively generate user queries and corresponding responses, which are then filtered to select high-quality data. ğŸ‘¨â€ğŸ“</span><br><span class="line">Note: The dataset is unfiltered but includes quality &amp; difficulty scores, embeddings, topics, and safety scores from ArmorRM and LlamaGuard. ğŸ›¡ï¸</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3</span><br><span class="line">8/4/24</span><br><span class="line">HuggingFaceM4: HuggingFaceM4 is the multimodal team at Hugging Face, working on vision-language models.</span><br><span class="line">Introducing Idefics 3 8B Llama 3, Apache 2.0 licensed VLM with enhanced Document QA capabilities! ğŸ”¥</span><br><span class="line">&gt; Vision backbone: SigLip, Text backbone: Llama 3.1 8B</span><br><span class="line">&gt; Text + Image input w/ text output</span><br><span class="line">&gt; 8.5B parameter model</span><br><span class="line">&gt; Supports up to 10K context</span><br><span class="line">&gt; Apache 2.0 licensed</span><br><span class="line">&gt; DocVQA 87.7; MMStar 55.9 (massive increase over Idefics 2)</span><br><span class="line">&gt; Integrated with Transformers</span><br><span class="line">Memory-wise, with 4-bit, you should be able to run it &lt; 5GB VRAM âš¡</span><br><span class="line">Open datasets and open models. Kudos to Hugo LaurenÃ§on</span><br><span class="line">&amp; Andi for sprinting and shipping; it&#x27;s such a brilliant checkpoint!</span><br><span class="line">Transformers version: until the next Transformers pypi release, please install Transformers from source and use this PR to be able to use Idefics3. TODO: change when new version.</span><br><span class="line"></span><br><span class="line">Idefics3</span><br><span class="line">Idefics3 is an open multimodal model that accepts arbitrary sequences of image and text inputs and produces text outputs. The model can answer questions about images, describe visual content, create stories grounded on multiple images, or simply behave as a pure language model without visual inputs. It improves upon Idefics1 and Idefics2, significantly enhancing capabilities around OCR, document understanding and visual reasoning.</span><br><span class="line"></span><br><span class="line">We release the checkpoints under the Apache 2.0.</span><br><span class="line"></span><br><span class="line">Model Summary</span><br><span class="line">Developed by: Hugging Face</span><br><span class="line">Model type: Multi-modal model (image+text)</span><br><span class="line">Language(s) (NLP): en</span><br><span class="line">License: Apache 2.0</span><br><span class="line">Parent Models: google/siglip-so400m-patch14-384 and meta-llama/Meta-Llama-3.1-8B-Instruct</span><br><span class="line">Resources for more information:</span><br><span class="line">Idefics1 paper: OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents</span><br><span class="line">Idefics2 paper: What matters when building vision-language models?</span><br><span class="line">Idefics3 paper: Coming soon (TODO)</span><br><span class="line">Uses</span><br><span class="line">Idefics3-8B can be used to perform inference on multimodal (image + text) tasks in which the input is composed of a text query along with one (or multiple) image(s). Text and images can be arbitrarily interleaved. That includes image captioning, visual question answering, etc. These model does not support image generation.</span><br><span class="line"></span><br><span class="line">The post-training of Idefics3-8B involves only a supervised fine-tuning stage, without RLHF alignment. As a result, the model may produce short answers or require prompt iterations to fully address the user&#x27;s request. Adding a prefix to the assistant&#x27;s response, such as &quot;Let&#x27;s fix this step by step&quot; has been found to effectively influence the generated output.</span><br><span class="line"></span><br><span class="line">To fine-tune Idefics3-8B on a specific task, we provide fine-tuning codes for Idefics2 that can be adapted (with almost no changes) to Idefics3:</span><br><span class="line"></span><br><span class="line">With the TRL library: Script</span><br><span class="line">With the Hugging Face Trainer: Tutorial notebook</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/black-forest-labs/FLUX.1-schnell</span><br><span class="line">Announcing Black Forest Labs</span><br><span class="line">Aug 1, 2024</span><br><span class="line">â€”</span><br><span class="line"></span><br><span class="line">by</span><br><span class="line"></span><br><span class="line">BlackForestLabs</span><br><span class="line">in News.</span><br><span class="line"></span><br><span class="line">Today, we are excited to announce the launch of Black Forest Labs. Deeply rooted in the generative AI research community, our mission is to develop and advance state-of-the-art generative deep learning models for media such as images and videos, and to push the boundaries of creativity, efficiency and diversity. We believe that generative AI will be a fundamental building block of all future technologies. By making our models available to a wide audience, we want to bring its benefits to everyone, educate the public and enhance trust in the safety of these models. We are determined to build the industry standard for generative media. Today, as the first step towards this goal, we release the FLUX.1 suite of models that push the frontiers of text-to-image synthesis.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The Black Forest Team</span><br><span class="line"></span><br><span class="line">We are a team of distinguished AI researchers and engineers with an outstanding track record in developing foundational generative AI models in academic, industrial, and open-source environments. Our innovations include creating VQGAN and Latent Diffusion, The Stable Diffusion models for image and video generation (Stable Diffusion XL, Stable Video Diffusion, Rectified Flow Transformers), and Adversarial Diffusion Distillation for ultra-fast, real-time image synthesis.</span><br><span class="line"></span><br><span class="line">Our core belief is that widely accessible models not only foster innovation and collaboration within the research community and academia, but also increase transparency, which is essential for trust and broad adoption. Our team strives to develop the highest quality technology and to make it accessible to the broadest audience possible.</span><br><span class="line"></span><br><span class="line">Funding</span><br><span class="line"></span><br><span class="line">We are excited to announce the successful closing of our Series Seed funding round of $31 million. This round was led by our main investor, Andreessen Horowitz, including notable participation from angel investors Brendan Iribe, Michael Ovitz, Garry Tan, Timo Aila and Vladlen Koltun and other renowned experts in AI research and company building. We have received follow-up investments from General Catalyst and MÃ¤tchVC to support us on our mission to bring state-of-the-art AI from Europe to everyone around the world.</span><br><span class="line"></span><br><span class="line">Furthermore, we are pleased to announce our advisory board, including Michael Ovitz, bringing extensive experience in the content creation industry, and Prof. Matthias Bethge, pioneer of neural style transfer and leading expert in open European AI research.</span><br><span class="line"></span><br><span class="line">Flux.1 Model Family</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">We release the FLUX.1 suite of text-to-image models that define a new state-of-the-art in image detail, prompt adherence, style diversity and scene complexity for text-to-image synthesis.</span><br><span class="line"></span><br><span class="line">To strike a balance between accessibility and model capabilities, FLUX.1 comes in three variants: FLUX.1 [pro], FLUX.1 [dev] and FLUX.1 [schnell]:</span><br><span class="line"></span><br><span class="line">FLUX.1 [pro]: The best of FLUX.1, offering state-of-the-art performance image generation with top of the line prompt following, visual quality, image detail and output diversity. Sign up for FLUX.1 [pro] access via our API here. FLUX.1 [pro] is also available via Replicate and fal.ai. Moreover we offer dedicated and customized enterprise solutions â€“ reach out via flux@blackforestlabs.ai to get in touch.</span><br><span class="line">FLUX.1 [dev]: FLUX.1 [dev] is an open-weight, guidance-distilled model for non-commercial applications. Directly distilled from FLUX.1 [pro], FLUX.1 [dev] obtains similar quality and prompt adherence capabilities, while being more efficient than a standard model of the same size. FLUX.1 [dev] weights are available on HuggingFace and can be directly tried out on Replicate or Fal.ai. For applications in commercial contexts, get in touch out via flux@blackforestlabs.ai.</span><br><span class="line">FLUX.1 [schnell]: our fastest model is tailored for local development and personal use. FLUX.1 [schnell] is openly available under an Apache2.0 license. Similar, FLUX.1 [dev], weights are available on Hugging Face and inference code can be found on GitHub and in HuggingFaceâ€™s Diffusers. Moreover weâ€™re happy to have day-1 integration for ComfyUI.</span><br><span class="line"></span><br><span class="line">Transformer-powered Flow Models at Scale</span><br><span class="line"></span><br><span class="line">All public FLUX.1 models are based on a hybrid architecture of multimodal and parallel diffusion transformer blocks and scaled to 12B parameters. We improve over previous state-of-the-art diffusion models by building on flow matching, a general and conceptually simple method for training generative models, which includes diffusion as a special case. In addition, we increase model performance and improve hardware efficiency by incorporating rotary positional embeddings and parallel attention layers. We will publish a more detailed tech report in the near future.</span><br><span class="line"></span><br><span class="line">A new Benchmark for Image Synthesis</span><br><span class="line"></span><br><span class="line">FLUX.1 defines the new state-of-the-art in image synthesis. Our models set new standards in their respective model class. FLUX.1 [pro] and [dev] surpass popular  models like Midjourney v6.0, DALLÂ·E 3 (HD) and SD3-Ultra in each of the following aspects: Visual Quality, Prompt Following, Size/Aspect Variability, Typography and Output Diversity. FLUX.1 [schnell] is the most advanced few-step model to date, outperforming not even its in-class competitors but also strong non-distilled models like Midjourney v6.0 and DALLÂ·E 3 (HD) .  Our models are specifically finetuned to preserve the entire output diversity from pretraining. Compared to the current state-of-the-art they offer drastically improved possibilities as shown below</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">All FLUX.1 model variants support a diverse range of aspect ratios and resolutions in 0.1 and 2.0 megapixels, as shown in the following example.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Up Next: SOTA Text-to-Video for All</span><br><span class="line"></span><br><span class="line">Today we release the FLUX.1 text-to-image model suite. With their strong creative capabilities, these models serve as a powerful foundation for our upcoming suite of competitive generative text-to-video systems. Our video models will unlock precise creation and editing at high definition and unprecedented speed. We are committed to continue pioneering the future of generative media.</span><br><span class="line"></span><br><span class="line">FLUX.1 [schnell] is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. For more information, please read our blog post.</span><br><span class="line"></span><br><span class="line">Key Features</span><br><span class="line">Cutting-edge output quality and competitive prompt following, matching the performance of closed source alternatives.</span><br><span class="line">Trained using latent adversarial diffusion distillation, FLUX.1 [schnell] can generate high-quality images in only 1 to 4 steps.</span><br><span class="line">Released under the apache-2.0 licence, the model can be used for personal, scientific, and commercial purposes.</span><br><span class="line">Usage</span><br><span class="line">We provide a reference implementation of FLUX.1 [schnell], as well as sampling code, in a dedicated github repository. Developers and creatives looking to build on top of FLUX.1 [schnell] are encouraged to use this as a starting point.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/Alpha-VLLM/Lumina-mGPT?tab=readme-ov-file#local-gradio-demos</span><br><span class="line">Lumina-mGPT</span><br><span class="line">A family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. ğŸ‘‹ join our WeChat</span><br><span class="line">[2024-07-08] ğŸ‰ğŸ‰ğŸ‰ Lumina-mGPT is released! ğŸ‰ğŸ‰ğŸ‰</span><br><span class="line">Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining</span><br><span class="line">Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, Peng Gao</span><br><span class="line">We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. Unlike existing autoregressive image generation approaches, Lumina-mGPT employs a pretrained decoder-only transformer as a unified framework for modeling multimodal token sequences. Our key insight is that a simple decoder-only transformer with multimodal Generative PreTraining (mGPT), utilizing the next-token prediction objective on massive interleaved text-image sequences, can learn broad and general multimodal capabilities, thereby illuminating photorealistic text-to-image generation. Building on these pretrained models, we propose Flexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text pairs to fully unlock their potential for high-aesthetic image synthesis at any resolution while maintaining their general multimodal capabilities. Furthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT), transforming Lumina-mGPT into a foundation model that seamlessly achieves omnipotent task unification. The resulting model demonstrates versatile multimodal capabilities, including visual generation tasks like flexible text-to-image generation and controllable generation, visual recognition tasks like segmentation and depth estimation, and vision-language tasks like multiturn visual question answering. Additionally, we analyze the differences and similarities between diffusion-based and autoregressive methods in a direct comparison.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/THUDM/CogVideo/tree/main?tab=readme-ov-file</span><br><span class="line">CogVideo &amp;&amp; CogVideoX</span><br><span class="line"> News: 2024/8/6: We have also open-sourced 3D Causal VAE used in CogVideoX-2B, which can reconstruct the video almost losslessly.</span><br><span class="line"> CogVideoX-2B is the latest open-source video generation model from ZhiPu AI, renowned for its powerful video creation capabilities. By simply inputting text or images, users can effortlessly generate high-quality video content. CogVideoX-2B is the first in the CogVideoX series, featuring 2 billion parameters and sharing the same lineage as ZhiPu AI&#x27;s AI video generation product, &quot;Qingying.&quot;</span><br><span class="line"></span><br><span class="line">CogVideoX-2B integrates several cutting-edge technologies, making it a leader in the video generation field.</span><br><span class="line"></span><br><span class="line">3D Variational Autoencoder (3D VAE): Utilizing an innovative three-dimensional convolution approach, the 3D VAE compresses video data across both spatial and temporal dimensions, achieving unprecedented compression rates and superior reconstruction quality. The model architecture includes an encoder, decoder, and a latent space regularizer, ensuring coherent and logical information processing through causal convolution mechanisms.</span><br><span class="line"></span><br><span class="line">End-to-End Video Understanding Model: This enhancement improves the model&#x27;s comprehension of text and adherence to instructions, ensuring the generated videos meet user requirements, even with long and complex prompts.</span><br><span class="line"></span><br><span class="line">Expert Transformer Technology: This technology allows for deep parsing of encoded video data, integrating textual inputs to create high-quality, narrative-rich video content.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/BCCard</span><br><span class="line">BCì¹´ë“œ, êµ­ë‚´ ìµœì í™” ê±°ëŒ€ì–¸ì–´ëª¨ë¸ ë¬´ìƒê³µê°œâ€¦â€˜ê¸ˆìœµ GPTâ€™ ì œê³µí•œë‹¤</span><br><span class="line">êµ¬í˜„ì£¼ ê¸°ì2024. 7. 25. 10:48</span><br><span class="line">ë²ˆì—­ ì„¤ì •ê¸€ì”¨í¬ê¸° ì¡°ì ˆí•˜ê¸°ì¸ì‡„í•˜ê¸°</span><br><span class="line">/BCì¹´ë“œ</span><br><span class="line"></span><br><span class="line">/BCì¹´ë“œ</span><br><span class="line">[ë§ˆì´ë°ì¼ë¦¬ = êµ¬í˜„ì£¼ ê¸°ì] êµ­ë‚´ ê¸ˆìœµì— ìµœì í™”ëœ ê±°ëŒ€ì–¸ì–´ëª¨ë¸(ì´í•˜ LLM)ì´ ë‚˜ì™”ë‹¤. LLMì€ ëŒ€ìš©ëŸ‰ ì¸ê°„ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ë„ë¡ í›ˆë ¨ëœ AI(ì¸ê³µì§€ëŠ¥) ëª¨ë¸ë¡œ ìƒì„±í˜• AI í•µì‹¬ ê¸°ìˆ ì´ë‹¤.</span><br><span class="line"></span><br><span class="line">25ì¼ BCì¹´ë“œëŠ” êµ­ë‚´ ê¸ˆìœµê¶Œì—ì„œ ì²˜ìŒìœ¼ë¡œ ê°œë°œí•œ K-ê¸ˆìœµ íŠ¹í™” AIë¥¼ ë¬´ìƒ ê³µê°œ í•œë‹¤ê³  25ì¼ ë°í˜”ë‹¤.</span><br><span class="line"></span><br><span class="line">ì´ë²ˆì— ê°œë°œëœ â€˜K-ê¸ˆìœµ íŠ¹í™” AIâ€™ëŠ” BCì¹´ë“œ ITê¸°íšë³¸ë¶€ê°€ KT ê¸°ìˆ í˜ì‹ ë¶€ë¬¸ ì‚°í•˜ KTì»¨ì„¤íŒ…ê·¸ë£¹ AI ë¦¬ë“œì™€ í˜‘ì—…í•´ ì§€ë‚œ 6ê°œì›”ê°„ ì—°êµ¬ ëì— êµ­ë‚´ì— ìµœì í™”í•œ LLMì´ë‹¤.</span><br><span class="line"></span><br><span class="line">K-ê¸ˆìœµ íŠ¹í™” AIëŠ” ë©”íƒ€(í˜ì´ìŠ¤ë¶)ì˜ ê±°ëŒ€ ì–¸ì–´ëª¨ë¸(LLama 3)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ì–´ í•™ìŠµ ëŠ¥ë ¥ì€ ë¬¼ë¡  ë‹¤ì–‘í•œ ê¸ˆìœµ ì§€ì‹ ì •ë³´ê¹Œì§€ íƒ‘ì¬í–ˆë‹¤.</span><br><span class="line"></span><br><span class="line">í˜„ì¬ êµ­ë‚´ì—ì„œ ê³µê°œëœ ëŒ€ë¶€ë¶„ LLMì€ 80ì–µê°œ ìˆ˜ì¤€ íŒŒë¼ë¯¸í„°ë¥¼ ê°–ì¶”ê³  ìˆì§€ë§Œ â€˜K-ê¸ˆìœµ íŠ¹í™” AIâ€™ëŠ” 200ì–µê°œ íŒŒë¼ë¯¸í„°ë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤. íŒŒë¼ë¯¸í„°ëŠ” ìƒì„±í˜• AIê°€ ì •ë³´ë¥¼ í•™ìŠµí•˜ê³  ê¸°ì–µí•˜ê¸° ìœ„í•´ í•„ìš”í•œ ê¸°ë³¸ ë‹¨ìœ„ë‹¤. íŒŒë¼ë¯¸í„°ê°€ ë§ì„ìˆ˜ë¡ ì¶•ì ëœ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³µì¡í•œ í•™ìŠµì„ í†µí•´ í•™ìŠµí•˜ì§€ ì•Šì•˜ë˜ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆì„ ë¿ë§Œ ì•„ë‹ˆë¼ ì •êµí•œ ì˜ˆì¸¡ê³¼ ë¶„ì„ë„ ê°€ëŠ¥í•´ì§„ë‹¤.</span><br><span class="line"></span><br><span class="line">K-ê¸ˆìœµ íŠ¹í™” AI ì •í™•ë„ëŠ” 91%ë¡œ ë²”ìš© AI ëŒ€ë¹„ ë†’ì€ ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ë©° í•œêµ­ ê¸ˆìœµì— ëŒ€í•œ LLM ì§€ì‹ìˆ˜ì¤€ì„ í•œ ë‹¨ê³„ ë” ëŒì–´ì˜¬ë ¸ë‹¤. ì´ëŠ” í•œêµ­ì€í–‰ ë“± ë‹¤ì–‘í•œ êµ­ì±…ê¸°ê´€ê³¼ ê¸ˆìœµê¸°ê´€ì˜ ê²€ì¦ëœ ë°ì´í„°ë§Œì„ í™œìš©í–ˆê¸° ë•Œë¬¸ì´ë‹¤.</span><br><span class="line"></span><br><span class="line">BCì¹´ë“œ ì¸¡ì€ K-ê¸ˆìœµ íŠ¹í™” AI ë„ì…ì„ ê¸°ì ìœ¼ë¡œ ê¸°ì—… ë‚´ë¶€ í”„ë¡œì„¸ìŠ¤ ê°œì„  ë° íš¨ìœ¨í™”ëŠ” ë¬¼ë¡  ì™œê³¡ëœ ê¸ˆìœµ ì •ë³´ë¡œ ì¸í•œ 2ì°¨ í”¼í•´ë¥¼ ì˜ˆë°©í•˜ëŠ” ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ê¸ì •ì ì¸ ì—­í• ì„ í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ë‚´ë‹¤ë´¤ë‹¤.</span><br><span class="line"></span><br><span class="line">7ì›” ì´ˆ AI ëª¨ë¸ í—ˆë¸Œ í”Œë«í¼ í—ˆê¹…í˜ì´ìŠ¤ë¥¼ í†µí•´ K-ê¸ˆìœµ íŠ¹í™” AI LLM ëª¨ë¸ê³¼ 2ë§Œì—¬ê°œ ê¸ˆìœµì§€ì‹ í•™ìŠµ ë°ì´í„°ë¥¼ ë¬´ìƒìœ¼ë¡œ ê³µê°œí–ˆë‹¤. í–¥í›„ K-ê¸ˆìœµ íŠ¹í™” AI ì§€ì†ì ì¸ ê³ ë„í™” ì‘ì—…ì„ í†µí•´ ê¸ˆìœµ AX ë¶„ì•¼ ë°œì „ì— ì´ë°”ì§€í•¨ì€ ë¬¼ë¡ , BCì¹´ë“œì— ì¹´ë“œ ìš´ì˜ì„ ë§¡ê¸°ê³  ìˆëŠ” ê¸ˆìœµì‚¬ë¥¼ ìœ„í•œ ë§ì¶¤í˜• â€˜ê¸ˆìœµ GPTâ€™ ë“±ì„ í†µí•´ ì°¨ë³„í™”ëœ ì„œë¹„ìŠ¤ë¥¼ ì§€ì† ì œê³µí•´ ë‚˜ê°ˆ ê³„íšì´ë‹¤.</span><br><span class="line"></span><br><span class="line">ê°•ëŒ€ì¼ BCì¹´ë“œ ìƒë¬´ëŠ” â€œê¸€ë¡œë²Œ AI ì‹œì¥ì—ì„œë„ ê²½ìŸí•  ìˆ˜ ìˆëŠ” í•œêµ­ì‚° ê¸ˆìœµ ì§€ì‹ ëª¨ë¸ì„ ì„ ë³´ì¼ ìˆ˜ ìˆê²Œ ë˜ì–´ ì˜ë¯¸ê°€ ë‚¨ë‹¤ë¥´ë‹¤â€ë©° â€œì•ìœ¼ë¡œë„ KT AI ê¸°ìˆ ë ¥ì„ ì ê·¹ í™œìš©í•´ êµ­ë‚´ ì—¬ëŸ¬ ì‚°ì—… ë¶„ì•¼ì—ì„œ ë‹¤ì–‘í•œ ì‹œë„ˆì§€ë¥¼ ë‚¼ ìˆ˜ ìˆë„ë¡ ì§€ì†ì ìœ¼ë¡œ í˜‘ì—…í•´ ë‚˜ê°ˆ ê³„íšâ€ì´ë¼ê³  ë§í–ˆë‹¤.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-08-07</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/08/07/2024-8-7-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 8ì›” 7ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/08/07/2024-8-9-AI-NEWS/" title="2024ë…„ 8ì›” 7ì¼ AI ì†Œì‹">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/08/01/2024-8-1-AI-NEWS/" title="2024ë…„ 8ì›” 1ì¼ AI ì†Œì‹">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>