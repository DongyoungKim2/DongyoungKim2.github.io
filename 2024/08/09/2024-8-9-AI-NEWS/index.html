<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 8ì›” 9ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="OpenAIì—ì„œëŠ” GPT-4o ëª¨ë¸ì— ëŒ€í•œ ì‹œìŠ¤í…œ ì¹´ë“œë¥¼ ë°œí‘œí•˜ë©°, ëª¨ë¸ì˜ ì•ˆì „ì„± í‰ê°€ ë° ì ì¬ì  ë¦¬ìŠ¤í¬ ê´€ë¦¬ì— ëŒ€í•´ ì„¤ëª…í•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, Zico Kolterê°€ ì´ì‚¬íšŒì˜ ìƒˆë¡œìš´ êµ¬ì„±ì›ìœ¼ë¡œ í•©ë¥˜í•˜ì˜€ìœ¼ë©°, ChatGPT ë¬´ë£Œ ì‚¬ìš©ìë“¤ì„ ìœ„í•œ DALLÂ·E 3 ì´ë¯¸ì§€ ìƒì„±"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 8ì›” 9ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><p>OpenAIì—ì„œëŠ” GPT-4o ëª¨ë¸ì— ëŒ€í•œ ì‹œìŠ¤í…œ ì¹´ë“œë¥¼ ë°œí‘œí•˜ë©°, ëª¨ë¸ì˜ ì•ˆì „ì„± í‰ê°€ ë° ì ì¬ì  ë¦¬ìŠ¤í¬ ê´€ë¦¬ì— ëŒ€í•´ ì„¤ëª…í•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, Zico Kolterê°€ ì´ì‚¬íšŒì˜ ìƒˆë¡œìš´ êµ¬ì„±ì›ìœ¼ë¡œ í•©ë¥˜í•˜ì˜€ìœ¼ë©°, ChatGPT ë¬´ë£Œ ì‚¬ìš©ìë“¤ì„ ìœ„í•œ DALLÂ·E 3 ì´ë¯¸ì§€ ìƒì„± ê¸°ëŠ¥ì´ ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤. Alibabaì˜ Qwen íŒ€ì€ ìƒˆë¡œìš´ ìˆ˜í•™ íŠ¹í™” ëª¨ë¸ Qwen2-Mathë¥¼ ê³µê°œí•˜ë©°, ìˆ˜í•™ì  ë¬¸ì œ í•´ê²°ì— ìˆì–´ GPT-4o ë° Claude 3.5 ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ì…ì¦í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ ì™¸ì—ë„ Parler TTSì˜ ê³ í’ˆì§ˆ TTS ëª¨ë¸ ê³µê°œ, Mistral AIì˜ ìƒˆë¡œìš´ ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì§• ë° ì—ì´ì „íŠ¸ ê¸°ëŠ¥ ë°œí‘œ, Whisper Medusaì˜ ê³ ì† ìŒì„± ì¸ì‹ ëª¨ë¸ ë°œí‘œ, ê·¸ë¦¬ê³  SENSE ë° RAGFoundryì˜ ìµœì‹  ì—°êµ¬ ì„±ê³¼ ë“±ì´ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
<h3 id="OpenAI-GPT-4o-System-Card-ë°œí‘œ"><a href="#OpenAI-GPT-4o-System-Card-ë°œí‘œ" class="headerlink" title="OpenAI, GPT-4o System Card ë°œí‘œ"></a>OpenAI, GPT-4o System Card ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/gpt-4o-system-card/">ë§í¬</a>, 2024ë…„ 8ì›” 8ì¼</p>
<ul>
<li>OpenAI, GPT-4o ëª¨ë¸ì˜ ì‹œìŠ¤í…œ ì¹´ë“œ ê³µê°œ,</li>
<li>GPT-4oëŠ” í…ìŠ¤íŠ¸, ë¹„ì „, ìŒì„± ì…ë ¥ì„ ì²˜ë¦¬í•˜ê³  ì¶œë ¥í•  ìˆ˜ ìˆëŠ” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë¡œ, ëª¨ë“  ì…ë ¥ê³¼ ì¶œë ¥ì´ ë™ì¼í•œ ì‹ ê²½ë§ì—ì„œ ì²˜ë¦¬ë¨,</li>
<li>GPT-4o ëª¨ë¸ì˜ ìŒì„± ëª¨ë“ˆì€ 232msì—ì„œ 320ms ì‚¬ì´ì˜ ì‘ë‹µ ì‹œê°„ì„ ë³´ì´ë©°, ì´ëŠ” ì¸ê°„ì˜ ëŒ€í™” ë°˜ì‘ ì‹œê°„ê³¼ ìœ ì‚¬í•¨,</li>
<li>ëª¨ë¸ í›ˆë ¨ì— ì‚¬ìš©ëœ ë°ì´í„°ëŠ” 2023ë…„ 10ì›”ê¹Œì§€ì˜ ê³µê°œ ë°ì´í„°ì™€ ì‚°ì—… í‘œì¤€ ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„°ì„¸íŠ¸, ê·¸ë¦¬ê³  ë…ì ì ì¸ ë°ì´í„°ë¡œ êµ¬ì„±ë¨,</li>
<li>GPT-4oëŠ” GPT-4 Turbo ëŒ€ë¹„ ë¹„ì˜ì–´ê¶Œ ì–¸ì–´ ì²˜ë¦¬ì—ì„œ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìœ¼ë©°, íŠ¹íˆ ìŒì„± ë° ë¹„ì „ ì´í•´ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜,</li>
<li>ì£¼ìš” ë¦¬ìŠ¤í¬ í‰ê°€ í•­ëª©ìœ¼ë¡œëŠ” ë¬´ë‹¨ ìŒì„± ìƒì„±, ìŠ¤í”¼ì»¤ ì‹ë³„, ê·¼ê±° ì—†ëŠ” ì¶”ë¡ , ë¯¼ê°í•œ íŠ¹ì„± ê·€ì†, ë¹„í—ˆìš© ì˜¤ë””ì˜¤ ì½˜í…ì¸  ìƒì„± ë“±ì´ ìˆìœ¼ë©°, ì´ëŸ¬í•œ ë¦¬ìŠ¤í¬ì— ëŒ€í•œ ëª¨ë¸ ë° ì‹œìŠ¤í…œ ë ˆë²¨ì˜ ì•ˆì „ ì¥ì¹˜ê°€ êµ¬í˜„ë¨,</li>
<li>Preparedness Frameworkì˜ í‰ê°€ì—ì„œ ì‚¬ì´ë²„ ë³´ì•ˆ, ìƒë¬¼í•™ì  ìœ„í˜‘, ëª¨ë¸ ììœ¨ì„± ì¹´í…Œê³ ë¦¬ì—ì„œ ë‚®ì€ ìœ„í—˜ë„ë¡œ í‰ê°€ë˜ì—ˆìœ¼ë©°, ì„¤ë“ë ¥ ì¹´í…Œê³ ë¦¬ì—ì„œ ì¤‘ê°„ ìœ„í—˜ë„ë¡œ í‰ê°€ë¨,</li>
<li>OpenAIëŠ” GPT-4o ëª¨ë¸ì„ ë°°í¬í•˜ê¸° ì „ì— ì•ˆì „ì„± í‰ê°€ì™€ ì™¸ë¶€ ë ˆë“œíŒ€ì˜ í…ŒìŠ¤íŠ¸ë¥¼ ê±°ì³¤ìœ¼ë©°, ì‹œìŠ¤í…œ ì¹´ë“œì™€ í•¨ê»˜ Preparedness Frameworkì˜ í‰ê°€ ê²°ê³¼ë¥¼ ê³µìœ í•˜ì—¬ GPT-4oì˜ ì•ˆì „ì„±ê³¼ ì ì¬ì  ë¦¬ìŠ¤í¬ì— ëŒ€í•œ ì¢…í•©ì ì¸ í‰ê°€ë¥¼ ì œê³µí•¨.</li>
</ul>
<h3 id="OpenAI-Zico-Kolter-ì´ì‚¬-ì„ëª…"><a href="#OpenAI-Zico-Kolter-ì´ì‚¬-ì„ëª…" class="headerlink" title="OpenAI, Zico Kolter ì´ì‚¬ ì„ëª…"></a>OpenAI, Zico Kolter ì´ì‚¬ ì„ëª…</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/zico-kolter-joins-openais-board-of-directors/">ë§í¬</a>, 2024ë…„ 8ì›” 8ì¼</p>
<ul>
<li>Zico Kolter, OpenAI ì´ì‚¬íšŒì˜ ìƒˆ êµ¬ì„±ì›ìœ¼ë¡œ í•©ë¥˜í•˜ë©°, AI ì•ˆì „ì„± ë° ê°•ê±´ì„± ë¶„ì•¼ì—ì„œì˜ ê¹Šì´ ìˆëŠ” ì „ë¬¸ì„±ì„ ì œê³µ,</li>
<li>KolterëŠ” Carnegie Mellon Universityì˜ ë¨¸ì‹ ëŸ¬ë‹ í•™ê³¼ì¥ì´ì AI ëª¨ë¸ì˜ ì•ˆì „ì„±, ê°•ê±´ì„± ë° ë°ì´í„° ì˜í–¥ì„ ì—°êµ¬í•˜ëŠ” ì „ë¬¸ê°€ë¡œ, ë‹¤ì–‘í•œ ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ì™€ ëª¨ë¸ì˜ ê°•ê±´ì„± í‰ê°€ ë°©ë²•ë¡ ì„ ê°œë°œí•´ì˜´,</li>
<li>KolterëŠ” AI ëª¨ë¸ì˜ ì·¨ì•½ì ì„ ìë™í™”ëœ ìµœì í™” ê¸°ë²•ìœ¼ë¡œ ë¶„ì„í•˜ê³ , ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ê°•ë ¥í•œ ì œì•½ ì¡°ê±´ì„ ë¶€ì—¬í•˜ëŠ” ê¸°ìˆ ì„ ê°œì²™,</li>
<li>ìµœê·¼ì—ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì•ˆì „ì„±ì„ ìë™ìœ¼ë¡œ í‰ê°€í•˜ëŠ” í˜ì‹ ì ì¸ ë°©ë²•ì„ ê°œë°œí•˜ì˜€ìœ¼ë©°, ì´ëŸ¬í•œ ê¸°ìˆ ì  ë°°ê²½ì„ ë°”íƒ•ìœ¼ë¡œ OpenAIì˜ ì´ì‚¬íšŒì—ì„œ AI ì•ˆì „ì„± ë° ë³´ì•ˆ ê´€ë ¨ ì£¼ìš” ê²°ì •ì„ ì§€ì›í•  ì˜ˆì •.</li>
</ul>
<h3 id="Alibaba-Qwen2-Math-ëª¨ë¸-ë°œí‘œ"><a href="#Alibaba-Qwen2-Math-ëª¨ë¸-ë°œí‘œ" class="headerlink" title="Alibaba, Qwen2-Math ëª¨ë¸ ë°œí‘œ"></a>Alibaba, Qwen2-Math ëª¨ë¸ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://qwenlm.github.io/blog/qwen2-math/">ë§í¬</a>, 2024ë…„ 8ì›” 8ì¼</p>
<ul>
<li>Alibaba Qwen íŒ€, ìˆ˜í•™ì  ë¬¸ì œ í•´ê²°ì— íŠ¹í™”ëœ Qwen2-Math ëª¨ë¸ ì‹œë¦¬ì¦ˆ ë°œí‘œ,</li>
<li>Qwen2-Math ì‹œë¦¬ì¦ˆëŠ” 1.5B, 7B, 72B íŒŒë¼ë¯¸í„°ë¡œ êµ¬ì„±ëœ ëª¨ë¸ë¡œ, GPT-4o ë° Claude 3.5ì™€ ê°™ì€ ìµœì‹  ëª¨ë¸ì„ ë›°ì–´ë„˜ëŠ” ì„±ëŠ¥ì„ ìë‘,</li>
<li>Olympiad Bench, College Math, MMLU STEM ë“± ë‹¤ì–‘í•œ ìˆ˜í•™ ë²¤ì¹˜ë§ˆí¬ì—ì„œ íƒì›”í•œ ì„±ê³¼ë¥¼ ê¸°ë¡, íŠ¹íˆ 72B ëª¨ë¸ì€ Olympiad Benchì—ì„œ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±,</li>
<li>Qwen2 ì•„í‚¤í…ì²˜ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í•™ì  ë°ì´í„°ì— íŠ¹í™”ëœ ì‚¬ì „ í›ˆë ¨ì„ ê±°ì³¤ìœ¼ë©°, ì¶”ê°€ë¡œ ìˆ˜í•™ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì¸ìŠ¤íŠ¸ëŸ­ì…˜ ëª¨ë¸(SFT)ì„ í†µí•´ ì„±ëŠ¥ì„ ê°•í™”,</li>
<li>ì´ ëª¨ë¸ì€ ì²´ì¸-ì˜¤ë¸Œ-ìƒê°(Chain-of-Thought) í”„ë¡¬í”„íŠ¸ ë°©ì‹ì„ í™œìš©í•˜ì—¬ ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œë¥¼ í•´ê²°í•˜ë©°, íŠ¹íˆ ë‹¤ë‹¨ê³„ ìˆ˜í•™ ë¬¸ì œì—ì„œë„ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì—¬ì¤Œ,</li>
<li>Qwen2-Math ì‹œë¦¬ì¦ˆì˜ ë°ì´í„°ì…‹ì€ ìˆ˜í•™ì  ì›¹ í…ìŠ¤íŠ¸, ì±…, ì½”ë“œ, ì‹œí—˜ ë¬¸ì œ ë“± ê³ í’ˆì§ˆì˜ ìˆ˜í•™ ë°ì´í„°ë¥¼ í¬í•¨í•˜ë©°, ì¶”ê°€ì ìœ¼ë¡œ Qwen2ì— ì˜í•´ ìƒì„±ëœ í•©ì„± ë°ì´í„°ë¡œ êµ¬ì„±ë¨,</li>
<li>ì´ ëª¨ë¸ì€ í•™ìŠµ ë°ì´í„°ì˜ ì¤‘ë³µì„ ì œê±°í•˜ê¸° ìœ„í•´ ì—„ê²©í•œ ë°ì´í„° í•„í„°ë§ ë°©ë²•ì„ ì ìš©, ì˜ˆë¥¼ ë“¤ì–´ ì •í™•í•œ ë§¤ì¹­ê³¼ 13-ê·¸ë¨ ì¤‘ë³µ ì œê±°ë¥¼ í†µí•´ í•™ìŠµ ë°ì´í„°ì˜ ì˜¤ì—¼ì„ ë°©ì§€.</li>
</ul>
<h3 id="Parler-TTS-ê³ í’ˆì§ˆ-TTS-ëª¨ë¸-ì¶œì‹œ"><a href="#Parler-TTS-ê³ í’ˆì§ˆ-TTS-ëª¨ë¸-ì¶œì‹œ" class="headerlink" title="Parler TTS, ê³ í’ˆì§ˆ TTS ëª¨ë¸ ì¶œì‹œ"></a>Parler TTS, ê³ í’ˆì§ˆ TTS ëª¨ë¸ ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/collections/parler-tts/parler-tts-fully-open-source-high-quality-tts-66164ad285ba03e8ffde214c">ë§í¬</a>, 2024ë…„ 8ì›” 8ì¼</p>
<ul>
<li>Parler TTS í”„ë¡œì íŠ¸, ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸-ìŒì„± ë³€í™˜(TTS) ëª¨ë¸ì¸ Parler TTS v1 ê³µê°œ,</li>
<li>ë‘ ê°€ì§€ ëª¨ë¸ í¬ê¸°(885M ë° 2.2B íŒŒë¼ë¯¸í„°)ë¡œ ì œê³µë˜ë©°, 45,000ì‹œê°„ì˜ ê³µê°œ ìŒì„± ë°ì´í„°ë¡œ í›ˆë ¨ë¨,</li>
<li>Torch Compile ë° Static KV ìºì‹œ ì ìš©ìœ¼ë¡œ ì´ì „ ëª¨ë¸ ëŒ€ë¹„ ìµœëŒ€ 4ë°° ë¹ ë¥¸ ìŒì„± ìƒì„± ì†ë„ë¥¼ ìë‘,</li>
<li>Parler TTS MiniëŠ” ë” í° í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¡œ í›ˆë ¨ë˜ì—ˆìœ¼ë©°, Parler TTS LargeëŠ” ë” í° í…ìŠ¤íŠ¸ ë° ë””ì½”ë”ë¡œ í›ˆë ¨ë˜ì–´ ì„±ëŠ¥ í–¥ìƒ,</li>
<li>Apache 2.0 ë¼ì´ì„ ìŠ¤ í•˜ì— ì½”ë“œë² ì´ìŠ¤ì™€ ê°€ì¤‘ì¹˜, ë°ì´í„°ì„¸íŠ¸ê°€ ëª¨ë‘ ê³µê°œë˜ì–´ ì˜¤í”ˆ ì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ììœ ë¡­ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ,</li>
<li>ëª¨ë¸ì€ ë” ë‚˜ì€ ìŒì„± ì¼ê´€ì„±ê³¼ ë‹¤ì–‘í•œ ìŠ¤í”¼ì»¤ ì„ íƒ ì˜µì…˜ì„ ì œê³µí•˜ë©°, ì‚¬ìš©ìê°€ í•„ìš”ì— ë”°ë¼ ëª¨ë¸ì„ ì„¸ë¶€ ì¡°ì •(fine-tuning)í•  ìˆ˜ ìˆìŒ, ë‹¨ ëª‡ ì‹œê°„ì˜ ë°ì´í„°ë¡œë„ ì¶”ê°€ì ì¸ í›ˆë ¨ì´ ê°€ëŠ¥.</li>
</ul>
<h3 id="Mistral-AI-ìƒˆë¡œìš´-ëª¨ë¸-ì»¤ìŠ¤í„°ë§ˆì´ì§•-ë°-ì—ì´ì „íŠ¸-ê¸°ëŠ¥-ë°œí‘œ"><a href="#Mistral-AI-ìƒˆë¡œìš´-ëª¨ë¸-ì»¤ìŠ¤í„°ë§ˆì´ì§•-ë°-ì—ì´ì „íŠ¸-ê¸°ëŠ¥-ë°œí‘œ" class="headerlink" title="Mistral AI, ìƒˆë¡œìš´ ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì§• ë° ì—ì´ì „íŠ¸ ê¸°ëŠ¥ ë°œí‘œ"></a>Mistral AI, ìƒˆë¡œìš´ ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì§• ë° ì—ì´ì „íŠ¸ ê¸°ëŠ¥ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://mistral.ai/news/build-tweak-repeat/">ë§í¬</a>, 2024ë…„ 8ì›” 7ì¼</p>
<ul>
<li>Mistral AI, La Plateformeì—ì„œì˜ ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê¸°ëŠ¥ ë°œí‘œ,</li>
<li>ì‚¬ìš©ìëŠ” Mistral Large 2 ë° Codestralê³¼ ê°™ì€ ì£¼ë ¥ ëª¨ë¸ë“¤ì„ ì‚¬ìš©ì ë°ì´í„°ì…‹ì„ ì´ìš©í•´ ë¯¸ì„¸ ì¡°ì • ê°€ëŠ¥,</li>
<li>ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•ì€ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸, few-shot í”„ë¡¬í”„íŒ…, ë˜ëŠ” ë¯¸ì„¸ ì¡°ì •(fine-tuning) ë°©ë²•ì„ í†µí•´ ì´ë£¨ì–´ì§€ë©°, ì´ë¥¼ í†µí•´ íŠ¹ì • ë„ë©”ì¸ ì§€ì‹, ë¬¸ë§¥, ë˜ëŠ” í†¤ì„ ë°˜ì˜í•œ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŒ,</li>
<li>ë˜í•œ, ì‚¬ìš©ìë“¤ì´ ë” ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ëŠ” ì—ì´ì „íŠ¸ ê¸°ëŠ¥ì˜ ì´ˆê¸° ë²„ì „ì„ ë°œí‘œ, ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•´ ì¡°ì§ ë‚´ì—ì„œ ì‰½ê²Œ ê³µìœ  ê°€ëŠ¥,</li>
<li>Mistralai ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ 1.0 ë²„ì „ì´ ë¦´ë¦¬ìŠ¤ë˜ì—ˆìœ¼ë©°, ì´ëŠ” Python ë° Typescriptì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ê³ , ì‚¬ìš© í¸ì˜ì„±ê³¼ ì¼ê´€ì„±ì´ í¬ê²Œ ê°œì„ ë¨.</li>
</ul>
<h3 id="Whisper-Medusa-ê³ ì†-ìŒì„±-ì¸ì‹-ëª¨ë¸-ë°œí‘œ"><a href="#Whisper-Medusa-ê³ ì†-ìŒì„±-ì¸ì‹-ëª¨ë¸-ë°œí‘œ" class="headerlink" title="Whisper Medusa, ê³ ì† ìŒì„± ì¸ì‹ ëª¨ë¸ ë°œí‘œ"></a>Whisper Medusa, ê³ ì† ìŒì„± ì¸ì‹ ëª¨ë¸ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/aiola/whisper-medusa-v1">ë§í¬</a>, 2024ë…„ 8ì›” 8ì¼</p>
<ul>
<li>Whisper Medusa ëª¨ë¸, ê¸°ì¡´ Whisper ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê³ ì† ìŒì„± ì¸ì‹ ë° ë²ˆì—­ ëª¨ë¸ ë°œí‘œ,</li>
<li>Medusa í—¤ë“œ êµ¬ì¡°ë¥¼ í†µí•´ ê° ë°˜ë³µì—ì„œ ì—¬ëŸ¬ í† í°ì„ ì˜ˆì¸¡í•˜ì—¬ ì†ë„ í–¥ìƒ (ìµœì†Œí•œì˜ WER ì €í•˜),</li>
<li>ì´ ëª¨ë¸ì€ LibriSpeech ë°ì´í„°ì…‹ì—ì„œ í›ˆë ¨ë˜ì—ˆìœ¼ë©°, ì˜ì–´ ì˜¤ë””ì˜¤ì— ìµœì í™”ëœ ì„±ëŠ¥ì„ ì œê³µ,</li>
<li>Medusa ëª¨ë¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ ì‚¬ìš©ëœ Medusa í—¤ë“œë¥¼ ASR(Automatic Speech Recognition)ì— ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”, Whisper ëª¨ë¸ë³´ë‹¤ 150% ë” ë¹ ë¥¸ ìŒì„± ìƒì„±ì´ ê°€ëŠ¥.</li>
</ul>
<h3 id="SENSE-ëª¨ë¸-Text-to-SQL-ë°ì´í„°-í•©ì„±-ì—°êµ¬-ë°œí‘œ"><a href="#SENSE-ëª¨ë¸-Text-to-SQL-ë°ì´í„°-í•©ì„±-ì—°êµ¬-ë°œí‘œ" class="headerlink" title="SENSE ëª¨ë¸, Text-to-SQL ë°ì´í„° í•©ì„± ì—°êµ¬ ë°œí‘œ"></a>SENSE ëª¨ë¸, Text-to-SQL ë°ì´í„° í•©ì„± ì—°êµ¬ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.03256">ë§í¬</a>, 2024ë…„ 8ì›” 6ì¼</p>
<ul>
<li>SENSE ëª¨ë¸, í…ìŠ¤íŠ¸-ë°ì´í„°ë² </li>
</ul>
<p>ì´ìŠ¤(SQL) ì§ˆì˜ ë³€í™˜ì—ì„œ ìµœì‹  ì„±ëŠ¥ì„ ê¸°ë¡í•œ ì—°êµ¬ ë°œí‘œ,</p>
<ul>
<li>ëŒ€í˜• ëª¨ë¸ì˜ í•©ì„± ë°ì´í„°ì™€ ì‘ì€ ëª¨ë¸ì˜ ì˜¤ë¥˜ ë°ì´í„°ë¥¼ í†µí•©í•´ ë°ì´í„° ë‹¤ì–‘ì„±ì„ ê°•í™”í•˜ê³ , ì‹¤í–‰ í”¼ë“œë°±ì„ í†µí•´ í•™ìŠµí•˜ëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆ,</li>
<li>ì„ í˜¸ í•™ìŠµ(Preference Learning)ì„ í™œìš©í•´ ì˜¬ë°”ë¥¸ ìƒ˜í”Œê³¼ ì˜¤ë¥˜ ìƒ˜í”Œ ëª¨ë‘ì—ì„œ í•™ìŠµì„ ìœ ë„,</li>
<li>SPIDER ë° BIRD ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë¸ê³¼ íì‡„í˜• ëª¨ë¸ ê°„ì˜ ì„±ëŠ¥ ê²©ì°¨ë¥¼ ì¤„ì´ë©° ìµœì‹  ì„±ê³¼ ë‹¬ì„±.</li>
</ul>
<h3 id="RAGFoundry-RAG-í™œìš©ì„-ìœ„í•œ-ì˜¤í”ˆ-ì†ŒìŠ¤-í”„ë ˆì„ì›Œí¬-ë°œí‘œ"><a href="#RAGFoundry-RAG-í™œìš©ì„-ìœ„í•œ-ì˜¤í”ˆ-ì†ŒìŠ¤-í”„ë ˆì„ì›Œí¬-ë°œí‘œ" class="headerlink" title="RAGFoundry, RAG í™œìš©ì„ ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ ë°œí‘œ"></a>RAGFoundry, RAG í™œìš©ì„ ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.02545">ë§í¬</a>, 2024ë…„ 8ì›” 5ì¼</p>
<ul>
<li>RAGFoundry, Retrieval-Augmented Generation (RAG) ì‹œìŠ¤í…œì„ ìœ„í•œ í†µí•© í”„ë ˆì„ì›Œí¬ ê³µê°œ,</li>
<li>ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë°ì´í„° ìƒì„±, í›ˆë ¨, ì¶”ë¡  ë° í‰ê°€ë¥¼ í•˜ë‚˜ì˜ ì›Œí¬í”Œë¡œìš°ë¡œ í†µí•©í•˜ì—¬, ë°ì´í„° ì¦ê°€í˜• ë°ì´í„°ì…‹ ìƒì„± ë° í‰ê°€ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•¨,</li>
<li>LLMsì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë‹¤ì–‘í•œ RAG ê¸°ë²•ì„ ì‹ ì†í•˜ê²Œ í”„ë¡œí† íƒ€ì´í•‘í•˜ê³  ì‹¤í—˜í•  ìˆ˜ ìˆë„ë¡ ì§€ì›,</li>
<li>Llama-3 ë° Phi-3 ëª¨ë¸ì„ RAGFoundryë¡œ ê°•í™”í•˜ì—¬ ì§€ì‹ ì§‘ì•½ì  ë°ì´í„°ì…‹ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ ê°œì„ ì„ ë‹¬ì„±,</li>
<li>ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì½”ë“œê°€ ì œê³µë˜ì–´ ì—°êµ¬ìì™€ ê°œë°œìë“¤ì´ ììœ ë¡­ê²Œ í™œìš© ê°€ëŠ¥.</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹"><a href="#todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹" class="headerlink" title="(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,"></a>(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAIì—ì„œëŠ” ~~~ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<p><a href="link">ë§í¬</a>, date,</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://openai.com/index/gpt-4o-system-card/</span><br><span class="line">August 8, 2024</span><br><span class="line"></span><br><span class="line">GPT-4o System Card</span><br><span class="line">This report outlines the safety work carried out prior to releasing GPT-4o including external red teaming, frontier risk evaluations according to our Preparedness Framework, and an overview of the mitigations we built in to address key risk areas.</span><br><span class="line"></span><br><span class="line">View PDF version</span><br><span class="line">GPT-4o Scorecard</span><br><span class="line">Key Areas of Risk Evaluation &amp; Mitigation</span><br><span class="line"></span><br><span class="line">Unauthorized voice generation</span><br><span class="line">Speaker identification</span><br><span class="line">Ungrounded inference &amp; sensitive trait attribution</span><br><span class="line">Generating disallowed audio content</span><br><span class="line">Generating erotic &amp; violent speech</span><br><span class="line">Preparedness Framework Scorecard</span><br><span class="line"></span><br><span class="line">Cybersecurity</span><br><span class="line">Low</span><br><span class="line">Biological Threats</span><br><span class="line">Low</span><br><span class="line">Persuasion</span><br><span class="line">Medium</span><br><span class="line">Model Autonomy</span><br><span class="line">Low</span><br><span class="line">Scorecard ratings</span><br><span class="line">Low</span><br><span class="line">Medium</span><br><span class="line">High</span><br><span class="line">Critical</span><br><span class="line">Only models with a post-mitigation score of &quot;medium&quot; or below can be deployed.</span><br><span class="line">Only models with a post-mitigation score of &quot;high&quot; or below can be developed further.</span><br><span class="line"></span><br><span class="line">We thoroughly evaluate new models for potential risks and build in appropriate safeguards before deploying them in ChatGPT or the API. Weâ€™re publishing the model System Card together with the Preparedness Framework scorecard to provide an end-to-end safety assessment of GPT-4o, including what weâ€™ve done to track and address todayâ€™s safety challenges as well as frontier risks.</span><br><span class="line"></span><br><span class="line">Building on the safety evaluations and mitigations we developed for GPT-4, and GPT-4V, weâ€™ve focused additional efforts on GPT-4o&#x27;s audio capabilities which present novel risks, while also evaluating its text and vision capabilities.</span><br><span class="line"></span><br><span class="line">Some of the risks we evaluated include speaker identification, unauthorized voice generation, the potential generation of copyrighted content, ungrounded inference, and disallowed content. Based on these evaluations, weâ€™ve implemented safeguards at both the model- and system-levels to mitigate these risks.</span><br><span class="line"></span><br><span class="line">Our findings indicate that GPT-4oâ€™s voice modality doesnâ€™t meaningfully increase Preparedness risks. Three of the four Preparedness Framework categories scored low, with persuasion, scoring borderline medium. The Safety Advisory Group(opens in a new window) reviewed our Preparedness evaluations and mitigations as part of our safe deployment process. We invite you to read the details of this work in the report below.</span><br><span class="line"></span><br><span class="line">Introduction</span><br><span class="line">GPT-4o1 is an autoregressive omni model, which accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. Itâ€™s trained end-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network.</span><br><span class="line"></span><br><span class="line">GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window)2 in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.</span><br><span class="line"></span><br><span class="line">In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House3, we are sharing the GPT-4o System Card, which includes our Preparedness Framework(opens in a new window)5 evaluations. In this System Card, we provide a detailed look at GPT-4oâ€™s capabilities, limitations, and safety evaluations across multiple categories, with a focus on speech-to-speech (voice)A while also evaluating text and image capabilities, and the measures weâ€™ve taken to enhance safety and alignment. We also include third party assessments on general autonomous capabilities, as well as discussion of potential societal impacts of GPT-4o text and vision capabilities.</span><br><span class="line"></span><br><span class="line">Model data &amp; training</span><br><span class="line">GPT-4o&#x27;s capabilities were pre-trained using data up to October 2023, sourced from a wide variety of materials including:</span><br><span class="line"></span><br><span class="line">Select publicly available data, mostly collected from industry-standard machine learning datasets and web crawls.</span><br><span class="line"></span><br><span class="line">Proprietary data from data partnerships. We form partnerships to access non-publicly available data, such as pay-walled content, archives, and metadata. For example, we partnered with Shutterstock(opens in a new window)5 on building and delivering AI-generated images.</span><br><span class="line"></span><br><span class="line">The key dataset components that contribute to GPT-4oâ€™s capabilities are:</span><br><span class="line"></span><br><span class="line">Web Data â€“ Data from public web pages provides a rich and diverse range of information, ensuring the model learns from a wide variety of perspectives and topics.</span><br><span class="line"></span><br><span class="line">Code and math â€“ Including code and math data in training helps the model develop robust reasoning skills by exposing it to structured logic and problem-solving processes.</span><br><span class="line"></span><br><span class="line">Multimodal data â€“ Our dataset includes images, audio, and video to teach the LLMs how to interpret and generate non-textual input and output. From this data, the model learns how to interpret visual images, actions and sequences in real-world contexts, language patterns, and speech nuances.</span><br><span class="line"></span><br><span class="line">Prior to deployment, OpenAI assesses and mitigates potential risks that may stem from generative models, such as information harms, bias and discrimination, or other content that violates our safety policies. We use a combination of methods, spanning all stages of development across pre-training, post-training, product development, and policy. For example, during post-training, we align the model to human preferences; we red team the resulting models and add product-level mitigations such as monitoring and enforcement; and we provide moderation tools and transparency reports to our users.</span><br><span class="line"></span><br><span class="line">We find that the majority of effective testing and mitigations are done after the pre-training stage because filtering pre-trained data alone cannot address nuanced and context-specific harms. At the same time, certain pre-training filtering mitigations can provide an additional layer of defense that, along with other safety mitigations, help exclude unwanted and harmful information from our datasets:</span><br><span class="line"></span><br><span class="line">We use our Moderation API and safety classifiers to filter out data that could contribute to harmful content or information hazards, including CSAM, hateful content, violence, and CBRN.</span><br><span class="line"></span><br><span class="line">As with our previous image generation systems, we filter our image generation datasets for explicit content such as graphic sexual material and CSAM.</span><br><span class="line"></span><br><span class="line">We use advanced data filtering processes to reduce personal information from training data.</span><br><span class="line"></span><br><span class="line">Upon releasing DALLÂ·E 3, we piloted a new approach to give users the power to opt images out of training. To respect those opt-outs, we fingerprinted the images and used the fingerprints to remove all instances of the images from the training dataset for the GPT-4o series of models.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Weâ€™re sharing the GPT-4o System Card, an end-to-end safety assessment that outlines what weâ€™ve done to track and address safety challenges, including frontier model risks in accordance with our Preparedness Framework.</span><br><span class="line">To ensure people could use this technology safely, we tested this model internally and with 100+ external red teamers across 45 languages. Our Preparedness evaluations were reviewed by our Safety Advisory Group before deploying the model.</span><br><span class="line">The System Card focuses on evaluating the novel risks presented by GPT-4o&#x27;s audio capabilities as well as the guardrails we implemented to prevent the generation of harmful, biased, or copyrighted content, and to ensure the model only generates audio in one of the preset voices.</span><br><span class="line">We care deeply about the impact our technology has on the people who use it, and will continue to assess, calibrate, and share our learnings to ensure everyone can enjoy the benefits of AI.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/zico-kolter-joins-openais-board-of-directors/</span><br><span class="line">OpenAI</span><br><span class="line">August 8, 2024</span><br><span class="line"></span><br><span class="line">Zico Kolter Joins OpenAIâ€™s Board of Directors</span><br><span class="line">Weâ€™re strengthening our governance with expertise in AI safety and alignment. Zico will also join the Safety &amp; Security Committee.</span><br><span class="line"></span><br><span class="line">Hero &gt; Zico Kolter Joins OpenAIâ€™s Board of Directors &gt; Media Asset</span><br><span class="line">Weâ€™re announcing the appointment of Zico Kolter to OpenAIâ€™s Board of Directors. As a professor and Director of the Machine Learning Department at Carnegie Mellon University, Zicoâ€™s work predominantly focuses on AI safety, alignment, and the robustness of machine learning classifiers. His research and expertise spans new deep network architectures, innovative methodologies for understanding the influence of data  on models, and automated methods for evaluating AI model robustness, making him an invaluable technical director for our governance.</span><br><span class="line"></span><br><span class="line">Zico will also join the Boardâ€™s Safety and Security Committee alongside directors Bret Taylor, Adam Dâ€™Angelo, Paul Nakasone, Nicole Seligman and Sam Altman (CEO) and OpenAI technical experts. The committee is responsible for making recommendations on critical safety and security decisions for all OpenAI projects.</span><br><span class="line"></span><br><span class="line">In welcoming Zico to the board, Bret Taylor, Chairman of the Board, remarked, â€œZico adds deep technical understanding and perspective in AI safety and robustness that will help us ensure general artificial intelligence benefits all of humanity.â€</span><br><span class="line"></span><br><span class="line">Zico Kolter is a Professor of Computer Science and the head of the Machine Learning Department at Carnegie Mellon University, where he has been a key figure for 12 years. Zico completed his Ph.D. in computer science at Stanford University in 2010, followed by a postdoctoral fellowship at MIT from 2010 to 2012. Throughout his career, he has made significant contributions to the field of machine learning, authoring numerous award-winning papers at prestigious conferences such as NeurIPS, ICML, and AISTATS.</span><br><span class="line"></span><br><span class="line">Zico&#x27;s research includes developing the first methods for creating deep learning models with guaranteed robustness. He pioneered techniques for embedding hard constraints into AI models using classical optimization within neural network layers. More recently, in 2023, his team developed innovative methods for automatically assessing the safety of large language models (LLMs), demonstrating the potential to bypass existing model safeguards through automated optimization techniques. Alongside his academic pursuits, Zico has worked closely within the industry throughout his career, formerly as Chief Data Scientist at C3.ai, and currently as Chief Expert at Bosch and Chief Technical Advisor at Gray Swan, a startup specializing in AI safety and security.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">8/8/24</span><br><span class="line">OpenAI</span><br><span class="line">Weâ€™re rolling out the ability for ChatGPT Free users to create up to two images per day with DALLÂ·E 3. Just ask ChatGPT to create an image for a slide deck, personalize a card for a friend, or show you what something looks like.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://qwenlm.github.io/blog/qwen2-math/</span><br><span class="line">Alibaba</span><br><span class="line">Wow! Qwen released Qwen2-Math - a 1.5B, 7B &amp; 72B models - beats GPT4o, Claude 3.5 on AIME 24/ AMC 23 ğŸ”¥</span><br><span class="line">&gt; 84 (72B), 75 (7B), 69.4 (1.5B) on MATH</span><br><span class="line">&gt; 72B SoTA in Olympiad Bench, College Math, MMLU STEM</span><br><span class="line">&gt; Release both base and instruct models</span><br><span class="line">&gt; Apache 2.0 license for 1.5B &amp; 7B, 72B released under Qianwen license</span><br><span class="line">&gt; Based on the same Qwen 2 Architecture</span><br><span class="line">&gt; Pretrained further on Math specific data (they don&#x27;t describe what) + synthetic data generated by Qwen 2</span><br><span class="line">&gt; Construct the SFT data with RM + rejection sampling</span><br><span class="line">&gt; Perform GRPO after SFT</span><br><span class="line">&gt; They decontaminate pre-training plus instruct datasets with exact match and 13-gram dedupe</span><br><span class="line">&gt; Integrated with Transformers! ğŸ¤—</span><br><span class="line">Kudos to Qwen team on yet another stellar release ğŸ</span><br><span class="line"></span><br><span class="line">Introducing Qwen2-Math</span><br><span class="line">August 8, 2024</span><br><span class="line"> Â· 28 min Â· 5758 words Â· Qwen Team</span><br><span class="line">GITHUB HUGGING FACE MODELSCOPE DISCORD</span><br><span class="line"></span><br><span class="line">ğŸš¨ This model mainly supports English. We will release bilingual (English and Chinese) math models soon.</span><br><span class="line">Introduction</span><br><span class="line">Over the past year, we have dedicated significant effort to researching and enhancing the reasoning capabilities of large language models, with a particular focus on their ability to solve arithmetic and mathematical problems. Today, we are delighted to introduce a series of math-specific large language models of our Qwen2 series, Qwen2-Math and Qwen2-Math-Instruct-1.5B/7B/72B. Qwen2-Math is a series of specialized math language models built upon the Qwen2 LLMs, which significantly outperforms the mathematical capabilities of open-source models and even closed-source models (e.g., GPT-4o). We hope that Qwen2-Math can contribute to the community for solving complex mathematical problems.</span><br><span class="line"></span><br><span class="line">We evaluate our math-specific models on a series of math benchmarks. The results below demonstrate that our largest math-specific model Qwen2-Math-72B-Instruct outperforms the state-of-the-art models, including GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro, and Llama-3.1-405B.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Qwen2-Math: Base Models</span><br><span class="line">The base models of Qwen2-Math are initialized with Qwen2-1.5B/7B/72B, and then pretrained on a meticulously designed Mathematics-specific Corpus. This corpus contains large-scale high-quality mathematical web texts, books, codes, exam questions, and mathematical pre-training data synthesized by Qwen2.</span><br><span class="line"></span><br><span class="line">We evaluate our Qwen2-Math base models on three widely used English math benchmarks GSM8K, Math, and MMLU-STEM. In addition, we also evaluate three Chinese math benchmarks CMATH, GaoKao Math Cloze, and GaoKao Math QA. All evaluations are tested with few-shot chain-of-thought prompting.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Qwen2-Math-Instruct: Instruction-Tuned Models</span><br><span class="line">We first trained a math-specific reward model based on Qwen2-Math-72B. We then combined this dense reward signal with a binary signal indicating whether the model answered correctly. This combined signal is used as supervision for constructing the SFT data through Rejection Sampling and also in the reinforcement learning with Group Relative Policy Optimization (GRPO) after SFT.</span><br><span class="line"></span><br><span class="line">We evaluate Qwen2-Math-Instruct on mathematical benchmarks in both English and Chinese. In addition to the widely-used benchmarks, such as GSM8K and Math, we also involve more exams that are much challenging to fully inspect the capabilities of Qwen2-Math-Instruct, such as OlympiadBench, CollegeMath, GaoKao, AIME2024, and AMC2023. For Chinese mathematical benchmarks, we use CMATH, Gaokao (Chinese college entrance examination 2024), and CN Middle School 24 (China High School Entrance Examination 2024).</span><br><span class="line"></span><br><span class="line">We report greedy , Maj@8 and RM@8 performance on all benchmarks in the zero-shot setting, except for the multi-choice benchmarks (including MMLU STEM and multiple-choice problems in GaoKao and CN Middle School 24) with a 5-shot setting. Qwen2-Math-Instruct achieves the best performance among models of the same size, with RM@8 outperforming Maj@8, particularly in the 1.5B and 7B models. This demonstrates the effectiveness of our math reward model.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In more complex mathematical competition evaluations such as AIME 2024 and AMC 2023, Qwen2-Math-Instruct also performs well across various settings, including Greedy, Maj@64, RM@64, and RM@256.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Case Study</span><br><span class="line">Here we list some test cases, which include some IMO math problems. From the experimental results and case study, we find that Qwen2-Math is capable of solving simple math competition problems. Feel free to click the expandable blocks to check the cases!</span><br><span class="line"></span><br><span class="line">All the solutions are generated by our model without modification. Please note that we do not guarantee the correctness of the claims in the process.</span><br><span class="line"></span><br><span class="line">Problem From IMO Shortlist 2002</span><br><span class="line">Problem From IMO Shortlist 2022</span><br><span class="line">Problem From IMO 2022</span><br><span class="line">Problem from International Zhautykov Olympiad 2020</span><br><span class="line">Problem From Baltic Way 2023</span><br><span class="line">Problem From Lusophon Mathematical Olympiad 2023</span><br><span class="line">Problem From Balkan MO 2023</span><br><span class="line">Problem From Math Odyssey</span><br><span class="line">Problem from USAMO 2010</span><br><span class="line">Problem from JBMO Shortlist 2011</span><br><span class="line">Decontamination</span><br><span class="line">We conduct decontamination methods on both our pretraining and post-training datasets. Specifically, for pretraining data, we target on math datasets, including GSM8K, MATH, and remove samples that have significant overlaps with the test sets. We use exact match to remove the identical samples and further apply 13-gram deduplication (with a condition that the ratio of longest common sequence should be larger than 0.6) to remove more samples that might cause contamination. For post-training data, we remove more postitive contaminated samples that have overlaps with GSM8K, MATH, Aqua, SAT Math, OlympiadBench, College Math, AIME24, AMC23, etc. with the same filtering method.</span><br><span class="line"></span><br><span class="line">Summary</span><br><span class="line">This time, weâ€™re releasing a new model series focused on mathematical capabilities, Qwen2-Math, built upon the Qwen2 foundation. Our flagship model, Qwen2-Math-72B-Instruct, outperforms proprietary models such as GPT-4o and Claude 3.5 in math-related tasks. Given the current limitation of English-only support, we plan to release bilingual models that support both English and Chinese shortly, with the development of multilingual models also in the pipeline. Moreover, we will continue to enhance our modelsâ€™ ability to solve complex and challenging mathematical problems.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/collections/parler-tts/parler-tts-fully-open-source-high-quality-tts-66164ad285ba03e8ffde214c</span><br><span class="line">8/8/24</span><br><span class="line">parler-tts</span><br><span class="line">Introducing Parler TTS v1 ğŸ”‰ - 885M (Mini) &amp; 2.2B (Large) - fully open-source Text-to-Speech models! ğŸ¤™</span><br><span class="line">&gt; Trained on 45,000 hours of open speech (datasets released as well)</span><br><span class="line">&gt; Upto 4x faster generation thanks to torch compile &amp; static KV cache (compared to previous v0.1 release)</span><br><span class="line">&gt; Mini trained on a larger text encoder, large trained on both larger text &amp; decoder</span><br><span class="line">&gt; Also supports SDPA &amp; Flash Attention 2 for an added speed boost</span><br><span class="line">&gt; In-built streaming, we provide a dedicated streaming class optimised for time to the first audio</span><br><span class="line">&gt; Better speaker consistency, more than a dozen speakers to choose from or create a speaker description prompt and use that</span><br><span class="line">&gt; Not convinced with a speaker? You can fine-tune the model on your dataset (only couple of hours would do)</span><br><span class="line">Apache 2.0 licensed codebase, weights and datasets! ğŸ</span><br><span class="line">Can&#x27;t wait to see what y&#x27;all would build with this! ğŸ¤—</span><br><span class="line">Parler-TTS Large v1</span><br><span class="line">Open in HuggingFace</span><br><span class="line">Parler-TTS Large v1 is a 2.2B-parameters text-to-speech (TTS) model, trained on 45K hours of audio data, that can generate high-quality, natural sounding speech with features that can be controlled using a simple text prompt (e.g. gender, background noise, speaking rate, pitch and reverberation).</span><br><span class="line"></span><br><span class="line">With Parler-TTS Mini v1, this is the second set of models published as part of the Parler-TTS project, which aims to provide the community with TTS training resources and dataset pre-processing code.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://mistral.ai/news/build-tweak-repeat/</span><br><span class="line">Build, tweak, repeat</span><br><span class="line">Making it easier to develop and share generative AI applications.</span><br><span class="line"></span><br><span class="line">August 7, 2024 Mistral AI team</span><br><span class="line">Detailed benchmarks</span><br><span class="line">Language models are changing the way we build software, serving as a flexible orchestrator in between knowledge sources and user interfaces. Building such software comes with new challenges to improve quality, reduce latency, and prototype quickly. Today, weâ€™re announcing various advancements in this direction.</span><br><span class="line"></span><br><span class="line">Simpler, more efficient model customization</span><br><span class="line">Because large language models are rapidly finding newer and more specialised use cases, it is critical that developers are able to quickly and efficiently tailor frontier models to their specific applications. To that end, weâ€™re announcing the ability to customise any of our flagship and specialist models on La Plateforme, including Mistral Large 2 and Codestral.</span><br><span class="line"></span><br><span class="line">Models can be customised using a base prompt, few-shot prompting, or fine-tuning, and you can bring your own dataset. Crucially, model customization follows the techniques developed by the Mistral AI science team for making strong reference models, so you can expect similar performance from your fine-tuned models. Developers can use model customization to integrate generative AI capabilities into their application with specific domain knowledge, context, or tone.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">We expect fine-tuning on our highly capable models to unlock a wealth of groundbreaking applications, and are eager to see what will be built with it. Check out our fine-tuning documentation, and try model customization on La Plateforme.</span><br><span class="line"></span><br><span class="line">Alpha release of Agents</span><br><span class="line">Weâ€™re also introducing an early version of Agents, that wraps models with additional context and instruction, for exposure on Le Chat or API. Agents help you create custom behaviour and workflows with a simple set of instructions and examples. With the advanced reasoning capabilities of Mistral Large 2, you can layer on increasingly complex workflows with multiple agents that are easy to share within your organisation. Weâ€™re working on connecting Agents to tools and data sources and are looking forward to your feedback on it.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Learn more about Agents.</span><br><span class="line"></span><br><span class="line">Stable version of our client SDK</span><br><span class="line">We have made significant updates to the mistralai library to improve its usability and consistency, and today we are releasing mistralai 1.0, available for both Python and Typescript. Learn more about our new SDK and check out the migration guide.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/aiola/whisper-medusa-v1</span><br><span class="line">8/8/24</span><br><span class="line">Whisper Medusa</span><br><span class="line">Whisper is an advanced encoder-decoder model for speech transcription and translation, processing audio through encoding and decoding stages. Given its large size and slow inference speed, various optimization strategies like Faster-Whisper and Speculative Decoding have been proposed to enhance performance. Our Medusa model builds on Whisper by predicting multiple tokens per iteration, which significantly improves speed with small degradation in WER. We train and evaluate our model on the LibriSpeech dataset, demonstrating speed improvements.</span><br><span class="line"></span><br><span class="line">Training Details</span><br><span class="line">aiola/whisper-medusa-v1 was trained on the LibriSpeech dataset to perform audio translation. The Medusa heads were optimized for English, so for optimal performance and speed improvements, please use English audio only.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">150% faster Whisper generations w/ medusa heads! ğŸ”¥</span><br><span class="line">Built on top of Transformers with minimal drop in accuracy.</span><br><span class="line">Quite exciting area of research, Medusa heads are proven to be incredibly fast for LLMs, seeing them now make way into ASR makes me hopeful!</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2408.03256</span><br><span class="line">[Submitted on 6 Aug 2024]</span><br><span class="line">Synthesizing Text-to-SQL Data from Weak and Strong LLMs</span><br><span class="line">Proposes integrated synthetic data to build a highly specialized SoTA text-to-SQL model called SENSE.</span><br><span class="line">The synthetic data from strong models enhances data diversity while valuable erroneous data from weaker models combined with an executor to learn from execution feedback. Preference learning is used to instruction-tune LLMs to learn from both correct and incorrect samples.</span><br><span class="line">SENSE achieves state-of-the-art results on the SPIDER and BIRD benchmarks, which bridges the performance gap between open-source models and methods that use closed-source models.</span><br><span class="line">Overall, this is an interesting framework where the outputs of weaker and smaller aligned models can be cleverly integrated to achieve more generalized systems.</span><br><span class="line"></span><br><span class="line">Jiaxi Yang, Binyuan Hui, Min Yang, Jian Yang, Junyang Lin, Chang Zhou</span><br><span class="line">The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we introduce a synthetic data approach that combines data produced by larger, more powerful models (strong models) with error information data generated by smaller, not well-aligned models (weak models). The method not only enhances the domain generalization of text-to-SQL models but also explores the potential of error data supervision through preference learning. Furthermore, we employ the synthetic data approach for instruction tuning on open-source LLMs, resulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is demonstrated through state-of-the-art results on the SPIDER and BIRD benchmarks, bridging the performance gap between open-source models and methods prompted by closed-source models.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2408.02545</span><br><span class="line">[Submitted on 5 Aug 2024]</span><br><span class="line">Enhancing LLMs for RAG</span><br><span class="line">Introduces RAGFoundry, an open-source framework for augmented LLMs for RAG use cases.</span><br><span class="line">It supports data creation, training, inference, and evaluation.</span><br><span class="line">One useful application is the creation of data-augmented datasets for tuning and evaluating  LLMs in RAG settings.</span><br><span class="line">After checking out their GitHub repo, I like how easy they make it to build and test quick prototypes with several RAG configurations and enhancements. It has a lot of potential even as just a learning tool.</span><br><span class="line">RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation</span><br><span class="line">Daniel Fleischer, Moshe Berchansky, Moshe Wasserblat, Peter Izsak</span><br><span class="line">Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in this https URL.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-08-09</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/08/09/2024-8-9-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 8ì›” 9ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/08/16/2024-8-16-AI-NEWS/" title="2024ë…„ 8ì›” 16ì¼ AI ì†Œì‹">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/08/07/2024-8-7-AI-NEWS/" title="2024ë…„ 8ì›” 7ì¼ AI ì†Œì‹">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>