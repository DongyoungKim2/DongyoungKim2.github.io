<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 8월 21일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="OpenAI에서는 GPT-4o 모델의 세부 조정 기능을 출시하여, RAG(최신 정보 검색) 기반 모델보다 더 높은 성능과 효율성을 제공하는 점을 강조했습니다. Microsoft는 Phi-3.5 MoE 모델을 발표하며, 이 모델이 RAG보다 강력한 추론 성능을 발휘하는 "><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 8월 21일 AI 소식</a></h3></div><div class="post-content"><p>OpenAI에서는 GPT-4o 모델의 세부 조정 기능을 출시하여, RAG(최신 정보 검색) 기반 모델보다 더 높은 성능과 효율성을 제공하는 점을 강조했습니다. Microsoft는 Phi-3.5 MoE 모델을 발표하며, 이 모델이 RAG보다 강력한 추론 성능을 발휘하는 점을 부각했습니다. Google은 YouTube Music 추천 시스템에 Transformer 모델을 도입하여 사용자 행동을 더 정밀하게 반영한 추천을 가능하게 했습니다. Meta AI는 개인화된 이미지 생성 모델을 발표하여 텍스트와 이미지의 복잡한 요구를 모두 충족시킬 수 있는 새로운 기술을 소개했습니다. Adobe는 텍스트 기반 이미지 편집에서 높은 정확도와 속도를 제공하는 TurboEdit 기술을 선보였습니다. 금융위원회는 금융보안 체계 개선 로드맵을 발표하며 생성형 AI의 활용을 촉진할 수 있는 방안을 제시했습니다.</p>
<h3 id="OpenAI-GPT-4o-모델-세부-조정-기능-출시"><a href="#OpenAI-GPT-4o-모델-세부-조정-기능-출시" class="headerlink" title="OpenAI, GPT-4o 모델 세부 조정 기능 출시"></a>OpenAI, GPT-4o 모델 세부 조정 기능 출시</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/gpt-4o-fine-tuning/">링크</a>, 2024년 8월 20일</p>
<ul>
<li>OpenAI는 GPT-4o 모델에 세부 조정 기능을 도입함.</li>
<li>개발자들은 맞춤형 데이터를 사용하여 모델의 응답 구조와 톤을 세밀하게 조정할 수 있음.</li>
<li>RAG(최신 정보 검색) 기반 모델에 비해, GPT-4o의 세부 조정 기능은 특정 도메인에서 더욱 높은 성능을 제공함.</li>
<li>특히, 복잡한 도메인별 명령어를 처리할 때 GPT-4o 모델이 더 적은 토큰으로 더 높은 정확도의 결과를 제공함.</li>
<li>OpenAI는 세부 조정 기능을 통해 RAG 기반 모델의 한계를 극복하고자 하며, 개발자들이 수십 개의 예시만으로도 강력한 맞춤형 모델을 구축할 수 있도록 지원함.</li>
<li>GPT-4o 세부 조정 기능은 높은 성능을 저비용으로 달성할 수 있도록 설계되었으며, 기존 RAG 시스템 대비 효율성이 증대됨.</li>
<li>프로모션으로 매일 100만 개의 학습 토큰을 무료로 제공하며, 이 프로모션은 9월 23일까지 진행됨.</li>
</ul>
<h3 id="Microsoft-Phi-3-5-MoE-모델-발표"><a href="#Microsoft-Phi-3-5-MoE-모델-발표" class="headerlink" title="Microsoft, Phi-3.5 MoE 모델 발표"></a>Microsoft, Phi-3.5 MoE 모델 발표</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/microsoft/Phi-3.5-MoE-instruct">링크</a>, 2024년 8월 20일</p>
<ul>
<li>Microsoft는 42억 개의 파라미터를 가진 Phi-3.5 MoE 모델을 공개함.</li>
<li>Phi-3.5 MoE 모델은 RAG 기반 시스템에 비해 우수한 추론 성능을 보여줌.</li>
<li>이 모델은 다국어 지원을 포함하며, 다양한 벤치마크에서 높은 성능을 입증함.</li>
<li>특히, 메모리 및 컴퓨팅 리소스가 제한된 환경에서 효율적으로 작동하도록 설계됨.</li>
<li>GPT-4o-mini 모델에 이어 성능이 매우 우수하며, Gemini 1.5 Flash보다 앞선 성능을 기록함.</li>
<li>이 모델은 다양한 산업 및 연구 환경에서 사용할 수 있는 경량화된 AI 솔루션으로, 실시간 응용 프로그램에 적합함.</li>
</ul>
<h3 id="Google-YouTube-Music-추천-시스템에-Transformer-모델-도입"><a href="#Google-YouTube-Music-추천-시스템에-Transformer-모델-도입" class="headerlink" title="Google, YouTube Music 추천 시스템에 Transformer 모델 도입"></a>Google, YouTube Music 추천 시스템에 Transformer 모델 도입</h3><p><a target="_blank" rel="noopener" href="https://research.google/blog/transformers-in-music-recommendation/">링크</a>, 2024년 8월 16일</p>
<ul>
<li>Google은 YouTube Music의 추천 시스템을 개선하기 위해 Transformer 모델을 도입함.</li>
<li>Transformer 모델은 사용자 행동 데이터를 정밀하게 분석하여, 사용자의 현재 상황에 맞춘 음악 추천을 제공함.</li>
<li>기존의 추천 시스템에서는 사용자 행동의 맥락을 충분히 반영하지 못했지만, Transformer를 활용한 새로운 시스템은 사용자 맥락에 맞춰 적절한 음악을 추천할 수 있음.</li>
<li>실험 결과, Transformer 기반 시스템이 사용자 행동을 반영하여 추천의 정확도와 사용자 만족도를 크게 향상시킴.</li>
<li>새로운 추천 시스템은 skip-rate(곡 건너뛰기율)를 낮추고, 세션 시간을 증가시켜 전반적인 사용자 경험을 향상시킴.</li>
</ul>
<h3 id="Meta-AI-개인화된-이미지-생성-모델-“Imagine-yourself”-발표"><a href="#Meta-AI-개인화된-이미지-생성-모델-“Imagine-yourself”-발표" class="headerlink" title="Meta AI, 개인화된 이미지 생성 모델 “Imagine yourself” 발표"></a>Meta AI, 개인화된 이미지 생성 모델 “Imagine yourself” 발표</h3><p><a target="_blank" rel="noopener" href="https://ai.meta.com/research/publications/imagine-yourself-tuning-free-personalized-image-generation">링크</a>, 2024년 7월 23일</p>
<ul>
<li>Meta AI는 텍스트와 이미지의 정교한 조화를 이루는 최신 개인화된 이미지 생성 모델을 발표함.</li>
<li>“Imagine yourself” 모델은 튜닝 없이도 높은 품질의 개인화된 이미지를 생성 가능함.</li>
<li>새로운 데이터 생성 메커니즘을 통해 이미지 다양성을 높이고, 평행 어텐션 아키텍처를 도입하여 텍스트 충실도와 시각적 품질을 향상시킴.</li>
<li>기존의 개인화 모델과 비교하여, 이 모델은 아이덴티티 보존, 텍스트 충실도, 시각적 매력에서 우수한 성능을 보여줌.</li>
<li>이 모델은 다양한 개인화 응용 프로그램의 기초를 제공하며, 사용자 요구를 반영한 이미지를 생성할 수 있음.</li>
</ul>
<h3 id="Adobe-TurboEdit-텍스트-기반-이미지-편집-기술-도입"><a href="#Adobe-TurboEdit-텍스트-기반-이미지-편집-기술-도입" class="headerlink" title="Adobe, TurboEdit: 텍스트 기반 이미지 편집 기술 도입"></a>Adobe, TurboEdit: 텍스트 기반 이미지 편집 기술 도입</h3><p><a target="_blank" rel="noopener" href="https://betterze.github.io/TurboEdit/">링크</a>, 2024년 8월 14일</p>
<ul>
<li>Adobe는 텍스트 기반으로 이미지를 실시간 편집할 수 있는 TurboEdit 기술을 도입함.</li>
<li>이 기술은 8회 기능 평가(NFEs)와 4회 편집 평가만으로 빠르고 정확한 이미지 편집을 가능하게 함.</li>
<li>TurboEdit는 기존의 다중 단계 확산 편집 기술보다 빠르고 정확하게 이미지 속성을 수정할 수 있음.</li>
<li>텍스트 프롬프트의 작은 변경을 통해 이미지의 특정 속성을 조정할 수 있으며, 사용자가 원하는 수준으로 편집 강도를 조절할 수 있음.</li>
<li>이 기술은 디퓨전 모델을 기반으로 하며, 이미지 복원 및 속성 편집을 동시에 수행할 수 있도록 설계됨.</li>
</ul>
<h3 id="금융위원회-금융분야-망분리-개선-로드맵-발표"><a href="#금융위원회-금융분야-망분리-개선-로드맵-발표" class="headerlink" title="금융위원회, 금융분야 망분리 개선 로드맵 발표"></a>금융위원회, 금융분야 망분리 개선 로드맵 발표</h3><p><a target="_blank" rel="noopener" href="https://www.kif.re.kr/kif2/publication/maildetview.aspx?nodeid=402&controlno=338353&ismail=1&email=dongyoung.kim@me.com&SL=0">링크</a>, 2024년 8월 13일</p>
<ul>
<li>금융위원회는 생성형 AI를 활용한 혁신적인 금융 서비스를 금융권에서 출시할 수 있도록 허용하는 망분리 개선 로드맵을 발표함.</li>
<li>새로운 로드맵은 클라우드(SaaS) 기반 서비스의 이용 범위를 확대하고, 연구 개발 환경을 대폭 개선할 계획임.</li>
<li>“자율보안-결과책임” 원칙에 기반한 새로운 금융보안 체계를 구축하여 금융업계의 경쟁력을 강화할 예정임.</li>
<li>장기적으로는 금융 보안 법·체계를 전면 개편하여 금융회사들이 자체 역량을 강화할 수 있도록 지원할 계획임.</li>
<li>금융권의 AI 및 클라우드 기술 활용이 증대됨에 따라, 금융 소비자 보호와 데이터 활용도 또한 증가할 것으로 기대됨.</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://openai.com/index/gpt-4o-fine-tuning/</span><br><span class="line">openAI</span><br><span class="line">August 20, 2024</span><br><span class="line"></span><br><span class="line">Fine-tuning now available for GPT-4o</span><br><span class="line">Fine-tune custom versions of GPT-4o to increase performance and accuracy for your applications.</span><br><span class="line"></span><br><span class="line">gpt-4o-fine-tuning &gt; cover image</span><br><span class="line">Today, we’re launching fine-tuning for GPT-4o, one of the most requested features from developers. We are also offering 1M training tokens per day for free for every organization through September 23.</span><br><span class="line"></span><br><span class="line">Developers can now fine-tune GPT-4o with custom datasets to get higher performance at a lower cost for their specific use cases. Fine-tuning enables the model to customize structure and tone of responses, or to follow complex domain-specific instructions. Developers can already produce strong results for their applications with as little as a few dozen examples in their training data set.</span><br><span class="line"></span><br><span class="line">From coding to creative writing, fine-tuning can have a large impact on model performance across a variety of domains. This is just the start—we’ll continue to invest in expanding our model customization options for developers.</span><br><span class="line"></span><br><span class="line">Getting started</span><br><span class="line">GPT-4o fine-tuning is available today to all developers on all paid usage tiers(opens in a new window).</span><br><span class="line"></span><br><span class="line">To get started, visit the fine-tuning dashboard(opens in a new window), click create, and select gpt-4o-2024-08-06 from the base model drop-down. GPT-4o fine-tuning training costs $25 per million tokens, and inference is $3.75 per million input tokens and $15 per million output tokens.</span><br><span class="line"></span><br><span class="line">GPT-4o mini fine-tuning is also available to all developers on all paid usage tiers. Visit the fine-tuning dashboard and select gpt-4o-mini-2024-07-18 from the base model drop-down. For GPT-4o mini, we’re offering 2M training tokens per day for free through September 23.</span><br><span class="line"></span><br><span class="line">To learn more about how to use fine-tuning, visit our docs(opens in a new window).</span><br><span class="line"></span><br><span class="line">Achieving state-of-the-art performance with GPT-4o fine-tuning</span><br><span class="line">Over the past couple of months, we’ve worked with a handful of trusted partners to test fine-tuning on GPT-4o and learn about their use cases. Here are a couple of success stories:</span><br><span class="line"></span><br><span class="line">Cosine achieves state-of-the-art results on the SWE-bench benchmark</span><br><span class="line"></span><br><span class="line">Cosine(opens in a new window)’s Genie is an AI software engineering assistant that’s able to autonomously identify and resolve bugs, build features, and refactor code in collaboration with users. It can reason across complex technical problems and make changes to code with higher accuracy and fewer tokens needed. Genie is powered by a fine-tuned GPT-4o model trained on examples of real software engineers at work, enabling the model to learn to respond in a specific way. The model was also trained to be able to output in specific formats, such as patches that could be committed easily to codebases.</span><br><span class="line"></span><br><span class="line">With a fine-tuned GPT-4o model, Genie achieves a SOTA score of 43.8% on the new SWE-bench(opens in a new window) Verified benchmark, announced last Tuesday. Genie also holds a SOTA score of 30.08% on SWE-bench Full, beating its previous SOTA score of 19.27%, the largest ever improvement in this benchmark.</span><br><span class="line"></span><br><span class="line">SWE-bench Verified Leaderboard</span><br><span class="line">43.8%</span><br><span class="line">38.8%</span><br><span class="line">38.4%</span><br><span class="line">37%</span><br><span class="line">33.6%</span><br><span class="line">26.2%</span><br><span class="line">25.6%</span><br><span class="line">22.4%</span><br><span class="line">18.2%</span><br><span class="line">7%</span><br><span class="line">4.4%</span><br><span class="line">2.8%</span><br><span class="line">1.4%</span><br><span class="line">1.2%</span><br><span class="line">0.4%</span><br><span class="line">Cosine Genie</span><br><span class="line">Amazon QDeveloper Agent(v20240719-dev)</span><br><span class="line">AutoCodeRover(v20240620) + GPT4o (2024-05-13)</span><br><span class="line">Factory Code Droid</span><br><span class="line">SWE-agent + Claude3.5 Sonnet</span><br><span class="line">AppMap Navie +GPT 4o (2024-05-13)</span><br><span class="line">Amazon QDeveloper Agent(v20240430-dev)</span><br><span class="line">SWE-agent + GPT 4(1106)</span><br><span class="line">SWE-agent + Claude3 Opus</span><br><span class="line">RAG + Claude 3Opus</span><br><span class="line">RAG + Claude 2</span><br><span class="line">RAG + GPT 4 (1106)</span><br><span class="line">RAG + SWE-Llama7B</span><br><span class="line">RAG + SWE-Llama13B</span><br><span class="line">RAG + ChatGPT 3.5</span><br><span class="line">0</span><br><span class="line">5</span><br><span class="line">10</span><br><span class="line">15</span><br><span class="line">20</span><br><span class="line">25</span><br><span class="line">30</span><br><span class="line">35</span><br><span class="line">40</span><br><span class="line">45</span><br><span class="line">50</span><br><span class="line">% Resolved</span><br><span class="line">Distyl ranks 1st on BIRD-SQL benchmark</span><br><span class="line"></span><br><span class="line">Distyl(opens in a new window), an AI solutions partner to Fortune 500 companies, recently placed 1st on the BIRD-SQL(opens in a new window) benchmark, the leading text-to-SQL benchmark. Distyl’s fine-tuned GPT-4o achieved an execution accuracy of 71.83% on the leaderboard and excelled across tasks like query reformulation, intent classification, chain-of-thought, and self-correction, with particularly high performance in SQL generation.</span><br><span class="line"></span><br><span class="line">GPT-4o Fine-Tuning &gt; Media &gt; Leaderboard</span><br><span class="line">Data Privacy and Safety</span><br><span class="line">Fine-tuned models remain entirely under your control, with full ownership of your business data, including all inputs and outputs. This ensures your data is never shared or used to train other models.</span><br><span class="line"></span><br><span class="line">We’ve also implemented layered safety mitigations for fine-tuned models to ensure they aren’t being misused. For example, we continuously run automated safety evals on fine-tuned models and monitor usage  to ensure applications adhere to our usage policies.</span><br><span class="line"></span><br><span class="line">We’re excited to see what you build by fine-tuning GPT-4o. If you’d like to explore more model customization options, please reach out to our team—we’d be happy to help!</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/microsoft/Phi-3.5-MoE-instruct</span><br><span class="line">Microsoft</span><br><span class="line">8/20/24</span><br><span class="line">Phi goes MoE! Microsoft just released Phi-3.5-MoE, a 42B parameter MoE built upon datasets used for Phi-3. Phi-3.5 MoE outperforms bigger models in reasoning capability and is only behind GPT-4o-mini. 👀</span><br><span class="line">TL;DR</span><br><span class="line">🧮 42B parameters with 6.6B activated during generation</span><br><span class="line">👨‍🏫  16 experts with 2 active in generation</span><br><span class="line">🪟 Instruct model with 128k context</span><br><span class="line">🚀 Outperforms Llama 3 8b, Gemma 2 9B across benchmarks</span><br><span class="line">🥈 Closely behind GPT-4o mini, but outperforms Gemini 1.5 Flash</span><br><span class="line">🌎 Multilingual support (unclear which languages)</span><br><span class="line">📜 MIT License</span><br><span class="line">📊 Trained on 4.9 trillion tokens (including 10% multilingual)</span><br><span class="line">🤗 Available on Hugging Face</span><br><span class="line"></span><br><span class="line">Phi-3.5-MoE is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available documents - with a focus on very high-quality, reasoning dense data. The model supports multilingual and comes with 128K context length (in tokens). The model underwent a rigorous enhancement process, incorporating supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.</span><br><span class="line"></span><br><span class="line">🏡 Phi-3 Portal</span><br><span class="line">📰 Phi-3 Microsoft Blog</span><br><span class="line">📖 Phi-3 Technical Report</span><br><span class="line">👩‍🍳 Phi-3 Cookbook</span><br><span class="line">🖥️ Try It</span><br><span class="line"></span><br><span class="line">Phi-3.5: [mini-instruct]; [MoE-instruct] ; [vision-instruct]</span><br><span class="line"></span><br><span class="line">Intended Uses</span><br><span class="line">Primary Use Cases</span><br><span class="line">The model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require:</span><br><span class="line"></span><br><span class="line">Memory/compute constrained environments</span><br><span class="line">Latency bound scenarios</span><br><span class="line">Strong reasoning (especially code, math and logic)</span><br><span class="line">Our model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.</span><br><span class="line">Let’s gooo.. Microsoft just release Phi 3.5 mini, MoE and vision with 128K context, multilingual &amp; MIT license! MoE beats Gemini flash, Vision competitive with GPT4o🔥</span><br><span class="line">&gt; Mini with 3.8B parameters, beats Llama3.1 8B and Mistral 7B and competitive with Mistral NeMo 12B</span><br><span class="line">&gt; Multilingual and Tokenizer with 32K vocab</span><br><span class="line">&gt; Trained on 3.4T tokens</span><br><span class="line">&gt; Used 512 H100s to train (10 days)</span><br><span class="line">&gt; MoE - 16x3.8B (6.6B active - 2 experts) - beats Gemini flash</span><br><span class="line">&gt; 128K context, Multilingual and same tokenizer (32K vocab)</span><br><span class="line">&gt; Trained on 4.9T tokens</span><br><span class="line">&gt; Used 512H100s to train (23 days)</span><br><span class="line">&gt; Ph3.5 Vision - 4.2B params - beats GPT4o on averaged benchmarks</span><br><span class="line">&gt; Trained on 500B tokens</span><br><span class="line">&gt; Used 256 A100 to train (6 days)</span><br><span class="line">&gt; Specialised in TextVQA + ScienceVQA</span><br><span class="line">Let&#x27;s goo, thanks a ton Microsoft for the goodies!⚡</span><br><span class="line">More stats in the thread</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://research.google/blog/transformers-in-music-recommendation/</span><br><span class="line">August 16, 2024</span><br><span class="line"></span><br><span class="line">Google</span><br><span class="line">In recommendation systems, user actions (e.g., skip, like, dislike) provide an important signal about user preference, but can also quickly become unwieldy. Learn how we’ve applied transformers’ ability to process sequences of data to improve YouTube Music</span><br><span class="line"></span><br><span class="line">Transformers in music recommendation</span><br><span class="line">August 16, 2024</span><br><span class="line"></span><br><span class="line">Anushya Subbiah and Vikram Aggarwal, Software Engineers, Google Research</span><br><span class="line"></span><br><span class="line">We present a music recommendation ranking system that uses Transformer models to better understand the sequential nature of user actions based on the current user context.</span><br><span class="line"></span><br><span class="line">Users have more choices for listening to music than ever before. Popular services boast of massive and varied catalogs. The YouTube Music catalog, for example, has over 100M songs globally. It follows that item recommendations are a core part of these products. Recommender systems make sense of the item catalog and are critical for tuning the catalog for the user’s tastes and needs. In products that provide recommendations, user actions on the recommended items — such as skip, like, or dislike — provide an important signal about user preferences. Observing and learning from these actions can lead to better recommendation systems. In YouTube Music, leveraging this signal is critical to understanding a user&#x27;s musical taste.</span><br><span class="line"></span><br><span class="line">Consider a scenario where a user typically likes slow-tempo songs. When presented with an uptempo song, the user would typically skip it. However, at the gym, when they’re in a workout session, they like more uptempo music. In such a situation, we want to continue learning from their prior history to understand their musical preferences. At the same time, we want to discount prior skips of uptempo songs when recommending workout music.</span><br><span class="line"></span><br><span class="line">Below we illustrate the users’ music listening experience, with music songs shown as items and with the user’s actions as text beneath. In current recommendation systems that don’t consider the broader context, we would predict that the user will skip an uptempo song, resulting in demoting a potentially relevant and valuable song.</span><br><span class="line"></span><br><span class="line">TransformersMusic1-Journey1final</span><br><span class="line">The below figure shows the same user journey as before, but in a different situation, where upbeat music may be more relevant. We still utilize their previous music listening, while recommending upbeat music that is close to their usual music listening. In effect, we are learning which previous actions are relevant in the current task of ranking music, and which actions are irrelevant.</span><br><span class="line"></span><br><span class="line">TransformersMusic2-Journey2final</span><br><span class="line">A typical user will perform hundreds of like, dislike, and skip actions, and this sequence of input data, though information-rich, quickly becomes unwieldy. To add to this complexity, users perform different numbers of actions. While a typical user might have hundreds of actions, user behavior can vary between a small number of actions to a very large number of actions, and a good ranking system must be flexible in handling different input sizes.</span><br><span class="line"></span><br><span class="line">In this post we discuss how we’ve applied transformers, which are well-suited to processing sequences of input data, to improve the recommendation system in YouTube Music. This recommendation system consists of three key stages: item retrieval, item ranking, and filtering. Prior user actions are usually added to the ranking models as an input feature. Our approach adapts the Transformer architecture from generative models for the task of understanding the sequential nature of user actions, and blends that with ranking models personalized for that user. Using transformers to incorporate different user actions based on the current user context helps steer music recommendations directly towards the user’s current need. For signed-in users, this approach allows us to incorporate a user’s history without having to explicitly identify what in a user’s history is valuable to the ranking task.</span><br><span class="line"></span><br><span class="line">Retrieval, ranking, and filtering</span><br><span class="line">In existing models, it was difficult to identify which user actions were relevant to the user’s current needs. To understand such models, we need to look at typical recommendation systems. These systems are usually set up as three distinct stages. First, retrieval systems retrieve thousands of relevant items (documents, songs, etc.) from a large corpus. Second, ranking systems evaluate the retrieved results, so that the items that are more relevant and important to the user’s needs are assigned a higher score. The key complexity of ranking comes from the value judgment between concepts such as relevance, importance, novelty, and assigning a numerical value to these fuzzy concepts. Finally, a filtering stage sorts the ranked list by scores, and reduces the sorted list to a short list that is shown to the user. When designing and deploying a ranking model, it is hard to manually select and apply relative weights to specific user actions out of the many hundreds or thousands that they may commonly take.</span><br><span class="line"></span><br><span class="line">Transformers make sense of sequences</span><br><span class="line">Transformers are well-suited to a class of problems where we need to make sense of a sequence of input data. While transformers have been used to improve ranking functions, previous approaches have not focused on user actions: Transformer models like RankFormer have used item candidates (as opposed to user-actions) as input, classical language transformers like BERT are used to rank language output, or BERT-like models are used in recommendations, as in Bert4Rec.</span><br><span class="line"></span><br><span class="line">The Transformer architecture consists of self-attention layers to make sense of sequential input. Transformer models have shown incredible performance on translation or classification tasks, even with ambiguous input text. The self-attention layers capture the relationship between words of text in a sentence, which suggests that they might be able to resolve the relationship between user actions as well. The attention layers in transformers learn attention weights between the pieces of input (tokens), which are akin to word relationships in the input sentence.</span><br><span class="line"></span><br><span class="line">TransformerMusic3-Transformer</span><br><span class="line">Transformers in generative models.</span><br><span class="line"></span><br><span class="line">This is how we utilize the Transformer architecture to encode user actions on YouTube Music. In the user journey involving uptempo music above, we saw how some actions were less important than others. For example, when the user is listening to music at the gym, the user may prefer high-energy upbeat music that they would normally skip, hence related actions (e.g., the skip action in this example) should get a lower attention weight. However, when a user is listening to music in other settings, user actions should get more attention. There should be a difference in attention weight applied to music context versus a user’s music history based upon the activity the user is performing. For example, when a user is at the gym they might listen to music that is more upbeat, but not too far from what they usually listen to. Or when they are driving, they might prefer to explore more new music.</span><br><span class="line"></span><br><span class="line">Transformers for ranking in YouTube Music</span><br><span class="line">Our architecture combines a Transformer with an existing ranking model to learn the combined ranking that best blends user actions with listening history (see diagram below). In this diagram, information flows from the bottom to the top: the inputs to the Transformer are shown at the bottom, and the produced ranking score is shown at the top. “Items” here are music tracks that we want to rank, with the goal to produce a ranking score for each music “item” given to it, with other signals (also called features) provided as input.</span><br><span class="line"></span><br><span class="line">TransformerMusic4-Hero</span><br><span class="line">Transformers and Ranker in the joint music recommendation task.</span><br><span class="line"></span><br><span class="line">Here are the signals describing the user action at each time step:</span><br><span class="line"></span><br><span class="line">Intention of the action: interrupt a music track, select a music track to listen to, autoplay.</span><br><span class="line">Salience of the action: percentage of the music track that was played, time since prior user-action.</span><br><span class="line">Other metadata: artist, language of the music.</span><br><span class="line">Music track: music track identifier corresponding to the user-action.</span><br><span class="line">Music tracks corresponding to user actions are represented by a vector of numbers called the track embedding. This music-track embedding is used as an input in both the Transformer and the existing ranking model. User-action signals, like intention and metadata, are turned into vectors with the same length as the length of the track embedding. This operation, called a projection, allows us to combine the signals simply by adding the two vectors: user-action signals and the track embedding, producing input vectors (called tokens) for the Transformer. The tokens provided as inputs to the Transformer are used to score the retrieved music items. When considering the user’s history, we include the previous user actions and the music the user is currently listening to, as both capture valuable user context. The output vector from the Transformer is combined with the existing ranking model inputs, using a multi-layer neural network. The Transformer is co-trained with the ranking model, for multiple ranking objectives.</span><br><span class="line"></span><br><span class="line">Offline analysis and live experiments demonstrate that using this Transformer significantly improves the performance of the ranking model, leading to a reduction in skip-rate and an increase in time users spend listening to music. Skipping less frequently indicates that on average, users like the recommendations more. Increased session length indicates that users are happier with the overall experience. These two metrics demonstrate the improvement in user satisfaction for YouTube Music.</span><br><span class="line"></span><br><span class="line">Future work</span><br><span class="line">We see two main opportunities to build on this work. The first would be to adapt the technique to other parts of the recommendation system such as retrieval models. There are also a variety of nonsequential features, which are used as inputs to the prior ranking model, that we are also exploring for incorporation. Currently, these are combined after the Transformer stage, and we predict that incorporating them within the Transformer would allow for improved self-attention between the sequential features, like user-actions, and non-sequential features such as artist popularity, user language, music popularity and more.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://ai.meta.com/research/publications/imagine-yourself-tuning-free-personalized-image-generation</span><br><span class="line">COMPUTER VISION</span><br><span class="line">Imagine yourself: Tuning-Free Personalized Image Generation</span><br><span class="line">July 23, 2024</span><br><span class="line">This paper introduces a state-of-the-art model designed for personal image generation. Want to try it? The feature is available now as a beta in Meta AI for users in the US.</span><br><span class="line"></span><br><span class="line">Abstract</span><br><span class="line">Diffusion models have demonstrated remarkable efficacy across various image-to-image tasks. In this research, we introduce Imagine yourself, a state-of-the-art model designed for personalized image generation. Unlike conventional tuning-based personalization techniques, Imagine yourself operates as a tuning-free model, enabling all users to leverage a shared framework without individualized adjustments. Moreover, previous work met challenges balancing identity preservation, following complex prompts and preserving good visual quality, resulting in models having strong copy-paste effect of the reference images. Thus, they can hardly generate images following prompts that require significant changes to the reference image, e.g., changing facial expression, head and body poses, and the diversity of the generated images is low. To address these limitations, our proposed method introduces 1) a new synthetic paired data generation mechanism to encourage image diversity, 2) a fully parallel attention architecture with three text encoders and a fully trainable vision encoder to improve the text faithfulness, and 3) a novel coarse-to-fine multi-stage finetuning methodology that gradually pushes the boundary of visual quality. Our study demonstrates that Imagine yourself surpasses the state-of-the-art personalization model, exhibiting superior capabilities in identity preservation, visual quality, and text alignment. This model establishes a robust foundation for various personalization applications. Human evaluation results validate the model’s SOTA superiority across all aspects (identity preservation, text faithfulness, and visual appeal) compared to the previous personalization models.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://betterze.github.io/TurboEdit/</span><br><span class="line">Adobe</span><br><span class="line">14 Aug 2024</span><br><span class="line"></span><br><span class="line">TurboEdit: Instant text-based image editing</span><br><span class="line">Zongze Wu, Nicholas Kolkin, Jonathan Brandt, Richard Zhang, Eli Shechtman</span><br><span class="line">Adobe Research</span><br><span class="line">To appear in ECCV, 2024</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Abstract</span><br><span class="line">We address the challenges of precise image inversion and disentangled image editing in the context of few-step diffusion models. We introduce an encoder based iterative inversion technique. The inversion network is conditioned on the input image and the reconstructed image from the previous step, allowing for correction of the next reconstruction towards the input image. We demonstrate that disentangled controls can be easily achieved in the few-step diffusion model by conditioning on an (automatically generated) detailed text prompt. To manipulate the inverted image, we freeze the noise maps and modify one attribute in the text prompt (either manually or via instruction based editing driven by an LLM), resulting in the generation of a new image similar to the input image with only one attribute changed. It can further control the editing strength and accept instructive text prompt. Our approach facilitates realistic text-guided image edits in real-time, requiring only 8 number of functional evaluations (NFEs) in inversion (one-time cost) and 4 NFEs per edit. Our method is not only fast, but also significantly outperforms state-of-the-art multi-step diffusion editing techniques.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Method</span><br><span class="line">Given an input real image x0, we utilize the LLaVA to generate a detailed caption c. Users can modify c to create a new text prompt c ′. The inversion process begins by feeding the x0, c, current time step t, and a previously reconstructed image x0,t+1 (initialized as a zero matrix) into the inversion network. This network then predicts the noise ϵt, which is subsequently input into a frozen SDXL-Turbo model to generate the new reconstruction image x0,t. Given the final inverted noise ϵt, along with c, we can use SDXL-Turbo to create an inversion trajectory and reconstruct x0,0, which is very similar to x0. Using the same noises ϵt and slightly different text prompt c ′, starting from t = T to smaller t, the editing trajectory will be very similar to the inversion trajectory, and the generated image will closely resemble the input image, differing only in the specified attribute in c&#x27;.</span><br><span class="line"></span><br><span class="line">TurboEdit: Instant text-based image editing</span><br><span class="line">Zongze Wu, Nicholas Kolkin, Jonathan Brandt, Richard Zhang, Eli Shechtman</span><br><span class="line">We address the challenges of precise image inversion and disentangled image editing in the context of few-step diffusion models. We introduce an encoder based iterative inversion technique. The inversion network is conditioned on the input image and the reconstructed image from the previous step, allowing for correction of the next reconstruction towards the input image. We demonstrate that disentangled controls can be easily achieved in the few-step diffusion model by conditioning on an (automatically generated) detailed text prompt. To manipulate the inverted image, we freeze the noise maps and modify one attribute in the text prompt (either manually or via instruction based editing driven by an LLM), resulting in the generation of a new image similar to the input image with only one attribute changed. It can further control the editing strength and accept instructive text prompt. Our approach facilitates realistic text-guided image edits in real-time, requiring only 8 number of functional evaluations (NFEs) in inversion (one-time cost) and 4 NFEs per edit. Our method is not only fast, but also significantly outperforms state-of-the-art multi-step diffusion editing techniques.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.kif.re.kr/kif2/publication/maildetview.aspx?nodeid=402&amp;controlno=338353&amp;ismail=1&amp;email=dongyoung.kim@me.com&amp;SL=0</span><br><span class="line">금융위원회</span><br><span class="line">2024. 8. 13.</span><br><span class="line"></span><br><span class="line">「금융분야 망분리 개선 로드맵」 발표</span><br><span class="line"></span><br><span class="line">- 금융권 망분리 10년, &quot;혁신과 보안의 새로운 균형으로의 도약&quot;</span><br><span class="line"></span><br><span class="line">- 금융권도 생성형 AI를 활용한 혁신적인 금융서비스 출시 가능</span><br><span class="line"></span><br><span class="line">- &quot;자율보안-결과책임&quot; 원칙에 입각한 新 금융보안체계 구축</span><br><span class="line"></span><br><span class="line">◈ 금융회사 등의 생성형 AI 활용을 허용하고, 클라우드(SaaS) 이용 범위를 대폭 확대하며, 연구·개발 환경을 적극 개선</span><br><span class="line"></span><br><span class="line">◈ 중·장기적으로는 금융보안 법·체계를 전면 개편하는 등 자율보안-결과책임 원칙으로의 규제 선진화 방향을 제시하고, 금융회사 등이 자체적인 역량 강화를 통해 미리 대비할 수 있도록 지원</span><br><span class="line">- 1 -</span><br><span class="line">보도시점 2024. 8. 13.(화) 14:00 배포 2024. 8. 12.(월) 10:00</span><br><span class="line">｢금융분야 망분리 개선 로드맵｣ 발표- 금융권 망분리 10년, “혁신과 보안의 새로운 균형으로의 도약”</span><br><span class="line">- 금융권도 생성형 AI를 활용한 혁신적인 금융서비스 출시 가능- “자율보안-결과책임” 원칙에 입각한 新 금융보안체계 구축◈ 금융회사 등의 생성형 AI 활용을 허용하고, 클라우드(SaaS) 이용 범위를대폭 확대하며, 연구·개발 환경을 적극 개선</span><br><span class="line">◈ 중·장기적으로는 금융보안 법·체계를 전면 개편하는 등 자율보안-결과책임 원칙으로의 규제 선진화 방향을 제시하고, 금융회사 등이 자체적인역량 강화를 통해 미리 대비할 수 있도록 지원</span><br><span class="line"> `24년 8월 13일(화), 금융위원회는 김병환 금융위원회 위원장 주재로</span><br><span class="line">민간 보안 전문가들과 금융협회, 금융감독원, 금융보안원 등 유관기관이</span><br><span class="line">참여한 가운데 ｢금융분야 망분리 개선 로드맵｣을 발표하였다. 【 금융분야 망분리 개선 로드맵 발표 행사 개요 】</span><br><span class="line">￭ 일시 / 장소 : 2024.8.13.(화) 14:00 / KB 국민은행 통합 IT센터(김포)</span><br><span class="line">￭ 참석자 : 【금융위원회】 김병환 금융위원장(주재), 디지털금융정책관</span><br><span class="line">【유관기관】 금융감독원, 금융보안원</span><br><span class="line"> 【업계】 금융협회(은행, 보험, 여신, 금융투자, 핀테크) 【민간전문가】 금융연구원 부원장, 신용정보원 AI 센터장</span><br><span class="line"> 김홍선 前안랩 CEO, 박춘식 교수</span><br><span class="line"> 금융위원회는 그동안 여러 차례에 걸친 간담회 등을 통해 금융 보안 규제에따른 금융회사 등의 애로를 직접 청취하였으며, ｢금융권 망분리 TF*｣를</span><br><span class="line">운영하여 보안전문가, 업계, 유관기관 등으로부터 의견을 수렴하였다. 그간</span><br><span class="line">금융위 원회 보도자료</span><br><span class="line">- 2 -</span><br><span class="line">논의된 내용을 종합적으로 반영하여 망분리 개선을 위한 세부 추진과제와 금융보안체계의 선진화 방향을 담은 로드맵을 마련하였다.</span><br><span class="line">* `24.4~6월동안 금융위원회, 디지털플랫폼정부위원회, 금융감독원, 금융보안원, IT 및</span><br><span class="line">보안 전문가, 학계, 법조계, 금융 업권 등과 함께 망분리 개선 방향 논의</span><br><span class="line">□ 추진 배경</span><br><span class="line"> 망분리로 인해 금융회사 및 전자금융업자(이하 “금융회사 등”)의 업무상 비효율이 클 뿐만 아니라, 신기술 활용이 저해되고 연구·개발이 어렵다는</span><br><span class="line">규제 개선요청이 지속 제기된 바 있다.</span><br><span class="line"> 특히, 소프트웨어 시장이 자체 구축형에서 클라우드 기반의 구독형(SaaS)으로빠르게 전환되고 생성형 AI의 활용이 산업의 미래를 좌우하는 상황에서, 망분리는 업무상 불편을 넘어 금융경쟁력 저하 요인으로 지적받고 있다.</span><br><span class="line"> 또한, 일부 금융회사 등은 인터넷 등 외부 통신과 분리된 환경만을 구축해놓고 선진 보안체계 도입에 소홀하거나, 규제 그늘에 숨어 변화하고 있는</span><br><span class="line">IT 환경에 부합하는 보안 조치도 적절히 갖추지 않는 등 오히려 금융권</span><br><span class="line">보안 발전이 저해되는 부작용이 존재하는 상황이다.</span><br><span class="line"> 금융권 망분리 도입 이후 약 10년이 경과한 지금, 금융당국은 낡은 규제를개선하고 중·장기적으로 금융 보안 법·제도를 전면 개편하는 등 혁신과 보안의새로운 균형을 찾기 위한 패러다임 전환을 추진할 계획이라고 밝혔다. □ 샌드박스를 통하여 규제 애로 즉시 해소 + 별도 보안대책 병행 현행 금융보안체계가 오랜 기간동안 인터넷 등 외부통신과 분리된 환경을전제로 구성되어 온 점을 고려하여, 급격한 규제 완화보다는 단계적 개선을 추진한다.</span><br><span class="line"> IT 환경 변화로 인해 신속한 대응이 필요한 과제는 샌드박스 등을 활용하여 규제 애로를 즉시 해소하되, 자율보안체계 확립까지는 시간이 소요되므로 보안상의 문제가 없도록 별도의 보안대책 등 충분한 안전장치를</span><br><span class="line">마련할 예정이다.</span><br><span class="line">- 3 -</span><br><span class="line"> 첫째, 금융회사 등의 생성형 AI 활용을 허용한다.</span><br><span class="line"> 대부분의 생성형 AI가 클라우드 기반의 인터넷 환경에서 제공되는 반면, 국내 금융권은 인터넷 등 외부 통신 활용 제한 등으로 인해 생성형 AI 도입에 제약이 있는 상황이다.</span><br><span class="line"> 이에, 샌드박스를 통해 인터넷 활용 제한 등에 대한 규제 특례를 허용한다. 이와 함께, 예상되는 리스크에 대한 보안대책을 조건으로 부과하고 금융감독원·금융보안원이 신청 기업별 보안 점검·컨설팅을 실시하는 등 충분한</span><br><span class="line">안전장치를 마련할 계획이다.</span><br><span class="line"> 둘째, 클라우드 기반의 응용 프로그램(SaaS) 이용 범위를 대폭 확대한다.</span><br><span class="line"> 기존에는 문서관리, 인사관리 등 비중요 업무에 대해서만 SaaS 이용이</span><br><span class="line">허용되고, 고객 개인신용정보를 처리할 수 없는 등 엄격한 샌드박스 부가조건이 부과되어 활용이 제한되었다.</span><br><span class="line"> 이에, 보안관리, 고객관리(CRM) 등의 업무까지 이용 범위를 확대하고, 가명정보 처리 및 모바일 단말기에서의 SaaS 이용까지 허용하는 등 SaaS</span><br><span class="line">활용도를 제고할 예정이다. 마찬가지로, 규제 특례 확대에 따른 보안 우려에대응하기 위해 보안대책을 마련하여 샌드박스 지정 조건으로 부과할 계획이다.</span><br><span class="line"> 셋째, 금융회사 등의 연구·개발 환경을 개선한다.</span><br><span class="line"> `22.11월 연구·개발 환경에서 인터넷을 자유롭게 활용할 수 있도록 한차례규제가 개선되었으나, 연구·개발 환경의 물리적 분리 및 개인신용정보 활용 금지 등에 따라 고객별 특성·수요에 맞는 혁신적인 서비스 연구·개발에제약이 크다는 지적이 지속 제기되었다.</span><br><span class="line"> 이에, ｢전자금융감독규정｣을 개정하여 금융회사 등이 연구·개발 결과물을보다 간편하게 이관할 수 있도록 물리적 제한을 완화하고, 가명정보 활용을허용하는 등 혁신적인 금융상품을 개발할 수 있는 환경을 제공하고자 한다.</span><br><span class="line">- 4 -</span><br><span class="line">&lt; 참고 : 망분리 개선 단기 추진 과제 종합 구성도 &gt;</span><br><span class="line">- 5 -</span><br><span class="line">□ 2단계 샌드박스에서는 개인신용정보까지 활용 확대</span><br><span class="line"> 가명정보 활용을 허용한 1단계 샌드박스의 운영 성과와 안전성이 충분히 검증될 경우, 빠르면 내년에는 2단계 샌드박스를 추진하여 금융회사가가명정보가 아닌 개인신용정보까지 직접 처리할 수 있도록 규제 특례의</span><br><span class="line">고도화도 추진한다. 다만, 데이터 활용 범위 증가에 따른 추가 보안대책등도 함께 부과할 예정이다. □ 샌드박스 운영 경험 등을 토대로 금융보안체계의 선진화 추진 : “목표·원칙 중심”의 보안규제, “자율보안-결과책임” 원칙에 입각한 보안체계 구축 누적된 샌드박스 사례를 통해 혁신성, 소비자 편익, 리스크 관리 등이</span><br><span class="line">충분히 검증된 과제는 정규 제도화하고, 중·장기적으로는 별도의 금융보안법</span><br><span class="line">(가칭 “｢디지털 금융보안법｣”)을 마련하여 “자율보안-결과책임” 원칙에입각한 新 금융보안체계를 구축해 나갈 계획이다.</span><br><span class="line"> 열거식 행위 규칙(Rule) 중심의 금융보안 규제를 목표·원칙(Principle) 중심으로 전환하고, 금융회사 등은 자체 리스크 평가를 바탕으로 세부 보안 통제를 자율적으로 구성할 수 있도록 한다.</span><br><span class="line"> 다만, 금융회사 등에 부여된 자율에 따른 책임은 강화할 필요가 있다. 중요 보안사항의 CEO·이사회 보고의무 등 금융회사 등의 내부 보안 거버넌스를 강화하고, 전산사고 발생시 배상책임 확대 및 실효성 있는 과징금 도입 등법적 근거 마련을 통해 금융회사 등의 보안 노력 제고를 유도할 예정이다.</span><br><span class="line"> 한편, 금융당국은 금융회사 등의 자율보안체계 수립·이행을 검증하여미흡한 경우 시정요구ㆍ이행명령을 부과하고 불이행시 엄중 제재하는 등</span><br><span class="line">금융권 보안 수준 강화를 위해 지속 노력해 나갈 것이다.</span><br><span class="line"> 또한, 제3자 리스크(3rd-party risk*)에 대한 관리를 강화하기 위하여 제도를정비한다. AI, 클라우드, 데이터센터 등 금융권의 제3자에 대한 정보처리</span><br><span class="line">- 6 -</span><br><span class="line">위탁이 지속적으로 증가하는 만큼 이에 대한 리스크 관리방안이 필요하다. EU·영국 등 해외 선진사례 연구를 토대로, 국내 환경에 맞는 도입 방향을</span><br><span class="line">검토하여 관련 제도 정비를 추진하고자 한다.</span><br><span class="line"> * 非금융부문의 장애 발생, 정보 유출 등의 사고가 금융 부문으로 전이되는 리스크□ 기대 효과</span><br><span class="line">첫째, 금번 망분리 개선을 통해 금융산업 전반의 경쟁력이 크게 제고될</span><br><span class="line">것으로 기대된다.</span><br><span class="line"> AI와 클라우드(SaaS) 기반의 업무 자동화, 전사적 경영관리(ERP :</span><br><span class="line">Enterprise Resource Planning), 준법 감시 프로그램 등 도입에 따라 금융권의</span><br><span class="line">업무 생산성이 향상되고, 빅데이터 분석 등 금융 데이터의 활용도 증가할</span><br><span class="line">것으로 예상된다.</span><br><span class="line"> 또한, 인터넷 접속이 자유롭게 허용되는 연구·개발망의 활용도 제고에</span><br><span class="line">따라 금융회사 등의 IT 개발이 활성화될 수 있으며, 연구·개발 환경 개선에 따른 금융권의 우수 IT 인력 유치 효과까지 기대된다.</span><br><span class="line"> 둘째, 망분리 개선의 이익이 금융산업 뿐만 아니라 금융소비자의 효용</span><br><span class="line">증진으로 이어질 수 있도록 유도해 나갈 계획이다.</span><br><span class="line"> 생성형 AI를 활용한 데이터 분석·예측 모델 고도화를 통해 다양한 특화 보험상품을 개발하고, 신용평가모델 고도화를 통해 중금리 대출의 저변을확대하는 등 금융 사각지대 해소에 기여할 것으로 기대된다.</span><br><span class="line"> 뿐만 아니라, 금융권은 생성형 AI를 활용한 이상금융거래탐지시스템(FDS : Fraud Detection System) 고도화를 통해 부정거래, 신종사기 시도등을보다 효과적으로 차단하는 등 금융소비자 보호를 강화할 수 있다.</span><br><span class="line">- 7 -</span><br><span class="line">□ 향후 계획</span><br><span class="line"> 금융당국은 8월 22일(목) 전 업권 업무 설명회*를 시작으로, 9월까지 업권별로 업무 설명회를 개최하고, 규제샌드박스 신청 기업별 보안 역량, 사업</span><br><span class="line">구조 등을 고려하여 부가 조건으로 지켜야 할 보안대책 등에 대한 컨설팅을제공할 예정이다.</span><br><span class="line"> * 구체적인 시간, 장소 및 진행 방식 등은 협회를 통해 안내할 예정이며, 금융보안원홈페이지(fsec.or.kr)를 통해서도 공지할 예정</span><br><span class="line"> 그리고, 9월 중 규제샌드박스 신청을 접수받아 연내 신규 과제에 대한 혁신 금융서비스를 지정하고자 한다. 이에 따라, 빠르면 올해 말부터 금융권에서도 생성형 AI 활용이 가능해 질 것으로 예상된다.</span><br><span class="line"> 더불어, 연말까지 연구·개발 환경 개선 등을 위한 ｢전자금융감독규정｣ 개정 절차 또한 차질 없이 수행할 계획이며, 新 금융보안체계 구축을 위한연구용역 등을 거쳐 금융보안 법·체계 개편을 추진해 나갈 예정이다.</span><br><span class="line"> 김병환 금융위원장은 “클라우드, 생성형 AI 등 급변하는 IT 환경변화에대응하기 위해 보다 효과적인 망분리 개선 방안이 필요한 시점”이라고언급하면서, “특히 모든 정책은 글로벌 스탠다드 관점에서 정비해 나간다는 기조 하에, 우리나라에만 존재하는 대표적인 갈라파고스 규제를과감하게 개선할 필요가 있다”고 말했다.</span><br><span class="line"> 또한, “이번 망분리 개선 로드맵이 금융산업의 경쟁력 제고와 금융소비자의 효용 증진으로 이어질 것”으로 기대하며, “어렵게 규제를 개선하는 만큼 금융업권도 보안사고 없이 새로운 제도가 잘 안착할 수 있도록힘써주시길 바란다”고 당부했다. ※ 금융분야 망분리 개선 로드맵 관련 자세한 내용은 별첨 자료를 참고해주시기 바랍니다.</span><br><span class="line">- 8 -</span><br><span class="line">[별첨] 1. 금융위원장 모두 발언</span><br><span class="line"> 2. 금융분야 망분리 개선 로드맵</span><br><span class="line">담당 부서 금융위원회</span><br><span class="line">금융안전과</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-08-21</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/08/21/2024-8-21-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 8월 21일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/08/16/2024-8-16-AI-NEWS/" title="2024년 8월 16일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>