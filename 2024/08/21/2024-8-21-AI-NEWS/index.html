<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 8ì›” 21ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="OpenAIì—ì„œëŠ” GPT-4o ëª¨ë¸ì˜ ì„¸ë¶€ ì¡°ì • ê¸°ëŠ¥ì„ ì¶œì‹œí•˜ì—¬, RAG(ìµœì‹  ì •ë³´ ê²€ìƒ‰) ê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ ë” ë†’ì€ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì„ ì œê³µí•˜ëŠ” ì ì„ ê°•ì¡°í–ˆìŠµë‹ˆë‹¤. MicrosoftëŠ” Phi-3.5 MoE ëª¨ë¸ì„ ë°œí‘œí•˜ë©°, ì´ ëª¨ë¸ì´ RAGë³´ë‹¤ ê°•ë ¥í•œ ì¶”ë¡  ì„±ëŠ¥ì„ ë°œíœ˜í•˜ëŠ” "><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 8ì›” 21ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><p>OpenAIì—ì„œëŠ” GPT-4o ëª¨ë¸ì˜ ì„¸ë¶€ ì¡°ì • ê¸°ëŠ¥ì„ ì¶œì‹œí•˜ì—¬, RAG(ìµœì‹  ì •ë³´ ê²€ìƒ‰) ê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ ë” ë†’ì€ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì„ ì œê³µí•˜ëŠ” ì ì„ ê°•ì¡°í–ˆìŠµë‹ˆë‹¤. MicrosoftëŠ” Phi-3.5 MoE ëª¨ë¸ì„ ë°œí‘œí•˜ë©°, ì´ ëª¨ë¸ì´ RAGë³´ë‹¤ ê°•ë ¥í•œ ì¶”ë¡  ì„±ëŠ¥ì„ ë°œíœ˜í•˜ëŠ” ì ì„ ë¶€ê°í–ˆìŠµë‹ˆë‹¤. Googleì€ YouTube Music ì¶”ì²œ ì‹œìŠ¤í…œì— Transformer ëª¨ë¸ì„ ë„ì…í•˜ì—¬ ì‚¬ìš©ì í–‰ë™ì„ ë” ì •ë°€í•˜ê²Œ ë°˜ì˜í•œ ì¶”ì²œì„ ê°€ëŠ¥í•˜ê²Œ í–ˆìŠµë‹ˆë‹¤. Meta AIëŠ” ê°œì¸í™”ëœ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì„ ë°œí‘œí•˜ì—¬ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ì˜ ë³µì¡í•œ ìš”êµ¬ë¥¼ ëª¨ë‘ ì¶©ì¡±ì‹œí‚¬ ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ê¸°ìˆ ì„ ì†Œê°œí–ˆìŠµë‹ˆë‹¤. AdobeëŠ” í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´ë¯¸ì§€ í¸ì§‘ì—ì„œ ë†’ì€ ì •í™•ë„ì™€ ì†ë„ë¥¼ ì œê³µí•˜ëŠ” TurboEdit ê¸°ìˆ ì„ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ê¸ˆìœµìœ„ì›íšŒëŠ” ê¸ˆìœµë³´ì•ˆ ì²´ê³„ ê°œì„  ë¡œë“œë§µì„ ë°œí‘œí•˜ë©° ìƒì„±í˜• AIì˜ í™œìš©ì„ ì´‰ì§„í•  ìˆ˜ ìˆëŠ” ë°©ì•ˆì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.</p>
<h3 id="OpenAI-GPT-4o-ëª¨ë¸-ì„¸ë¶€-ì¡°ì •-ê¸°ëŠ¥-ì¶œì‹œ"><a href="#OpenAI-GPT-4o-ëª¨ë¸-ì„¸ë¶€-ì¡°ì •-ê¸°ëŠ¥-ì¶œì‹œ" class="headerlink" title="OpenAI, GPT-4o ëª¨ë¸ ì„¸ë¶€ ì¡°ì • ê¸°ëŠ¥ ì¶œì‹œ"></a>OpenAI, GPT-4o ëª¨ë¸ ì„¸ë¶€ ì¡°ì • ê¸°ëŠ¥ ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/gpt-4o-fine-tuning/">ë§í¬</a>, 2024ë…„ 8ì›” 20ì¼</p>
<ul>
<li>OpenAIëŠ” GPT-4o ëª¨ë¸ì— ì„¸ë¶€ ì¡°ì • ê¸°ëŠ¥ì„ ë„ì…í•¨.</li>
<li>ê°œë°œìë“¤ì€ ë§ì¶¤í˜• ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì‘ë‹µ êµ¬ì¡°ì™€ í†¤ì„ ì„¸ë°€í•˜ê²Œ ì¡°ì •í•  ìˆ˜ ìˆìŒ.</li>
<li>RAG(ìµœì‹  ì •ë³´ ê²€ìƒ‰) ê¸°ë°˜ ëª¨ë¸ì— ë¹„í•´, GPT-4oì˜ ì„¸ë¶€ ì¡°ì • ê¸°ëŠ¥ì€ íŠ¹ì • ë„ë©”ì¸ì—ì„œ ë”ìš± ë†’ì€ ì„±ëŠ¥ì„ ì œê³µí•¨.</li>
<li>íŠ¹íˆ, ë³µì¡í•œ ë„ë©”ì¸ë³„ ëª…ë ¹ì–´ë¥¼ ì²˜ë¦¬í•  ë•Œ GPT-4o ëª¨ë¸ì´ ë” ì ì€ í† í°ìœ¼ë¡œ ë” ë†’ì€ ì •í™•ë„ì˜ ê²°ê³¼ë¥¼ ì œê³µí•¨.</li>
<li>OpenAIëŠ” ì„¸ë¶€ ì¡°ì • ê¸°ëŠ¥ì„ í†µí•´ RAG ê¸°ë°˜ ëª¨ë¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ ì í•˜ë©°, ê°œë°œìë“¤ì´ ìˆ˜ì‹­ ê°œì˜ ì˜ˆì‹œë§Œìœ¼ë¡œë„ ê°•ë ¥í•œ ë§ì¶¤í˜• ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•¨.</li>
<li>GPT-4o ì„¸ë¶€ ì¡°ì • ê¸°ëŠ¥ì€ ë†’ì€ ì„±ëŠ¥ì„ ì €ë¹„ìš©ìœ¼ë¡œ ë‹¬ì„±í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ê¸°ì¡´ RAG ì‹œìŠ¤í…œ ëŒ€ë¹„ íš¨ìœ¨ì„±ì´ ì¦ëŒ€ë¨.</li>
<li>í”„ë¡œëª¨ì…˜ìœ¼ë¡œ ë§¤ì¼ 100ë§Œ ê°œì˜ í•™ìŠµ í† í°ì„ ë¬´ë£Œë¡œ ì œê³µí•˜ë©°, ì´ í”„ë¡œëª¨ì…˜ì€ 9ì›” 23ì¼ê¹Œì§€ ì§„í–‰ë¨.</li>
</ul>
<h3 id="Microsoft-Phi-3-5-MoE-ëª¨ë¸-ë°œí‘œ"><a href="#Microsoft-Phi-3-5-MoE-ëª¨ë¸-ë°œí‘œ" class="headerlink" title="Microsoft, Phi-3.5 MoE ëª¨ë¸ ë°œí‘œ"></a>Microsoft, Phi-3.5 MoE ëª¨ë¸ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/microsoft/Phi-3.5-MoE-instruct">ë§í¬</a>, 2024ë…„ 8ì›” 20ì¼</p>
<ul>
<li>MicrosoftëŠ” 42ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ Phi-3.5 MoE ëª¨ë¸ì„ ê³µê°œí•¨.</li>
<li>Phi-3.5 MoE ëª¨ë¸ì€ RAG ê¸°ë°˜ ì‹œìŠ¤í…œì— ë¹„í•´ ìš°ìˆ˜í•œ ì¶”ë¡  ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ.</li>
<li>ì´ ëª¨ë¸ì€ ë‹¤êµ­ì–´ ì§€ì›ì„ í¬í•¨í•˜ë©°, ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ì…ì¦í•¨.</li>
<li>íŠ¹íˆ, ë©”ëª¨ë¦¬ ë° ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ê°€ ì œí•œëœ í™˜ê²½ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ì‘ë™í•˜ë„ë¡ ì„¤ê³„ë¨.</li>
<li>GPT-4o-mini ëª¨ë¸ì— ì´ì–´ ì„±ëŠ¥ì´ ë§¤ìš° ìš°ìˆ˜í•˜ë©°, Gemini 1.5 Flashë³´ë‹¤ ì•ì„  ì„±ëŠ¥ì„ ê¸°ë¡í•¨.</li>
<li>ì´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì‚°ì—… ë° ì—°êµ¬ í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê²½ëŸ‰í™”ëœ AI ì†”ë£¨ì…˜ìœ¼ë¡œ, ì‹¤ì‹œê°„ ì‘ìš© í”„ë¡œê·¸ë¨ì— ì í•©í•¨.</li>
</ul>
<h3 id="Google-YouTube-Music-ì¶”ì²œ-ì‹œìŠ¤í…œì—-Transformer-ëª¨ë¸-ë„ì…"><a href="#Google-YouTube-Music-ì¶”ì²œ-ì‹œìŠ¤í…œì—-Transformer-ëª¨ë¸-ë„ì…" class="headerlink" title="Google, YouTube Music ì¶”ì²œ ì‹œìŠ¤í…œì— Transformer ëª¨ë¸ ë„ì…"></a>Google, YouTube Music ì¶”ì²œ ì‹œìŠ¤í…œì— Transformer ëª¨ë¸ ë„ì…</h3><p><a target="_blank" rel="noopener" href="https://research.google/blog/transformers-in-music-recommendation/">ë§í¬</a>, 2024ë…„ 8ì›” 16ì¼</p>
<ul>
<li>Googleì€ YouTube Musicì˜ ì¶”ì²œ ì‹œìŠ¤í…œì„ ê°œì„ í•˜ê¸° ìœ„í•´ Transformer ëª¨ë¸ì„ ë„ì…í•¨.</li>
<li>Transformer ëª¨ë¸ì€ ì‚¬ìš©ì í–‰ë™ ë°ì´í„°ë¥¼ ì •ë°€í•˜ê²Œ ë¶„ì„í•˜ì—¬, ì‚¬ìš©ìì˜ í˜„ì¬ ìƒí™©ì— ë§ì¶˜ ìŒì•… ì¶”ì²œì„ ì œê³µí•¨.</li>
<li>ê¸°ì¡´ì˜ ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œëŠ” ì‚¬ìš©ì í–‰ë™ì˜ ë§¥ë½ì„ ì¶©ë¶„íˆ ë°˜ì˜í•˜ì§€ ëª»í–ˆì§€ë§Œ, Transformerë¥¼ í™œìš©í•œ ìƒˆë¡œìš´ ì‹œìŠ¤í…œì€ ì‚¬ìš©ì ë§¥ë½ì— ë§ì¶° ì ì ˆí•œ ìŒì•…ì„ ì¶”ì²œí•  ìˆ˜ ìˆìŒ.</li>
<li>ì‹¤í—˜ ê²°ê³¼, Transformer ê¸°ë°˜ ì‹œìŠ¤í…œì´ ì‚¬ìš©ì í–‰ë™ì„ ë°˜ì˜í•˜ì—¬ ì¶”ì²œì˜ ì •í™•ë„ì™€ ì‚¬ìš©ì ë§Œì¡±ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚´.</li>
<li>ìƒˆë¡œìš´ ì¶”ì²œ ì‹œìŠ¤í…œì€ skip-rate(ê³¡ ê±´ë„ˆë›°ê¸°ìœ¨)ë¥¼ ë‚®ì¶”ê³ , ì„¸ì…˜ ì‹œê°„ì„ ì¦ê°€ì‹œì¼œ ì „ë°˜ì ì¸ ì‚¬ìš©ì ê²½í—˜ì„ í–¥ìƒì‹œí‚´.</li>
</ul>
<h3 id="Meta-AI-ê°œì¸í™”ëœ-ì´ë¯¸ì§€-ìƒì„±-ëª¨ë¸-â€œImagine-yourselfâ€-ë°œí‘œ"><a href="#Meta-AI-ê°œì¸í™”ëœ-ì´ë¯¸ì§€-ìƒì„±-ëª¨ë¸-â€œImagine-yourselfâ€-ë°œí‘œ" class="headerlink" title="Meta AI, ê°œì¸í™”ëœ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ â€œImagine yourselfâ€ ë°œí‘œ"></a>Meta AI, ê°œì¸í™”ëœ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ â€œImagine yourselfâ€ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://ai.meta.com/research/publications/imagine-yourself-tuning-free-personalized-image-generation">ë§í¬</a>, 2024ë…„ 7ì›” 23ì¼</p>
<ul>
<li>Meta AIëŠ” í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ì˜ ì •êµí•œ ì¡°í™”ë¥¼ ì´ë£¨ëŠ” ìµœì‹  ê°œì¸í™”ëœ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì„ ë°œí‘œí•¨.</li>
<li>â€œImagine yourselfâ€ ëª¨ë¸ì€ íŠœë‹ ì—†ì´ë„ ë†’ì€ í’ˆì§ˆì˜ ê°œì¸í™”ëœ ì´ë¯¸ì§€ë¥¼ ìƒì„± ê°€ëŠ¥í•¨.</li>
<li>ìƒˆë¡œìš´ ë°ì´í„° ìƒì„± ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì´ë¯¸ì§€ ë‹¤ì–‘ì„±ì„ ë†’ì´ê³ , í‰í–‰ ì–´í…ì…˜ ì•„í‚¤í…ì²˜ë¥¼ ë„ì…í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶©ì‹¤ë„ì™€ ì‹œê°ì  í’ˆì§ˆì„ í–¥ìƒì‹œí‚´.</li>
<li>ê¸°ì¡´ì˜ ê°œì¸í™” ëª¨ë¸ê³¼ ë¹„êµí•˜ì—¬, ì´ ëª¨ë¸ì€ ì•„ì´ë´í‹°í‹° ë³´ì¡´, í…ìŠ¤íŠ¸ ì¶©ì‹¤ë„, ì‹œê°ì  ë§¤ë ¥ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ.</li>
<li>ì´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ê°œì¸í™” ì‘ìš© í”„ë¡œê·¸ë¨ì˜ ê¸°ì´ˆë¥¼ ì œê³µí•˜ë©°, ì‚¬ìš©ì ìš”êµ¬ë¥¼ ë°˜ì˜í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŒ.</li>
</ul>
<h3 id="Adobe-TurboEdit-í…ìŠ¤íŠ¸-ê¸°ë°˜-ì´ë¯¸ì§€-í¸ì§‘-ê¸°ìˆ -ë„ì…"><a href="#Adobe-TurboEdit-í…ìŠ¤íŠ¸-ê¸°ë°˜-ì´ë¯¸ì§€-í¸ì§‘-ê¸°ìˆ -ë„ì…" class="headerlink" title="Adobe, TurboEdit: í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´ë¯¸ì§€ í¸ì§‘ ê¸°ìˆ  ë„ì…"></a>Adobe, TurboEdit: í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´ë¯¸ì§€ í¸ì§‘ ê¸°ìˆ  ë„ì…</h3><p><a target="_blank" rel="noopener" href="https://betterze.github.io/TurboEdit/">ë§í¬</a>, 2024ë…„ 8ì›” 14ì¼</p>
<ul>
<li>AdobeëŠ” í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ì‹¤ì‹œê°„ í¸ì§‘í•  ìˆ˜ ìˆëŠ” TurboEdit ê¸°ìˆ ì„ ë„ì…í•¨.</li>
<li>ì´ ê¸°ìˆ ì€ 8íšŒ ê¸°ëŠ¥ í‰ê°€(NFEs)ì™€ 4íšŒ í¸ì§‘ í‰ê°€ë§Œìœ¼ë¡œ ë¹ ë¥´ê³  ì •í™•í•œ ì´ë¯¸ì§€ í¸ì§‘ì„ ê°€ëŠ¥í•˜ê²Œ í•¨.</li>
<li>TurboEditëŠ” ê¸°ì¡´ì˜ ë‹¤ì¤‘ ë‹¨ê³„ í™•ì‚° í¸ì§‘ ê¸°ìˆ ë³´ë‹¤ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ì´ë¯¸ì§€ ì†ì„±ì„ ìˆ˜ì •í•  ìˆ˜ ìˆìŒ.</li>
<li>í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì˜ ì‘ì€ ë³€ê²½ì„ í†µí•´ ì´ë¯¸ì§€ì˜ íŠ¹ì • ì†ì„±ì„ ì¡°ì •í•  ìˆ˜ ìˆìœ¼ë©°, ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ìˆ˜ì¤€ìœ¼ë¡œ í¸ì§‘ ê°•ë„ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆìŒ.</li>
<li>ì´ ê¸°ìˆ ì€ ë””í“¨ì „ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì´ë¯¸ì§€ ë³µì› ë° ì†ì„± í¸ì§‘ì„ ë™ì‹œì— ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë¨.</li>
</ul>
<h3 id="ê¸ˆìœµìœ„ì›íšŒ-ê¸ˆìœµë¶„ì•¼-ë§ë¶„ë¦¬-ê°œì„ -ë¡œë“œë§µ-ë°œí‘œ"><a href="#ê¸ˆìœµìœ„ì›íšŒ-ê¸ˆìœµë¶„ì•¼-ë§ë¶„ë¦¬-ê°œì„ -ë¡œë“œë§µ-ë°œí‘œ" class="headerlink" title="ê¸ˆìœµìœ„ì›íšŒ, ê¸ˆìœµë¶„ì•¼ ë§ë¶„ë¦¬ ê°œì„  ë¡œë“œë§µ ë°œí‘œ"></a>ê¸ˆìœµìœ„ì›íšŒ, ê¸ˆìœµë¶„ì•¼ ë§ë¶„ë¦¬ ê°œì„  ë¡œë“œë§µ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://www.kif.re.kr/kif2/publication/maildetview.aspx?nodeid=402&controlno=338353&ismail=1&email=dongyoung.kim@me.com&SL=0">ë§í¬</a>, 2024ë…„ 8ì›” 13ì¼</p>
<ul>
<li>ê¸ˆìœµìœ„ì›íšŒëŠ” ìƒì„±í˜• AIë¥¼ í™œìš©í•œ í˜ì‹ ì ì¸ ê¸ˆìœµ ì„œë¹„ìŠ¤ë¥¼ ê¸ˆìœµê¶Œì—ì„œ ì¶œì‹œí•  ìˆ˜ ìˆë„ë¡ í—ˆìš©í•˜ëŠ” ë§ë¶„ë¦¬ ê°œì„  ë¡œë“œë§µì„ ë°œí‘œí•¨.</li>
<li>ìƒˆë¡œìš´ ë¡œë“œë§µì€ í´ë¼ìš°ë“œ(SaaS) ê¸°ë°˜ ì„œë¹„ìŠ¤ì˜ ì´ìš© ë²”ìœ„ë¥¼ í™•ëŒ€í•˜ê³ , ì—°êµ¬ ê°œë°œ í™˜ê²½ì„ ëŒ€í­ ê°œì„ í•  ê³„íšì„.</li>
<li>â€œììœ¨ë³´ì•ˆ-ê²°ê³¼ì±…ì„â€ ì›ì¹™ì— ê¸°ë°˜í•œ ìƒˆë¡œìš´ ê¸ˆìœµë³´ì•ˆ ì²´ê³„ë¥¼ êµ¬ì¶•í•˜ì—¬ ê¸ˆìœµì—…ê³„ì˜ ê²½ìŸë ¥ì„ ê°•í™”í•  ì˜ˆì •ì„.</li>
<li>ì¥ê¸°ì ìœ¼ë¡œëŠ” ê¸ˆìœµ ë³´ì•ˆ ë²•Â·ì²´ê³„ë¥¼ ì „ë©´ ê°œí¸í•˜ì—¬ ê¸ˆìœµíšŒì‚¬ë“¤ì´ ìì²´ ì—­ëŸ‰ì„ ê°•í™”í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•  ê³„íšì„.</li>
<li>ê¸ˆìœµê¶Œì˜ AI ë° í´ë¼ìš°ë“œ ê¸°ìˆ  í™œìš©ì´ ì¦ëŒ€ë¨ì— ë”°ë¼, ê¸ˆìœµ ì†Œë¹„ì ë³´í˜¸ì™€ ë°ì´í„° í™œìš©ë„ ë˜í•œ ì¦ê°€í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë¨.</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹"><a href="#todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹" class="headerlink" title="(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,"></a>(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAIì—ì„œëŠ” ~~~ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<p><a href="link">ë§í¬</a>, date,</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://openai.com/index/gpt-4o-fine-tuning/</span><br><span class="line">openAI</span><br><span class="line">August 20, 2024</span><br><span class="line"></span><br><span class="line">Fine-tuning now available for GPT-4o</span><br><span class="line">Fine-tune custom versions of GPT-4o to increase performance and accuracy for your applications.</span><br><span class="line"></span><br><span class="line">gpt-4o-fine-tuning &gt; cover image</span><br><span class="line">Today, weâ€™re launching fine-tuning for GPT-4o, one of the most requested features from developers. We are also offering 1M training tokens per day for free for every organization through September 23.</span><br><span class="line"></span><br><span class="line">Developers can now fine-tune GPT-4o with custom datasets to get higher performance at a lower cost for their specific use cases. Fine-tuning enables the model to customize structure and tone of responses, or to follow complex domain-specific instructions. Developers can already produce strong results for their applications with as little as a few dozen examples in their training data set.</span><br><span class="line"></span><br><span class="line">From coding to creative writing, fine-tuning can have a large impact on model performance across a variety of domains. This is just the startâ€”weâ€™ll continue to invest in expanding our model customization options for developers.</span><br><span class="line"></span><br><span class="line">Getting started</span><br><span class="line">GPT-4o fine-tuning is available today to all developers on all paid usage tiers(opens in a new window).</span><br><span class="line"></span><br><span class="line">To get started, visit the fine-tuning dashboard(opens in a new window), click create, and select gpt-4o-2024-08-06 from the base model drop-down. GPT-4o fine-tuning training costs $25 per million tokens, and inference is $3.75 per million input tokens and $15 per million output tokens.</span><br><span class="line"></span><br><span class="line">GPT-4o mini fine-tuning is also available to all developers on all paid usage tiers. Visit the fine-tuning dashboard and select gpt-4o-mini-2024-07-18 from the base model drop-down. For GPT-4o mini, weâ€™re offering 2M training tokens per day for free through September 23.</span><br><span class="line"></span><br><span class="line">To learn more about how to use fine-tuning, visit our docs(opens in a new window).</span><br><span class="line"></span><br><span class="line">Achieving state-of-the-art performance with GPT-4o fine-tuning</span><br><span class="line">Over the past couple of months, weâ€™ve worked with a handful of trusted partners to test fine-tuning on GPT-4o and learn about their use cases. Here are a couple of success stories:</span><br><span class="line"></span><br><span class="line">Cosine achieves state-of-the-art results on the SWE-bench benchmark</span><br><span class="line"></span><br><span class="line">Cosine(opens in a new window)â€™s Genie is an AI software engineering assistant thatâ€™s able to autonomously identify and resolve bugs, build features, and refactor code in collaboration with users. It can reason across complex technical problems and make changes to code with higher accuracy and fewer tokens needed. Genie is powered by a fine-tuned GPT-4o model trained on examples of real software engineers at work, enabling the model to learn to respond in a specific way. The model was also trained to be able to output in specific formats, such as patches that could be committed easily to codebases.</span><br><span class="line"></span><br><span class="line">With a fine-tuned GPT-4o model, Genie achieves a SOTA score of 43.8% on the new SWE-bench(opens in a new window) Verified benchmark, announced last Tuesday. Genie also holds a SOTA score of 30.08% on SWE-bench Full, beating its previous SOTA score of 19.27%, the largest ever improvement in this benchmark.</span><br><span class="line"></span><br><span class="line">SWE-bench Verified Leaderboard</span><br><span class="line">43.8%</span><br><span class="line">38.8%</span><br><span class="line">38.4%</span><br><span class="line">37%</span><br><span class="line">33.6%</span><br><span class="line">26.2%</span><br><span class="line">25.6%</span><br><span class="line">22.4%</span><br><span class="line">18.2%</span><br><span class="line">7%</span><br><span class="line">4.4%</span><br><span class="line">2.8%</span><br><span class="line">1.4%</span><br><span class="line">1.2%</span><br><span class="line">0.4%</span><br><span class="line">Cosine Genie</span><br><span class="line">Amazon QDeveloper Agent(v20240719-dev)</span><br><span class="line">AutoCodeRover(v20240620) + GPT4o (2024-05-13)</span><br><span class="line">Factory Code Droid</span><br><span class="line">SWE-agent + Claude3.5 Sonnet</span><br><span class="line">AppMap Navie +GPT 4o (2024-05-13)</span><br><span class="line">Amazon QDeveloper Agent(v20240430-dev)</span><br><span class="line">SWE-agent + GPT 4(1106)</span><br><span class="line">SWE-agent + Claude3 Opus</span><br><span class="line">RAG + Claude 3Opus</span><br><span class="line">RAG + Claude 2</span><br><span class="line">RAG + GPT 4 (1106)</span><br><span class="line">RAG + SWE-Llama7B</span><br><span class="line">RAG + SWE-Llama13B</span><br><span class="line">RAG + ChatGPT 3.5</span><br><span class="line">0</span><br><span class="line">5</span><br><span class="line">10</span><br><span class="line">15</span><br><span class="line">20</span><br><span class="line">25</span><br><span class="line">30</span><br><span class="line">35</span><br><span class="line">40</span><br><span class="line">45</span><br><span class="line">50</span><br><span class="line">% Resolved</span><br><span class="line">Distyl ranks 1st on BIRD-SQL benchmark</span><br><span class="line"></span><br><span class="line">Distyl(opens in a new window), an AI solutions partner to Fortune 500 companies, recently placed 1st on the BIRD-SQL(opens in a new window) benchmark, the leading text-to-SQL benchmark. Distylâ€™s fine-tuned GPT-4o achieved an execution accuracy of 71.83% on the leaderboard and excelled across tasks like query reformulation, intent classification, chain-of-thought, and self-correction, with particularly high performance in SQL generation.</span><br><span class="line"></span><br><span class="line">GPT-4o Fine-Tuning &gt; Media &gt; Leaderboard</span><br><span class="line">Data Privacy and Safety</span><br><span class="line">Fine-tuned models remain entirely under your control, with full ownership of your business data, including all inputs and outputs. This ensures your data is never shared or used to train other models.</span><br><span class="line"></span><br><span class="line">Weâ€™ve also implemented layered safety mitigations for fine-tuned models to ensure they arenâ€™t being misused. For example, we continuously run automated safety evals on fine-tuned models and monitor usage  to ensure applications adhere to our usage policies.</span><br><span class="line"></span><br><span class="line">Weâ€™re excited to see what you build by fine-tuning GPT-4o. If youâ€™d like to explore more model customization options, please reach out to our teamâ€”weâ€™d be happy to help!</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/microsoft/Phi-3.5-MoE-instruct</span><br><span class="line">Microsoft</span><br><span class="line">8/20/24</span><br><span class="line">Phi goes MoE! Microsoft just released Phi-3.5-MoE, a 42B parameter MoE built upon datasets used for Phi-3. Phi-3.5 MoE outperforms bigger models in reasoning capability and is only behind GPT-4o-mini. ğŸ‘€</span><br><span class="line">TL;DR</span><br><span class="line">ğŸ§® 42B parameters with 6.6B activated during generation</span><br><span class="line">ğŸ‘¨â€ğŸ«  16 experts with 2 active in generation</span><br><span class="line">ğŸªŸ Instruct model with 128k context</span><br><span class="line">ğŸš€ Outperforms Llama 3 8b, Gemma 2 9B across benchmarks</span><br><span class="line">ğŸ¥ˆ Closely behind GPT-4o mini, but outperforms Gemini 1.5 Flash</span><br><span class="line">ğŸŒ Multilingual support (unclear which languages)</span><br><span class="line">ğŸ“œ MIT License</span><br><span class="line">ğŸ“Š Trained on 4.9 trillion tokens (including 10% multilingual)</span><br><span class="line">ğŸ¤— Available on Hugging Face</span><br><span class="line"></span><br><span class="line">Phi-3.5-MoE is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available documents - with a focus on very high-quality, reasoning dense data. The model supports multilingual and comes with 128K context length (in tokens). The model underwent a rigorous enhancement process, incorporating supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.</span><br><span class="line"></span><br><span class="line">ğŸ¡ Phi-3 Portal</span><br><span class="line">ğŸ“° Phi-3 Microsoft Blog</span><br><span class="line">ğŸ“– Phi-3 Technical Report</span><br><span class="line">ğŸ‘©â€ğŸ³ Phi-3 Cookbook</span><br><span class="line">ğŸ–¥ï¸ Try It</span><br><span class="line"></span><br><span class="line">Phi-3.5: [mini-instruct]; [MoE-instruct] ; [vision-instruct]</span><br><span class="line"></span><br><span class="line">Intended Uses</span><br><span class="line">Primary Use Cases</span><br><span class="line">The model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require:</span><br><span class="line"></span><br><span class="line">Memory/compute constrained environments</span><br><span class="line">Latency bound scenarios</span><br><span class="line">Strong reasoning (especially code, math and logic)</span><br><span class="line">Our model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.</span><br><span class="line">Letâ€™s gooo.. Microsoft just release Phi 3.5 mini, MoE and vision with 128K context, multilingual &amp; MIT license! MoE beats Gemini flash, Vision competitive with GPT4oğŸ”¥</span><br><span class="line">&gt; Mini with 3.8B parameters, beats Llama3.1 8B and Mistral 7B and competitive with Mistral NeMo 12B</span><br><span class="line">&gt; Multilingual and Tokenizer with 32K vocab</span><br><span class="line">&gt; Trained on 3.4T tokens</span><br><span class="line">&gt; Used 512 H100s to train (10 days)</span><br><span class="line">&gt; MoE - 16x3.8B (6.6B active - 2 experts) - beats Gemini flash</span><br><span class="line">&gt; 128K context, Multilingual and same tokenizer (32K vocab)</span><br><span class="line">&gt; Trained on 4.9T tokens</span><br><span class="line">&gt; Used 512H100s to train (23 days)</span><br><span class="line">&gt; Ph3.5 Vision - 4.2B params - beats GPT4o on averaged benchmarks</span><br><span class="line">&gt; Trained on 500B tokens</span><br><span class="line">&gt; Used 256 A100 to train (6 days)</span><br><span class="line">&gt; Specialised in TextVQA + ScienceVQA</span><br><span class="line">Let&#x27;s goo, thanks a ton Microsoft for the goodies!âš¡</span><br><span class="line">More stats in the thread</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://research.google/blog/transformers-in-music-recommendation/</span><br><span class="line">August 16, 2024</span><br><span class="line"></span><br><span class="line">Google</span><br><span class="line">In recommendation systems, user actions (e.g., skip, like, dislike) provide an important signal about user preference, but can also quickly become unwieldy. Learn how weâ€™ve applied transformersâ€™ ability to process sequences of data to improve YouTube Music</span><br><span class="line"></span><br><span class="line">Transformers in music recommendation</span><br><span class="line">August 16, 2024</span><br><span class="line"></span><br><span class="line">Anushya Subbiah and Vikram Aggarwal, Software Engineers, Google Research</span><br><span class="line"></span><br><span class="line">We present a music recommendation ranking system that uses Transformer models to better understand the sequential nature of user actions based on the current user context.</span><br><span class="line"></span><br><span class="line">Users have more choices for listening to music than ever before. Popular services boast of massive and varied catalogs. The YouTube Music catalog, for example, has over 100M songs globally. It follows that item recommendations are a core part of these products. Recommender systems make sense of the item catalog and are critical for tuning the catalog for the userâ€™s tastes and needs. In products that provide recommendations, user actions on the recommended items â€” such as skip, like, or dislike â€” provide an important signal about user preferences. Observing and learning from these actions can lead to better recommendation systems. In YouTube Music, leveraging this signal is critical to understanding a user&#x27;s musical taste.</span><br><span class="line"></span><br><span class="line">Consider a scenario where a user typically likes slow-tempo songs. When presented with an uptempo song, the user would typically skip it. However, at the gym, when theyâ€™re in a workout session, they like more uptempo music. In such a situation, we want to continue learning from their prior history to understand their musical preferences. At the same time, we want to discount prior skips of uptempo songs when recommending workout music.</span><br><span class="line"></span><br><span class="line">Below we illustrate the usersâ€™ music listening experience, with music songs shown as items and with the userâ€™s actions as text beneath. In current recommendation systems that donâ€™t consider the broader context, we would predict that the user will skip an uptempo song, resulting in demoting a potentially relevant and valuable song.</span><br><span class="line"></span><br><span class="line">TransformersMusic1-Journey1final</span><br><span class="line">The below figure shows the same user journey as before, but in a different situation, where upbeat music may be more relevant. We still utilize their previous music listening, while recommending upbeat music that is close to their usual music listening. In effect, we are learning which previous actions are relevant in the current task of ranking music, and which actions are irrelevant.</span><br><span class="line"></span><br><span class="line">TransformersMusic2-Journey2final</span><br><span class="line">A typical user will perform hundreds of like, dislike, and skip actions, and this sequence of input data, though information-rich, quickly becomes unwieldy. To add to this complexity, users perform different numbers of actions. While a typical user might have hundreds of actions, user behavior can vary between a small number of actions to a very large number of actions, and a good ranking system must be flexible in handling different input sizes.</span><br><span class="line"></span><br><span class="line">In this post we discuss how weâ€™ve applied transformers, which are well-suited to processing sequences of input data, to improve the recommendation system in YouTube Music. This recommendation system consists of three key stages: item retrieval, item ranking, and filtering. Prior user actions are usually added to the ranking models as an input feature. Our approach adapts the Transformer architecture from generative models for the task of understanding the sequential nature of user actions, and blends that with ranking models personalized for that user. Using transformers to incorporate different user actions based on the current user context helps steer music recommendations directly towards the userâ€™s current need. For signed-in users, this approach allows us to incorporate a userâ€™s history without having to explicitly identify what in a userâ€™s history is valuable to the ranking task.</span><br><span class="line"></span><br><span class="line">Retrieval, ranking, and filtering</span><br><span class="line">In existing models, it was difficult to identify which user actions were relevant to the userâ€™s current needs. To understand such models, we need to look at typical recommendation systems. These systems are usually set up as three distinct stages. First, retrieval systems retrieve thousands of relevant items (documents, songs, etc.) from a large corpus. Second, ranking systems evaluate the retrieved results, so that the items that are more relevant and important to the userâ€™s needs are assigned a higher score. The key complexity of ranking comes from the value judgment between concepts such as relevance, importance, novelty, and assigning a numerical value to these fuzzy concepts. Finally, a filtering stage sorts the ranked list by scores, and reduces the sorted list to a short list that is shown to the user. When designing and deploying a ranking model, it is hard to manually select and apply relative weights to specific user actions out of the many hundreds or thousands that they may commonly take.</span><br><span class="line"></span><br><span class="line">Transformers make sense of sequences</span><br><span class="line">Transformers are well-suited to a class of problems where we need to make sense of a sequence of input data. While transformers have been used to improve ranking functions, previous approaches have not focused on user actions: Transformer models like RankFormer have used item candidates (as opposed to user-actions) as input, classical language transformers like BERT are used to rank language output, or BERT-like models are used in recommendations, as in Bert4Rec.</span><br><span class="line"></span><br><span class="line">The Transformer architecture consists of self-attention layers to make sense of sequential input. Transformer models have shown incredible performance on translation or classification tasks, even with ambiguous input text. The self-attention layers capture the relationship between words of text in a sentence, which suggests that they might be able to resolve the relationship between user actions as well. The attention layers in transformers learn attention weights between the pieces of input (tokens), which are akin to word relationships in the input sentence.</span><br><span class="line"></span><br><span class="line">TransformerMusic3-Transformer</span><br><span class="line">Transformers in generative models.</span><br><span class="line"></span><br><span class="line">This is how we utilize the Transformer architecture to encode user actions on YouTube Music. In the user journey involving uptempo music above, we saw how some actions were less important than others. For example, when the user is listening to music at the gym, the user may prefer high-energy upbeat music that they would normally skip, hence related actions (e.g., the skip action in this example) should get a lower attention weight. However, when a user is listening to music in other settings, user actions should get more attention. There should be a difference in attention weight applied to music context versus a userâ€™s music history based upon the activity the user is performing. For example, when a user is at the gym they might listen to music that is more upbeat, but not too far from what they usually listen to. Or when they are driving, they might prefer to explore more new music.</span><br><span class="line"></span><br><span class="line">Transformers for ranking in YouTube Music</span><br><span class="line">Our architecture combines a Transformer with an existing ranking model to learn the combined ranking that best blends user actions with listening history (see diagram below). In this diagram, information flows from the bottom to the top: the inputs to the Transformer are shown at the bottom, and the produced ranking score is shown at the top. â€œItemsâ€ here are music tracks that we want to rank, with the goal to produce a ranking score for each music â€œitemâ€ given to it, with other signals (also called features) provided as input.</span><br><span class="line"></span><br><span class="line">TransformerMusic4-Hero</span><br><span class="line">Transformers and Ranker in the joint music recommendation task.</span><br><span class="line"></span><br><span class="line">Here are the signals describing the user action at each time step:</span><br><span class="line"></span><br><span class="line">Intention of the action: interrupt a music track, select a music track to listen to, autoplay.</span><br><span class="line">Salience of the action: percentage of the music track that was played, time since prior user-action.</span><br><span class="line">Other metadata: artist, language of the music.</span><br><span class="line">Music track: music track identifier corresponding to the user-action.</span><br><span class="line">Music tracks corresponding to user actions are represented by a vector of numbers called the track embedding. This music-track embedding is used as an input in both the Transformer and the existing ranking model. User-action signals, like intention and metadata, are turned into vectors with the same length as the length of the track embedding. This operation, called a projection, allows us to combine the signals simply by adding the two vectors: user-action signals and the track embedding, producing input vectors (called tokens) for the Transformer. The tokens provided as inputs to the Transformer are used to score the retrieved music items. When considering the userâ€™s history, we include the previous user actions and the music the user is currently listening to, as both capture valuable user context. The output vector from the Transformer is combined with the existing ranking model inputs, using a multi-layer neural network. The Transformer is co-trained with the ranking model, for multiple ranking objectives.</span><br><span class="line"></span><br><span class="line">Offline analysis and live experiments demonstrate that using this Transformer significantly improves the performance of the ranking model, leading to a reduction in skip-rate and an increase in time users spend listening to music. Skipping less frequently indicates that on average, users like the recommendations more. Increased session length indicates that users are happier with the overall experience. These two metrics demonstrate the improvement in user satisfaction for YouTube Music.</span><br><span class="line"></span><br><span class="line">Future work</span><br><span class="line">We see two main opportunities to build on this work. The first would be to adapt the technique to other parts of the recommendation system such as retrieval models. There are also a variety of nonsequential features, which are used as inputs to the prior ranking model, that we are also exploring for incorporation. Currently, these are combined after the Transformer stage, and we predict that incorporating them within the Transformer would allow for improved self-attention between the sequential features, like user-actions, and non-sequential features such as artist popularity, user language, music popularity and more.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://ai.meta.com/research/publications/imagine-yourself-tuning-free-personalized-image-generation</span><br><span class="line">COMPUTER VISION</span><br><span class="line">Imagine yourself: Tuning-Free Personalized Image Generation</span><br><span class="line">July 23, 2024</span><br><span class="line">This paper introduces a state-of-the-art model designed for personal image generation. Want to try it? The feature is available now as a beta in Meta AI for users in the US.</span><br><span class="line"></span><br><span class="line">Abstract</span><br><span class="line">Diffusion models have demonstrated remarkable efficacy across various image-to-image tasks. In this research, we introduce Imagine yourself, a state-of-the-art model designed for personalized image generation. Unlike conventional tuning-based personalization techniques, Imagine yourself operates as a tuning-free model, enabling all users to leverage a shared framework without individualized adjustments. Moreover, previous work met challenges balancing identity preservation, following complex prompts and preserving good visual quality, resulting in models having strong copy-paste effect of the reference images. Thus, they can hardly generate images following prompts that require significant changes to the reference image, e.g., changing facial expression, head and body poses, and the diversity of the generated images is low. To address these limitations, our proposed method introduces 1) a new synthetic paired data generation mechanism to encourage image diversity, 2) a fully parallel attention architecture with three text encoders and a fully trainable vision encoder to improve the text faithfulness, and 3) a novel coarse-to-fine multi-stage finetuning methodology that gradually pushes the boundary of visual quality. Our study demonstrates that Imagine yourself surpasses the state-of-the-art personalization model, exhibiting superior capabilities in identity preservation, visual quality, and text alignment. This model establishes a robust foundation for various personalization applications. Human evaluation results validate the modelâ€™s SOTA superiority across all aspects (identity preservation, text faithfulness, and visual appeal) compared to the previous personalization models.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://betterze.github.io/TurboEdit/</span><br><span class="line">Adobe</span><br><span class="line">14 Aug 2024</span><br><span class="line"></span><br><span class="line">TurboEdit: Instant text-based image editing</span><br><span class="line">Zongze Wu, Nicholas Kolkin, Jonathan Brandt, Richard Zhang, Eli Shechtman</span><br><span class="line">Adobe Research</span><br><span class="line">To appear in ECCV, 2024</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Abstract</span><br><span class="line">We address the challenges of precise image inversion and disentangled image editing in the context of few-step diffusion models. We introduce an encoder based iterative inversion technique. The inversion network is conditioned on the input image and the reconstructed image from the previous step, allowing for correction of the next reconstruction towards the input image. We demonstrate that disentangled controls can be easily achieved in the few-step diffusion model by conditioning on an (automatically generated) detailed text prompt. To manipulate the inverted image, we freeze the noise maps and modify one attribute in the text prompt (either manually or via instruction based editing driven by an LLM), resulting in the generation of a new image similar to the input image with only one attribute changed. It can further control the editing strength and accept instructive text prompt. Our approach facilitates realistic text-guided image edits in real-time, requiring only 8 number of functional evaluations (NFEs) in inversion (one-time cost) and 4 NFEs per edit. Our method is not only fast, but also significantly outperforms state-of-the-art multi-step diffusion editing techniques.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Method</span><br><span class="line">Given an input real image x0, we utilize the LLaVA to generate a detailed caption c. Users can modify c to create a new text prompt c â€². The inversion process begins by feeding the x0, c, current time step t, and a previously reconstructed image x0,t+1 (initialized as a zero matrix) into the inversion network. This network then predicts the noise Ïµt, which is subsequently input into a frozen SDXL-Turbo model to generate the new reconstruction image x0,t. Given the final inverted noise Ïµt, along with c, we can use SDXL-Turbo to create an inversion trajectory and reconstruct x0,0, which is very similar to x0. Using the same noises Ïµt and slightly different text prompt c â€², starting from t = T to smaller t, the editing trajectory will be very similar to the inversion trajectory, and the generated image will closely resemble the input image, differing only in the specified attribute in c&#x27;.</span><br><span class="line"></span><br><span class="line">TurboEdit: Instant text-based image editing</span><br><span class="line">Zongze Wu, Nicholas Kolkin, Jonathan Brandt, Richard Zhang, Eli Shechtman</span><br><span class="line">We address the challenges of precise image inversion and disentangled image editing in the context of few-step diffusion models. We introduce an encoder based iterative inversion technique. The inversion network is conditioned on the input image and the reconstructed image from the previous step, allowing for correction of the next reconstruction towards the input image. We demonstrate that disentangled controls can be easily achieved in the few-step diffusion model by conditioning on an (automatically generated) detailed text prompt. To manipulate the inverted image, we freeze the noise maps and modify one attribute in the text prompt (either manually or via instruction based editing driven by an LLM), resulting in the generation of a new image similar to the input image with only one attribute changed. It can further control the editing strength and accept instructive text prompt. Our approach facilitates realistic text-guided image edits in real-time, requiring only 8 number of functional evaluations (NFEs) in inversion (one-time cost) and 4 NFEs per edit. Our method is not only fast, but also significantly outperforms state-of-the-art multi-step diffusion editing techniques.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.kif.re.kr/kif2/publication/maildetview.aspx?nodeid=402&amp;controlno=338353&amp;ismail=1&amp;email=dongyoung.kim@me.com&amp;SL=0</span><br><span class="line">ê¸ˆìœµìœ„ì›íšŒ</span><br><span class="line">2024. 8. 13.</span><br><span class="line"></span><br><span class="line">ã€Œê¸ˆìœµë¶„ì•¼ ë§ë¶„ë¦¬ ê°œì„  ë¡œë“œë§µã€ ë°œí‘œ</span><br><span class="line"></span><br><span class="line">- ê¸ˆìœµê¶Œ ë§ë¶„ë¦¬ 10ë…„, &quot;í˜ì‹ ê³¼ ë³´ì•ˆì˜ ìƒˆë¡œìš´ ê· í˜•ìœ¼ë¡œì˜ ë„ì•½&quot;</span><br><span class="line"></span><br><span class="line">- ê¸ˆìœµê¶Œë„ ìƒì„±í˜• AIë¥¼ í™œìš©í•œ í˜ì‹ ì ì¸ ê¸ˆìœµì„œë¹„ìŠ¤ ì¶œì‹œ ê°€ëŠ¥</span><br><span class="line"></span><br><span class="line">- &quot;ììœ¨ë³´ì•ˆ-ê²°ê³¼ì±…ì„&quot; ì›ì¹™ì— ì…ê°í•œ æ–° ê¸ˆìœµë³´ì•ˆì²´ê³„ êµ¬ì¶•</span><br><span class="line"></span><br><span class="line">â—ˆ ê¸ˆìœµíšŒì‚¬ ë“±ì˜ ìƒì„±í˜• AI í™œìš©ì„ í—ˆìš©í•˜ê³ , í´ë¼ìš°ë“œ(SaaS) ì´ìš© ë²”ìœ„ë¥¼ ëŒ€í­ í™•ëŒ€í•˜ë©°, ì—°êµ¬Â·ê°œë°œ í™˜ê²½ì„ ì ê·¹ ê°œì„ </span><br><span class="line"></span><br><span class="line">â—ˆ ì¤‘Â·ì¥ê¸°ì ìœ¼ë¡œëŠ” ê¸ˆìœµë³´ì•ˆ ë²•Â·ì²´ê³„ë¥¼ ì „ë©´ ê°œí¸í•˜ëŠ” ë“± ììœ¨ë³´ì•ˆ-ê²°ê³¼ì±…ì„ ì›ì¹™ìœ¼ë¡œì˜ ê·œì œ ì„ ì§„í™” ë°©í–¥ì„ ì œì‹œí•˜ê³ , ê¸ˆìœµíšŒì‚¬ ë“±ì´ ìì²´ì ì¸ ì—­ëŸ‰ ê°•í™”ë¥¼ í†µí•´ ë¯¸ë¦¬ ëŒ€ë¹„í•  ìˆ˜ ìˆë„ë¡ ì§€ì›</span><br><span class="line">- 1 -</span><br><span class="line">ë³´ë„ì‹œì  2024. 8. 13.(í™”) 14:00 ë°°í¬ 2024. 8. 12.(ì›”) 10:00</span><br><span class="line">ï½¢ê¸ˆìœµë¶„ì•¼ ë§ë¶„ë¦¬ ê°œì„  ë¡œë“œë§µï½£ ë°œí‘œ- ê¸ˆìœµê¶Œ ë§ë¶„ë¦¬ 10ë…„, â€œí˜ì‹ ê³¼ ë³´ì•ˆì˜ ìƒˆë¡œìš´ ê· í˜•ìœ¼ë¡œì˜ ë„ì•½â€</span><br><span class="line">- ê¸ˆìœµê¶Œë„ ìƒì„±í˜• AIë¥¼ í™œìš©í•œ í˜ì‹ ì ì¸ ê¸ˆìœµì„œë¹„ìŠ¤ ì¶œì‹œ ê°€ëŠ¥- â€œììœ¨ë³´ì•ˆ-ê²°ê³¼ì±…ì„â€ ì›ì¹™ì— ì…ê°í•œ æ–° ê¸ˆìœµë³´ì•ˆì²´ê³„ êµ¬ì¶•â—ˆ ê¸ˆìœµíšŒì‚¬ ë“±ì˜ ìƒì„±í˜• AI í™œìš©ì„ í—ˆìš©í•˜ê³ , í´ë¼ìš°ë“œ(SaaS) ì´ìš© ë²”ìœ„ë¥¼ëŒ€í­ í™•ëŒ€í•˜ë©°, ì—°êµ¬Â·ê°œë°œ í™˜ê²½ì„ ì ê·¹ ê°œì„ </span><br><span class="line">â—ˆ ì¤‘Â·ì¥ê¸°ì ìœ¼ë¡œëŠ” ê¸ˆìœµë³´ì•ˆ ë²•Â·ì²´ê³„ë¥¼ ì „ë©´ ê°œí¸í•˜ëŠ” ë“± ììœ¨ë³´ì•ˆ-ê²°ê³¼ì±…ì„ ì›ì¹™ìœ¼ë¡œì˜ ê·œì œ ì„ ì§„í™” ë°©í–¥ì„ ì œì‹œí•˜ê³ , ê¸ˆìœµíšŒì‚¬ ë“±ì´ ìì²´ì ì¸ì—­ëŸ‰ ê°•í™”ë¥¼ í†µí•´ ë¯¸ë¦¬ ëŒ€ë¹„í•  ìˆ˜ ìˆë„ë¡ ì§€ì›</span><br><span class="line"> `24ë…„ 8ì›” 13ì¼(í™”), ê¸ˆìœµìœ„ì›íšŒëŠ” ê¹€ë³‘í™˜ ê¸ˆìœµìœ„ì›íšŒ ìœ„ì›ì¥ ì£¼ì¬ë¡œ</span><br><span class="line">ë¯¼ê°„ ë³´ì•ˆ ì „ë¬¸ê°€ë“¤ê³¼ ê¸ˆìœµí˜‘íšŒ, ê¸ˆìœµê°ë…ì›, ê¸ˆìœµë³´ì•ˆì› ë“± ìœ ê´€ê¸°ê´€ì´</span><br><span class="line">ì°¸ì—¬í•œ ê°€ìš´ë° ï½¢ê¸ˆìœµë¶„ì•¼ ë§ë¶„ë¦¬ ê°œì„  ë¡œë“œë§µï½£ì„ ë°œí‘œí•˜ì˜€ë‹¤. ã€ ê¸ˆìœµë¶„ì•¼ ë§ë¶„ë¦¬ ê°œì„  ë¡œë“œë§µ ë°œí‘œ í–‰ì‚¬ ê°œìš” ã€‘</span><br><span class="line">ï¿­ ì¼ì‹œ / ì¥ì†Œ : 2024.8.13.(í™”) 14:00 / KB êµ­ë¯¼ì€í–‰ í†µí•© ITì„¼í„°(ê¹€í¬)</span><br><span class="line">ï¿­ ì°¸ì„ì : ã€ê¸ˆìœµìœ„ì›íšŒã€‘ ê¹€ë³‘í™˜ ê¸ˆìœµìœ„ì›ì¥(ì£¼ì¬), ë””ì§€í„¸ê¸ˆìœµì •ì±…ê´€</span><br><span class="line">ã€ìœ ê´€ê¸°ê´€ã€‘ ê¸ˆìœµê°ë…ì›, ê¸ˆìœµë³´ì•ˆì›</span><br><span class="line"> ã€ì—…ê³„ã€‘ ê¸ˆìœµí˜‘íšŒ(ì€í–‰, ë³´í—˜, ì—¬ì‹ , ê¸ˆìœµíˆ¬ì, í•€í…Œí¬) ã€ë¯¼ê°„ì „ë¬¸ê°€ã€‘ ê¸ˆìœµì—°êµ¬ì› ë¶€ì›ì¥, ì‹ ìš©ì •ë³´ì› AI ì„¼í„°ì¥</span><br><span class="line"> ê¹€í™ì„  å‰ì•ˆë© CEO, ë°•ì¶˜ì‹ êµìˆ˜</span><br><span class="line"> ê¸ˆìœµìœ„ì›íšŒëŠ” ê·¸ë™ì•ˆ ì—¬ëŸ¬ ì°¨ë¡€ì— ê±¸ì¹œ ê°„ë‹´íšŒ ë“±ì„ í†µí•´ ê¸ˆìœµ ë³´ì•ˆ ê·œì œì—ë”°ë¥¸ ê¸ˆìœµíšŒì‚¬ ë“±ì˜ ì• ë¡œë¥¼ ì§ì ‘ ì²­ì·¨í•˜ì˜€ìœ¼ë©°, ï½¢ê¸ˆìœµê¶Œ ë§ë¶„ë¦¬ TF*ï½£ë¥¼</span><br><span class="line">ìš´ì˜í•˜ì—¬ ë³´ì•ˆì „ë¬¸ê°€, ì—…ê³„, ìœ ê´€ê¸°ê´€ ë“±ìœ¼ë¡œë¶€í„° ì˜ê²¬ì„ ìˆ˜ë ´í•˜ì˜€ë‹¤. ê·¸ê°„</span><br><span class="line">ê¸ˆìœµìœ„ ì›íšŒ ë³´ë„ìë£Œ</span><br><span class="line">- 2 -</span><br><span class="line">ë…¼ì˜ëœ ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ë°˜ì˜í•˜ì—¬ ë§ë¶„ë¦¬ ê°œì„ ì„ ìœ„í•œ ì„¸ë¶€ ì¶”ì§„ê³¼ì œì™€ ê¸ˆìœµë³´ì•ˆì²´ê³„ì˜ ì„ ì§„í™” ë°©í–¥ì„ ë‹´ì€ ë¡œë“œë§µì„ ë§ˆë ¨í•˜ì˜€ë‹¤.</span><br><span class="line">* `24.4~6ì›”ë™ì•ˆ ê¸ˆìœµìœ„ì›íšŒ, ë””ì§€í„¸í”Œë«í¼ì •ë¶€ìœ„ì›íšŒ, ê¸ˆìœµê°ë…ì›, ê¸ˆìœµë³´ì•ˆì›, IT ë°</span><br><span class="line">ë³´ì•ˆ ì „ë¬¸ê°€, í•™ê³„, ë²•ì¡°ê³„, ê¸ˆìœµ ì—…ê¶Œ ë“±ê³¼ í•¨ê»˜ ë§ë¶„ë¦¬ ê°œì„  ë°©í–¥ ë…¼ì˜</span><br><span class="line">â–¡ ì¶”ì§„ ë°°ê²½</span><br><span class="line"> ë§ë¶„ë¦¬ë¡œ ì¸í•´ ê¸ˆìœµíšŒì‚¬ ë° ì „ìê¸ˆìœµì—…ì(ì´í•˜ â€œê¸ˆìœµíšŒì‚¬ ë“±â€)ì˜ ì—…ë¬´ìƒ ë¹„íš¨ìœ¨ì´ í´ ë¿ë§Œ ì•„ë‹ˆë¼, ì‹ ê¸°ìˆ  í™œìš©ì´ ì €í•´ë˜ê³  ì—°êµ¬Â·ê°œë°œì´ ì–´ë µë‹¤ëŠ”</span><br><span class="line">ê·œì œ ê°œì„ ìš”ì²­ì´ ì§€ì† ì œê¸°ëœ ë°” ìˆë‹¤.</span><br><span class="line"> íŠ¹íˆ, ì†Œí”„íŠ¸ì›¨ì–´ ì‹œì¥ì´ ìì²´ êµ¬ì¶•í˜•ì—ì„œ í´ë¼ìš°ë“œ ê¸°ë°˜ì˜ êµ¬ë…í˜•(SaaS)ìœ¼ë¡œë¹ ë¥´ê²Œ ì „í™˜ë˜ê³  ìƒì„±í˜• AIì˜ í™œìš©ì´ ì‚°ì—…ì˜ ë¯¸ë˜ë¥¼ ì¢Œìš°í•˜ëŠ” ìƒí™©ì—ì„œ, ë§ë¶„ë¦¬ëŠ” ì—…ë¬´ìƒ ë¶ˆí¸ì„ ë„˜ì–´ ê¸ˆìœµê²½ìŸë ¥ ì €í•˜ ìš”ì¸ìœ¼ë¡œ ì§€ì ë°›ê³  ìˆë‹¤.</span><br><span class="line"> ë˜í•œ, ì¼ë¶€ ê¸ˆìœµíšŒì‚¬ ë“±ì€ ì¸í„°ë„· ë“± ì™¸ë¶€ í†µì‹ ê³¼ ë¶„ë¦¬ëœ í™˜ê²½ë§Œì„ êµ¬ì¶•í•´ë†“ê³  ì„ ì§„ ë³´ì•ˆì²´ê³„ ë„ì…ì— ì†Œí™€í•˜ê±°ë‚˜, ê·œì œ ê·¸ëŠ˜ì— ìˆ¨ì–´ ë³€í™”í•˜ê³  ìˆëŠ”</span><br><span class="line">IT í™˜ê²½ì— ë¶€í•©í•˜ëŠ” ë³´ì•ˆ ì¡°ì¹˜ë„ ì ì ˆíˆ ê°–ì¶”ì§€ ì•ŠëŠ” ë“± ì˜¤íˆë ¤ ê¸ˆìœµê¶Œ</span><br><span class="line">ë³´ì•ˆ ë°œì „ì´ ì €í•´ë˜ëŠ” ë¶€ì‘ìš©ì´ ì¡´ì¬í•˜ëŠ” ìƒí™©ì´ë‹¤.</span><br><span class="line"> ê¸ˆìœµê¶Œ ë§ë¶„ë¦¬ ë„ì… ì´í›„ ì•½ 10ë…„ì´ ê²½ê³¼í•œ ì§€ê¸ˆ, ê¸ˆìœµë‹¹êµ­ì€ ë‚¡ì€ ê·œì œë¥¼ê°œì„ í•˜ê³  ì¤‘Â·ì¥ê¸°ì ìœ¼ë¡œ ê¸ˆìœµ ë³´ì•ˆ ë²•Â·ì œë„ë¥¼ ì „ë©´ ê°œí¸í•˜ëŠ” ë“± í˜ì‹ ê³¼ ë³´ì•ˆì˜ìƒˆë¡œìš´ ê· í˜•ì„ ì°¾ê¸° ìœ„í•œ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì„ ì¶”ì§„í•  ê³„íšì´ë¼ê³  ë°í˜”ë‹¤. â–¡ ìƒŒë“œë°•ìŠ¤ë¥¼ í†µí•˜ì—¬ ê·œì œ ì• ë¡œ ì¦‰ì‹œ í•´ì†Œ + ë³„ë„ ë³´ì•ˆëŒ€ì±… ë³‘í–‰ í˜„í–‰ ê¸ˆìœµë³´ì•ˆì²´ê³„ê°€ ì˜¤ëœ ê¸°ê°„ë™ì•ˆ ì¸í„°ë„· ë“± ì™¸ë¶€í†µì‹ ê³¼ ë¶„ë¦¬ëœ í™˜ê²½ì„ì „ì œë¡œ êµ¬ì„±ë˜ì–´ ì˜¨ ì ì„ ê³ ë ¤í•˜ì—¬, ê¸‰ê²©í•œ ê·œì œ ì™„í™”ë³´ë‹¤ëŠ” ë‹¨ê³„ì  ê°œì„ ì„ ì¶”ì§„í•œë‹¤.</span><br><span class="line"> IT í™˜ê²½ ë³€í™”ë¡œ ì¸í•´ ì‹ ì†í•œ ëŒ€ì‘ì´ í•„ìš”í•œ ê³¼ì œëŠ” ìƒŒë“œë°•ìŠ¤ ë“±ì„ í™œìš©í•˜ì—¬ ê·œì œ ì• ë¡œë¥¼ ì¦‰ì‹œ í•´ì†Œí•˜ë˜, ììœ¨ë³´ì•ˆì²´ê³„ í™•ë¦½ê¹Œì§€ëŠ” ì‹œê°„ì´ ì†Œìš”ë˜ë¯€ë¡œ ë³´ì•ˆìƒì˜ ë¬¸ì œê°€ ì—†ë„ë¡ ë³„ë„ì˜ ë³´ì•ˆëŒ€ì±… ë“± ì¶©ë¶„í•œ ì•ˆì „ì¥ì¹˜ë¥¼</span><br><span class="line">ë§ˆë ¨í•  ì˜ˆì •ì´ë‹¤.</span><br><span class="line">- 3 -</span><br><span class="line"> ì²«ì§¸, ê¸ˆìœµíšŒì‚¬ ë“±ì˜ ìƒì„±í˜• AI í™œìš©ì„ í—ˆìš©í•œë‹¤.</span><br><span class="line"> ëŒ€ë¶€ë¶„ì˜ ìƒì„±í˜• AIê°€ í´ë¼ìš°ë“œ ê¸°ë°˜ì˜ ì¸í„°ë„· í™˜ê²½ì—ì„œ ì œê³µë˜ëŠ” ë°˜ë©´, êµ­ë‚´ ê¸ˆìœµê¶Œì€ ì¸í„°ë„· ë“± ì™¸ë¶€ í†µì‹  í™œìš© ì œí•œ ë“±ìœ¼ë¡œ ì¸í•´ ìƒì„±í˜• AI ë„ì…ì— ì œì•½ì´ ìˆëŠ” ìƒí™©ì´ë‹¤.</span><br><span class="line"> ì´ì—, ìƒŒë“œë°•ìŠ¤ë¥¼ í†µí•´ ì¸í„°ë„· í™œìš© ì œí•œ ë“±ì— ëŒ€í•œ ê·œì œ íŠ¹ë¡€ë¥¼ í—ˆìš©í•œë‹¤. ì´ì™€ í•¨ê»˜, ì˜ˆìƒë˜ëŠ” ë¦¬ìŠ¤í¬ì— ëŒ€í•œ ë³´ì•ˆëŒ€ì±…ì„ ì¡°ê±´ìœ¼ë¡œ ë¶€ê³¼í•˜ê³  ê¸ˆìœµê°ë…ì›Â·ê¸ˆìœµë³´ì•ˆì›ì´ ì‹ ì²­ ê¸°ì—…ë³„ ë³´ì•ˆ ì ê²€Â·ì»¨ì„¤íŒ…ì„ ì‹¤ì‹œí•˜ëŠ” ë“± ì¶©ë¶„í•œ</span><br><span class="line">ì•ˆì „ì¥ì¹˜ë¥¼ ë§ˆë ¨í•  ê³„íšì´ë‹¤.</span><br><span class="line"> ë‘˜ì§¸, í´ë¼ìš°ë“œ ê¸°ë°˜ì˜ ì‘ìš© í”„ë¡œê·¸ë¨(SaaS) ì´ìš© ë²”ìœ„ë¥¼ ëŒ€í­ í™•ëŒ€í•œë‹¤.</span><br><span class="line"> ê¸°ì¡´ì—ëŠ” ë¬¸ì„œê´€ë¦¬, ì¸ì‚¬ê´€ë¦¬ ë“± ë¹„ì¤‘ìš” ì—…ë¬´ì— ëŒ€í•´ì„œë§Œ SaaS ì´ìš©ì´</span><br><span class="line">í—ˆìš©ë˜ê³ , ê³ ê° ê°œì¸ì‹ ìš©ì •ë³´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” ë“± ì—„ê²©í•œ ìƒŒë“œë°•ìŠ¤ ë¶€ê°€ì¡°ê±´ì´ ë¶€ê³¼ë˜ì–´ í™œìš©ì´ ì œí•œë˜ì—ˆë‹¤.</span><br><span class="line"> ì´ì—, ë³´ì•ˆê´€ë¦¬, ê³ ê°ê´€ë¦¬(CRM) ë“±ì˜ ì—…ë¬´ê¹Œì§€ ì´ìš© ë²”ìœ„ë¥¼ í™•ëŒ€í•˜ê³ , ê°€ëª…ì •ë³´ ì²˜ë¦¬ ë° ëª¨ë°”ì¼ ë‹¨ë§ê¸°ì—ì„œì˜ SaaS ì´ìš©ê¹Œì§€ í—ˆìš©í•˜ëŠ” ë“± SaaS</span><br><span class="line">í™œìš©ë„ë¥¼ ì œê³ í•  ì˜ˆì •ì´ë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ, ê·œì œ íŠ¹ë¡€ í™•ëŒ€ì— ë”°ë¥¸ ë³´ì•ˆ ìš°ë ¤ì—ëŒ€ì‘í•˜ê¸° ìœ„í•´ ë³´ì•ˆëŒ€ì±…ì„ ë§ˆë ¨í•˜ì—¬ ìƒŒë“œë°•ìŠ¤ ì§€ì • ì¡°ê±´ìœ¼ë¡œ ë¶€ê³¼í•  ê³„íšì´ë‹¤.</span><br><span class="line"> ì…‹ì§¸, ê¸ˆìœµíšŒì‚¬ ë“±ì˜ ì—°êµ¬Â·ê°œë°œ í™˜ê²½ì„ ê°œì„ í•œë‹¤.</span><br><span class="line"> `22.11ì›” ì—°êµ¬Â·ê°œë°œ í™˜ê²½ì—ì„œ ì¸í„°ë„·ì„ ììœ ë¡­ê²Œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•œì°¨ë¡€ê·œì œê°€ ê°œì„ ë˜ì—ˆìœ¼ë‚˜, ì—°êµ¬Â·ê°œë°œ í™˜ê²½ì˜ ë¬¼ë¦¬ì  ë¶„ë¦¬ ë° ê°œì¸ì‹ ìš©ì •ë³´ í™œìš© ê¸ˆì§€ ë“±ì— ë”°ë¼ ê³ ê°ë³„ íŠ¹ì„±Â·ìˆ˜ìš”ì— ë§ëŠ” í˜ì‹ ì ì¸ ì„œë¹„ìŠ¤ ì—°êµ¬Â·ê°œë°œì—ì œì•½ì´ í¬ë‹¤ëŠ” ì§€ì ì´ ì§€ì† ì œê¸°ë˜ì—ˆë‹¤.</span><br><span class="line"> ì´ì—, ï½¢ì „ìê¸ˆìœµê°ë…ê·œì •ï½£ì„ ê°œì •í•˜ì—¬ ê¸ˆìœµíšŒì‚¬ ë“±ì´ ì—°êµ¬Â·ê°œë°œ ê²°ê³¼ë¬¼ì„ë³´ë‹¤ ê°„í¸í•˜ê²Œ ì´ê´€í•  ìˆ˜ ìˆë„ë¡ ë¬¼ë¦¬ì  ì œí•œì„ ì™„í™”í•˜ê³ , ê°€ëª…ì •ë³´ í™œìš©ì„í—ˆìš©í•˜ëŠ” ë“± í˜ì‹ ì ì¸ ê¸ˆìœµìƒí’ˆì„ ê°œë°œí•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ì œê³µí•˜ê³ ì í•œë‹¤.</span><br><span class="line">- 4 -</span><br><span class="line">&lt; ì°¸ê³  : ë§ë¶„ë¦¬ ê°œì„  ë‹¨ê¸° ì¶”ì§„ ê³¼ì œ ì¢…í•© êµ¬ì„±ë„ &gt;</span><br><span class="line">- 5 -</span><br><span class="line">â–¡ 2ë‹¨ê³„ ìƒŒë“œë°•ìŠ¤ì—ì„œëŠ” ê°œì¸ì‹ ìš©ì •ë³´ê¹Œì§€ í™œìš© í™•ëŒ€</span><br><span class="line"> ê°€ëª…ì •ë³´ í™œìš©ì„ í—ˆìš©í•œ 1ë‹¨ê³„ ìƒŒë“œë°•ìŠ¤ì˜ ìš´ì˜ ì„±ê³¼ì™€ ì•ˆì „ì„±ì´ ì¶©ë¶„íˆ ê²€ì¦ë  ê²½ìš°, ë¹ ë¥´ë©´ ë‚´ë…„ì—ëŠ” 2ë‹¨ê³„ ìƒŒë“œë°•ìŠ¤ë¥¼ ì¶”ì§„í•˜ì—¬ ê¸ˆìœµíšŒì‚¬ê°€ê°€ëª…ì •ë³´ê°€ ì•„ë‹Œ ê°œì¸ì‹ ìš©ì •ë³´ê¹Œì§€ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ê·œì œ íŠ¹ë¡€ì˜</span><br><span class="line">ê³ ë„í™”ë„ ì¶”ì§„í•œë‹¤. ë‹¤ë§Œ, ë°ì´í„° í™œìš© ë²”ìœ„ ì¦ê°€ì— ë”°ë¥¸ ì¶”ê°€ ë³´ì•ˆëŒ€ì±…ë“±ë„ í•¨ê»˜ ë¶€ê³¼í•  ì˜ˆì •ì´ë‹¤. â–¡ ìƒŒë“œë°•ìŠ¤ ìš´ì˜ ê²½í—˜ ë“±ì„ í† ëŒ€ë¡œ ê¸ˆìœµë³´ì•ˆì²´ê³„ì˜ ì„ ì§„í™” ì¶”ì§„ : â€œëª©í‘œÂ·ì›ì¹™ ì¤‘ì‹¬â€ì˜ ë³´ì•ˆê·œì œ, â€œììœ¨ë³´ì•ˆ-ê²°ê³¼ì±…ì„â€ ì›ì¹™ì— ì…ê°í•œ ë³´ì•ˆì²´ê³„ êµ¬ì¶• ëˆ„ì ëœ ìƒŒë“œë°•ìŠ¤ ì‚¬ë¡€ë¥¼ í†µí•´ í˜ì‹ ì„±, ì†Œë¹„ì í¸ìµ, ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë“±ì´</span><br><span class="line">ì¶©ë¶„íˆ ê²€ì¦ëœ ê³¼ì œëŠ” ì •ê·œ ì œë„í™”í•˜ê³ , ì¤‘Â·ì¥ê¸°ì ìœ¼ë¡œëŠ” ë³„ë„ì˜ ê¸ˆìœµë³´ì•ˆë²•</span><br><span class="line">(ê°€ì¹­ â€œï½¢ë””ì§€í„¸ ê¸ˆìœµë³´ì•ˆë²•ï½£â€)ì„ ë§ˆë ¨í•˜ì—¬ â€œììœ¨ë³´ì•ˆ-ê²°ê³¼ì±…ì„â€ ì›ì¹™ì—ì…ê°í•œ æ–° ê¸ˆìœµë³´ì•ˆì²´ê³„ë¥¼ êµ¬ì¶•í•´ ë‚˜ê°ˆ ê³„íšì´ë‹¤.</span><br><span class="line"> ì—´ê±°ì‹ í–‰ìœ„ ê·œì¹™(Rule) ì¤‘ì‹¬ì˜ ê¸ˆìœµë³´ì•ˆ ê·œì œë¥¼ ëª©í‘œÂ·ì›ì¹™(Principle) ì¤‘ì‹¬ìœ¼ë¡œ ì „í™˜í•˜ê³ , ê¸ˆìœµíšŒì‚¬ ë“±ì€ ìì²´ ë¦¬ìŠ¤í¬ í‰ê°€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¸ë¶€ ë³´ì•ˆ í†µì œë¥¼ ììœ¨ì ìœ¼ë¡œ êµ¬ì„±í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.</span><br><span class="line"> ë‹¤ë§Œ, ê¸ˆìœµíšŒì‚¬ ë“±ì— ë¶€ì—¬ëœ ììœ¨ì— ë”°ë¥¸ ì±…ì„ì€ ê°•í™”í•  í•„ìš”ê°€ ìˆë‹¤. ì¤‘ìš” ë³´ì•ˆì‚¬í•­ì˜ CEOÂ·ì´ì‚¬íšŒ ë³´ê³ ì˜ë¬´ ë“± ê¸ˆìœµíšŒì‚¬ ë“±ì˜ ë‚´ë¶€ ë³´ì•ˆ ê±°ë²„ë„ŒìŠ¤ë¥¼ ê°•í™”í•˜ê³ , ì „ì‚°ì‚¬ê³  ë°œìƒì‹œ ë°°ìƒì±…ì„ í™•ëŒ€ ë° ì‹¤íš¨ì„± ìˆëŠ” ê³¼ì§•ê¸ˆ ë„ì… ë“±ë²•ì  ê·¼ê±° ë§ˆë ¨ì„ í†µí•´ ê¸ˆìœµíšŒì‚¬ ë“±ì˜ ë³´ì•ˆ ë…¸ë ¥ ì œê³ ë¥¼ ìœ ë„í•  ì˜ˆì •ì´ë‹¤.</span><br><span class="line"> í•œí¸, ê¸ˆìœµë‹¹êµ­ì€ ê¸ˆìœµíšŒì‚¬ ë“±ì˜ ììœ¨ë³´ì•ˆì²´ê³„ ìˆ˜ë¦½Â·ì´í–‰ì„ ê²€ì¦í•˜ì—¬ë¯¸í¡í•œ ê²½ìš° ì‹œì •ìš”êµ¬ã†ì´í–‰ëª…ë ¹ì„ ë¶€ê³¼í•˜ê³  ë¶ˆì´í–‰ì‹œ ì—„ì¤‘ ì œì¬í•˜ëŠ” ë“±</span><br><span class="line">ê¸ˆìœµê¶Œ ë³´ì•ˆ ìˆ˜ì¤€ ê°•í™”ë¥¼ ìœ„í•´ ì§€ì† ë…¸ë ¥í•´ ë‚˜ê°ˆ ê²ƒì´ë‹¤.</span><br><span class="line"> ë˜í•œ, ì œ3ì ë¦¬ìŠ¤í¬(3rd-party risk*)ì— ëŒ€í•œ ê´€ë¦¬ë¥¼ ê°•í™”í•˜ê¸° ìœ„í•˜ì—¬ ì œë„ë¥¼ì •ë¹„í•œë‹¤. AI, í´ë¼ìš°ë“œ, ë°ì´í„°ì„¼í„° ë“± ê¸ˆìœµê¶Œì˜ ì œ3ìì— ëŒ€í•œ ì •ë³´ì²˜ë¦¬</span><br><span class="line">- 6 -</span><br><span class="line">ìœ„íƒì´ ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ë§Œí¼ ì´ì— ëŒ€í•œ ë¦¬ìŠ¤í¬ ê´€ë¦¬ë°©ì•ˆì´ í•„ìš”í•˜ë‹¤. EUÂ·ì˜êµ­ ë“± í•´ì™¸ ì„ ì§„ì‚¬ë¡€ ì—°êµ¬ë¥¼ í† ëŒ€ë¡œ, êµ­ë‚´ í™˜ê²½ì— ë§ëŠ” ë„ì… ë°©í–¥ì„</span><br><span class="line">ê²€í† í•˜ì—¬ ê´€ë ¨ ì œë„ ì •ë¹„ë¥¼ ì¶”ì§„í•˜ê³ ì í•œë‹¤.</span><br><span class="line"> * éê¸ˆìœµë¶€ë¬¸ì˜ ì¥ì•  ë°œìƒ, ì •ë³´ ìœ ì¶œ ë“±ì˜ ì‚¬ê³ ê°€ ê¸ˆìœµ ë¶€ë¬¸ìœ¼ë¡œ ì „ì´ë˜ëŠ” ë¦¬ìŠ¤í¬â–¡ ê¸°ëŒ€ íš¨ê³¼</span><br><span class="line">ì²«ì§¸, ê¸ˆë²ˆ ë§ë¶„ë¦¬ ê°œì„ ì„ í†µí•´ ê¸ˆìœµì‚°ì—… ì „ë°˜ì˜ ê²½ìŸë ¥ì´ í¬ê²Œ ì œê³ ë </span><br><span class="line">ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤.</span><br><span class="line"> AIì™€ í´ë¼ìš°ë“œ(SaaS) ê¸°ë°˜ì˜ ì—…ë¬´ ìë™í™”, ì „ì‚¬ì  ê²½ì˜ê´€ë¦¬(ERP :</span><br><span class="line">Enterprise Resource Planning), ì¤€ë²• ê°ì‹œ í”„ë¡œê·¸ë¨ ë“± ë„ì…ì— ë”°ë¼ ê¸ˆìœµê¶Œì˜</span><br><span class="line">ì—…ë¬´ ìƒì‚°ì„±ì´ í–¥ìƒë˜ê³ , ë¹…ë°ì´í„° ë¶„ì„ ë“± ê¸ˆìœµ ë°ì´í„°ì˜ í™œìš©ë„ ì¦ê°€í• </span><br><span class="line">ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.</span><br><span class="line"> ë˜í•œ, ì¸í„°ë„· ì ‘ì†ì´ ììœ ë¡­ê²Œ í—ˆìš©ë˜ëŠ” ì—°êµ¬Â·ê°œë°œë§ì˜ í™œìš©ë„ ì œê³ ì—</span><br><span class="line">ë”°ë¼ ê¸ˆìœµíšŒì‚¬ ë“±ì˜ IT ê°œë°œì´ í™œì„±í™”ë  ìˆ˜ ìˆìœ¼ë©°, ì—°êµ¬Â·ê°œë°œ í™˜ê²½ ê°œì„ ì— ë”°ë¥¸ ê¸ˆìœµê¶Œì˜ ìš°ìˆ˜ IT ì¸ë ¥ ìœ ì¹˜ íš¨ê³¼ê¹Œì§€ ê¸°ëŒ€ëœë‹¤.</span><br><span class="line"> ë‘˜ì§¸, ë§ë¶„ë¦¬ ê°œì„ ì˜ ì´ìµì´ ê¸ˆìœµì‚°ì—… ë¿ë§Œ ì•„ë‹ˆë¼ ê¸ˆìœµì†Œë¹„ìì˜ íš¨ìš©</span><br><span class="line">ì¦ì§„ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆë„ë¡ ìœ ë„í•´ ë‚˜ê°ˆ ê³„íšì´ë‹¤.</span><br><span class="line"> ìƒì„±í˜• AIë¥¼ í™œìš©í•œ ë°ì´í„° ë¶„ì„Â·ì˜ˆì¸¡ ëª¨ë¸ ê³ ë„í™”ë¥¼ í†µí•´ ë‹¤ì–‘í•œ íŠ¹í™” ë³´í—˜ìƒí’ˆì„ ê°œë°œí•˜ê³ , ì‹ ìš©í‰ê°€ëª¨ë¸ ê³ ë„í™”ë¥¼ í†µí•´ ì¤‘ê¸ˆë¦¬ ëŒ€ì¶œì˜ ì €ë³€ì„í™•ëŒ€í•˜ëŠ” ë“± ê¸ˆìœµ ì‚¬ê°ì§€ëŒ€ í•´ì†Œì— ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤.</span><br><span class="line"> ë¿ë§Œ ì•„ë‹ˆë¼, ê¸ˆìœµê¶Œì€ ìƒì„±í˜• AIë¥¼ í™œìš©í•œ ì´ìƒê¸ˆìœµê±°ë˜íƒì§€ì‹œìŠ¤í…œ(FDS : Fraud Detection System) ê³ ë„í™”ë¥¼ í†µí•´ ë¶€ì •ê±°ë˜, ì‹ ì¢…ì‚¬ê¸° ì‹œë„ë“±ì„ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ì°¨ë‹¨í•˜ëŠ” ë“± ê¸ˆìœµì†Œë¹„ì ë³´í˜¸ë¥¼ ê°•í™”í•  ìˆ˜ ìˆë‹¤.</span><br><span class="line">- 7 -</span><br><span class="line">â–¡ í–¥í›„ ê³„íš</span><br><span class="line"> ê¸ˆìœµë‹¹êµ­ì€ 8ì›” 22ì¼(ëª©) ì „ ì—…ê¶Œ ì—…ë¬´ ì„¤ëª…íšŒ*ë¥¼ ì‹œì‘ìœ¼ë¡œ, 9ì›”ê¹Œì§€ ì—…ê¶Œë³„ë¡œ ì—…ë¬´ ì„¤ëª…íšŒë¥¼ ê°œìµœí•˜ê³ , ê·œì œìƒŒë“œë°•ìŠ¤ ì‹ ì²­ ê¸°ì—…ë³„ ë³´ì•ˆ ì—­ëŸ‰, ì‚¬ì—…</span><br><span class="line">êµ¬ì¡° ë“±ì„ ê³ ë ¤í•˜ì—¬ ë¶€ê°€ ì¡°ê±´ìœ¼ë¡œ ì§€ì¼œì•¼ í•  ë³´ì•ˆëŒ€ì±… ë“±ì— ëŒ€í•œ ì»¨ì„¤íŒ…ì„ì œê³µí•  ì˜ˆì •ì´ë‹¤.</span><br><span class="line"> * êµ¬ì²´ì ì¸ ì‹œê°„, ì¥ì†Œ ë° ì§„í–‰ ë°©ì‹ ë“±ì€ í˜‘íšŒë¥¼ í†µí•´ ì•ˆë‚´í•  ì˜ˆì •ì´ë©°, ê¸ˆìœµë³´ì•ˆì›í™ˆí˜ì´ì§€(fsec.or.kr)ë¥¼ í†µí•´ì„œë„ ê³µì§€í•  ì˜ˆì •</span><br><span class="line"> ê·¸ë¦¬ê³ , 9ì›” ì¤‘ ê·œì œìƒŒë“œë°•ìŠ¤ ì‹ ì²­ì„ ì ‘ìˆ˜ë°›ì•„ ì—°ë‚´ ì‹ ê·œ ê³¼ì œì— ëŒ€í•œ í˜ì‹  ê¸ˆìœµì„œë¹„ìŠ¤ë¥¼ ì§€ì •í•˜ê³ ì í•œë‹¤. ì´ì— ë”°ë¼, ë¹ ë¥´ë©´ ì˜¬í•´ ë§ë¶€í„° ê¸ˆìœµê¶Œì—ì„œë„ ìƒì„±í˜• AI í™œìš©ì´ ê°€ëŠ¥í•´ ì§ˆ ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.</span><br><span class="line"> ë”ë¶ˆì–´, ì—°ë§ê¹Œì§€ ì—°êµ¬Â·ê°œë°œ í™˜ê²½ ê°œì„  ë“±ì„ ìœ„í•œ ï½¢ì „ìê¸ˆìœµê°ë…ê·œì •ï½£ ê°œì • ì ˆì°¨ ë˜í•œ ì°¨ì§ˆ ì—†ì´ ìˆ˜í–‰í•  ê³„íšì´ë©°, æ–° ê¸ˆìœµë³´ì•ˆì²´ê³„ êµ¬ì¶•ì„ ìœ„í•œì—°êµ¬ìš©ì—­ ë“±ì„ ê±°ì³ ê¸ˆìœµë³´ì•ˆ ë²•Â·ì²´ê³„ ê°œí¸ì„ ì¶”ì§„í•´ ë‚˜ê°ˆ ì˜ˆì •ì´ë‹¤.</span><br><span class="line"> ê¹€ë³‘í™˜ ê¸ˆìœµìœ„ì›ì¥ì€ â€œí´ë¼ìš°ë“œ, ìƒì„±í˜• AI ë“± ê¸‰ë³€í•˜ëŠ” IT í™˜ê²½ë³€í™”ì—ëŒ€ì‘í•˜ê¸° ìœ„í•´ ë³´ë‹¤ íš¨ê³¼ì ì¸ ë§ë¶„ë¦¬ ê°œì„  ë°©ì•ˆì´ í•„ìš”í•œ ì‹œì â€ì´ë¼ê³ ì–¸ê¸‰í•˜ë©´ì„œ, â€œíŠ¹íˆ ëª¨ë“  ì •ì±…ì€ ê¸€ë¡œë²Œ ìŠ¤íƒ ë‹¤ë“œ ê´€ì ì—ì„œ ì •ë¹„í•´ ë‚˜ê°„ë‹¤ëŠ” ê¸°ì¡° í•˜ì—, ìš°ë¦¬ë‚˜ë¼ì—ë§Œ ì¡´ì¬í•˜ëŠ” ëŒ€í‘œì ì¸ ê°ˆë¼íŒŒê³ ìŠ¤ ê·œì œë¥¼ê³¼ê°í•˜ê²Œ ê°œì„ í•  í•„ìš”ê°€ ìˆë‹¤â€ê³  ë§í–ˆë‹¤.</span><br><span class="line"> ë˜í•œ, â€œì´ë²ˆ ë§ë¶„ë¦¬ ê°œì„  ë¡œë“œë§µì´ ê¸ˆìœµì‚°ì—…ì˜ ê²½ìŸë ¥ ì œê³ ì™€ ê¸ˆìœµì†Œë¹„ìì˜ íš¨ìš© ì¦ì§„ìœ¼ë¡œ ì´ì–´ì§ˆ ê²ƒâ€ìœ¼ë¡œ ê¸°ëŒ€í•˜ë©°, â€œì–´ë µê²Œ ê·œì œë¥¼ ê°œì„ í•˜ëŠ” ë§Œí¼ ê¸ˆìœµì—…ê¶Œë„ ë³´ì•ˆì‚¬ê³  ì—†ì´ ìƒˆë¡œìš´ ì œë„ê°€ ì˜ ì•ˆì°©í•  ìˆ˜ ìˆë„ë¡í˜ì¨ì£¼ì‹œê¸¸ ë°”ë€ë‹¤â€ê³  ë‹¹ë¶€í–ˆë‹¤. â€» ê¸ˆìœµë¶„ì•¼ ë§ë¶„ë¦¬ ê°œì„  ë¡œë“œë§µ ê´€ë ¨ ìì„¸í•œ ë‚´ìš©ì€ ë³„ì²¨ ìë£Œë¥¼ ì°¸ê³ í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.</span><br><span class="line">- 8 -</span><br><span class="line">[ë³„ì²¨] 1. ê¸ˆìœµìœ„ì›ì¥ ëª¨ë‘ ë°œì–¸</span><br><span class="line"> 2. ê¸ˆìœµë¶„ì•¼ ë§ë¶„ë¦¬ ê°œì„  ë¡œë“œë§µ</span><br><span class="line">ë‹´ë‹¹ ë¶€ì„œ ê¸ˆìœµìœ„ì›íšŒ</span><br><span class="line">ê¸ˆìœµì•ˆì „ê³¼</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-08-21</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/08/21/2024-8-21-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 8ì›” 21ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/08/16/2024-8-16-AI-NEWS/" title="2024ë…„ 8ì›” 16ì¼ AI ì†Œì‹">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>