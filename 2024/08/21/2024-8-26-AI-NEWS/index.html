<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 8월 26일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="Nvidia는 고성능 소형 언어 모델인 Mistral-NeMo-Minitron 8B를, AI21 Labs는 긴 문맥 처리에 특화된 Jamba 1.5 모델을 선보였습니다. 또한, Jina AI는 장문 임베딩 모델의 새로운 처리 방법을 소개하였으며, Meta는 인간 중심 "><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 8월 26일 AI 소식</a></h3></div><div class="post-content"><p>Nvidia는 고성능 소형 언어 모델인 Mistral-NeMo-Minitron 8B를, AI21 Labs는 긴 문맥 처리에 특화된 Jamba 1.5 모델을 선보였습니다. 또한, Jina AI는 장문 임베딩 모델의 새로운 처리 방법을 소개하였으며, Meta는 인간 중심 비전 작업을 위한 새로운 모델 패밀리인 Sapiens를 발표했습니다. LinkedIn과 Neural Magic도 AI 모델의 효율성을 극대화하기 위한 새로운 도구를 공개하였고, KB금융그룹은 AI 기술을 과장하여 홍보하는 ‘AI 워싱’의 문제점을 경고했습니다.</p>
<h3 id="Nvidia-Mistral-NeMo-Minitron-8B-소형-언어-모델-출시"><a href="#Nvidia-Mistral-NeMo-Minitron-8B-소형-언어-모델-출시" class="headerlink" title="Nvidia, Mistral-NeMo-Minitron 8B: 소형 언어 모델 출시"></a>Nvidia, Mistral-NeMo-Minitron 8B: 소형 언어 모델 출시</h3><p><a target="_blank" rel="noopener" href="https://blogs.nvidia.com/blog/mistral-nemo-minitron-8b-small-language-model/">링크</a>, 2024년 8월 21일</p>
<ul>
<li>Nvidia는 Mistral NeMo 12B 모델을 소형화한 Mistral-NeMo-Minitron 8B를 출시함</li>
<li>이 모델은 높은 정확도를 유지하면서도 낮은 컴퓨팅 비용으로 실행 가능</li>
<li>Pruning과 Distillation 기법을 결합하여 모델 크기를 줄이면서도 성능을 최적화</li>
<li>소형 언어 모델 중 최고 성능을 자랑하며, 다양한 AI 작업에서 활용 가능</li>
<li>해당 모델은 Nvidia RTX 워크스테이션에서 실시간으로 구동 가능하며, Hugging Face에서도 다운로드 가능</li>
<li>모델은 9가지 주요 벤치마크에서 우수한 성능을 기록하며, 언어 이해, 상식 추론, 수학적 추론, 요약, 코딩 및 진실한 답변 생성 능력을 입증함</li>
<li>Nvidia AI Foundry 플랫폼을 통해 모델을 더욱 소형화하여 스마트폰이나 로봇 등 임베디드 장치에 적합한 버전으로도 활용 가능</li>
</ul>
<h3 id="AI21-Labs-Jamba-1-5-모델-출시"><a href="#AI21-Labs-Jamba-1-5-모델-출시" class="headerlink" title="AI21 Labs, Jamba 1.5 모델 출시"></a>AI21 Labs, Jamba 1.5 모델 출시</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251">링크</a>, 2024년 8월 22일</p>
<ul>
<li>AI21 Labs는 새로운 Jamba 1.5 모델 시리즈를 발표함</li>
<li>Jamba 1.5 모델은 SSM-Transformer 아키텍처를 사용하여 긴 문맥 처리와 속도, 효율성을 극대화</li>
<li>Jamba 1.5 모델은 256K의 긴 문맥 창을 지원하여 대형 문서 요약, 분석, 에이전트 및 RAG(리트리벌 증강 생성) 워크플로우에 적합</li>
<li>Jamba 1.5 Mini와 Large 모델은 각각 Arena Hard 벤치마크에서 최고 성능을 기록함</li>
<li>모델은 다중 언어를 지원하며, 구조화된 JSON 출력, 함수 호출, 문서 객체 처리, 인용 생성 기능을 제공</li>
<li>AI21 Studio, Google Cloud Vertex AI, Microsoft Azure, Nvidia NIM 등 다양한 클라우드 플랫폼에서 즉시 사용할 수 있으며, Amazon Bedrock, Databricks Marketplace, Snowflake Cortex 등에서도 곧 출시 예정</li>
<li>ExpertsInt8라는 새로운 양자화 기술을 통해 MoE(Mixture of Experts) 모델에서 메모리 사용량을 줄이고, 단일 8 GPU 노드에서 모델을 실행 가능</li>
</ul>
<h3 id="Jina-AI-장문-임베딩-모델을-위한-‘Late-Chunking’-기술-발표"><a href="#Jina-AI-장문-임베딩-모델을-위한-‘Late-Chunking’-기술-발표" class="headerlink" title="Jina AI, 장문 임베딩 모델을 위한 ‘Late Chunking’ 기술 발표"></a>Jina AI, 장문 임베딩 모델을 위한 ‘Late Chunking’ 기술 발표</h3><p><a target="_blank" rel="noopener" href="https://jina.ai/news/late-chunking-in-long-context-embedding-models/">링크</a>, 2024년 8월 23일</p>
<ul>
<li>Jina AI는 긴 문맥을 다루는 임베딩 모델을 위한 새로운 처리 방법인 ‘Late Chunking’ 기술을 발표함</li>
<li>이 방법은 문서의 긴 문맥을 효과적으로 처리하여, RAG(리트리벌 증강 생성) 시스템에서 보다 나은 검색 성능을 제공</li>
<li>‘Late Chunking’ 기술은 문서를 미리 분할하는 대신, 임베딩 모델의 트랜스포머 레이어를 사용해 문서 전체를 처리한 후, 각 청크에 문맥 정보를 반영한 임베딩을 생성</li>
<li>이 기술은 긴 문서에서 문맥 정보를 더욱 효과적으로 유지하며, BEIR 벤치마크에서 높은 성능을 보임</li>
<li>Late Chunking은 긴 문맥을 효과적으로 처리하여, 기존의 청킹 방식보다 검색 정확도를 크게 개선</li>
</ul>
<h3 id="Meta-인간-중심-비전-작업을-위한-‘Sapiens’-모델-발표"><a href="#Meta-인간-중심-비전-작업을-위한-‘Sapiens’-모델-발표" class="headerlink" title="Meta, 인간 중심 비전 작업을 위한 ‘Sapiens’ 모델 발표"></a>Meta, 인간 중심 비전 작업을 위한 ‘Sapiens’ 모델 발표</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2408.12569">링크</a>, 2024년 8월 23일</p>
<ul>
<li>Meta는 인간 중심 비전 작업을 위한 Sapiens 모델 패밀리를 발표함</li>
<li>이 모델은 2D 포즈 추정, 신체 부위 분할, 깊이 추정, 표면 법선 예측 등 4가지 핵심 작업을 지원</li>
<li>1K 해상도에서의 추론을 기본적으로 지원하며, 개별 작업에 맞게 간단히 미세 조정 가능</li>
<li>3억 개 이상의 인류 이미지 데이터셋을 기반으로 자가 학습을 통해 성능을 크게 향상</li>
<li>Sapiens 모델은 다양한 인간 중심 벤치마크에서 기존 최고 성능을 초과 달성</li>
<li>Humans-5K, Humans-2K, Hi4D, THuman2 등에서 상대적 RMSE 및 각도 오류를 크게 개선</li>
</ul>
<h3 id="LinkedIn-Liger-Kernel-출시"><a href="#LinkedIn-Liger-Kernel-출시" class="headerlink" title="LinkedIn, Liger-Kernel 출시"></a>LinkedIn, Liger-Kernel 출시</h3><p><a target="_blank" rel="noopener" href="https://github.com/linkedin/Liger-Kernel">링크</a>, 2024년 8월 23일</p>
<ul>
<li>LinkedIn의 LLM 연구팀은 멀티 GPU 파인 튜닝을 위한 새로운 효율적 GPU 커널인 Liger-Kernel을 출시함</li>
<li>이 커널은 멀티 GPU 환경에서 20%의 처리량 증가와 60%의 메모리 감소를 제공</li>
<li>Flash Attention, PyTorch FSDP, Microsoft DeepSpeed와 호환 가능하며, 다양한 LLM 모델에 적용 가능</li>
<li>Hugging Face 호환 RMSNorm, RoPE, SwiGLU, CrossEntropy, FusedLinearCrossEntropy 등을 지원</li>
<li>Triton 커널을 사용하여 정확한 계산을 수행하며, 더 긴 문맥 길이와 더 큰 배치 크기를 처리 가능</li>
<li>이 커널은 단일 라인 코드로 쉽게 적용 가능하며, 오픈 소스로 커뮤니티 주도의 개발을 유도</li>
</ul>
<h3 id="Neural-Magic-LLM-Compressor-프레임워크-공개"><a href="#Neural-Magic-LLM-Compressor-프레임워크-공개" class="headerlink" title="Neural Magic, LLM Compressor 프레임워크 공개"></a>Neural Magic, LLM Compressor 프레임워크 공개</h3><p><a target="_blank" rel="noopener" href="https://neuralmagic.com/blog/llm-compressor-is-here-faster-inference-with-vllm/">링크</a>, 2024년 8월 14일</p>
<ul>
<li>Neural Magic은 LLM Compressor라는 오픈 소스 프레임워크를 공개하여, LLM(대형 언어 모델)의 압축과 성능 향상을 지원</li>
<li>이 프레임워크는 다양한 양자화 기술과 희소성 옵션을 제공하여 LLM 모델의 유연성을 극대화</li>
<li>Neural Magic 팀은 이 도구를 사용하여 Llama 3.1 405B 모델을 포함한 다양한 모델을 성공적으로 압축</li>
<li>Activation Quantization을 통해 INT8 또는 FP8 텐서 코어를 활용하여 최대 3배 더 빠른 서버&#x2F;처리량 배치를 실현</li>
<li>LLM Compressor는 vLLM과 완벽하게 통합되어 바로 사용 가능하며, AI 연구자와 기업들에게 최적화된 모델 생성 및 사용을 지원</li>
</ul>
<h3 id="KB금융그룹-AI-워싱-경고"><a href="#KB금융그룹-AI-워싱-경고" class="headerlink" title="KB금융그룹, AI 워싱 경고"></a>KB금융그룹, AI 워싱 경고</h3><p><a target="_blank" rel="noopener" href="https://www.kbfg.com/kbresearch/report/reportView.do?reportId=2000501">링크</a>, 2024년 8월 19일</p>
<ul>
<li>KB금융그룹은 AI 기술을 실제로 사용하지 않으면서 마치 사용하는 것처럼 홍보하는 ‘AI 워싱’의 위험성을 경고함</li>
<li>AI 워싱은 소비자 신뢰 저하, 투자 자원의 낭비, 과도한 기대 유발 등의 부작용을 초래할 수 있음</li>
<li>아마존의 무인 매장 ‘아마존고’와 채용 스타트업 ‘준코’ 등 다수의 사례가 AI 워싱으로 지적됨</li>
<li>AI 워싱에 대한 규제를 강화하고, 투명한 기술 사용과 정보 제공이 필요하다고 강조</li>
<li>소비자와 투자자들에게는 AI 기술에 대한 비판적 시각과 투명한 설명을 요구할 것을 권장</li>
</ul>
<p>이상 오늘의 AI 소식이었습니다.</p>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://blogs.nvidia.com/blog/mistral-nemo-minitron-8b-small-language-model/</span><br><span class="line">Lightweight Champ: NVIDIA Releases Small Language Model With State-of-the-Art Accuracy</span><br><span class="line">Mistral-NeMo-Minitron 8B is a miniaturized version of the recently released Mistral NeMo 12B model, delivering high accuracy combined with the compute efficiency to run the model across GPU-accelerated data centers, clouds and workstations.</span><br><span class="line">August 21, 2024 by Kari Briski</span><br><span class="line"> Share</span><br><span class="line"></span><br><span class="line">Developers of generative AI typically face a tradeoff between model size and accuracy. But a new language model released by NVIDIA delivers the best of both, providing state-of-the-art accuracy in a compact form factor.</span><br><span class="line"></span><br><span class="line">Mistral-NeMo-Minitron 8B — a miniaturized version of the open Mistral NeMo 12B model released by Mistral AI and NVIDIA last month — is small enough to run on an NVIDIA RTX-powered workstation while still excelling across multiple benchmarks for AI-powered chatbots, virtual assistants, content generators and educational tools. Minitron models are distilled by NVIDIA using NVIDIA NeMo, an end-to-end platform for developing custom generative AI.</span><br><span class="line"></span><br><span class="line">“We combined two different AI optimization methods — pruning to shrink Mistral NeMo’s 12 billion parameters into 8 billion, and distillation to improve accuracy,” said Bryan Catanzaro, vice president of applied deep learning research at NVIDIA. “By doing so, Mistral-NeMo-Minitron 8B delivers comparable accuracy to the original model at lower computational cost.”</span><br><span class="line"></span><br><span class="line">Unlike their larger counterparts, small language models can run in real time on workstations and laptops. This makes it easier for organizations with limited resources to deploy generative AI capabilities across their infrastructure while optimizing for cost, operational efficiency and energy use. Running language models locally on edge devices also delivers security benefits, since data doesn’t need to be passed to a server from an edge device.</span><br><span class="line"></span><br><span class="line">Developers can get started with Mistral-NeMo-Minitron 8B packaged as an NVIDIA NIM microservice with a standard application programming interface (API) — or they can download the model from Hugging Face. A downloadable NVIDIA NIM, which can be deployed on any GPU-accelerated system in minutes, will be available soon.</span><br><span class="line"></span><br><span class="line">State-of-the-Art for 8 Billion Parameters</span><br><span class="line">For a model of its size, Mistral-NeMo-Minitron 8B leads on nine popular benchmarks for language models. These benchmarks cover a variety of tasks including language understanding, common sense reasoning, mathematical reasoning, summarization, coding and ability to generate truthful answers.</span><br><span class="line"></span><br><span class="line">Packaged as an NVIDIA NIM microservice, the model is optimized for low latency, which means faster responses for users, and high throughput, which corresponds to higher computational efficiency in production.</span><br><span class="line"></span><br><span class="line">In some cases, developers may want an even smaller version of the model to run on a smartphone or an embedded device like a robot. To do so, they can download the 8-billion-parameter model and, using NVIDIA AI Foundry, prune and distill it into a smaller, optimized neural network customized for enterprise-specific applications.</span><br><span class="line"></span><br><span class="line">The AI Foundry platform and service offers developers a full-stack solution for creating a customized foundation model packaged as a NIM microservice. It includes popular foundation models, the NVIDIA NeMo platform and dedicated capacity on NVIDIA DGX Cloud. Developers using NVIDIA AI Foundry can also access NVIDIA AI Enterprise, a software platform that provides security, stability and support for production deployments.</span><br><span class="line"></span><br><span class="line">Since the original Mistral-NeMo-Minitron 8B model starts with a baseline of state-of-the-art accuracy, versions downsized using AI Foundry would still offer users high accuracy with a fraction of the training data and compute infrastructure.</span><br><span class="line"></span><br><span class="line">Harnessing the Perks of Pruning and Distillation</span><br><span class="line">To achieve high accuracy with a smaller model, the team used a process that combines pruning and distillation. Pruning downsizes a neural network by removing model weights that contribute the least to accuracy. During distillation, the team retrained this pruned model on a small dataset to significantly boost accuracy, which had decreased through the pruning process.</span><br><span class="line"></span><br><span class="line">The end result is a smaller, more efficient model with the predictive accuracy of its larger counterpart.</span><br><span class="line"></span><br><span class="line">This technique means that a fraction of the original dataset is required to train each additional model within a family of related models, saving up to 40x the compute cost when pruning and distilling a larger model compared to training a smaller model from scratch.</span><br><span class="line"></span><br><span class="line">Read the NVIDIA Technical Blog and a technical report for details.</span><br><span class="line"></span><br><span class="line">NVIDIA also announced this week Nemotron-Mini-4B-Instruct, another small language model optimized for low memory usage and faster response times on NVIDIA GeForce RTX AI PCs and laptops. The model is available as an NVIDIA NIM microservice for cloud and on-device deployment and is part of NVIDIA ACE, a suite of digital human technologies that provide speech, intelligence and animation powered by generative AI.</span><br><span class="line"></span><br><span class="line">Experience both models as NIM microservices from a browser or an API at ai.nvidia.com.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Nvidia just dropped Mistral NeMo Minitron 8B - Distilled + pruned from 12B, commercially permissive license, and beats the teacher (12B) on multiple benchmarks!</span><br><span class="line">&gt; Achieves similar benchmarks as Mistral NeMo 12B, beats Llama 3.1 8B</span><br><span class="line">&gt; MMLU - L3.1 8B (65), NeMo Minitron 8B (69.5), NeMo 12B (69)</span><br><span class="line">&gt; HumanEval - L3.1 8B (24.75), NeMo Minitron 8B (36.2), NeMo 12B (23.7)</span><br><span class="line">&gt; Trained on 380B tokens</span><br><span class="line">&gt; Iterative pruning and distillation</span><br><span class="line">&gt; Width-only pruning - pruned both embedding + MLP hidden representations</span><br><span class="line">&gt; Pruned MLP intermediate dimension from 14336 to 11520 and hidden size from 5120 to 4096</span><br><span class="line">&gt; Retain the attention head count and number of layers</span><br><span class="line">&gt; Works out of the box with Transformers 🤗</span><br><span class="line">This is a solid base model for further fine-tuning, and task specific use-cases, pretty much the strongest &lt;10B range - which makes it easy to deploy across variety of cheaper GPUs.</span><br><span class="line">8B =&gt; bf16 (16 GB), fp8/ 8 bit (8 GB), 4 bit (4GB)</span><br><span class="line">Perfect to deploy on a L4 :D</span><br><span class="line">Kudos to Nvidia for releasing their research and the model weights! 🤗</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251</span><br><span class="line">𝗔𝗜𝟮𝟭 𝗿𝗲𝗹𝗲𝗮𝘀𝗲𝘀 𝗻𝗲𝘄 𝗝𝗮𝗺𝗯𝗮 𝟭.𝟱 𝗺𝗼𝗱𝗲𝗹𝘀: 𝗡𝗲𝘄 𝘀𝘁𝗮𝗻𝗱𝗮𝗿𝗱 𝗳𝗼𝗿 𝗹𝗼𝗻𝗴-𝗰𝗼𝗻𝘁𝗲𝘅𝘁 𝘂𝘀𝗲-𝗰𝗮𝘀𝗲𝘀!🏅</span><br><span class="line">AI21 Labs used a different architecture to beat the status-quo Transformers models: Jamba architecture combines classic Transformers layers with the new Mamba layers, for which the complexity is a linear (instead of quadratic) function of the context length.</span><br><span class="line">What does this imply?</span><br><span class="line">The Jamba 1.5 Open Model Family: The Most Powerful and Efficient Long Context Models</span><br><span class="line">August 22, 2024</span><br><span class="line">Blog posts</span><br><span class="line">The new family of open models from AI21, offering unrivaled speed, efficiency, and quality and the longest context window among open models.</span><br><span class="line"></span><br><span class="line">Today, we are debuting the Jamba 1.5 family of open models: Jamba 1.5 Mini and Jamba 1.5 Large. Built on our novel SSM-Transformer architecture, these models demonstrate superior long context handling, speed, and quality—outranking competitors in their size class and marking the first time a non-Transformer model has been successfully scaled to the quality and strength of the market’s leading models.</span><br><span class="line"></span><br><span class="line">We are releasing these models under the Jamba Open Model License, upholding our commitment to democratizing access to quality models and opening the door to further experimentation.</span><br><span class="line"></span><br><span class="line">Today’s language models are impressive in their capabilities—but too often fail to deliver real value for businesses.</span><br><span class="line"></span><br><span class="line">At AI21, we are on a mission to change this by designing AI systems that are purpose-built for the enterprise. These models are built keeping in mind the key measures large businesses care most about when it comes to GenAI implementation: resource efficiency, quality, speed, and ability to actually solve critical tasks.</span><br><span class="line">‍</span><br><span class="line"></span><br><span class="line">Long context handling: With a 256K effective context window, the longest in the market, Jamba 1.5 models can improve the quality of key enterprise applications, such as lengthy document summarization and analysis, as well as agentic and RAG workflows</span><br><span class="line">Speed: Up to 2.5X faster on long contexts and fastest across all context lengths in their size class</span><br><span class="line">Quality: Jamba 1.5 Mini is the strongest open model in its size class with a score of 46.1 on the Arena Hard benchmark, surpassing larger models like Mixtral 8x22B and Command-R+. Jamba 1.5 Large, with a score of 65.4, outpaces both Llama 3.1 70B and 405B</span><br><span class="line">Multilingual: In addition to English, the models support Spanish, French, Portuguese, Italian, Dutch, German, Arabic and Hebrew</span><br><span class="line">Developer ready: Jamba natively supports structured JSON output, function calling, digesting document objects, and generating citations</span><br><span class="line">Open for builders: Both models are available for immediate download on Hugging Face (and coming soon to leading frameworks LangChain and LlamaIndex)</span><br><span class="line">Deploy anywhere: In addition to AI21 Studio, the models are available on cloud partners Google Cloud Vertex AI, Microsoft Azure, and NVIDIA NIM and coming soon to Amazon Bedrock, Databricks Marketplace, Snowflake Cortex, Together.AI as well as for private on-prem and VPC deployment</span><br><span class="line">Resource-efficient hybrid architecture</span><br><span class="line">Jamba 1.5 Large and Mini are built on the novel SSM-Transformer Jamba architecture, which weaves together Transformer’s outstanding quality with Mamba’s groundbreaking efficiency.</span><br><span class="line"></span><br><span class="line">As a result, the models offer a lower memory footprint than competitors, allowing clients to handle context lengths up to 140K tokens on a single GPU using Jamba 1.5 Mini. The same advantage also makes fine-tuning over long contexts easier and more accessible than with transformer-based models. Thanks to this efficiency-optimized architecture, our models can deliver top quality and speed without skyrocketing costs.</span><br><span class="line"></span><br><span class="line">Like all models in its size class, Jamba 1.5 Large can’t be loaded in full (FP32) or half (FP16/BF16) precision on a single node of 8 GPUs. Dissatisfied with currently available quantization techniques, we developed ExpertsInt8, a novel quantization technique tailored for MoE models.</span><br><span class="line"></span><br><span class="line">With ExpertsInt8, we only quantize weights that are parts of the MoE (or MLP) layers, which for many MoE models account for over 85% of the model weights. In our implementation, we quantize and save these weights in INT8, an 8-bit precision format, and dequantize them at runtime directly inside the MoE GPU kernel.</span><br><span class="line"></span><br><span class="line">This technique offers four advantages: It is fast, with quantization taking up to just a few minutes; it does not rely on calibration, a sometimes unstable process which ordinarily can take hours or days; it can still use BF16 to hold large activations; and, importantly, it allows Jamba 1.5 Large to fit on a single 8 GPU node, while utilizing its full context length of 256K. In our experiments, ExpertsInt8 proved to have the lowest latency of all vLLM quantization techniques for MoE models, without a loss in quality.</span><br><span class="line"></span><br><span class="line">‍</span><br><span class="line"></span><br><span class="line">Long context that actually delivers</span><br><span class="line">The 256K context window offered by the Jamba 1.5 models is not only the longest amongst open models, but also the only one to back this claim on the RULER benchmark.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Whereas most other models claim a long context window but fail to sustain the same quality of performance at the upper limits of their context window, the Jamba 1.5 family maintains its long context handling throughout the entire span of its 256K context window.</span><br><span class="line"></span><br><span class="line">A model that can effectively handle long context is crucial for almost every enterprise scale GenAI application. In addition to thoroughly and precisely summarizing and analyzing lengthy documents, a long context model substantially improves the quality of RAG and agentic workflows—and reduces their cost—by eliminating the need for continuous chunking and repetitive retrievals.</span><br><span class="line"></span><br><span class="line">While it’s sometimes claimed that RAG is a substitute for long context, a successful enterprise AI system needs both. In pairing long context and RAG, the long context model improves the quality and cost-efficiency of RAG’s retrieval stage at scale.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Fastest on the market</span><br><span class="line">For the use cases enterprises are interested in, such as customer support agent assistants and chatbots, rapid turnaround is essential. The model needs to be able to keep pace with the scale of operations, even as usage requests and batch sizes increase.</span><br><span class="line"></span><br><span class="line">Both Jamba 1.5 models are faster than competitors of a similar size, with up to 2.5X faster inference on long contexts, offering customers major cost, quality, and speed gains under high utilization when deployed in their own environment.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The Jamba 1.5 Mini comparisons were done over 2xA100 80GB GPUs and the Jamba 1.5 Large comparisons were done over 8xA100 80GB GPUs. The test was performed on vLLM, with batch_size=1, output_tokens=512, input_tokens=(context_length-512)</span><br><span class="line">Jamba 1.5 Mini and Jamba 1.5 Large show excellent speed and throughput results in tests run by Artificial Analysis, as can be seen in the chart below, with Jamba 1.5 Mini ranking as the fastest model on 10K contexts.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Output tokens per second on 10K contexts, as independently tested by Artificial Analysis.</span><br><span class="line">Outstanding quality across the board</span><br><span class="line">As measured on the Arena Hard benchmark, Jamba 1.5 Mini emerges as the strongest model in its size class, outshining competitors Claude 3 Haiku, Mixtral 8x22B and Command-R+. Jamba 1.5 Large similarly rises above leading models like Claude 3 Opus, Llama 3.1 70B, and Llama 3.1 405B, offering excellent value per cost for its size class.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Getting started</span><br><span class="line">Build with Jamba 1.5 Mini or Jamba 1.5 Large wherever you like to work. The models are available on the following platforms and cloud partners:</span><br><span class="line">‍</span><br><span class="line"></span><br><span class="line">AI21 Studio</span><br><span class="line">Google Cloud Vertex AI</span><br><span class="line">Hugging Face</span><br><span class="line">Microsoft Azure</span><br><span class="line">NVIDIA NIM</span><br><span class="line">And coming soon to Amazon Bedrock, Databricks Marketplace, LangChain, LlamaIndex, Snowflake Cortex, and Together.AI.</span><br><span class="line"></span><br><span class="line">For customers who wish to avoid a lengthy experimentation process and keep their data onsite, we offer private deployments and custom models. In this white-glove service, we tailor our models exactly to your needs and use case through continuous pre-training and fine-tuning so you can move more quickly from ideation to production.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://jina.ai/news/late-chunking-in-long-context-embedding-models/</span><br><span class="line">August 23, 2024</span><br><span class="line">Late Chunking in Long-Context Embedding Models</span><br><span class="line">Chunking long documents while preserving contextual information is challenging. We introduce the &quot;Late Chunking&quot; that leverages long-context embedding models to generate contextual chunk embeddings for better retrieval applications.</span><br><span class="line">Diagram illustrating the &#x27;Late Chunking&#x27; and &#x27;Long Document Model&#x27; processes in machine learning on a black background.</span><br><span class="line">Michael Günther</span><br><span class="line">Han Xiao</span><br><span class="line">Michael Günther, Han Xiao • 8 minutes read</span><br><span class="line">About a year ago, in October 2023, we released the world&#x27;s first open-source embedding model with an 8K context length, jina-embeddings-v2-base-en. Since then, there has been quite some debate about the usefulness of long-context in embedding models. For many applications, encoding a document thousands of words long into a single embedding representation is not ideal. Many use cases require retrieving smaller portions of the text, and dense vector-based retrieval systems often perform better with smaller text segments, as the semantics are less likely to be &quot;over-compressed&quot; in the embedding vectors.</span><br><span class="line"></span><br><span class="line">Retrieval-Augmented Generation (RAG) is one of the most well-known applications that requires splitting documents into smaller text chunks (say within 512 tokens). These chunks are usually stored in a vector database, with vector representations generated by a text embedding model. During runtime, the same embedding model encodes a query into a vector representation, which is then used to identify relevant stored text chunks. These chunks are subsequently passed to a large language model (LLM), which synthesizes a response to the query based on the retrieved texts.</span><br><span class="line"></span><br><span class="line">Flowchart detailing a query processing system, starting from &quot;Query&quot; to &quot;Document Chunks&quot; and &quot;Embedding Model,&quot; then to &quot;Vec</span><br><span class="line">A typical RAG pipeline of chunking-embedding-retrieving-generating.</span><br><span class="line">In short, embedding smaller chunks seems to be more preferable, partly due to the limited input sizes of downstream LLMs, but also because there’s a concern that important contextual information in a long context may get diluted when compressed into a single vector.</span><br><span class="line"></span><br><span class="line">But if the industry only ever needs embedding models with a 512-context length, what’s the point of training models with an 8192-context length at all?</span><br><span class="line"></span><br><span class="line">In this article, we revisit this important, albeit uncomfortable, question by exploring the limitations of the naive chunking-embedding pipeline in RAG. We introduce a new approach called &quot;Late Chunking,&quot; which leverages the rich contextual information provided by 8192-length embedding models to more effectively embed chunks.</span><br><span class="line"></span><br><span class="line">The Lost Context Problem</span><br><span class="line">The simple RAG pipeline of chunking-embedding-retrieving-generating is not without its challenges. Specifically, this process can destroy long-distance contextual dependencies. In other words, when relevant information is spread across multiple chunks, taking text segments out of context can render them ineffective, making this approach particularly problematic.</span><br><span class="line"></span><br><span class="line">In the image below, a Wikipedia article is split into chunks of sentences. You can see that phrases like &quot;its&quot; and &quot;the city&quot; reference &quot;Berlin,&quot; which is mentioned only in the first sentence. This makes it harder for the embedding model to link these references to the correct entity, thereby producing a lower-quality vector representation.</span><br><span class="line"></span><br><span class="line">Comparative panels display Berlin&#x27;s Wikipedia article and its chunked text to highlight clarity and readability benefits.</span><br><span class="line">This means, if we split a long article into sentence-length chunks, as in the example above, a RAG system might struggle to answer a query like &quot;What is the population of Berlin?&quot; Because the city name and the population never appear together in a single chunk, and without any larger document context, an LLM presented with one of these chunks cannot resolve anaphoric references like &quot;it&quot; or &quot;the city.&quot;</span><br><span class="line"></span><br><span class="line">There are some heuristics to alleviate this issue, such as resampling with a sliding window, using multiple context window lengths, and performing multi-pass document scans. However, like all heuristics, these approaches are hit-or-miss; they may work in some cases, but there&#x27;s no theoretical guarantee of their effectiveness.</span><br><span class="line"></span><br><span class="line">The Solution: Late Chunking</span><br><span class="line">The naive encoding approach (as seen on the left side of the image below) involves using sentences, paragraphs, or maximum length limits to split the text a priori. Afterward, an embedding model is repetitively applied to these resulting chunks. To generate a single embedding for each chunk, many embedding models use mean pooling on these token-level embeddings to output a single embedding vector.</span><br><span class="line"></span><br><span class="line">Flowchart comparing naive and late chunking methods in document processing with labeled steps and embeddings.</span><br><span class="line">An illustration of the naive chunking strategy (left) and the late chunking strategy (right).</span><br><span class="line">In contrast, the &quot;Late Chunking&quot; approach we propose in this article first applies the transformer layer of the embedding model to the entire text or as much of it as possible. This generates a sequence of vector representations for each token that encompasses textual information from the entire text. Subsequently, mean pooling is applied to each chunk of this sequence of token vectors, yielding embeddings for each chunk that consider the entire text&#x27;s context. Unlike the naive encoding approach, which generates independent and identically distributed (i.i.d.) chunk embeddings, late chunking creates a set of chunk embeddings where each one is &quot;conditioned on&quot; the previous ones, thereby encoding more contextual information for each chunk.</span><br><span class="line"></span><br><span class="line">Obviously to effectively apply late chunking, we need long-context embedding models like jina-embeddings-v2-base-en, which support up to 8192 tokens—roughly ten standard pages of text. Text segments of this size are much less likely to have contextual dependencies that require an even longer context to resolve.</span><br><span class="line"></span><br><span class="line">It&#x27;s important to highlight that late chunking still requires boundary cues, but these cues are used only after obtaining the token-level embeddings—hence the term &quot;late&quot; in its naming.</span><br><span class="line"></span><br><span class="line">Naive Chunking	Late Chunking</span><br><span class="line">The need of boundary cues	Yes	Yes</span><br><span class="line">The use of boundary cues	Directly in preprocessing	After getting the token-level embeddings from the transformer layer</span><br><span class="line">The resulting chunk embeddings	i.i.d.	Conditional</span><br><span class="line">Contextual information of nearby chunks	Lost. Some heuristics (like overlap sampling) to alleviate this	Well-preserved by long-context embedding models</span><br><span class="line">Implementation and Qualitative Evaluation</span><br><span class="line">Google Colab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The implementation of late chunking can be found in the Google Colab linked above. Here, we utilize our recent feature release in the Tokenizer API, which leverages all possible boundary cues to segment a long document into meaningful chunks. More discussion on the algorithm behind this feature can be found on X.</span><br><span class="line"></span><br><span class="line">Tokenizer API</span><br><span class="line">Free API to tokenize text and segment long text into chunks.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">When applying late chunking to the Wikipedia example above, you can immediately see an improvement in semantic similarity. For instance, in the case of &quot;the city&quot; and &quot;Berlin&quot; within a Wikipedia article, the vectors representing &quot;the city&quot; now contain information linking it to the previous mention of &quot;Berlin,&quot; making it a much better match for queries involving that city name.</span><br><span class="line"></span><br><span class="line">Query	Chunk	Sim. on naive chunking	Sim. on late chunking</span><br><span class="line">Berlin	Berlin is the capital and largest city of Germany, both by area and by population.	0.849	0.850</span><br><span class="line">Berlin	Its more than 3.85 million inhabitants make it the European Union&#x27;s most populous city, as measured by population within city limits.	0.708	0.825</span><br><span class="line">Berlin	The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.	0.753	0.850</span><br><span class="line">You can observe this in the numerical results above, which compare the embedding of the term &quot;Berlin&quot; to various sentences from the article about Berlin using cosine similarity. The column &quot;Sim. on IID chunk embeddings&quot; shows the similarity values between the query embedding of &quot;Berlin&quot; and the embeddings using a priori chunking, while &quot;Sim. under contextual chunk embedding&quot; represents the results with late chunking method.</span><br><span class="line"></span><br><span class="line">Quantitative Evaluation on BEIR</span><br><span class="line">To verify the effectiveness of late chunking beyond a toy example, we tested it using some of the retrieval benchmarks from BeIR. These retrieval tasks consist of a query set, a corpus of text documents, and a QRels file that stores information about the IDs of documents relevant to each query.</span><br><span class="line"></span><br><span class="line">To identify the relevant documents for a query, the documents are chunked, encoded into an embedding index, and the most similar chunks are determined for each query embedding using k-nearest neighbors (kNN). Since each chunk corresponds to a document, the kNN ranking of chunks can be converted into a kNN ranking of documents (retaining only the first occurrence for documents appearing multiple times in the ranking). This resulting ranking is then compared to the ranking provided by the ground-truth QRels file, and retrieval metrics like nDCG@10 are calculated. This procedure is depicted below, and the evaluation script can be found in this repository for reproducibility.</span><br><span class="line"></span><br><span class="line">GitHub - jina-ai/late-chunking: Code for explaining and evaluating late chunking (chunked pooling)</span><br><span class="line">Code for explaining and evaluating late chunking (chunked pooling) - jina-ai/late-chunking</span><br><span class="line"></span><br><span class="line">GitHub</span><br><span class="line">jina-ai</span><br><span class="line"></span><br><span class="line">We ran this evaluation on various BeIR datasets, comparing naive chunking with our late chunking method. For getting the boundary cues, we used a regex that splits the texts into strings of roughly 256 tokens. Both the naive and late chunking evaluation used jina-embeddings-v2-small-en as the embedding model; a smaller version of the v2-base-en model that still supports up to 8192-token length. Results can be found in the table below.</span><br><span class="line"></span><br><span class="line">Dataset	Avg. Document Length (characters)	Naive Chunking (nDCG@10)	Late Chunking (nDCG@10)	No Chunking (nDCG@10)</span><br><span class="line">SciFact	1498.4	64.20%	66.10%	63.89%</span><br><span class="line">TRECCOVID	1116.7	63.36%	64.70%	65.18%</span><br><span class="line">FiQA2018	767.2	33.25%	33.84%	33.43%</span><br><span class="line">NFCorpus	1589.8	23.46%	29.98%	30.40%</span><br><span class="line">Quora	62.2	87.19%	87.19%	87.19%</span><br><span class="line">In all cases, late chunking improved the scores compared to the naive approach. In some instances, it also outperformed encoding the entire document into a single embedding, while in other datasets, not chunking at all yielded the best results (Of course, no chunking only makes sense if there is no need to rank chunks, which is rare in practice). If we plot the performance gap between the naive approach and late chunking against document length, it becomes evident that the average length of the documents correlates with greater improvements in nDCG scores through late chunking. In other words, the longer the document, the more effective the late chunking strategy becomes.</span><br><span class="line"></span><br><span class="line">Line graph showing the decline in relative improvement with increasing document length, from 0 to 1500 characters.</span><br><span class="line">Late chunking&#x27;s improvement over naive chunking is correlated with the avg. document length.</span><br><span class="line">Conclusion</span><br><span class="line">In this article, we introduced a simple approach called &quot;late chunking&quot; to embed short chunks by leveraging the power of long-context embedding models. We demonstrated how traditional i.i.d. chunk embedding fails to preserve contextual information, leading to suboptimal retrieval; and how late chunking offers a simple yet highly effective solution to maintain and condition contextual information within each chunk. The effectiveness of late chunking becomes increasingly significant on longer documents—a capability made possible only by advanced long-context embedding models like jina-embeddings-v2-base-en. We hope this work not only validates the importance of long-context embedding models but also inspires further research on this topic.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/OpenMeditron</span><br><span class="line"></span><br><span class="line">🐝⛑️It’s here…meet #Meditron3</span><br><span class="line">…and an opportunity to join the upcoming publication as a clinical expert by critically evaluating Meditron 🤖🤺 in the #MOOVE: a Massive Open Online Validation and Evaluation platform)</span><br><span class="line">Meditron is the latest update to our suite of state-of-the-art open Large Medical Language Models trained from #Llama3point1 [8B] and [70B]</span><br><span class="line">---</span><br><span class="line">💯#OPEN: weights released here 🔗</span><br><span class="line">https://lnkd.in/eF8q9A94</span><br><span class="line"></span><br><span class="line">🤝#CODESIGNED: from day one with an interdisciplinary team of practicing clinicians from around the world, global humanitarian organizations, AI ethicists, and data scientists</span><br><span class="line">📊#STATEOFTHEART: Out-performing GPT4 on the standard medical benchmarks (*see footnote)</span><br><span class="line">🌍#REPRESENTATIVE: Continually pre-trained on an enormous corpus of expert-curated open-access medical literature and global clinical practice guidelines that is specifically inclusive of low-resource and humanitarian settings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2408.12569</span><br><span class="line">Meta presents Sapiens</span><br><span class="line">Foundation for Human Vision Models</span><br><span class="line"></span><br><span class="line">We present Sapiens, a family of models for four fundamental human-centric vision tasks - 2D pose estimation, body-part segmentation, depth estimation, and surface normal prediction. Our models natively support 1K high-resolution inference and are extremely easy to adapt for individual tasks by simply fine-tuning models pretrained on over 300 million in-the-wild human images. We observe that, given the same computational budget, self-supervised pretraining on a curated dataset of human images significantly boosts the performance for a diverse set of human-centric tasks. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. Our simple model design also brings scalability - model performance across tasks improves as we scale the number of parameters from 0.3 to 2 billion. Sapiens consistently surpasses existing baselines across various human-centric benchmarks. We achieve significant improvements over the prior state-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1 mIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5% relative angular error.</span><br><span class="line">Sapiens: Foundation for Human Vision Models</span><br><span class="line">Published on Aug 23</span><br><span class="line">·</span><br><span class="line">Submitted by</span><br><span class="line">akhaliq</span><br><span class="line">on Aug 23</span><br><span class="line">#1 Paper of the day</span><br><span class="line">Authors:</span><br><span class="line">Rawal Khirodkar</span><br><span class="line">,</span><br><span class="line">Timur Bagautdinov</span><br><span class="line">,</span><br><span class="line">Julieta Martinez</span><br><span class="line">,</span><br><span class="line">Su Zhaoen</span><br><span class="line">,</span><br><span class="line">Austin James</span><br><span class="line">,</span><br><span class="line">Peter Selednik</span><br><span class="line">,</span><br><span class="line">Stuart Anderson</span><br><span class="line">,</span><br><span class="line">Shunsuke Saito</span><br><span class="line">Abstract</span><br><span class="line">We present Sapiens, a family of models for four fundamental human-centric vision tasks - 2D pose estimation, body-part segmentation, depth estimation, and surface normal prediction. Our models natively support 1K high-resolution inference and are extremely easy to adapt for individual tasks by simply fine-tuning models pretrained on over 300 million in-the-wild human images. We observe that, given the same computational budget, self-supervised pretraining on a curated dataset of human images significantly boosts the performance for a diverse set of human-centric tasks. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. Our simple model design also brings scalability - model performance across tasks improves as we scale the number of parameters from 0.3 to 2 billion. Sapiens consistently surpasses existing baselines across various human-centric benchmarks. We achieve significant improvements over the prior state-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1 mIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5% relative angular error.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/linkedin/Liger-Kernel</span><br><span class="line">Big Update! 20% higher throughput and 60% memory reduction for multi-GPU fine-tuning with Hugging Face  Transformers! 🤯 The LLM research team from LinkedIn released new efficient GPU Kernels (Liger Kernels) to speed up and reduce memory when fine-tuning LLMs! 🚀</span><br><span class="line"></span><br><span class="line">TL;DR:</span><br><span class="line"></span><br><span class="line">🚀 Boost multi-GPU training throughput by 20% and slash memory usage by 60%</span><br><span class="line"></span><br><span class="line">💻 Works with Flash Attention, PyTorch FSDP, and Microsoft DeepSpeed</span><br><span class="line"></span><br><span class="line">🧠 Supported models include Llama 3, Mistral, Mixtral, Gemma 2</span><br><span class="line"></span><br><span class="line">🔧 Adds Hugging Face Compatible RMSNorm, RoPE, SwiGLU, CrossEntropy, and FusedLinearCrossEntropy</span><br><span class="line"></span><br><span class="line">🧮 Exact computations with no approximations, longer context lengths, larger batch sizes</span><br><span class="line"></span><br><span class="line">⚡️ Train Meta Llama 3 8b ~20% faster with over 40% memory using 4 A100s with FSDP</span><br><span class="line"></span><br><span class="line">🔌 Get started with pip install liger-kernel and add 1-line of code</span><br><span class="line"></span><br><span class="line">🌟 Open-source and community-driven development</span><br><span class="line"></span><br><span class="line">Github: https://lnkd.in/e4XKs4-F</span><br><span class="line">LinkedIn</span><br><span class="line">This link will take you to a page that’s not on LinkedIn</span><br><span class="line"> [2024/8/31] CUDA MODE talk, Liger-Kernel: Real-world Triton kernel for LLM Training</span><br><span class="line">[2024/8/23] Official release: check out our X post</span><br><span class="line">Liger (Linkedin GPU Efficient Runtime) Kernel is a collection of Triton kernels designed specifically for LLM training. It can effectively increase multi-GPU training throughput by 20% and reduces memory usage by 60%. We have implemented Hugging Face Compatible RMSNorm, RoPE, SwiGLU, CrossEntropy, FusedLinearCrossEntropy, and more to come. The kernel works out of the box with Flash Attention, PyTorch FSDP, and Microsoft DeepSpeed. We welcome contributions from the community to gather the best kernels for LLM training.</span><br><span class="line"></span><br><span class="line">Supercharge Your Model with Liger Kernel</span><br><span class="line">Banner</span><br><span class="line"></span><br><span class="line">With one line of code, Liger Kernel can increase throughput by more than 20% and reduce memory usage by 60%, thereby enabling longer context lengths, larger batch sizes, and massive vocabularies.</span><br><span class="line"></span><br><span class="line">Speed Up	Memory Reduction</span><br><span class="line">Speed up	Memory</span><br><span class="line">Note:</span><br><span class="line"></span><br><span class="line">Benchmark conditions: LLaMA 3-8B, Batch Size = 8, Data Type = bf16, Optimizer = AdamW, Gradient Checkpointing = True, Distributed Strategy = FSDP1 on 8 A100s.</span><br><span class="line">Hugging Face models start to OOM at a 4K context length, whereas Hugging Face + Liger Kernel scales up to 16K.</span><br><span class="line">Examples</span><br><span class="line">Basic</span><br><span class="line">Example	Description	Lightning Studio</span><br><span class="line">Hugging Face Trainer	Train LLaMA 3-8B ~20% faster with over 40% memory reduction on Alpaca dataset using 4 A100s with FSDP	TBA</span><br><span class="line">Lightning Trainer	Increase 15% throughput and reduce memory usage by 40% with LLaMA3-8B on MMLU dataset using 8 A100s with DeepSpeed ZeRO3	TBA</span><br><span class="line">Advanced</span><br><span class="line">Example	Description	Lightning Studio</span><br><span class="line">Medusa Multi-head LLM (Retraining Phase)	Reduce memory usage by 80% with 5 LM heads and improve throughput by 40% using 8 A100s with FSDP	TBA</span><br><span class="line">Key Features</span><br><span class="line">Ease of use: Simply patch your Hugging Face model with one line of code, or compose your own model using our Liger Kernel modules.</span><br><span class="line">Time and memory efficient: In the same spirit as Flash-Attn, but for layers like RMSNorm, RoPE, SwiGLU, and CrossEntropy! Increases multi-GPU training throughput by 20% and reduces memory usage by 60% with kernel fusion, in-place replacement, and chunking techniques.</span><br><span class="line">Exact: Computation is exact—no approximations! Both forward and backward passes are implemented with rigorous unit tests and undergo convergence testing against training runs without Liger Kernel to ensure accuracy.</span><br><span class="line">Lightweight: Liger Kernel has minimal dependencies, requiring only Torch and Triton—no extra libraries needed! Say goodbye to dependency headaches!</span><br><span class="line">Multi-GPU supported: Compatible with multi-GPU setups (PyTorch FSDP, DeepSpeed, DDP, etc.).</span><br><span class="line">Target Audiences</span><br><span class="line">Researchers: Looking to compose models using efficient and reliable kernels for frontier experiments.</span><br><span class="line">ML Practitioners: Focused on maximizing GPU training efficiency with optimal, high-performance kernels.</span><br><span class="line">Curious Novices: Eager to learn how to write reliable Triton kernels to enhance training efficiency.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://neuralmagic.com/blog/llm-compressor-is-here-faster-inference-with-vllm/</span><br><span class="line">📢 Introducing LLM Compressor: SOTA open-sourced framework for compressing LLMs (including Llamas)! 📢</span><br><span class="line">Starting as an internal research project at Neural Magic, I could never imagine that nearly six years later, what I called neuralmagicML would evolve into the incredibly powerful and capable LLM Compressor framework. Thanks to the hard work of our incredible engineering team, this tool can compress LLMs of any size to remarkable levels while recovering full accuracy. (such as our recent Llama 3.1 405B results!)</span><br><span class="line">We are thrilled to announce that we have donated this library and its innovative techniques to the vLLM community. By offering efficient, performant, and accurate solutions for large language models, we aim to empower researchers, hackers, and enterprises.</span><br><span class="line">Why This Matters:</span><br><span class="line">- Cutting-Edge Algorithms: Implement the latest techniques and best practices for top-tier model performance without extensive research.</span><br><span class="line">- Superior Flexibility: Enjoy various compression techniques, quantization schemes, and sparsity options for a solution that fits your use cases.</span><br><span class="line">- Community-Driven: This tool is open-sourced and seamlessly integrated with vLLM and Hugging Face to ensure compatibility and encourage future contributions.</span><br><span class="line">We are excited to witness how the community will leverage this incredible tool! Dive deeper into LLM Compressor by exploring our blog and the repo:</span><br><span class="line"></span><br><span class="line">Aug 14, 2024</span><br><span class="line"></span><br><span class="line">Announcing LLM Compressor</span><br><span class="line">We are excited to announce LLM Compressor, a unified library for creating compressed models for faster inference with vLLM. Neural Magic&#x27;s research team has successfully utilized it to create our latest compressed models, including fully quantized and accurate versions of Llama 3.1, and with that, we are excited to open up the toolkit to the community with its first 0.1 release for general usage to compress your models!</span><br><span class="line"></span><br><span class="line">LLM Compressor architecture diagram.</span><br><span class="line">In recent months, the high-performance computing team at Neural Magic has brought performant inference for various quantization schemes to vLLM, including custom Marlin kernels for weight-only quantization and custom CUTLASS kernels for INT8 and FP8 activation quantization.</span><br><span class="line"></span><br><span class="line">However, before today, creating quantized checkpoints required navigating a fragmented ecosystem of bespoke compression libraries such as AutoGPTQ, AutoAWQ, AutoFP8, etc. We built LLM Compressor from the ground up as a single library for applying the latest compression best practices, including GPTQ, SmoothQuant, SparseGPT, and RTN, with many more actively being added. It works natively with Hugging Face models for seamless ease of use in the open-source ecosystem, and vLLM supports directly loading checkpoints from LLM Compressor for accelerated inference.</span><br><span class="line"></span><br><span class="line">Using LLM Compressor, you can create compressed, accurate versions of your models, including:</span><br><span class="line"></span><br><span class="line">Activation and weight quantization for up to 3X faster server/throughput deployments. This includes FP8 models using RTN for NVIDIA&#x27;s Ada Lovelace and Hopper GPUs, and INT8 models using SmoothQuant and GPTQ for Nvidia&#x27;s Turing and Ampere GPUs.</span><br><span class="line">Weight quantization for up to 4X faster latency with INT4 weight-only models using GPTQ for Nvidia&#x27;s Ampere GPUs and newer.</span><br><span class="line">Weight pruning for up to 1.5X faster general performance with 2:4, 50% sparse models utilizing SparseGPT for Nvidia&#x27;s Ampere GPUs and newer.</span><br><span class="line">Enabling Activation Quantization in vLLM</span><br><span class="line">Thanks to LLM Compressor&#x27;s flexibility, it enables a critical new feature: activation quantization.</span><br><span class="line"></span><br><span class="line">The open-source compression ecosystem thus far has focused mainly on weight-only quantization, including AutoGPTQ and AutoAWQ. Weight-only quantization enables smaller models and faster latency, but with 16-bit activations, the compute runs through the same 16-bit tensor cores as the unquantized model. This leads to slower inference for compute-heavy workloads due to the penalty of dequantizing the weights. Activation quantization, where the inputs to each layer are quantized, combined with weight quantization, enables utilization of the faster INT8 or FP8 tensor cores for the matrix multiplies, doubling the performance for compute-bound inference.</span><br><span class="line"></span><br><span class="line">Weight-only quantization often fails to deliver speed improvements in production serving deployments. These environments typically result in compute-bound workloads with minimal benefits from weight-only quantization. Activation quantization, however, offers a substantial performance boost in such high-compute scenarios and faster inference at lower queries per second (QPS). The chart below demonstrates a 1.6X speedup at 5 QPS for the INT8 weight and activation quantized model (w8a8) compared to the 16-bit baseline (w16a16), while the 4-bit weight quantized model (w4a16) shows little improvement.</span><br><span class="line"></span><br><span class="line">This chart demonstrates a 1.6X speedup at 5 QPS for the INT8 weight and activation quantized model (w8a8) compared to the 16-bit baseline (w16a16), while the 4-bit weight quantized model (w4a16) shows little improvement.</span><br><span class="line">Full replication instructions for the benchmark are available in the appendix.</span><br><span class="line">Activation Quantization Performance in vLLM</span><br><span class="line">Let’s take an example of a Llama 3.1 70B running in vLLM on a 4xA100 GPU setup to see if this analysis holds up!</span><br><span class="line"></span><br><span class="line">We will compare the serving latency for three variants for Llama 3.1 70B</span><br><span class="line"></span><br><span class="line">Unquantized FP16 (w16a16):</span><br><span class="line">meta-llama/Meta-Llama-3.1-70B-Instruct</span><br><span class="line">Weight and activation quantization to INT8 (w8a8):</span><br><span class="line">neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w8a8</span><br><span class="line">Weight-only quantization to INT4 (w4a16):</span><br><span class="line">neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w4a16</span><br><span class="line">The chart below illustrates the average time to generate each new token (TPOT) across different server loads, measured in queries per second (QPS). Additionally, a deployment constraint of 5 seconds is set for the time to generate the first token (TTFT) to ensure the serving application maintains reasonable initial response times.</span><br><span class="line"></span><br><span class="line">This chart illustrates the average time to generate each new token (TPOT) across different server loads, measured in queries per second (QPS). Additionally, a deployment constraint of 5 seconds is set for the time to generate the first token (TTFT) to ensure the serving application maintains reasonable initial response times.</span><br><span class="line">Full replication instructions for the benchmark are available in the appendix.</span><br><span class="line">At low QPS, weight-only quantization offers improved latency relative to an unquantized model. However, as the server load increases and becomes compute-bound, the performance of the weight-only model levels off, matching the unquantized model. In contrast, the activation quantized model performs better under high load, supporting more queries per second before the system becomes overloaded and TTFT exceeds our limits for a responsive application.</span><br><span class="line"></span><br><span class="line">For a 70B model on an A100 system, we see that the W8A8 model achieves similar latency performance with just 2 GPUs compared to the unquantized model running with 4, meaning similar latency guarantees with half the resources!</span><br><span class="line"></span><br><span class="line">Llama 3.1 70B Time per Output Token coparing w16a16 on 4 GPUs and w8a8 on 2 GPUs.</span><br><span class="line">Full replication instructions for the benchmark are available in the appendix.</span><br><span class="line">Activation Quantization Accuracy</span><br><span class="line">vLLM’s CUTLASS kernels for activation quantization offer flexible support for various schemes, allowing for a high degree of customization, including any combination of:</span><br><span class="line"></span><br><span class="line">Per-tensor or per-channel quantization for weights</span><br><span class="line">Per-tensor or per-token quantization for activations</span><br><span class="line">Symmetric or asymmetric quantized activations (for int8).</span><br><span class="line">Side note: We are doing a CUTLASS deep dive during our bi-weekly vLLM office hours on September 5, 2024. Sign up here.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">This flexibility in vLLM, combined with LLM Compressor&#x27;s advanced algorithms such as GPTQ and SmoothQuant, ensures that model accuracy is maintained even after quantization. As we can see from the model card for neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w8a8, we see a negligible drop using static per-channel weight scales and dynamic per token activation scales in comparison to the FP16 baseline on Open LLM:</span><br><span class="line"></span><br><span class="line">Evaluation scores for Meta Llama 3.1 70B model after optimizations with the LLM Compressor.</span><br><span class="line">This combination of fine-grained quantization and sophisticated algorithms enables users to achieve faster inference without compromising on the precision and reliability of their models.</span><br><span class="line"></span><br><span class="line">Try LLM Compressor</span><br><span class="line">The following snippet is a minimal example of quantizing meta-llama/Meta-Llama-3.1-8B-Instruct with INT8 weights and activations.</span><br><span class="line"></span><br><span class="line">Install LLM Compressor via PyPi</span><br><span class="line">LLM Compressor is available for installation via PyPI:</span><br><span class="line"></span><br><span class="line">pip install llmcompressor</span><br><span class="line">Apply Quantization with the LLM Compressor</span><br><span class="line">Quantization is applied by selecting an algorithm and calling the oneshot API, which applies the selections in a post-training setting.</span><br><span class="line"></span><br><span class="line">In this case, we apply SmoothQuant to make the activations easier to quantize and GPTQ to apply the weight and activation quantization. We apply these algorithms to all linear layers of the network using the built-in open_platypus dataset (note: see the examples for how to use your own calibration set).</span><br><span class="line"></span><br><span class="line">from llmcompressor.modifiers.quantization import GPTQModifier</span><br><span class="line">from llmcompressor.modifiers.smoothquant import SmoothQuantModifier</span><br><span class="line">from llmcompressor.transformers import oneshot</span><br><span class="line"># Select quantization algorithm. In this case, we:</span><br><span class="line">#   * apply SmoothQuant to make the activations easier to quantize</span><br><span class="line">#   * quantize the weights to int8 with GPTQ (static per channel)</span><br><span class="line">#   * quantize the activations to int8 (dynamic per token)</span><br><span class="line">recipe = [</span><br><span class="line">    SmoothQuantModifier(smoothing_strength=0.8),</span><br><span class="line">    GPTQModifier(scheme=&quot;W8A8&quot;, targets=&quot;Linear&quot;, ignore=[&quot;lm_head&quot;]),</span><br><span class="line">]</span><br><span class="line"># Apply quantization using the built in open_platypus dataset.</span><br><span class="line">oneshot(</span><br><span class="line">    model=&quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot;,</span><br><span class="line">    dataset=&quot;open_platypus&quot;,</span><br><span class="line">    recipe=recipe,</span><br><span class="line">    output_dir=&quot;Meta-Llama-3.1-8B-Instruct-INT8&quot;,</span><br><span class="line">    max_seq_length=2048,</span><br><span class="line">    num_calibration_samples=512,</span><br><span class="line">)</span><br><span class="line">Inference Compressed Models with vLLM</span><br><span class="line">The resulting model is ready to be loaded and run in vLLM out-of-the-box:</span><br><span class="line"></span><br><span class="line">from vllm import LLM</span><br><span class="line">model = LLM(&quot;./Meta-Llama-3.1-8B-Instruct-INT8&quot;)</span><br><span class="line">output = model.generate(&quot;My name is&quot;)</span><br><span class="line">print(&quot;Output:&quot;, output[0].outputs[0].text)</span><br><span class="line"># Output: Jakob Schmid.  I live in the Republic of South Moluccas</span><br><span class="line">Under the hood, vLLM understands how to load and run the compressed model by looking at the config.yaml next to the weight files. Check out some of our more detailed examples to try out other quantization flows:</span><br><span class="line"></span><br><span class="line">FP8 activation quantization with PTQ</span><br><span class="line">INT8 activation quantization with GPTQ and SmoothQuant</span><br><span class="line">INT4 weight-only quantization With GPTQ</span><br><span class="line">LLM Compressor Roadmap</span><br><span class="line">We have a robust roadmap planned to expand support for model compression in LLM Compressor. Our roadmap is prioritized across the following initiatives:</span><br><span class="line"></span><br><span class="line">Expand model support: Mixture of Experts (MoE) and vision-language models</span><br><span class="line">Expand algorithm and scheme support: AWQ, additional quantized floating point formats (fp8 and fp4), and KV cache quantization</span><br><span class="line">Support for non-Nvidia hardware: We are actively collaborating with AMD, Google, and Intel teams to support models created by LLM Compressor on non-Nvidia hardware devices.</span><br><span class="line">Tools for creating non-uniform quantization schemes</span><br><span class="line">2:4 sparsity: Sparse foundation models, sparse fine-tuning from sparse foundational models, combining sparsity and quantization</span><br><span class="line">Expand support for training aware methods: Quantization-Aware Training (QAT) and Low-Rank Adaptation (LoRA)</span><br><span class="line">If you have any feature requests, large or small, please comment on our Roadmap Issue in GitHub.</span><br><span class="line"></span><br><span class="line">Final Thoughts</span><br><span class="line">At Neural Magic, we believe the Future of AI is Open, and we are on a mission to bring the power of open-source models and vLLM to every enterprise on the planet.</span><br><span class="line"></span><br><span class="line">We offer nm-vllm, an enterprise distribution of vLLM, with:</span><br><span class="line"></span><br><span class="line">Stable builds with bug fixes and selected model backporting</span><br><span class="line">Enterprise support with SLAs for production deployments of vLLM</span><br><span class="line">Tools and access to our teams for applying model optimizations via LLM Compressor</span><br><span class="line">Pre-optimized model registry</span><br><span class="line">Kubernetes reference architectures for production deployments</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.kbfg.com/kbresearch/report/reportView.do?reportId=2000501</span><br><span class="line">너도나도 AI? 말로만 AI 외치는 &#x27;AI 워싱&#x27; 주의보</span><br><span class="line">2024-08-19</span><br><span class="line">KB금융그룹</span><br><span class="line"></span><br><span class="line">2024. 8</span><br><span class="line">너도나도 AI? 말로만 AI 외치는 ‘AI 워싱’ 주의보</span><br><span class="line"></span><br><span class="line">□ AI 워싱의 주요 특징과 부작용</span><br><span class="line">□ AI 워싱 사례</span><br><span class="line">□ AI 워싱을 피하려면?</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1</span><br><span class="line"> AI 워싱(AI Washing)이란 기업이나 제품이 실제로는 인공지능 기술을 사용하지 않으면서 광범위</span><br><span class="line">하게 활용하고 있는 것처럼 홍보하는 행위로 AI 열풍을 이용하는 마케팅 전략의 일환</span><br><span class="line">• 환경친화적이지 않은 기업이 친환경적인 것처럼 홍보하는 그린워싱(Greenwashing) 행태와 유사</span><br><span class="line">• 기업은 AI 기술 사용을 강조함으로써 투자자에게 매력적인 투자 대상으로 인식되어 더 많은 자본을 유</span><br><span class="line">치할 수 있으며, 보다 혁신적이고 기술 선도적인 이미지를 구축하여 비즈니스에서 경쟁사보다 유리한</span><br><span class="line">위치를 점함으로써 기업 가치의 상승 효과를 기대할 수 있음</span><br><span class="line"> [문제점] AI 워싱은 적절한 투자 자원의 배분 저해, 소비자 신뢰 저하, 과도한 기대감 유발 등 부</span><br><span class="line">정적인 결과를 초래할 수 있음</span><br><span class="line"> [사례] AI 워싱의 유형은 다양하나, 자사 제품이나 서비스가 출시되는 과정에서 AI 기여도가 불분</span><br><span class="line">명함에도 광고나 홍보에서 AI 기술 사용을 강조하며 소비자를 기만하는 행위가 대표적</span><br><span class="line">• [아마존고] 무인 매장 아마존고(Amazon Go)의 ‘저스트 워크 아웃(Just Walk Out)’은 고객 퇴점 시 자동으</span><br><span class="line">로 결제가 청구되는 시스템으로 알려졌으나, 사실은 인도 지사 직원들이 수동으로 검토하는 것으로 밝혀짐</span><br><span class="line">• [준코] 채용 스타트업 준코(Joonko)는 AI를 기반으로 기업에 적합한 지원자를 추천한다는 허위 정보</span><br><span class="line">를 유포해 투자 자금을 유치했다는 이유로 미국 증권거래위원회(SEC)와 법무부로부터 기소됨</span><br><span class="line">• [델피아 및 글로벌프레딕션스] 투자자문사 델피아(Delphia)와 글로벌프레딕션스(Global Predictions)는 AI</span><br><span class="line">워싱 행위로 미국 증권거래위원회로부터 각각 22만 5천 달러와 17만 5천 달러의 벌금이 부과됨</span><br><span class="line"> [대응 방안] AI 기술이 지속적으로 발전함에 따라 AI 워싱에 대한 규제와 소비자 보호의 중요성이 더</span><br><span class="line">욱 강조될 것이므로, 기업은 투명한 AI 기술 사용을 통해 소비자와 투자자의 신뢰를 확보함으로써 진</span><br><span class="line">정한 혁신을 도모할 필요</span><br><span class="line">• [규제 강화] 정부는 AI 워싱에 대한 규제를 강화함으로써 기업이 AI 사용에 대한 투명성을 높이고 허위</span><br><span class="line">주장을 남발하지 않도록 유도할 필요</span><br><span class="line">• [투명한 기술 사용 및 정보 제공] 기업은 실제 사용한 AI 기술과 일치하는 투명한 정보를 제공하고, 규</span><br><span class="line">제 기관 등의 감사나 외부 의혹에 대비하여 관련 자료를 기록하고 문서화할 필요</span><br><span class="line">• [소비자 교육] 정부나 기업은 소비자가 AI 기술에 대한 이해도를 높이고 AI와 관련된 과장된 주장에 현</span><br><span class="line">혹되지 않도록 소비자 대상 교육 프로그램을 운영할 필요</span><br><span class="line">• [비판적 태도 갖기] 소비자와 투자자는 주체적으로 정보를 검토하고 검증하는 태도를 가져야 함. 기업</span><br><span class="line">이 AI 기술에 대해 단순히 ‘혁신적’ 또는 ‘지능적’이라는 단어를 남발하는 것이 아닌지 비판적인 시각으</span><br><span class="line">로 바라볼 필요</span><br><span class="line">• [투명한 설명 요구하기] 소비자는 AI 작동 방식에 대한 투명한 설명을 요구하여 기업이 책임감을 갖고</span><br><span class="line">기술의 실제 성능과 한계 등을 포함해 정확한 정보를 제공하도록 유도해야 함</span><br><span class="line">&lt; 요 약 &gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2</span><br><span class="line"> AI 워싱의 주요 특징과 부작용</span><br><span class="line">○ AI 워싱(AI Washing)은 기업이나 제품이 실제로는 인공지능 기술을 사용하지 않으면서 광범</span><br><span class="line">위하게 활용하고 있는 것처럼 홍보하는 행위로 AI 열풍을 이용하는 마케팅 전략의 일환</span><br><span class="line"> 환경친화적이지 않은 기업이 친환경적인 것처럼 홍보하는 그린워싱(Greenwashing) 행태와 유사</span><br><span class="line"> AI 에 대한 정의가 광범위하고 느슨하게 사용되고 있다는 점도 AI 워싱의 등장 배경 중 하나</span><br><span class="line">【AI 워싱을 하는 이유】</span><br><span class="line">○ 기업은 AI 사용을 강조함으로써 투자자에게 매력적인 투자 대상으로 인식되어 더 많은 자본</span><br><span class="line">을 유치할 수 있으며, 보다 혁신적이고 기술 선도적인 이미지를 구축하여 비즈니스에서 경</span><br><span class="line">쟁사보다 유리한 위치를 점할 수 있음</span><br><span class="line"> [투자 유치] 《포브스(Forbes)》에 따르면 투자 유치 시 AI를 언급한 스타트업이 그렇지 않은 경우보다</span><br><span class="line">적게는 15%, 많게는 50% 더 많은 자금을 확보하면서 AI 역량을 과대 광고하는 기업이 늘고 있음1</span><br><span class="line">- 일부 기업이 자금 조달을 위해 AI 역량을 과장하고, 창업자는 투자 유치 시 AI를 언급</span><br><span class="line">하지 않으면 불리해질 수 있다고 생각하는 경향이 있음2</span><br><span class="line">. 이 때문에 AI 역량을 주장하</span><br><span class="line">는 기업과 실질적인 AI 기업 사이에는 상당한 간극이 존재</span><br><span class="line">- 영국 벤처캐피털 펀드 MMC벤처스(MMC Ventures)의 2019년 조사에 따르면 유럽</span><br><span class="line">AI 스타트업의 40%는 사실상 AI를 전혀 사용하지 않은 것으로 나타남</span><br><span class="line"> [시장 경쟁력 확보] 가전제품 제조업체가 자사 제품에 AI 기술이 적용되었다고 주장하면, 소비자</span><br><span class="line">는 그 제품이 더 스마트하고 혁신적이라 생각하여 적극적인 구매 의사를 나타낼 가능성이 높음</span><br><span class="line">- 기업은 AI 기술을 사용하여 기술적 우월성을 강조하고 시장에서 리더로 자리매김하려</span><br><span class="line">의도하지만, 실제로는 인터넷을 통해 작동하는 것을 AI 시스템을 구축했다고 홍보하거</span><br><span class="line">나 상담 챗봇을 추가하는 정도에 그치는 경우가 많음</span><br><span class="line">【AI 워싱의 문제점】</span><br><span class="line">○ AI 워싱은 흔한 마케팅 방법 중 하나로 보일 수 있지만, 투자 자원의 적절한 배분을 저해하고 소비</span><br><span class="line">자의 과도한 기대감을 유도해 결국 신뢰 저하로 이어지게 하는 등 부정적인 결과를 초래할 수 있음</span><br><span class="line"></span><br><span class="line">1 Bernard Marr, Apr. 25, 2024, “Spotting AI Washing: How Companies Overhype Artificial Intelligence”, Forbes</span><br><span class="line">2 영국과 핀란드에 본사를 둔 신기술 투자회사 오픈오션(OpenOcean)에 따르면 2022년 기술 스타트업의 10%만이 투자 유치 시 AI</span><br><span class="line">를 사용한다고 언급했으나 2023년에는 이 비율이 4분의 1 이상으로 증가했고 올해는 3분의 1 이상으로 증가할 것으로 예상</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3</span><br><span class="line">○ [적절한 자원 배분 저해] 혁신적인 기술을 보유하고 있다고 포장하는 기업에 소비자와 투자</span><br><span class="line">자의 이목과 투자가 집중될 수 있음</span><br><span class="line"> AI 워싱 기업에 투자 재원이 몰릴 경우 실제 혁신 기술을 보유한 기업이나 프로젝트에는 원활한</span><br><span class="line">자금 공급이 어려울 수 있음</span><br><span class="line">○ [소비자 신뢰 저하] AI 워싱을 마케팅 수단으로 사용하는 기업이 증가하여 대부분의 기업이</span><br><span class="line">AI에 대해 언급한다면 소비자는 AI의 성능에 의문을 갖게 될 것이고, 실제 사용을 통해 부</span><br><span class="line">정적 경험이 누적될 경우 AI 기술 자체에 대한 신뢰도가 저하될 수 있음</span><br><span class="line"> 소비자는 과장된 주장을 반복적으로 접하면서 AI 기술을 불신하게 되며, 이는 결과적으로 AI 제품</span><br><span class="line">과 서비스의 구매 기피로 이어져 진정한 AI 기술을 제공하는 기업에 부정적 영향을 미칠 수 있음</span><br><span class="line">○ [기업의 자원 낭비] AI 워싱 기업의 과장된 주장을 의식하여 정상적인 기업마저 현실성이 떨</span><br><span class="line">어지는 목표를 설정하고 추진해 나갈 경우 프로젝트의 실패로 인한 손실뿐 아니라 선택과</span><br><span class="line">집중의 실패로 인해 혁신이 지체되는 등의 부정적 결과를 초래할 수 있음</span><br><span class="line"> 기업이 의미 있는 AI 역량을 개발하는 대신 피상적인 개선에 투자를 남발할 경우 기술의 진전을 늦출</span><br><span class="line">수 있음</span><br><span class="line"> 진정 가치 있는 AI 솔루션을 모색하는 기업의 의사 결정을 복잡하게 만들어 디지털 전환 노력을</span><br><span class="line">방해하고 혁신을 저해하며 성과를 악화시킬 수 있음</span><br><span class="line"> AI 워싱 사례</span><br><span class="line">○ [아마존고] 무인 매장 아마존고(Amazon Go)의 ‘저스트 워크 아웃(Just Walk Out)’은 고객</span><br><span class="line">퇴점 시 자동으로 결제가 청구되는 시스템으로 알려졌으나, 사실은 인도 지사 직원들이 수</span><br><span class="line">동으로 검토하는 것으로 밝혀짐3</span><br><span class="line"> 매장 내 개별 제품에 센서나 칩을 부착하지 않고, 천장에 달린 센서가 실시간으로 고객과 제품을 추적</span><br><span class="line">하는 딥러닝 기술을 적용했다고 밝혔으나, 시스템의 상당 부분이 1천여 명의 인도 지사 직원이 각 매</span><br><span class="line">장 카메라를 통해 수동으로 체크하는 방식으로 운영된다는 사실이 드러나면서 AI 워싱 논란이 제기됨</span><br><span class="line">- 아마존고는 인간이 결제 청구 과정에 관여한다는 사실은 부인하지 않았지만, 직원들이</span><br><span class="line">‘저스트 워크 아웃’ 시스템을 개선하기 위해 AI가 생성한 실제 데이터에 주석을 달 뿐이</span><br><span class="line">며 전체 운영에 관여하는 것은 아니라고 주장4</span><br><span class="line"></span><br><span class="line">3 Alex Bitter, Apr. 4, 2024, “Amazon’s just walk out actually uses 1,000 people in India”, Business Insider</span><br><span class="line">4 Emma Roth, Apr. 18, 2024, “Amazon insists Just Walk Out isn’t secretly run by workers watching you shop”, The Verge</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">4</span><br><span class="line">[그림 1] 아마존고 구조도 [그림 2] 아마존고 외부 전경</span><br><span class="line">자료: 위키피디아 자료: 위키피디아</span><br><span class="line">○ [준코] 채용 스타트업 준코(Joonko)는 AI를 기반으로 기업에 적합한 지원자를 추천한다는</span><br><span class="line">허위 정보를 유포해 투자 자금을 유치했다는 이유로 미국 증권거래위원회(SEC)와 법무부로</span><br><span class="line">부터 기소됨</span><br><span class="line"> 증권거래위원회는 지난 6월 AI를 기반으로 지원자를 선발</span><br><span class="line">한다고 속이고 고객 정보와 지원자 수, 기업 수익에 관한</span><br><span class="line">허위 및 오해의 소지가 있는 진술을 하여 최소 2천100만</span><br><span class="line">달러의 투자를 유치한 혐의로 AI 기반 채용 스타트업 준코</span><br><span class="line">의 CEO이자 창업자인 일릿 라즈(llit Raz)를 기소</span><br><span class="line">- 준코는 AI를 사용하여 기업이 다양성, 형평성, 포</span><br><span class="line">용성이 존중되는 인력 조직을 구축할 수 있도록</span><br><span class="line">다양한 지원자를 선발하는 데 도움을 준다고 주장</span><br><span class="line">했으나 조사 결과 거짓으로 밝혀짐</span><br><span class="line">- 라즈는 투자 유치 시 준코가 《포천(Fortune)》 500대 기업을 포함하여 100개 이상의 고</span><br><span class="line">객사를 보유하고 있으며, 100만 달러 이상의 수익을 올렸을 뿐 아니라 10만 명 이상</span><br><span class="line">의 활성 구직자와 협력하고 있다는 등 경영 성과에 대해서도 허위 정보를 제공</span><br><span class="line">- 라즈의 주장에 의심을 품는 투자자에게 허위 정보 유포 사실을 은폐하기 위해 위조된</span><br><span class="line">은행 거래 내역서와 계약서도 제공</span><br><span class="line">○ [델피아 및 글로벌프레딕션스] 미국 증권거래위원회는 AI 사용 범위에 대해 허위 및 오해의 소지가</span><br><span class="line">있는 진술을 한 혐의로 투자자문사 델피아(Delphia)와 글로벌프레딕션스(Global Predictions)를 기</span><br><span class="line">소. 양사는 제기된 혐의를 해결하고 각각 22만 5천 달러와 17만 5천 달러의 벌금을 지불하는</span><br><span class="line">선에서 합의</span><br><span class="line">[그림 3] 준코 홍보 이미지</span><br><span class="line">자료: 준코 링크드인 계정</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">5</span><br><span class="line"> [델피아] AI 를 활용하여 어떤 기업과 트렌드가 크게</span><br><span class="line">성장할 것인지 예측하여 고객이 선제적으로 투자할</span><br><span class="line">수 있도록 정보를 제공했다고 주장했지만, 증권거래</span><br><span class="line">위원회는 델피아가 실제로는 AI 및 머신러닝 기술을</span><br><span class="line">갖추지 않았다고 판단</span><br><span class="line">- 델피아는 등록 투자자문사가 중요 사실에 대</span><br><span class="line">한 허위 진술이 포함된 광고를 유포하는 것</span><br><span class="line">을 금지한 마케팅 규칙을 위반한 혐의로 추</span><br><span class="line">가 기소됨</span><br><span class="line"> [글로벌프레딕션스] 홈페이지, 소셜미디어, 이메일에</span><br><span class="line">서 자신들이 최초의 AI 금융 자문사로 전문가 수준의 AI 기반 예측(Expert AI-driven Forecasts)</span><br><span class="line">을 제공한다고 주장했지만, 이에 대한 근거를 제시하지 못했고 실제로도 AI 기술을 사용하지 않음</span><br><span class="line">- 홈페이지와 유튜브 채널에 실제 고객 데이터</span><br><span class="line">가 아닌 가상의 성과 데이터를 공개하면서 이</span><br><span class="line">사실을 명시하지 않아 증권거래위원회 규정을</span><br><span class="line">위반. 또한 AI 기반 예측 시스템의 우수성을</span><br><span class="line">홍보했지만 실제로는 AI를 사용하지 않은 것</span><br><span class="line">으로 드러남</span><br><span class="line">- 이러한 규정 위반에 대한 시정 조치로 마케팅</span><br><span class="line">및 교육 자료 검토를 위한 규정 준수 컨설턴</span><br><span class="line">트를 고용하기로 증권거래위원회와 합의</span><br><span class="line">○ [오토메이터스 AI] 오토메이터스 AI(Automators AI)는 AI 기술을 활용해 온라인 스토어의 판매</span><br><span class="line">량을 자동으로 증가시킬 수 있다는 허위 정보를 유포하여 투자를 유치한 혐의로 연방거래위</span><br><span class="line">원회(FTC)에 의해 기소됨5</span><br><span class="line"> 아마존이나 월마트와 같은 플랫폼에서 운영되는 온라인 스토어에 입점한 사업자들에게 자신들의</span><br><span class="line">AI 기술을 활용하면 매월 4 천 달러에서 6 천 달러의 순이익을 얻고, 8 개월 후 100% 투자수익률</span><br><span class="line">을 달성할 수 있다고 홍보하여 2 천 200 만 달러의 투자를 유치했지만 허위 정보로 드러남</span><br><span class="line"></span><br><span class="line">5 2023년 8월 로만 크레스토(Roman Cresto), 존 크레스토(John Cresto), 앤드루 채프먼(Andrew Chapman)은 오토메이터스</span><br><span class="line">AI를 포함해 여러 회사를 운영하면서 아마존 및 월마트의 이커머스에 AI를 적용하면 불로소득(Passive income)을 얻을 수 있다</span><br><span class="line">고 홍보하며 투자자를 유치했으나 AI 기술이 사용되지 않은 것으로 드러남(Federal Trade Commission, Feb. 27, 2024, “FTC</span><br><span class="line">Action Leads to Ban for Owners of Automators AI E-Commerce Money-Making Scheme”)</span><br><span class="line">[그림 4] 델피아 홍보 이미지</span><br><span class="line">자료: 델피아 홈페이지</span><br><span class="line">[그림 5] 글로벌프레딕션스 홍보 이미지</span><br><span class="line">자료: 글로벌프레딕션스 홈페이지</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">6</span><br><span class="line">- 그들의 주장과는 달리 실제로는 AI 기술을 활용하지 않았고, 대부분의 고객이 수익은커녕</span><br><span class="line">투자금마저 회수하지 못한 것으로 드러남. 이에 연방법원은 회사 운영을 중단시키고, 관</span><br><span class="line">련자들에게 영구적인 사업 기회 박탈과 2천200만 달러 배상을 명령</span><br><span class="line">○ [오랄비] P&amp;G의 구강 관리 브랜드 오랄비(Oral-B)는 고가의 전동칫솔을 판매하면서 AI가 치아 위</span><br><span class="line">치와 밝기 등을 파악해 이가 잘 닦였는지 확인할 수 있다고 광고. 그러나 《워싱턴포스트》는 “이 칫</span><br><span class="line">솔에 AI 기능이 정확히 어떻게 적용되는지 물었지만 회사는</span><br><span class="line">대답하지 못했다”고 보도6</span><br><span class="line"> 오랄비는 해당 칫솔이 AI 기술로 사용자의 칫솔질 습관</span><br><span class="line">을 실시간 분석하여 맞춤형 피드백을 제공한다고 주장했</span><br><span class="line">으나, 소비자와 전문가들은 이러한 기능이 얼마나 효과</span><br><span class="line">적인지에 대해 의문을 제기</span><br><span class="line">- 일부 사용자는 칫솔이 제공하는 피드백이 개인화되었다</span><br><span class="line">고 보기 어렵거나, 예상보다 단순하다며 불만을 표시</span><br><span class="line">○ [코카콜라] 코카콜라는 AI를 사용하여 새로운 음료</span><br><span class="line">를 만들었다고 홍보했지만 AI 워싱임이 드러나 업</span><br><span class="line">계의 비난을 받음</span><br><span class="line"> 3000 년대를 상상하며 만들었다고 홍보한 코카</span><br><span class="line">콜라 Y3000 은 AI 와 공동으로 제품을 개발했다</span><br><span class="line">고 밝혔지만, AI 가 개발 과정에서 어떻게 관여했</span><br><span class="line">는지에 대해서는 설명하지 않음</span><br><span class="line"> AI 워싱을 피하려면?</span><br><span class="line">○ AI 기술이 지속적으로 발전함에 따라 AI 워싱에 대한 규제와 소비자 보호의 중요성이 더욱 강조될 것이며,</span><br><span class="line">기업은 투명한 AI 사용을 통해 소비자와 투자자의 신뢰를 확보함으로써 진정한 혁신을 도모할 필요</span><br><span class="line">【정부 및 기업】</span><br><span class="line">○ [규제 강화] 정부는 AI 워싱에 대한 규제를 강화함으로써 기업이 AI 사용에 대한 투명성을 높</span><br><span class="line">이고 허위 주장을 남발하지 않도록 유도할 필요</span><br><span class="line"></span><br><span class="line">6 Shira Ovide, Apr. 5, 2024, “This $400 toothbrush is peak AI mania”, The Washington Post</span><br><span class="line">[그림 6] 오랄비 iO시리즈 홍보문구</span><br><span class="line">자료: 오랄비 스마트스토어</span><br><span class="line">[그림 7] 코카콜라 Y3000</span><br><span class="line">자료: 코카콜라 홈페이지</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">7</span><br><span class="line"> 미국 증권거래위원회와 법무부, 연방거래위원회는 AI 워싱 기업을 대상으로 기소하거나 거액의</span><br><span class="line">벌금을 부과하는 등의 조치를 취함으로써 AI 와 관련된 허위 또는 오해의 소지가 있는 주장을 할</span><br><span class="line">경우 단속 대상이 될 수 있다는 경고 메시지를 전달</span><br><span class="line">- 증권거래위원회는 지난해 말 JP모건체이스 등 대형 금융사의 AI 사용 실태에 대한 전수</span><br><span class="line">조사에 착수. 고객 포트폴리오 관리에 활용되는 AI 알고리즘 모델 및 관련 마케팅 서류,</span><br><span class="line">데이터에 대한 제3자 제공 현황, 컴플라이언스 교육 사항 등이 요청 내역에 포함됨7</span><br><span class="line">- 연방거래위원회는 기업이 “AI 제품의 기능을 과장하고 있지 않나요? AI 제품이 비AI 제</span><br><span class="line">품보다 더 나은 것처럼 홍보하고 있나요? 제품이 실제로 AI를 사용하고 있나요?” 등의</span><br><span class="line">질문을 통해 자기 검열을 강화할 것을 권고</span><br><span class="line"> 영국 광고표준위원회(Advertising Standards Authority)는 AI 워싱에 대한 규칙과 법률을 제정.</span><br><span class="line">여기에는 AI 와 관련하여 실질적으로 오해의 소지가 있는 진술을 금지하는 내용이 포함됨</span><br><span class="line">○ [투명한 기술 사용 및 정보 제공] 기업은 실제 사용한 AI 기술과 일치하는 투명한 정보를 제공</span><br><span class="line">하고, 규제 기관 등의 감사나 외부 의혹에 대비하여 기록하고 문서화할 필요</span><br><span class="line"> AI 기술을 홍보할 때 허위 주장을 피하고 관련 정보를 투명하게 제공함으로써 소비자와 투자자에</span><br><span class="line">게 신뢰를 얻어야 함</span><br><span class="line">- 추가적으로 기업 내부의 각 부서가 협력하여 AI 기술 사용에 대한 정확한 정보를 공유해</span><br><span class="line">야 하며, AI 기술이 실제로 어떻게 사용되는지에 대한 상세한 기록을 남기고 정기적으로</span><br><span class="line">업데이트를 해야 함</span><br><span class="line">○ [소비자 교육] 정부나 기업은 소비자가 AI 기술에 대한 이해도를 높이고 AI와 관련된 과장된</span><br><span class="line">주장에 현혹되지 않도록 소비자 대상 교육 프로그램을 운영할 필요</span><br><span class="line"> 교육 프로그램 등을 제공하여 소비자가 AI 기술의 한계와 실제 활용 가능성에 대해 명확하게 이</span><br><span class="line">해함으로써 과대 광고 및 허위 투자 유치 등에 현혹되지 않는 역량을 함양시키는 것이 중요</span><br><span class="line">【소비자 입장】</span><br><span class="line">○ [비판적 태도 갖기] 소비자와 투자자는 기업의 AI 활용 주장에 대해 비판적 태도를 가져야 함.</span><br><span class="line">사용된 AI 모델알고리즘과 같은 기술적인 부분에 대한 구체적 언급이 있는지 확인하고, 기업</span><br><span class="line">이 관련 데이터와 알고리즘 유형에 대한 투명성을 유지하는지 점검할 필요</span><br><span class="line"></span><br><span class="line">7 김현수신아형, 2023.12.12, “美 SEC ‘투자수익 과장하는 AI워싱 위험’… 월街 실태 전수조사”, 《동아일보》</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">8</span><br><span class="line"> 소비자와 투자자가 주체적으로 정보를 검토하고 검증하는 태도를 갖는 것이 중요. 기업이 AI 기술</span><br><span class="line">에 대해 단순히 ‘혁신적’ 또는 ‘지능적’이라는 단어를 남발하는 것이 아닌지 비판적인 시각으로 바</span><br><span class="line">라볼 필요</span><br><span class="line">○ [투명한 설명 요구하기] 소비자는 AI 작동 방식에 대한 투명한 설명을 요구하여 기업이 책임감을</span><br><span class="line">갖고 기술의 실제 성능과 한계 등을 포함해 정확한 정보를 제공하도록 유도해야 함</span><br><span class="line"> AI 시스템의 데이터 출처나 알고리즘 작동 방식, 정확성을 높이기 위해 어떤 조치를 취했는지 등</span><br><span class="line">에 대한 설명을 요구해야 함. 데이터와 알고리즘의 편향이나 AI 환각을 피하기 위한 해결책이 없</span><br><span class="line">는 경우 실제로 AI 를 사용하지 않았을 가능성이 존재</span><br><span class="line">&lt;책임연구원 송원호(wonho.song@kbfg.com) ☎02)2073-5730&gt;</span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-08-21</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/08/21/2024-8-26-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 8월 26일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/08/21/2024-8-21-AI-NEWS/" title="2024년 8월 21일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/08/16/2024-8-16-AI-NEWS/" title="2024년 8월 16일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>