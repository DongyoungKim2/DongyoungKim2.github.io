<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 8ì›” 26ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="NvidiaëŠ” ê³ ì„±ëŠ¥ ì†Œí˜• ì–¸ì–´ ëª¨ë¸ì¸ Mistral-NeMo-Minitron 8Bë¥¼, AI21 LabsëŠ” ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ì— íŠ¹í™”ëœ Jamba 1.5 ëª¨ë¸ì„ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, Jina AIëŠ” ì¥ë¬¸ ì„ë² ë”© ëª¨ë¸ì˜ ìƒˆë¡œìš´ ì²˜ë¦¬ ë°©ë²•ì„ ì†Œê°œí•˜ì˜€ìœ¼ë©°, MetaëŠ” ì¸ê°„ ì¤‘ì‹¬ "><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 8ì›” 26ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><p>NvidiaëŠ” ê³ ì„±ëŠ¥ ì†Œí˜• ì–¸ì–´ ëª¨ë¸ì¸ Mistral-NeMo-Minitron 8Bë¥¼, AI21 LabsëŠ” ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ì— íŠ¹í™”ëœ Jamba 1.5 ëª¨ë¸ì„ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ, Jina AIëŠ” ì¥ë¬¸ ì„ë² ë”© ëª¨ë¸ì˜ ìƒˆë¡œìš´ ì²˜ë¦¬ ë°©ë²•ì„ ì†Œê°œí•˜ì˜€ìœ¼ë©°, MetaëŠ” ì¸ê°„ ì¤‘ì‹¬ ë¹„ì „ ì‘ì—…ì„ ìœ„í•œ ìƒˆë¡œìš´ ëª¨ë¸ íŒ¨ë°€ë¦¬ì¸ Sapiensë¥¼ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. LinkedInê³¼ Neural Magicë„ AI ëª¨ë¸ì˜ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë„êµ¬ë¥¼ ê³µê°œí•˜ì˜€ê³ , KBê¸ˆìœµê·¸ë£¹ì€ AI ê¸°ìˆ ì„ ê³¼ì¥í•˜ì—¬ í™ë³´í•˜ëŠ” â€˜AI ì›Œì‹±â€™ì˜ ë¬¸ì œì ì„ ê²½ê³ í–ˆìŠµë‹ˆë‹¤.</p>
<h3 id="Nvidia-Mistral-NeMo-Minitron-8B-ì†Œí˜•-ì–¸ì–´-ëª¨ë¸-ì¶œì‹œ"><a href="#Nvidia-Mistral-NeMo-Minitron-8B-ì†Œí˜•-ì–¸ì–´-ëª¨ë¸-ì¶œì‹œ" class="headerlink" title="Nvidia, Mistral-NeMo-Minitron 8B: ì†Œí˜• ì–¸ì–´ ëª¨ë¸ ì¶œì‹œ"></a>Nvidia, Mistral-NeMo-Minitron 8B: ì†Œí˜• ì–¸ì–´ ëª¨ë¸ ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://blogs.nvidia.com/blog/mistral-nemo-minitron-8b-small-language-model/">ë§í¬</a>, 2024ë…„ 8ì›” 21ì¼</p>
<ul>
<li>NvidiaëŠ” Mistral NeMo 12B ëª¨ë¸ì„ ì†Œí˜•í™”í•œ Mistral-NeMo-Minitron 8Bë¥¼ ì¶œì‹œí•¨</li>
<li>ì´ ëª¨ë¸ì€ ë†’ì€ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œë„ ë‚®ì€ ì»´í“¨íŒ… ë¹„ìš©ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥</li>
<li>Pruningê³¼ Distillation ê¸°ë²•ì„ ê²°í•©í•˜ì—¬ ëª¨ë¸ í¬ê¸°ë¥¼ ì¤„ì´ë©´ì„œë„ ì„±ëŠ¥ì„ ìµœì í™”</li>
<li>ì†Œí˜• ì–¸ì–´ ëª¨ë¸ ì¤‘ ìµœê³  ì„±ëŠ¥ì„ ìë‘í•˜ë©°, ë‹¤ì–‘í•œ AI ì‘ì—…ì—ì„œ í™œìš© ê°€ëŠ¥</li>
<li>í•´ë‹¹ ëª¨ë¸ì€ Nvidia RTX ì›Œí¬ìŠ¤í…Œì´ì…˜ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ êµ¬ë™ ê°€ëŠ¥í•˜ë©°, Hugging Faceì—ì„œë„ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥</li>
<li>ëª¨ë¸ì€ 9ê°€ì§€ ì£¼ìš” ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ê¸°ë¡í•˜ë©°, ì–¸ì–´ ì´í•´, ìƒì‹ ì¶”ë¡ , ìˆ˜í•™ì  ì¶”ë¡ , ìš”ì•½, ì½”ë”© ë° ì§„ì‹¤í•œ ë‹µë³€ ìƒì„± ëŠ¥ë ¥ì„ ì…ì¦í•¨</li>
<li>Nvidia AI Foundry í”Œë«í¼ì„ í†µí•´ ëª¨ë¸ì„ ë”ìš± ì†Œí˜•í™”í•˜ì—¬ ìŠ¤ë§ˆíŠ¸í°ì´ë‚˜ ë¡œë´‡ ë“± ì„ë² ë””ë“œ ì¥ì¹˜ì— ì í•©í•œ ë²„ì „ìœ¼ë¡œë„ í™œìš© ê°€ëŠ¥</li>
</ul>
<h3 id="AI21-Labs-Jamba-1-5-ëª¨ë¸-ì¶œì‹œ"><a href="#AI21-Labs-Jamba-1-5-ëª¨ë¸-ì¶œì‹œ" class="headerlink" title="AI21 Labs, Jamba 1.5 ëª¨ë¸ ì¶œì‹œ"></a>AI21 Labs, Jamba 1.5 ëª¨ë¸ ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251">ë§í¬</a>, 2024ë…„ 8ì›” 22ì¼</p>
<ul>
<li>AI21 LabsëŠ” ìƒˆë¡œìš´ Jamba 1.5 ëª¨ë¸ ì‹œë¦¬ì¦ˆë¥¼ ë°œí‘œí•¨</li>
<li>Jamba 1.5 ëª¨ë¸ì€ SSM-Transformer ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ì™€ ì†ë„, íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”</li>
<li>Jamba 1.5 ëª¨ë¸ì€ 256Kì˜ ê¸´ ë¬¸ë§¥ ì°½ì„ ì§€ì›í•˜ì—¬ ëŒ€í˜• ë¬¸ì„œ ìš”ì•½, ë¶„ì„, ì—ì´ì „íŠ¸ ë° RAG(ë¦¬íŠ¸ë¦¬ë²Œ ì¦ê°• ìƒì„±) ì›Œí¬í”Œë¡œìš°ì— ì í•©</li>
<li>Jamba 1.5 Miniì™€ Large ëª¨ë¸ì€ ê°ê° Arena Hard ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœê³  ì„±ëŠ¥ì„ ê¸°ë¡í•¨</li>
<li>ëª¨ë¸ì€ ë‹¤ì¤‘ ì–¸ì–´ë¥¼ ì§€ì›í•˜ë©°, êµ¬ì¡°í™”ëœ JSON ì¶œë ¥, í•¨ìˆ˜ í˜¸ì¶œ, ë¬¸ì„œ ê°ì²´ ì²˜ë¦¬, ì¸ìš© ìƒì„± ê¸°ëŠ¥ì„ ì œê³µ</li>
<li>AI21 Studio, Google Cloud Vertex AI, Microsoft Azure, Nvidia NIM ë“± ë‹¤ì–‘í•œ í´ë¼ìš°ë“œ í”Œë«í¼ì—ì„œ ì¦‰ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, Amazon Bedrock, Databricks Marketplace, Snowflake Cortex ë“±ì—ì„œë„ ê³§ ì¶œì‹œ ì˜ˆì •</li>
<li>ExpertsInt8ë¼ëŠ” ìƒˆë¡œìš´ ì–‘ìí™” ê¸°ìˆ ì„ í†µí•´ MoE(Mixture of Experts) ëª¨ë¸ì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³ , ë‹¨ì¼ 8 GPU ë…¸ë“œì—ì„œ ëª¨ë¸ì„ ì‹¤í–‰ ê°€ëŠ¥</li>
</ul>
<h3 id="Jina-AI-ì¥ë¬¸-ì„ë² ë”©-ëª¨ë¸ì„-ìœ„í•œ-â€˜Late-Chunkingâ€™-ê¸°ìˆ -ë°œí‘œ"><a href="#Jina-AI-ì¥ë¬¸-ì„ë² ë”©-ëª¨ë¸ì„-ìœ„í•œ-â€˜Late-Chunkingâ€™-ê¸°ìˆ -ë°œí‘œ" class="headerlink" title="Jina AI, ì¥ë¬¸ ì„ë² ë”© ëª¨ë¸ì„ ìœ„í•œ â€˜Late Chunkingâ€™ ê¸°ìˆ  ë°œí‘œ"></a>Jina AI, ì¥ë¬¸ ì„ë² ë”© ëª¨ë¸ì„ ìœ„í•œ â€˜Late Chunkingâ€™ ê¸°ìˆ  ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://jina.ai/news/late-chunking-in-long-context-embedding-models/">ë§í¬</a>, 2024ë…„ 8ì›” 23ì¼</p>
<ul>
<li>Jina AIëŠ” ê¸´ ë¬¸ë§¥ì„ ë‹¤ë£¨ëŠ” ì„ë² ë”© ëª¨ë¸ì„ ìœ„í•œ ìƒˆë¡œìš´ ì²˜ë¦¬ ë°©ë²•ì¸ â€˜Late Chunkingâ€™ ê¸°ìˆ ì„ ë°œí‘œí•¨</li>
<li>ì´ ë°©ë²•ì€ ë¬¸ì„œì˜ ê¸´ ë¬¸ë§¥ì„ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬, RAG(ë¦¬íŠ¸ë¦¬ë²Œ ì¦ê°• ìƒì„±) ì‹œìŠ¤í…œì—ì„œ ë³´ë‹¤ ë‚˜ì€ ê²€ìƒ‰ ì„±ëŠ¥ì„ ì œê³µ</li>
<li>â€˜Late Chunkingâ€™ ê¸°ìˆ ì€ ë¬¸ì„œë¥¼ ë¯¸ë¦¬ ë¶„í• í•˜ëŠ” ëŒ€ì‹ , ì„ë² ë”© ëª¨ë¸ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•´ ë¬¸ì„œ ì „ì²´ë¥¼ ì²˜ë¦¬í•œ í›„, ê° ì²­í¬ì— ë¬¸ë§¥ ì •ë³´ë¥¼ ë°˜ì˜í•œ ì„ë² ë”©ì„ ìƒì„±</li>
<li>ì´ ê¸°ìˆ ì€ ê¸´ ë¬¸ì„œì—ì„œ ë¬¸ë§¥ ì •ë³´ë¥¼ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ ìœ ì§€í•˜ë©°, BEIR ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì„</li>
<li>Late Chunkingì€ ê¸´ ë¬¸ë§¥ì„ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬, ê¸°ì¡´ì˜ ì²­í‚¹ ë°©ì‹ë³´ë‹¤ ê²€ìƒ‰ ì •í™•ë„ë¥¼ í¬ê²Œ ê°œì„ </li>
</ul>
<h3 id="Meta-ì¸ê°„-ì¤‘ì‹¬-ë¹„ì „-ì‘ì—…ì„-ìœ„í•œ-â€˜Sapiensâ€™-ëª¨ë¸-ë°œí‘œ"><a href="#Meta-ì¸ê°„-ì¤‘ì‹¬-ë¹„ì „-ì‘ì—…ì„-ìœ„í•œ-â€˜Sapiensâ€™-ëª¨ë¸-ë°œí‘œ" class="headerlink" title="Meta, ì¸ê°„ ì¤‘ì‹¬ ë¹„ì „ ì‘ì—…ì„ ìœ„í•œ â€˜Sapiensâ€™ ëª¨ë¸ ë°œí‘œ"></a>Meta, ì¸ê°„ ì¤‘ì‹¬ ë¹„ì „ ì‘ì—…ì„ ìœ„í•œ â€˜Sapiensâ€™ ëª¨ë¸ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2408.12569">ë§í¬</a>, 2024ë…„ 8ì›” 23ì¼</p>
<ul>
<li>MetaëŠ” ì¸ê°„ ì¤‘ì‹¬ ë¹„ì „ ì‘ì—…ì„ ìœ„í•œ Sapiens ëª¨ë¸ íŒ¨ë°€ë¦¬ë¥¼ ë°œí‘œí•¨</li>
<li>ì´ ëª¨ë¸ì€ 2D í¬ì¦ˆ ì¶”ì •, ì‹ ì²´ ë¶€ìœ„ ë¶„í• , ê¹Šì´ ì¶”ì •, í‘œë©´ ë²•ì„  ì˜ˆì¸¡ ë“± 4ê°€ì§€ í•µì‹¬ ì‘ì—…ì„ ì§€ì›</li>
<li>1K í•´ìƒë„ì—ì„œì˜ ì¶”ë¡ ì„ ê¸°ë³¸ì ìœ¼ë¡œ ì§€ì›í•˜ë©°, ê°œë³„ ì‘ì—…ì— ë§ê²Œ ê°„ë‹¨íˆ ë¯¸ì„¸ ì¡°ì • ê°€ëŠ¥</li>
<li>3ì–µ ê°œ ì´ìƒì˜ ì¸ë¥˜ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ ìê°€ í•™ìŠµì„ í†µí•´ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒ</li>
<li>Sapiens ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì¸ê°„ ì¤‘ì‹¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ì¡´ ìµœê³  ì„±ëŠ¥ì„ ì´ˆê³¼ ë‹¬ì„±</li>
<li>Humans-5K, Humans-2K, Hi4D, THuman2 ë“±ì—ì„œ ìƒëŒ€ì  RMSE ë° ê°ë„ ì˜¤ë¥˜ë¥¼ í¬ê²Œ ê°œì„ </li>
</ul>
<h3 id="LinkedIn-Liger-Kernel-ì¶œì‹œ"><a href="#LinkedIn-Liger-Kernel-ì¶œì‹œ" class="headerlink" title="LinkedIn, Liger-Kernel ì¶œì‹œ"></a>LinkedIn, Liger-Kernel ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://github.com/linkedin/Liger-Kernel">ë§í¬</a>, 2024ë…„ 8ì›” 23ì¼</p>
<ul>
<li>LinkedInì˜ LLM ì—°êµ¬íŒ€ì€ ë©€í‹° GPU íŒŒì¸ íŠœë‹ì„ ìœ„í•œ ìƒˆë¡œìš´ íš¨ìœ¨ì  GPU ì»¤ë„ì¸ Liger-Kernelì„ ì¶œì‹œí•¨</li>
<li>ì´ ì»¤ë„ì€ ë©€í‹° GPU í™˜ê²½ì—ì„œ 20%ì˜ ì²˜ë¦¬ëŸ‰ ì¦ê°€ì™€ 60%ì˜ ë©”ëª¨ë¦¬ ê°ì†Œë¥¼ ì œê³µ</li>
<li>Flash Attention, PyTorch FSDP, Microsoft DeepSpeedì™€ í˜¸í™˜ ê°€ëŠ¥í•˜ë©°, ë‹¤ì–‘í•œ LLM ëª¨ë¸ì— ì ìš© ê°€ëŠ¥</li>
<li>Hugging Face í˜¸í™˜ RMSNorm, RoPE, SwiGLU, CrossEntropy, FusedLinearCrossEntropy ë“±ì„ ì§€ì›</li>
<li>Triton ì»¤ë„ì„ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ë©°, ë” ê¸´ ë¬¸ë§¥ ê¸¸ì´ì™€ ë” í° ë°°ì¹˜ í¬ê¸°ë¥¼ ì²˜ë¦¬ ê°€ëŠ¥</li>
<li>ì´ ì»¤ë„ì€ ë‹¨ì¼ ë¼ì¸ ì½”ë“œë¡œ ì‰½ê²Œ ì ìš© ê°€ëŠ¥í•˜ë©°, ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ì»¤ë®¤ë‹ˆí‹° ì£¼ë„ì˜ ê°œë°œì„ ìœ ë„</li>
</ul>
<h3 id="Neural-Magic-LLM-Compressor-í”„ë ˆì„ì›Œí¬-ê³µê°œ"><a href="#Neural-Magic-LLM-Compressor-í”„ë ˆì„ì›Œí¬-ê³µê°œ" class="headerlink" title="Neural Magic, LLM Compressor í”„ë ˆì„ì›Œí¬ ê³µê°œ"></a>Neural Magic, LLM Compressor í”„ë ˆì„ì›Œí¬ ê³µê°œ</h3><p><a target="_blank" rel="noopener" href="https://neuralmagic.com/blog/llm-compressor-is-here-faster-inference-with-vllm/">ë§í¬</a>, 2024ë…„ 8ì›” 14ì¼</p>
<ul>
<li>Neural Magicì€ LLM Compressorë¼ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ë¥¼ ê³µê°œí•˜ì—¬, LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì˜ ì••ì¶•ê³¼ ì„±ëŠ¥ í–¥ìƒì„ ì§€ì›</li>
<li>ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë‹¤ì–‘í•œ ì–‘ìí™” ê¸°ìˆ ê³¼ í¬ì†Œì„± ì˜µì…˜ì„ ì œê³µí•˜ì—¬ LLM ëª¨ë¸ì˜ ìœ ì—°ì„±ì„ ê·¹ëŒ€í™”</li>
<li>Neural Magic íŒ€ì€ ì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Llama 3.1 405B ëª¨ë¸ì„ í¬í•¨í•œ ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ì••ì¶•</li>
<li>Activation Quantizationì„ í†µí•´ INT8 ë˜ëŠ” FP8 í…ì„œ ì½”ì–´ë¥¼ í™œìš©í•˜ì—¬ ìµœëŒ€ 3ë°° ë” ë¹ ë¥¸ ì„œë²„&#x2F;ì²˜ë¦¬ëŸ‰ ë°°ì¹˜ë¥¼ ì‹¤í˜„</li>
<li>LLM CompressorëŠ” vLLMê³¼ ì™„ë²½í•˜ê²Œ í†µí•©ë˜ì–´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë©°, AI ì—°êµ¬ìì™€ ê¸°ì—…ë“¤ì—ê²Œ ìµœì í™”ëœ ëª¨ë¸ ìƒì„± ë° ì‚¬ìš©ì„ ì§€ì›</li>
</ul>
<h3 id="KBê¸ˆìœµê·¸ë£¹-AI-ì›Œì‹±-ê²½ê³ "><a href="#KBê¸ˆìœµê·¸ë£¹-AI-ì›Œì‹±-ê²½ê³ " class="headerlink" title="KBê¸ˆìœµê·¸ë£¹, AI ì›Œì‹± ê²½ê³ "></a>KBê¸ˆìœµê·¸ë£¹, AI ì›Œì‹± ê²½ê³ </h3><p><a target="_blank" rel="noopener" href="https://www.kbfg.com/kbresearch/report/reportView.do?reportId=2000501">ë§í¬</a>, 2024ë…„ 8ì›” 19ì¼</p>
<ul>
<li>KBê¸ˆìœµê·¸ë£¹ì€ AI ê¸°ìˆ ì„ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ì„œ ë§ˆì¹˜ ì‚¬ìš©í•˜ëŠ” ê²ƒì²˜ëŸ¼ í™ë³´í•˜ëŠ” â€˜AI ì›Œì‹±â€™ì˜ ìœ„í—˜ì„±ì„ ê²½ê³ í•¨</li>
<li>AI ì›Œì‹±ì€ ì†Œë¹„ì ì‹ ë¢° ì €í•˜, íˆ¬ì ìì›ì˜ ë‚­ë¹„, ê³¼ë„í•œ ê¸°ëŒ€ ìœ ë°œ ë“±ì˜ ë¶€ì‘ìš©ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŒ</li>
<li>ì•„ë§ˆì¡´ì˜ ë¬´ì¸ ë§¤ì¥ â€˜ì•„ë§ˆì¡´ê³ â€™ì™€ ì±„ìš© ìŠ¤íƒ€íŠ¸ì—… â€˜ì¤€ì½”â€™ ë“± ë‹¤ìˆ˜ì˜ ì‚¬ë¡€ê°€ AI ì›Œì‹±ìœ¼ë¡œ ì§€ì ë¨</li>
<li>AI ì›Œì‹±ì— ëŒ€í•œ ê·œì œë¥¼ ê°•í™”í•˜ê³ , íˆ¬ëª…í•œ ê¸°ìˆ  ì‚¬ìš©ê³¼ ì •ë³´ ì œê³µì´ í•„ìš”í•˜ë‹¤ê³  ê°•ì¡°</li>
<li>ì†Œë¹„ìì™€ íˆ¬ììë“¤ì—ê²ŒëŠ” AI ê¸°ìˆ ì— ëŒ€í•œ ë¹„íŒì  ì‹œê°ê³¼ íˆ¬ëª…í•œ ì„¤ëª…ì„ ìš”êµ¬í•  ê²ƒì„ ê¶Œì¥</li>
</ul>
<p>ì´ìƒ ì˜¤ëŠ˜ì˜ AI ì†Œì‹ì´ì—ˆìŠµë‹ˆë‹¤.</p>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹"><a href="#todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹" class="headerlink" title="(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,"></a>(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAIì—ì„œëŠ” ~~~ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<p><a href="link">ë§í¬</a>, date,</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://blogs.nvidia.com/blog/mistral-nemo-minitron-8b-small-language-model/</span><br><span class="line">Lightweight Champ: NVIDIA Releases Small Language Model With State-of-the-Art Accuracy</span><br><span class="line">Mistral-NeMo-Minitron 8B is a miniaturized version of the recently released Mistral NeMo 12B model, delivering high accuracy combined with the compute efficiency to run the model across GPU-accelerated data centers, clouds and workstations.</span><br><span class="line">August 21, 2024 by Kari Briski</span><br><span class="line"> Share</span><br><span class="line"></span><br><span class="line">Developers of generative AI typically face a tradeoff between model size and accuracy. But a new language model released by NVIDIA delivers the best of both, providing state-of-the-art accuracy in a compact form factor.</span><br><span class="line"></span><br><span class="line">Mistral-NeMo-Minitron 8B â€” a miniaturized version of the open Mistral NeMo 12B model released by Mistral AI and NVIDIA last month â€” is small enough to run on an NVIDIA RTX-powered workstation while still excelling across multiple benchmarks for AI-powered chatbots, virtual assistants, content generators and educational tools. Minitron models are distilled by NVIDIA using NVIDIA NeMo, an end-to-end platform for developing custom generative AI.</span><br><span class="line"></span><br><span class="line">â€œWe combined two different AI optimization methods â€” pruning to shrink Mistral NeMoâ€™s 12 billion parameters into 8 billion, and distillation to improve accuracy,â€ said Bryan Catanzaro, vice president of applied deep learning research at NVIDIA. â€œBy doing so, Mistral-NeMo-Minitron 8B delivers comparable accuracy to the original model at lower computational cost.â€</span><br><span class="line"></span><br><span class="line">Unlike their larger counterparts, small language models can run in real time on workstations and laptops. This makes it easier for organizations with limited resources to deploy generative AI capabilities across their infrastructure while optimizing for cost, operational efficiency and energy use. Running language models locally on edge devices also delivers security benefits, since data doesnâ€™t need to be passed to a server from an edge device.</span><br><span class="line"></span><br><span class="line">Developers can get started with Mistral-NeMo-Minitron 8B packaged as an NVIDIA NIM microservice with a standard application programming interface (API) â€” or they can download the model from Hugging Face. A downloadable NVIDIA NIM, which can be deployed on any GPU-accelerated system in minutes, will be available soon.</span><br><span class="line"></span><br><span class="line">State-of-the-Art for 8 Billion Parameters</span><br><span class="line">For a model of its size, Mistral-NeMo-Minitron 8B leads on nine popular benchmarks for language models. These benchmarks cover a variety of tasks including language understanding, common sense reasoning, mathematical reasoning, summarization, coding and ability to generate truthful answers.</span><br><span class="line"></span><br><span class="line">Packaged as an NVIDIA NIM microservice, the model is optimized for low latency, which means faster responses for users, and high throughput, which corresponds to higher computational efficiency in production.</span><br><span class="line"></span><br><span class="line">In some cases, developers may want an even smaller version of the model to run on a smartphone or an embedded device like a robot. To do so, they can download the 8-billion-parameter model and, using NVIDIA AI Foundry, prune and distill it into a smaller, optimized neural network customized for enterprise-specific applications.</span><br><span class="line"></span><br><span class="line">The AI Foundry platform and service offers developers a full-stack solution for creating a customized foundation model packaged as a NIM microservice. It includes popular foundation models, the NVIDIA NeMo platform and dedicated capacity on NVIDIA DGX Cloud. Developers using NVIDIA AI Foundry can also access NVIDIA AI Enterprise, a software platform that provides security, stability and support for production deployments.</span><br><span class="line"></span><br><span class="line">Since the original Mistral-NeMo-Minitron 8B model starts with a baseline of state-of-the-art accuracy, versions downsized using AI Foundry would still offer users high accuracy with a fraction of the training data and compute infrastructure.</span><br><span class="line"></span><br><span class="line">Harnessing the Perks of Pruning and Distillation</span><br><span class="line">To achieve high accuracy with a smaller model, the team used a process that combines pruning and distillation. Pruning downsizes a neural network by removing model weights that contribute the least to accuracy. During distillation, the team retrained this pruned model on a small dataset to significantly boost accuracy, which had decreased through the pruning process.</span><br><span class="line"></span><br><span class="line">The end result is a smaller, more efficient model with the predictive accuracy of its larger counterpart.</span><br><span class="line"></span><br><span class="line">This technique means that a fraction of the original dataset is required to train each additional model within a family of related models, saving up to 40x the compute cost when pruning and distilling a larger model compared to training a smaller model from scratch.</span><br><span class="line"></span><br><span class="line">Read the NVIDIA Technical Blog and a technical report for details.</span><br><span class="line"></span><br><span class="line">NVIDIA also announced this week Nemotron-Mini-4B-Instruct, another small language model optimized for low memory usage and faster response times on NVIDIA GeForce RTX AI PCs and laptops. The model is available as an NVIDIA NIM microservice for cloud and on-device deployment and is part of NVIDIA ACE, a suite of digital human technologies that provide speech, intelligence and animation powered by generative AI.</span><br><span class="line"></span><br><span class="line">Experience both models as NIM microservices from a browser or an API at ai.nvidia.com.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Nvidia just dropped Mistral NeMo Minitron 8B - Distilled + pruned from 12B, commercially permissive license, and beats the teacher (12B) on multiple benchmarks!</span><br><span class="line">&gt; Achieves similar benchmarks as Mistral NeMo 12B, beats Llama 3.1 8B</span><br><span class="line">&gt; MMLU - L3.1 8B (65), NeMo Minitron 8B (69.5), NeMo 12B (69)</span><br><span class="line">&gt; HumanEval - L3.1 8B (24.75), NeMo Minitron 8B (36.2), NeMo 12B (23.7)</span><br><span class="line">&gt; Trained on 380B tokens</span><br><span class="line">&gt; Iterative pruning and distillation</span><br><span class="line">&gt; Width-only pruning - pruned both embedding + MLP hidden representations</span><br><span class="line">&gt; Pruned MLP intermediate dimension from 14336 to 11520 and hidden size from 5120 to 4096</span><br><span class="line">&gt; Retain the attention head count and number of layers</span><br><span class="line">&gt; Works out of the box with Transformers ğŸ¤—</span><br><span class="line">This is a solid base model for further fine-tuning, and task specific use-cases, pretty much the strongest &lt;10B range - which makes it easy to deploy across variety of cheaper GPUs.</span><br><span class="line">8B =&gt; bf16 (16 GB), fp8/ 8 bit (8 GB), 4 bit (4GB)</span><br><span class="line">Perfect to deploy on a L4 :D</span><br><span class="line">Kudos to Nvidia for releasing their research and the model weights! ğŸ¤—</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251</span><br><span class="line">ğ—”ğ—œğŸ®ğŸ­ ğ—¿ğ—²ğ—¹ğ—²ğ—®ğ˜€ğ—²ğ˜€ ğ—»ğ—²ğ˜„ ğ—ğ—®ğ—ºğ—¯ğ—® ğŸ­.ğŸ± ğ—ºğ—¼ğ—±ğ—²ğ—¹ğ˜€: ğ—¡ğ—²ğ˜„ ğ˜€ğ˜ğ—®ğ—»ğ—±ğ—®ğ—¿ğ—± ğ—³ğ—¼ğ—¿ ğ—¹ğ—¼ğ—»ğ—´-ğ—°ğ—¼ğ—»ğ˜ğ—²ğ˜…ğ˜ ğ˜‚ğ˜€ğ—²-ğ—°ğ—®ğ˜€ğ—²ğ˜€!ğŸ…</span><br><span class="line">AI21 Labs used a different architecture to beat the status-quo Transformers models: Jamba architecture combines classic Transformers layers with the new Mamba layers, for which the complexity is a linear (instead of quadratic) function of the context length.</span><br><span class="line">What does this imply?</span><br><span class="line">The Jamba 1.5 Open Model Family: The Most Powerful and Efficient Long Context Models</span><br><span class="line">August 22, 2024</span><br><span class="line">Blog posts</span><br><span class="line">The new family of open models from AI21, offering unrivaled speed, efficiency, and quality and the longest context window among open models.</span><br><span class="line"></span><br><span class="line">Today, we are debuting the Jamba 1.5 family of open models: Jamba 1.5 Mini and Jamba 1.5 Large. Built on our novel SSM-Transformer architecture, these models demonstrate superior long context handling, speed, and qualityâ€”outranking competitors in their size class and marking the first time a non-Transformer model has been successfully scaled to the quality and strength of the marketâ€™s leading models.</span><br><span class="line"></span><br><span class="line">We are releasing these models under the Jamba Open Model License, upholding our commitment to democratizing access to quality models and opening the door to further experimentation.</span><br><span class="line"></span><br><span class="line">Todayâ€™s language models are impressive in their capabilitiesâ€”but too often fail to deliver real value for businesses.</span><br><span class="line"></span><br><span class="line">At AI21, we are on a mission to change this by designing AI systems that are purpose-built for the enterprise. These models are built keeping in mind the key measures large businesses care most about when it comes to GenAI implementation: resource efficiency, quality, speed, and ability to actually solve critical tasks.</span><br><span class="line">â€</span><br><span class="line"></span><br><span class="line">Long context handling: With a 256K effective context window, the longest in the market, Jamba 1.5 models can improve the quality of key enterprise applications, such as lengthy document summarization and analysis, as well as agentic and RAG workflows</span><br><span class="line">Speed: Up to 2.5X faster on long contexts and fastest across all context lengths in their size class</span><br><span class="line">Quality: Jamba 1.5 Mini is the strongest open model in its size class with a score of 46.1 on the Arena Hard benchmark, surpassing larger models like Mixtral 8x22B and Command-R+. Jamba 1.5 Large, with a score of 65.4, outpaces both Llama 3.1 70B and 405B</span><br><span class="line">Multilingual: In addition to English, the models support Spanish, French, Portuguese, Italian, Dutch, German, Arabic and Hebrew</span><br><span class="line">Developer ready: Jamba natively supports structured JSON output, function calling, digesting document objects, and generating citations</span><br><span class="line">Open for builders: Both models are available for immediate download on Hugging Face (and coming soon to leading frameworks LangChain and LlamaIndex)</span><br><span class="line">Deploy anywhere: In addition to AI21 Studio, the models are available on cloud partners Google Cloud Vertex AI, Microsoft Azure, and NVIDIA NIM and coming soon to Amazon Bedrock, Databricks Marketplace, Snowflake Cortex, Together.AI as well as for private on-prem and VPC deployment</span><br><span class="line">Resource-efficient hybrid architecture</span><br><span class="line">Jamba 1.5 Large and Mini are built on the novel SSM-Transformer Jamba architecture, which weaves together Transformerâ€™s outstanding quality with Mambaâ€™s groundbreaking efficiency.</span><br><span class="line"></span><br><span class="line">As a result, the models offer a lower memory footprint than competitors, allowing clients to handle context lengths up to 140K tokens on a single GPU using Jamba 1.5 Mini. The same advantage also makes fine-tuning over long contexts easier and more accessible than with transformer-based models. Thanks to this efficiency-optimized architecture, our models can deliver top quality and speed without skyrocketing costs.</span><br><span class="line"></span><br><span class="line">Like all models in its size class, Jamba 1.5 Large canâ€™t be loaded in full (FP32) or half (FP16/BF16) precision on a single node of 8 GPUs. Dissatisfied with currently available quantization techniques, we developed ExpertsInt8, a novel quantization technique tailored for MoE models.</span><br><span class="line"></span><br><span class="line">With ExpertsInt8, we only quantize weights that are parts of the MoE (or MLP) layers, which for many MoE models account for over 85% of the model weights. In our implementation, we quantize and save these weights in INT8, an 8-bit precision format, and dequantize them at runtime directly inside the MoE GPU kernel.</span><br><span class="line"></span><br><span class="line">This technique offers four advantages: It is fast, with quantization taking up to just a few minutes; it does not rely on calibration, a sometimes unstable process which ordinarily can take hours or days; it can still use BF16 to hold large activations; and, importantly, it allows Jamba 1.5 Large to fit on a single 8 GPU node, while utilizing its full context length of 256K. In our experiments, ExpertsInt8 proved to have the lowest latency of all vLLM quantization techniques for MoE models, without a loss in quality.</span><br><span class="line"></span><br><span class="line">â€</span><br><span class="line"></span><br><span class="line">Long context that actually delivers</span><br><span class="line">The 256K context window offered by the Jamba 1.5 models is not only the longest amongst open models, but also the only one to back this claim on the RULER benchmark.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Whereas most other models claim a long context window but fail to sustain the same quality of performance at the upper limits of their context window, the Jamba 1.5 family maintains its long context handling throughout the entire span of its 256K context window.</span><br><span class="line"></span><br><span class="line">A model that can effectively handle long context is crucial for almost every enterprise scale GenAI application. In addition to thoroughly and precisely summarizing and analyzing lengthy documents, a long context model substantially improves the quality of RAG and agentic workflowsâ€”and reduces their costâ€”by eliminating the need for continuous chunking and repetitive retrievals.</span><br><span class="line"></span><br><span class="line">While itâ€™s sometimes claimed that RAG is a substitute for long context, a successful enterprise AI system needs both. In pairing long context and RAG, the long context model improves the quality and cost-efficiency of RAGâ€™s retrieval stage at scale.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Fastest on the market</span><br><span class="line">For the use cases enterprises are interested in, such as customer support agent assistants and chatbots, rapid turnaround is essential. The model needs to be able to keep pace with the scale of operations, even as usage requests and batch sizes increase.</span><br><span class="line"></span><br><span class="line">Both Jamba 1.5 models are faster than competitors of a similar size, with up to 2.5X faster inference on long contexts, offering customers major cost, quality, and speed gains under high utilization when deployed in their own environment.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The Jamba 1.5 Mini comparisons were done over 2xA100 80GB GPUs and the Jamba 1.5 Large comparisons were done over 8xA100 80GB GPUs. The test was performed on vLLM, with batch_size=1, output_tokens=512, input_tokens=(context_length-512)</span><br><span class="line">Jamba 1.5 Mini and Jamba 1.5 Large show excellent speed and throughput results in tests run by Artificial Analysis, as can be seen in the chart below, with Jamba 1.5 Mini ranking as the fastest model on 10K contexts.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Output tokens per second on 10K contexts, as independently tested by Artificial Analysis.</span><br><span class="line">Outstanding quality across the board</span><br><span class="line">As measured on the Arena Hard benchmark, Jamba 1.5 Mini emerges as the strongest model in its size class, outshining competitors Claude 3 Haiku, Mixtral 8x22B and Command-R+. Jamba 1.5 Large similarly rises above leading models like Claude 3 Opus, Llama 3.1 70B, and Llama 3.1 405B, offering excellent value per cost for its size class.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Getting started</span><br><span class="line">Build with Jamba 1.5 Mini or Jamba 1.5 Large wherever you like to work. The models are available on the following platforms and cloud partners:</span><br><span class="line">â€</span><br><span class="line"></span><br><span class="line">AI21 Studio</span><br><span class="line">Google Cloud Vertex AI</span><br><span class="line">Hugging Face</span><br><span class="line">Microsoft Azure</span><br><span class="line">NVIDIA NIM</span><br><span class="line">And coming soon to Amazon Bedrock, Databricks Marketplace, LangChain, LlamaIndex, Snowflake Cortex, and Together.AI.</span><br><span class="line"></span><br><span class="line">For customers who wish to avoid a lengthy experimentation process and keep their data onsite, we offer private deployments and custom models. In this white-glove service, we tailor our models exactly to your needs and use case through continuous pre-training and fine-tuning so you can move more quickly from ideation to production.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://jina.ai/news/late-chunking-in-long-context-embedding-models/</span><br><span class="line">August 23, 2024</span><br><span class="line">Late Chunking in Long-Context Embedding Models</span><br><span class="line">Chunking long documents while preserving contextual information is challenging. We introduce the &quot;Late Chunking&quot; that leverages long-context embedding models to generate contextual chunk embeddings for better retrieval applications.</span><br><span class="line">Diagram illustrating the &#x27;Late Chunking&#x27; and &#x27;Long Document Model&#x27; processes in machine learning on a black background.</span><br><span class="line">Michael GÃ¼nther</span><br><span class="line">Han Xiao</span><br><span class="line">Michael GÃ¼nther, Han Xiao â€¢ 8 minutes read</span><br><span class="line">About a year ago, in October 2023, we released the world&#x27;s first open-source embedding model with an 8K context length, jina-embeddings-v2-base-en. Since then, there has been quite some debate about the usefulness of long-context in embedding models. For many applications, encoding a document thousands of words long into a single embedding representation is not ideal. Many use cases require retrieving smaller portions of the text, and dense vector-based retrieval systems often perform better with smaller text segments, as the semantics are less likely to be &quot;over-compressed&quot; in the embedding vectors.</span><br><span class="line"></span><br><span class="line">Retrieval-Augmented Generation (RAG) is one of the most well-known applications that requires splitting documents into smaller text chunks (say within 512 tokens). These chunks are usually stored in a vector database, with vector representations generated by a text embedding model. During runtime, the same embedding model encodes a query into a vector representation, which is then used to identify relevant stored text chunks. These chunks are subsequently passed to a large language model (LLM), which synthesizes a response to the query based on the retrieved texts.</span><br><span class="line"></span><br><span class="line">Flowchart detailing a query processing system, starting from &quot;Query&quot; to &quot;Document Chunks&quot; and &quot;Embedding Model,&quot; then to &quot;Vec</span><br><span class="line">A typical RAG pipeline of chunking-embedding-retrieving-generating.</span><br><span class="line">In short, embedding smaller chunks seems to be more preferable, partly due to the limited input sizes of downstream LLMs, but also because thereâ€™s a concern that important contextual information in a long context may get diluted when compressed into a single vector.</span><br><span class="line"></span><br><span class="line">But if the industry only ever needs embedding models with a 512-context length, whatâ€™s the point of training models with an 8192-context length at all?</span><br><span class="line"></span><br><span class="line">In this article, we revisit this important, albeit uncomfortable, question by exploring the limitations of the naive chunking-embedding pipeline in RAG. We introduce a new approach called &quot;Late Chunking,&quot; which leverages the rich contextual information provided by 8192-length embedding models to more effectively embed chunks.</span><br><span class="line"></span><br><span class="line">The Lost Context Problem</span><br><span class="line">The simple RAG pipeline of chunking-embedding-retrieving-generating is not without its challenges. Specifically, this process can destroy long-distance contextual dependencies. In other words, when relevant information is spread across multiple chunks, taking text segments out of context can render them ineffective, making this approach particularly problematic.</span><br><span class="line"></span><br><span class="line">In the image below, a Wikipedia article is split into chunks of sentences. You can see that phrases like &quot;its&quot; and &quot;the city&quot; reference &quot;Berlin,&quot; which is mentioned only in the first sentence. This makes it harder for the embedding model to link these references to the correct entity, thereby producing a lower-quality vector representation.</span><br><span class="line"></span><br><span class="line">Comparative panels display Berlin&#x27;s Wikipedia article and its chunked text to highlight clarity and readability benefits.</span><br><span class="line">This means, if we split a long article into sentence-length chunks, as in the example above, a RAG system might struggle to answer a query like &quot;What is the population of Berlin?&quot; Because the city name and the population never appear together in a single chunk, and without any larger document context, an LLM presented with one of these chunks cannot resolve anaphoric references like &quot;it&quot; or &quot;the city.&quot;</span><br><span class="line"></span><br><span class="line">There are some heuristics to alleviate this issue, such as resampling with a sliding window, using multiple context window lengths, and performing multi-pass document scans. However, like all heuristics, these approaches are hit-or-miss; they may work in some cases, but there&#x27;s no theoretical guarantee of their effectiveness.</span><br><span class="line"></span><br><span class="line">The Solution: Late Chunking</span><br><span class="line">The naive encoding approach (as seen on the left side of the image below) involves using sentences, paragraphs, or maximum length limits to split the text a priori. Afterward, an embedding model is repetitively applied to these resulting chunks. To generate a single embedding for each chunk, many embedding models use mean pooling on these token-level embeddings to output a single embedding vector.</span><br><span class="line"></span><br><span class="line">Flowchart comparing naive and late chunking methods in document processing with labeled steps and embeddings.</span><br><span class="line">An illustration of the naive chunking strategy (left) and the late chunking strategy (right).</span><br><span class="line">In contrast, the &quot;Late Chunking&quot; approach we propose in this article first applies the transformer layer of the embedding model to the entire text or as much of it as possible. This generates a sequence of vector representations for each token that encompasses textual information from the entire text. Subsequently, mean pooling is applied to each chunk of this sequence of token vectors, yielding embeddings for each chunk that consider the entire text&#x27;s context. Unlike the naive encoding approach, which generates independent and identically distributed (i.i.d.) chunk embeddings, late chunking creates a set of chunk embeddings where each one is &quot;conditioned on&quot; the previous ones, thereby encoding more contextual information for each chunk.</span><br><span class="line"></span><br><span class="line">Obviously to effectively apply late chunking, we need long-context embedding models like jina-embeddings-v2-base-en, which support up to 8192 tokensâ€”roughly ten standard pages of text. Text segments of this size are much less likely to have contextual dependencies that require an even longer context to resolve.</span><br><span class="line"></span><br><span class="line">It&#x27;s important to highlight that late chunking still requires boundary cues, but these cues are used only after obtaining the token-level embeddingsâ€”hence the term &quot;late&quot; in its naming.</span><br><span class="line"></span><br><span class="line">Naive Chunking	Late Chunking</span><br><span class="line">The need of boundary cues	Yes	Yes</span><br><span class="line">The use of boundary cues	Directly in preprocessing	After getting the token-level embeddings from the transformer layer</span><br><span class="line">The resulting chunk embeddings	i.i.d.	Conditional</span><br><span class="line">Contextual information of nearby chunks	Lost. Some heuristics (like overlap sampling) to alleviate this	Well-preserved by long-context embedding models</span><br><span class="line">Implementation and Qualitative Evaluation</span><br><span class="line">Google Colab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The implementation of late chunking can be found in the Google Colab linked above. Here, we utilize our recent feature release in the Tokenizer API, which leverages all possible boundary cues to segment a long document into meaningful chunks. More discussion on the algorithm behind this feature can be found on X.</span><br><span class="line"></span><br><span class="line">Tokenizer API</span><br><span class="line">Free API to tokenize text and segment long text into chunks.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">When applying late chunking to the Wikipedia example above, you can immediately see an improvement in semantic similarity. For instance, in the case of &quot;the city&quot; and &quot;Berlin&quot; within a Wikipedia article, the vectors representing &quot;the city&quot; now contain information linking it to the previous mention of &quot;Berlin,&quot; making it a much better match for queries involving that city name.</span><br><span class="line"></span><br><span class="line">Query	Chunk	Sim. on naive chunking	Sim. on late chunking</span><br><span class="line">Berlin	Berlin is the capital and largest city of Germany, both by area and by population.	0.849	0.850</span><br><span class="line">Berlin	Its more than 3.85 million inhabitants make it the European Union&#x27;s most populous city, as measured by population within city limits.	0.708	0.825</span><br><span class="line">Berlin	The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.	0.753	0.850</span><br><span class="line">You can observe this in the numerical results above, which compare the embedding of the term &quot;Berlin&quot; to various sentences from the article about Berlin using cosine similarity. The column &quot;Sim. on IID chunk embeddings&quot; shows the similarity values between the query embedding of &quot;Berlin&quot; and the embeddings using a priori chunking, while &quot;Sim. under contextual chunk embedding&quot; represents the results with late chunking method.</span><br><span class="line"></span><br><span class="line">Quantitative Evaluation on BEIR</span><br><span class="line">To verify the effectiveness of late chunking beyond a toy example, we tested it using some of the retrieval benchmarks from BeIR. These retrieval tasks consist of a query set, a corpus of text documents, and a QRels file that stores information about the IDs of documents relevant to each query.</span><br><span class="line"></span><br><span class="line">To identify the relevant documents for a query, the documents are chunked, encoded into an embedding index, and the most similar chunks are determined for each query embedding using k-nearest neighbors (kNN). Since each chunk corresponds to a document, the kNN ranking of chunks can be converted into a kNN ranking of documents (retaining only the first occurrence for documents appearing multiple times in the ranking). This resulting ranking is then compared to the ranking provided by the ground-truth QRels file, and retrieval metrics like nDCG@10 are calculated. This procedure is depicted below, and the evaluation script can be found in this repository for reproducibility.</span><br><span class="line"></span><br><span class="line">GitHub - jina-ai/late-chunking: Code for explaining and evaluating late chunking (chunked pooling)</span><br><span class="line">Code for explaining and evaluating late chunking (chunked pooling) - jina-ai/late-chunking</span><br><span class="line"></span><br><span class="line">GitHub</span><br><span class="line">jina-ai</span><br><span class="line"></span><br><span class="line">We ran this evaluation on various BeIR datasets, comparing naive chunking with our late chunking method. For getting the boundary cues, we used a regex that splits the texts into strings of roughly 256 tokens. Both the naive and late chunking evaluation used jina-embeddings-v2-small-en as the embedding model; a smaller version of the v2-base-en model that still supports up to 8192-token length. Results can be found in the table below.</span><br><span class="line"></span><br><span class="line">Dataset	Avg. Document Length (characters)	Naive Chunking (nDCG@10)	Late Chunking (nDCG@10)	No Chunking (nDCG@10)</span><br><span class="line">SciFact	1498.4	64.20%	66.10%	63.89%</span><br><span class="line">TRECCOVID	1116.7	63.36%	64.70%	65.18%</span><br><span class="line">FiQA2018	767.2	33.25%	33.84%	33.43%</span><br><span class="line">NFCorpus	1589.8	23.46%	29.98%	30.40%</span><br><span class="line">Quora	62.2	87.19%	87.19%	87.19%</span><br><span class="line">In all cases, late chunking improved the scores compared to the naive approach. In some instances, it also outperformed encoding the entire document into a single embedding, while in other datasets, not chunking at all yielded the best results (Of course, no chunking only makes sense if there is no need to rank chunks, which is rare in practice). If we plot the performance gap between the naive approach and late chunking against document length, it becomes evident that the average length of the documents correlates with greater improvements in nDCG scores through late chunking. In other words, the longer the document, the more effective the late chunking strategy becomes.</span><br><span class="line"></span><br><span class="line">Line graph showing the decline in relative improvement with increasing document length, from 0 to 1500 characters.</span><br><span class="line">Late chunking&#x27;s improvement over naive chunking is correlated with the avg. document length.</span><br><span class="line">Conclusion</span><br><span class="line">In this article, we introduced a simple approach called &quot;late chunking&quot; to embed short chunks by leveraging the power of long-context embedding models. We demonstrated how traditional i.i.d. chunk embedding fails to preserve contextual information, leading to suboptimal retrieval; and how late chunking offers a simple yet highly effective solution to maintain and condition contextual information within each chunk. The effectiveness of late chunking becomes increasingly significant on longer documentsâ€”a capability made possible only by advanced long-context embedding models like jina-embeddings-v2-base-en. We hope this work not only validates the importance of long-context embedding models but also inspires further research on this topic.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/OpenMeditron</span><br><span class="line"></span><br><span class="line">ğŸâ›‘ï¸Itâ€™s hereâ€¦meet #Meditron3</span><br><span class="line">â€¦and an opportunity to join the upcoming publication as a clinical expert by critically evaluating Meditron ğŸ¤–ğŸ¤º in the #MOOVE: a Massive Open Online Validation and Evaluation platform)</span><br><span class="line">Meditron is the latest update to our suite of state-of-the-art open Large Medical Language Models trained from #Llama3point1 [8B] and [70B]</span><br><span class="line">---</span><br><span class="line">ğŸ’¯#OPEN: weights released here ğŸ”—</span><br><span class="line">https://lnkd.in/eF8q9A94</span><br><span class="line"></span><br><span class="line">ğŸ¤#CODESIGNED: from day one with an interdisciplinary team of practicing clinicians from around the world, global humanitarian organizations, AI ethicists, and data scientists</span><br><span class="line">ğŸ“Š#STATEOFTHEART: Out-performing GPT4 on the standard medical benchmarks (*see footnote)</span><br><span class="line">ğŸŒ#REPRESENTATIVE: Continually pre-trained on an enormous corpus of expert-curated open-access medical literature and global clinical practice guidelines that is specifically inclusive of low-resource and humanitarian settings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2408.12569</span><br><span class="line">Meta presents Sapiens</span><br><span class="line">Foundation for Human Vision Models</span><br><span class="line"></span><br><span class="line">We present Sapiens, a family of models for four fundamental human-centric vision tasks - 2D pose estimation, body-part segmentation, depth estimation, and surface normal prediction. Our models natively support 1K high-resolution inference and are extremely easy to adapt for individual tasks by simply fine-tuning models pretrained on over 300 million in-the-wild human images. We observe that, given the same computational budget, self-supervised pretraining on a curated dataset of human images significantly boosts the performance for a diverse set of human-centric tasks. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. Our simple model design also brings scalability - model performance across tasks improves as we scale the number of parameters from 0.3 to 2 billion. Sapiens consistently surpasses existing baselines across various human-centric benchmarks. We achieve significant improvements over the prior state-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1 mIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5% relative angular error.</span><br><span class="line">Sapiens: Foundation for Human Vision Models</span><br><span class="line">Published on Aug 23</span><br><span class="line">Â·</span><br><span class="line">Submitted by</span><br><span class="line">akhaliq</span><br><span class="line">on Aug 23</span><br><span class="line">#1 Paper of the day</span><br><span class="line">Authors:</span><br><span class="line">Rawal Khirodkar</span><br><span class="line">,</span><br><span class="line">Timur Bagautdinov</span><br><span class="line">,</span><br><span class="line">Julieta Martinez</span><br><span class="line">,</span><br><span class="line">Su Zhaoen</span><br><span class="line">,</span><br><span class="line">Austin James</span><br><span class="line">,</span><br><span class="line">Peter Selednik</span><br><span class="line">,</span><br><span class="line">Stuart Anderson</span><br><span class="line">,</span><br><span class="line">Shunsuke Saito</span><br><span class="line">Abstract</span><br><span class="line">We present Sapiens, a family of models for four fundamental human-centric vision tasks - 2D pose estimation, body-part segmentation, depth estimation, and surface normal prediction. Our models natively support 1K high-resolution inference and are extremely easy to adapt for individual tasks by simply fine-tuning models pretrained on over 300 million in-the-wild human images. We observe that, given the same computational budget, self-supervised pretraining on a curated dataset of human images significantly boosts the performance for a diverse set of human-centric tasks. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. Our simple model design also brings scalability - model performance across tasks improves as we scale the number of parameters from 0.3 to 2 billion. Sapiens consistently surpasses existing baselines across various human-centric benchmarks. We achieve significant improvements over the prior state-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1 mIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5% relative angular error.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/linkedin/Liger-Kernel</span><br><span class="line">Big Update! 20% higher throughput and 60% memory reduction for multi-GPU fine-tuning with Hugging Face  Transformers! ğŸ¤¯ The LLM research team from LinkedIn released new efficient GPU Kernels (Liger Kernels) to speed up and reduce memory when fine-tuning LLMs! ğŸš€</span><br><span class="line"></span><br><span class="line">TL;DR:</span><br><span class="line"></span><br><span class="line">ğŸš€ Boost multi-GPU training throughput by 20% and slash memory usage by 60%</span><br><span class="line"></span><br><span class="line">ğŸ’» Works with Flash Attention, PyTorch FSDP, and Microsoft DeepSpeed</span><br><span class="line"></span><br><span class="line">ğŸ§  Supported models include Llama 3, Mistral, Mixtral, Gemma 2</span><br><span class="line"></span><br><span class="line">ğŸ”§ Adds Hugging Face Compatible RMSNorm, RoPE, SwiGLU, CrossEntropy, and FusedLinearCrossEntropy</span><br><span class="line"></span><br><span class="line">ğŸ§® Exact computations with no approximations, longer context lengths, larger batch sizes</span><br><span class="line"></span><br><span class="line">âš¡ï¸ Train Meta Llama 3 8b ~20% faster with over 40% memory using 4 A100s with FSDP</span><br><span class="line"></span><br><span class="line">ğŸ”Œ Get started with pip install liger-kernel and add 1-line of code</span><br><span class="line"></span><br><span class="line">ğŸŒŸ Open-source and community-driven development</span><br><span class="line"></span><br><span class="line">Github: https://lnkd.in/e4XKs4-F</span><br><span class="line">LinkedIn</span><br><span class="line">This link will take you to a page thatâ€™s not on LinkedIn</span><br><span class="line"> [2024/8/31] CUDA MODE talk, Liger-Kernel: Real-world Triton kernel for LLM Training</span><br><span class="line">[2024/8/23] Official release: check out our X post</span><br><span class="line">Liger (Linkedin GPU Efficient Runtime) Kernel is a collection of Triton kernels designed specifically for LLM training. It can effectively increase multi-GPU training throughput by 20% and reduces memory usage by 60%. We have implemented Hugging Face Compatible RMSNorm, RoPE, SwiGLU, CrossEntropy, FusedLinearCrossEntropy, and more to come. The kernel works out of the box with Flash Attention, PyTorch FSDP, and Microsoft DeepSpeed. We welcome contributions from the community to gather the best kernels for LLM training.</span><br><span class="line"></span><br><span class="line">Supercharge Your Model with Liger Kernel</span><br><span class="line">Banner</span><br><span class="line"></span><br><span class="line">With one line of code, Liger Kernel can increase throughput by more than 20% and reduce memory usage by 60%, thereby enabling longer context lengths, larger batch sizes, and massive vocabularies.</span><br><span class="line"></span><br><span class="line">Speed Up	Memory Reduction</span><br><span class="line">Speed up	Memory</span><br><span class="line">Note:</span><br><span class="line"></span><br><span class="line">Benchmark conditions: LLaMA 3-8B, Batch Size = 8, Data Type = bf16, Optimizer = AdamW, Gradient Checkpointing = True, Distributed Strategy = FSDP1 on 8 A100s.</span><br><span class="line">Hugging Face models start to OOM at a 4K context length, whereas Hugging Face + Liger Kernel scales up to 16K.</span><br><span class="line">Examples</span><br><span class="line">Basic</span><br><span class="line">Example	Description	Lightning Studio</span><br><span class="line">Hugging Face Trainer	Train LLaMA 3-8B ~20% faster with over 40% memory reduction on Alpaca dataset using 4 A100s with FSDP	TBA</span><br><span class="line">Lightning Trainer	Increase 15% throughput and reduce memory usage by 40% with LLaMA3-8B on MMLU dataset using 8 A100s with DeepSpeed ZeRO3	TBA</span><br><span class="line">Advanced</span><br><span class="line">Example	Description	Lightning Studio</span><br><span class="line">Medusa Multi-head LLM (Retraining Phase)	Reduce memory usage by 80% with 5 LM heads and improve throughput by 40% using 8 A100s with FSDP	TBA</span><br><span class="line">Key Features</span><br><span class="line">Ease of use: Simply patch your Hugging Face model with one line of code, or compose your own model using our Liger Kernel modules.</span><br><span class="line">Time and memory efficient: In the same spirit as Flash-Attn, but for layers like RMSNorm, RoPE, SwiGLU, and CrossEntropy! Increases multi-GPU training throughput by 20% and reduces memory usage by 60% with kernel fusion, in-place replacement, and chunking techniques.</span><br><span class="line">Exact: Computation is exactâ€”no approximations! Both forward and backward passes are implemented with rigorous unit tests and undergo convergence testing against training runs without Liger Kernel to ensure accuracy.</span><br><span class="line">Lightweight: Liger Kernel has minimal dependencies, requiring only Torch and Tritonâ€”no extra libraries needed! Say goodbye to dependency headaches!</span><br><span class="line">Multi-GPU supported: Compatible with multi-GPU setups (PyTorch FSDP, DeepSpeed, DDP, etc.).</span><br><span class="line">Target Audiences</span><br><span class="line">Researchers: Looking to compose models using efficient and reliable kernels for frontier experiments.</span><br><span class="line">ML Practitioners: Focused on maximizing GPU training efficiency with optimal, high-performance kernels.</span><br><span class="line">Curious Novices: Eager to learn how to write reliable Triton kernels to enhance training efficiency.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://neuralmagic.com/blog/llm-compressor-is-here-faster-inference-with-vllm/</span><br><span class="line">ğŸ“¢ Introducing LLM Compressor: SOTA open-sourced framework for compressing LLMs (including Llamas)! ğŸ“¢</span><br><span class="line">Starting as an internal research project at Neural Magic, I could never imagine that nearly six years later, what I called neuralmagicML would evolve into the incredibly powerful and capable LLM Compressor framework. Thanks to the hard work of our incredible engineering team, this tool can compress LLMs of any size to remarkable levels while recovering full accuracy. (such as our recent Llama 3.1 405B results!)</span><br><span class="line">We are thrilled to announce that we have donated this library and its innovative techniques to the vLLM community. By offering efficient, performant, and accurate solutions for large language models, we aim to empower researchers, hackers, and enterprises.</span><br><span class="line">Why This Matters:</span><br><span class="line">- Cutting-Edge Algorithms: Implement the latest techniques and best practices for top-tier model performance without extensive research.</span><br><span class="line">- Superior Flexibility: Enjoy various compression techniques, quantization schemes, and sparsity options for a solution that fits your use cases.</span><br><span class="line">- Community-Driven: This tool is open-sourced and seamlessly integrated with vLLM and Hugging Face to ensure compatibility and encourage future contributions.</span><br><span class="line">We are excited to witness how the community will leverage this incredible tool! Dive deeper into LLM Compressor by exploring our blog and the repo:</span><br><span class="line"></span><br><span class="line">Aug 14, 2024</span><br><span class="line"></span><br><span class="line">Announcing LLM Compressor</span><br><span class="line">We are excited to announce LLM Compressor, a unified library for creating compressed models for faster inference with vLLM. Neural Magic&#x27;s research team has successfully utilized it to create our latest compressed models, including fully quantized and accurate versions of Llama 3.1, and with that, we are excited to open up the toolkit to the community with its first 0.1 release for general usage to compress your models!</span><br><span class="line"></span><br><span class="line">LLM Compressor architecture diagram.</span><br><span class="line">In recent months, the high-performance computing team at Neural Magic has brought performant inference for various quantization schemes to vLLM, including custom Marlin kernels for weight-only quantization and custom CUTLASS kernels for INT8 and FP8 activation quantization.</span><br><span class="line"></span><br><span class="line">However, before today, creating quantized checkpoints required navigating a fragmented ecosystem of bespoke compression libraries such as AutoGPTQ, AutoAWQ, AutoFP8, etc. We built LLM Compressor from the ground up as a single library for applying the latest compression best practices, including GPTQ, SmoothQuant, SparseGPT, and RTN, with many more actively being added. It works natively with Hugging Face models for seamless ease of use in the open-source ecosystem, and vLLM supports directly loading checkpoints from LLM Compressor for accelerated inference.</span><br><span class="line"></span><br><span class="line">Using LLM Compressor, you can create compressed, accurate versions of your models, including:</span><br><span class="line"></span><br><span class="line">Activation and weight quantization for up to 3X faster server/throughput deployments. This includes FP8 models using RTN for NVIDIA&#x27;s Ada Lovelace and Hopper GPUs, and INT8 models using SmoothQuant and GPTQ for Nvidia&#x27;s Turing and Ampere GPUs.</span><br><span class="line">Weight quantization for up to 4X faster latency with INT4 weight-only models using GPTQ for Nvidia&#x27;s Ampere GPUs and newer.</span><br><span class="line">Weight pruning for up to 1.5X faster general performance with 2:4, 50% sparse models utilizing SparseGPT for Nvidia&#x27;s Ampere GPUs and newer.</span><br><span class="line">Enabling Activation Quantization in vLLM</span><br><span class="line">Thanks to LLM Compressor&#x27;s flexibility, it enables a critical new feature: activation quantization.</span><br><span class="line"></span><br><span class="line">The open-source compression ecosystem thus far has focused mainly on weight-only quantization, including AutoGPTQ and AutoAWQ. Weight-only quantization enables smaller models and faster latency, but with 16-bit activations, the compute runs through the same 16-bit tensor cores as the unquantized model. This leads to slower inference for compute-heavy workloads due to the penalty of dequantizing the weights. Activation quantization, where the inputs to each layer are quantized, combined with weight quantization, enables utilization of the faster INT8 or FP8 tensor cores for the matrix multiplies, doubling the performance for compute-bound inference.</span><br><span class="line"></span><br><span class="line">Weight-only quantization often fails to deliver speed improvements in production serving deployments. These environments typically result in compute-bound workloads with minimal benefits from weight-only quantization. Activation quantization, however, offers a substantial performance boost in such high-compute scenarios and faster inference at lower queries per second (QPS). The chart below demonstrates a 1.6X speedup at 5 QPS for the INT8 weight and activation quantized model (w8a8) compared to the 16-bit baseline (w16a16), while the 4-bit weight quantized model (w4a16) shows little improvement.</span><br><span class="line"></span><br><span class="line">This chart demonstrates a 1.6X speedup at 5 QPS for the INT8 weight and activation quantized model (w8a8) compared to the 16-bit baseline (w16a16), while the 4-bit weight quantized model (w4a16) shows little improvement.</span><br><span class="line">Full replication instructions for the benchmark are available in the appendix.</span><br><span class="line">Activation Quantization Performance in vLLM</span><br><span class="line">Letâ€™s take an example of a Llama 3.1 70B running in vLLM on a 4xA100 GPU setup to see if this analysis holds up!</span><br><span class="line"></span><br><span class="line">We will compare the serving latency for three variants for Llama 3.1 70B</span><br><span class="line"></span><br><span class="line">Unquantized FP16 (w16a16):</span><br><span class="line">meta-llama/Meta-Llama-3.1-70B-Instruct</span><br><span class="line">Weight and activation quantization to INT8 (w8a8):</span><br><span class="line">neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w8a8</span><br><span class="line">Weight-only quantization to INT4 (w4a16):</span><br><span class="line">neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w4a16</span><br><span class="line">The chart below illustrates the average time to generate each new token (TPOT) across different server loads, measured in queries per second (QPS). Additionally, a deployment constraint of 5 seconds is set for the time to generate the first token (TTFT) to ensure the serving application maintains reasonable initial response times.</span><br><span class="line"></span><br><span class="line">This chart illustrates the average time to generate each new token (TPOT) across different server loads, measured in queries per second (QPS). Additionally, a deployment constraint of 5 seconds is set for the time to generate the first token (TTFT) to ensure the serving application maintains reasonable initial response times.</span><br><span class="line">Full replication instructions for the benchmark are available in the appendix.</span><br><span class="line">At low QPS, weight-only quantization offers improved latency relative to an unquantized model. However, as the server load increases and becomes compute-bound, the performance of the weight-only model levels off, matching the unquantized model. In contrast, the activation quantized model performs better under high load, supporting more queries per second before the system becomes overloaded and TTFT exceeds our limits for a responsive application.</span><br><span class="line"></span><br><span class="line">For a 70B model on an A100 system, we see that the W8A8 model achieves similar latency performance with just 2 GPUs compared to the unquantized model running with 4, meaning similar latency guarantees with half the resources!</span><br><span class="line"></span><br><span class="line">Llama 3.1 70B Time per Output Token coparing w16a16 on 4 GPUs and w8a8 on 2 GPUs.</span><br><span class="line">Full replication instructions for the benchmark are available in the appendix.</span><br><span class="line">Activation Quantization Accuracy</span><br><span class="line">vLLMâ€™s CUTLASS kernels for activation quantization offer flexible support for various schemes, allowing for a high degree of customization, including any combination of:</span><br><span class="line"></span><br><span class="line">Per-tensor or per-channel quantization for weights</span><br><span class="line">Per-tensor or per-token quantization for activations</span><br><span class="line">Symmetric or asymmetric quantized activations (for int8).</span><br><span class="line">Side note: We are doing a CUTLASS deep dive during our bi-weekly vLLM office hours on September 5, 2024. Sign up here.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">This flexibility in vLLM, combined with LLM Compressor&#x27;s advanced algorithms such as GPTQ and SmoothQuant, ensures that model accuracy is maintained even after quantization. As we can see from the model card for neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w8a8, we see a negligible drop using static per-channel weight scales and dynamic per token activation scales in comparison to the FP16 baseline on Open LLM:</span><br><span class="line"></span><br><span class="line">Evaluation scores for Meta Llama 3.1 70B model after optimizations with the LLM Compressor.</span><br><span class="line">This combination of fine-grained quantization and sophisticated algorithms enables users to achieve faster inference without compromising on the precision and reliability of their models.</span><br><span class="line"></span><br><span class="line">Try LLM Compressor</span><br><span class="line">The following snippet is a minimal example of quantizing meta-llama/Meta-Llama-3.1-8B-Instruct with INT8 weights and activations.</span><br><span class="line"></span><br><span class="line">Install LLM Compressor via PyPi</span><br><span class="line">LLM Compressor is available for installation via PyPI:</span><br><span class="line"></span><br><span class="line">pip install llmcompressor</span><br><span class="line">Apply Quantization with the LLM Compressor</span><br><span class="line">Quantization is applied by selecting an algorithm and calling the oneshot API, which applies the selections in a post-training setting.</span><br><span class="line"></span><br><span class="line">In this case, we apply SmoothQuant to make the activations easier to quantize and GPTQ to apply the weight and activation quantization. We apply these algorithms to all linear layers of the network using the built-in open_platypus dataset (note: see the examples for how to use your own calibration set).</span><br><span class="line"></span><br><span class="line">from llmcompressor.modifiers.quantization import GPTQModifier</span><br><span class="line">from llmcompressor.modifiers.smoothquant import SmoothQuantModifier</span><br><span class="line">from llmcompressor.transformers import oneshot</span><br><span class="line"># Select quantization algorithm. In this case, we:</span><br><span class="line">#   * apply SmoothQuant to make the activations easier to quantize</span><br><span class="line">#   * quantize the weights to int8 with GPTQ (static per channel)</span><br><span class="line">#   * quantize the activations to int8 (dynamic per token)</span><br><span class="line">recipe = [</span><br><span class="line">    SmoothQuantModifier(smoothing_strength=0.8),</span><br><span class="line">    GPTQModifier(scheme=&quot;W8A8&quot;, targets=&quot;Linear&quot;, ignore=[&quot;lm_head&quot;]),</span><br><span class="line">]</span><br><span class="line"># Apply quantization using the built in open_platypus dataset.</span><br><span class="line">oneshot(</span><br><span class="line">    model=&quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot;,</span><br><span class="line">    dataset=&quot;open_platypus&quot;,</span><br><span class="line">    recipe=recipe,</span><br><span class="line">    output_dir=&quot;Meta-Llama-3.1-8B-Instruct-INT8&quot;,</span><br><span class="line">    max_seq_length=2048,</span><br><span class="line">    num_calibration_samples=512,</span><br><span class="line">)</span><br><span class="line">Inference Compressed Models with vLLM</span><br><span class="line">The resulting model is ready to be loaded and run in vLLM out-of-the-box:</span><br><span class="line"></span><br><span class="line">from vllm import LLM</span><br><span class="line">model = LLM(&quot;./Meta-Llama-3.1-8B-Instruct-INT8&quot;)</span><br><span class="line">output = model.generate(&quot;My name is&quot;)</span><br><span class="line">print(&quot;Output:&quot;, output[0].outputs[0].text)</span><br><span class="line"># Output: Jakob Schmid.  I live in the Republic of South Moluccas</span><br><span class="line">Under the hood, vLLM understands how to load and run the compressed model by looking at the config.yaml next to the weight files. Check out some of our more detailed examples to try out other quantization flows:</span><br><span class="line"></span><br><span class="line">FP8 activation quantization with PTQ</span><br><span class="line">INT8 activation quantization with GPTQ and SmoothQuant</span><br><span class="line">INT4 weight-only quantization With GPTQ</span><br><span class="line">LLM Compressor Roadmap</span><br><span class="line">We have a robust roadmap planned to expand support for model compression in LLM Compressor. Our roadmap is prioritized across the following initiatives:</span><br><span class="line"></span><br><span class="line">Expand model support: Mixture of Experts (MoE) and vision-language models</span><br><span class="line">Expand algorithm and scheme support: AWQ, additional quantized floating point formats (fp8 and fp4), and KV cache quantization</span><br><span class="line">Support for non-Nvidia hardware: We are actively collaborating with AMD, Google, and Intel teams to support models created by LLM Compressor on non-Nvidia hardware devices.</span><br><span class="line">Tools for creating non-uniform quantization schemes</span><br><span class="line">2:4 sparsity: Sparse foundation models, sparse fine-tuning from sparse foundational models, combining sparsity and quantization</span><br><span class="line">Expand support for training aware methods: Quantization-Aware Training (QAT) and Low-Rank Adaptation (LoRA)</span><br><span class="line">If you have any feature requests, large or small, please comment on our Roadmap Issue in GitHub.</span><br><span class="line"></span><br><span class="line">Final Thoughts</span><br><span class="line">At Neural Magic, we believe the Future of AI is Open, and we are on a mission to bring the power of open-source models and vLLM to every enterprise on the planet.</span><br><span class="line"></span><br><span class="line">We offer nm-vllm, an enterprise distribution of vLLM, with:</span><br><span class="line"></span><br><span class="line">Stable builds with bug fixes and selected model backporting</span><br><span class="line">Enterprise support with SLAs for production deployments of vLLM</span><br><span class="line">Tools and access to our teams for applying model optimizations via LLM Compressor</span><br><span class="line">Pre-optimized model registry</span><br><span class="line">Kubernetes reference architectures for production deployments</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.kbfg.com/kbresearch/report/reportView.do?reportId=2000501</span><br><span class="line">ë„ˆë„ë‚˜ë„ AI? ë§ë¡œë§Œ AI ì™¸ì¹˜ëŠ” &#x27;AI ì›Œì‹±&#x27; ì£¼ì˜ë³´</span><br><span class="line">2024-08-19</span><br><span class="line">KBê¸ˆìœµê·¸ë£¹</span><br><span class="line"></span><br><span class="line">2024. 8</span><br><span class="line">ë„ˆë„ë‚˜ë„ AI? ë§ë¡œë§Œ AI ì™¸ì¹˜ëŠ” â€˜AI ì›Œì‹±â€™ ì£¼ì˜ë³´</span><br><span class="line"></span><br><span class="line">â–¡ AI ì›Œì‹±ì˜ ì£¼ìš” íŠ¹ì§•ê³¼ ë¶€ì‘ìš©</span><br><span class="line">â–¡ AI ì›Œì‹± ì‚¬ë¡€</span><br><span class="line">â–¡ AI ì›Œì‹±ì„ í”¼í•˜ë ¤ë©´?</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1</span><br><span class="line">ï± AI ì›Œì‹±(AI Washing)ì´ë€ ê¸°ì—…ì´ë‚˜ ì œí’ˆì´ ì‹¤ì œë¡œëŠ” ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ì„œ ê´‘ë²”ìœ„</span><br><span class="line">í•˜ê²Œ í™œìš©í•˜ê³  ìˆëŠ” ê²ƒì²˜ëŸ¼ í™ë³´í•˜ëŠ” í–‰ìœ„ë¡œ AI ì—´í’ì„ ì´ìš©í•˜ëŠ” ë§ˆì¼€íŒ… ì „ëµì˜ ì¼í™˜</span><br><span class="line">â€¢ í™˜ê²½ì¹œí™”ì ì´ì§€ ì•Šì€ ê¸°ì—…ì´ ì¹œí™˜ê²½ì ì¸ ê²ƒì²˜ëŸ¼ í™ë³´í•˜ëŠ” ê·¸ë¦°ì›Œì‹±(Greenwashing) í–‰íƒœì™€ ìœ ì‚¬</span><br><span class="line">â€¢ ê¸°ì—…ì€ AI ê¸°ìˆ  ì‚¬ìš©ì„ ê°•ì¡°í•¨ìœ¼ë¡œì¨ íˆ¬ììì—ê²Œ ë§¤ë ¥ì ì¸ íˆ¬ì ëŒ€ìƒìœ¼ë¡œ ì¸ì‹ë˜ì–´ ë” ë§ì€ ìë³¸ì„ ìœ </span><br><span class="line">ì¹˜í•  ìˆ˜ ìˆìœ¼ë©°, ë³´ë‹¤ í˜ì‹ ì ì´ê³  ê¸°ìˆ  ì„ ë„ì ì¸ ì´ë¯¸ì§€ë¥¼ êµ¬ì¶•í•˜ì—¬ ë¹„ì¦ˆë‹ˆìŠ¤ì—ì„œ ê²½ìŸì‚¬ë³´ë‹¤ ìœ ë¦¬í•œ</span><br><span class="line">ìœ„ì¹˜ë¥¼ ì í•¨ìœ¼ë¡œì¨ ê¸°ì—… ê°€ì¹˜ì˜ ìƒìŠ¹ íš¨ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆìŒ</span><br><span class="line">ï± [ë¬¸ì œì ] AI ì›Œì‹±ì€ ì ì ˆí•œ íˆ¬ì ìì›ì˜ ë°°ë¶„ ì €í•´, ì†Œë¹„ì ì‹ ë¢° ì €í•˜, ê³¼ë„í•œ ê¸°ëŒ€ê° ìœ ë°œ ë“± ë¶€</span><br><span class="line">ì •ì ì¸ ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŒ</span><br><span class="line">ï± [ì‚¬ë¡€] AI ì›Œì‹±ì˜ ìœ í˜•ì€ ë‹¤ì–‘í•˜ë‚˜, ìì‚¬ ì œí’ˆì´ë‚˜ ì„œë¹„ìŠ¤ê°€ ì¶œì‹œë˜ëŠ” ê³¼ì •ì—ì„œ AI ê¸°ì—¬ë„ê°€ ë¶ˆë¶„</span><br><span class="line">ëª…í•¨ì—ë„ ê´‘ê³ ë‚˜ í™ë³´ì—ì„œ AI ê¸°ìˆ  ì‚¬ìš©ì„ ê°•ì¡°í•˜ë©° ì†Œë¹„ìë¥¼ ê¸°ë§Œí•˜ëŠ” í–‰ìœ„ê°€ ëŒ€í‘œì </span><br><span class="line">â€¢ [ì•„ë§ˆì¡´ê³ ] ë¬´ì¸ ë§¤ì¥ ì•„ë§ˆì¡´ê³ (Amazon Go)ì˜ â€˜ì €ìŠ¤íŠ¸ ì›Œí¬ ì•„ì›ƒ(Just Walk Out)â€™ì€ ê³ ê° í‡´ì  ì‹œ ìë™ìœ¼</span><br><span class="line">ë¡œ ê²°ì œê°€ ì²­êµ¬ë˜ëŠ” ì‹œìŠ¤í…œìœ¼ë¡œ ì•Œë ¤ì¡Œìœ¼ë‚˜, ì‚¬ì‹¤ì€ ì¸ë„ ì§€ì‚¬ ì§ì›ë“¤ì´ ìˆ˜ë™ìœ¼ë¡œ ê²€í† í•˜ëŠ” ê²ƒìœ¼ë¡œ ë°í˜€ì§</span><br><span class="line">â€¢ [ì¤€ì½”] ì±„ìš© ìŠ¤íƒ€íŠ¸ì—… ì¤€ì½”(Joonko)ëŠ” AIë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ì—…ì— ì í•©í•œ ì§€ì›ìë¥¼ ì¶”ì²œí•œë‹¤ëŠ” í—ˆìœ„ ì •ë³´</span><br><span class="line">ë¥¼ ìœ í¬í•´ íˆ¬ì ìê¸ˆì„ ìœ ì¹˜í–ˆë‹¤ëŠ” ì´ìœ ë¡œ ë¯¸êµ­ ì¦ê¶Œê±°ë˜ìœ„ì›íšŒ(SEC)ì™€ ë²•ë¬´ë¶€ë¡œë¶€í„° ê¸°ì†Œë¨</span><br><span class="line">â€¢ [ë¸í”¼ì•„ ë° ê¸€ë¡œë²Œí”„ë ˆë”•ì…˜ìŠ¤] íˆ¬ììë¬¸ì‚¬ ë¸í”¼ì•„(Delphia)ì™€ ê¸€ë¡œë²Œí”„ë ˆë”•ì…˜ìŠ¤(Global Predictions)ëŠ” AI</span><br><span class="line">ì›Œì‹± í–‰ìœ„ë¡œ ë¯¸êµ­ ì¦ê¶Œê±°ë˜ìœ„ì›íšŒë¡œë¶€í„° ê°ê° 22ë§Œ 5ì²œ ë‹¬ëŸ¬ì™€ 17ë§Œ 5ì²œ ë‹¬ëŸ¬ì˜ ë²Œê¸ˆì´ ë¶€ê³¼ë¨</span><br><span class="line">ï± [ëŒ€ì‘ ë°©ì•ˆ] AI ê¸°ìˆ ì´ ì§€ì†ì ìœ¼ë¡œ ë°œì „í•¨ì— ë”°ë¼ AI ì›Œì‹±ì— ëŒ€í•œ ê·œì œì™€ ì†Œë¹„ì ë³´í˜¸ì˜ ì¤‘ìš”ì„±ì´ ë”</span><br><span class="line">ìš± ê°•ì¡°ë  ê²ƒì´ë¯€ë¡œ, ê¸°ì—…ì€ íˆ¬ëª…í•œ AI ê¸°ìˆ  ì‚¬ìš©ì„ í†µí•´ ì†Œë¹„ìì™€ íˆ¬ììì˜ ì‹ ë¢°ë¥¼ í™•ë³´í•¨ìœ¼ë¡œì¨ ì§„</span><br><span class="line">ì •í•œ í˜ì‹ ì„ ë„ëª¨í•  í•„ìš”</span><br><span class="line">â€¢ [ê·œì œ ê°•í™”] ì •ë¶€ëŠ” AI ì›Œì‹±ì— ëŒ€í•œ ê·œì œë¥¼ ê°•í™”í•¨ìœ¼ë¡œì¨ ê¸°ì—…ì´ AI ì‚¬ìš©ì— ëŒ€í•œ íˆ¬ëª…ì„±ì„ ë†’ì´ê³  í—ˆìœ„</span><br><span class="line">ì£¼ì¥ì„ ë‚¨ë°œí•˜ì§€ ì•Šë„ë¡ ìœ ë„í•  í•„ìš”</span><br><span class="line">â€¢ [íˆ¬ëª…í•œ ê¸°ìˆ  ì‚¬ìš© ë° ì •ë³´ ì œê³µ] ê¸°ì—…ì€ ì‹¤ì œ ì‚¬ìš©í•œ AI ê¸°ìˆ ê³¼ ì¼ì¹˜í•˜ëŠ” íˆ¬ëª…í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ê·œ</span><br><span class="line">ì œ ê¸°ê´€ ë“±ì˜ ê°ì‚¬ë‚˜ ì™¸ë¶€ ì˜í˜¹ì— ëŒ€ë¹„í•˜ì—¬ ê´€ë ¨ ìë£Œë¥¼ ê¸°ë¡í•˜ê³  ë¬¸ì„œí™”í•  í•„ìš”</span><br><span class="line">â€¢ [ì†Œë¹„ì êµìœ¡] ì •ë¶€ë‚˜ ê¸°ì—…ì€ ì†Œë¹„ìê°€ AI ê¸°ìˆ ì— ëŒ€í•œ ì´í•´ë„ë¥¼ ë†’ì´ê³  AIì™€ ê´€ë ¨ëœ ê³¼ì¥ëœ ì£¼ì¥ì— í˜„</span><br><span class="line">í˜¹ë˜ì§€ ì•Šë„ë¡ ì†Œë¹„ì ëŒ€ìƒ êµìœ¡ í”„ë¡œê·¸ë¨ì„ ìš´ì˜í•  í•„ìš”</span><br><span class="line">â€¢ [ë¹„íŒì  íƒœë„ ê°–ê¸°] ì†Œë¹„ìì™€ íˆ¬ììëŠ” ì£¼ì²´ì ìœ¼ë¡œ ì •ë³´ë¥¼ ê²€í† í•˜ê³  ê²€ì¦í•˜ëŠ” íƒœë„ë¥¼ ê°€ì ¸ì•¼ í•¨. ê¸°ì—…</span><br><span class="line">ì´ AI ê¸°ìˆ ì— ëŒ€í•´ ë‹¨ìˆœíˆ â€˜í˜ì‹ ì â€™ ë˜ëŠ” â€˜ì§€ëŠ¥ì â€™ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ë‚¨ë°œí•˜ëŠ” ê²ƒì´ ì•„ë‹Œì§€ ë¹„íŒì ì¸ ì‹œê°ìœ¼</span><br><span class="line">ë¡œ ë°”ë¼ë³¼ í•„ìš”</span><br><span class="line">â€¢ [íˆ¬ëª…í•œ ì„¤ëª… ìš”êµ¬í•˜ê¸°] ì†Œë¹„ìëŠ” AI ì‘ë™ ë°©ì‹ì— ëŒ€í•œ íˆ¬ëª…í•œ ì„¤ëª…ì„ ìš”êµ¬í•˜ì—¬ ê¸°ì—…ì´ ì±…ì„ê°ì„ ê°–ê³ </span><br><span class="line">ê¸°ìˆ ì˜ ì‹¤ì œ ì„±ëŠ¥ê³¼ í•œê³„ ë“±ì„ í¬í•¨í•´ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ë„ë¡ ìœ ë„í•´ì•¼ í•¨</span><br><span class="line">&lt; ìš” ì•½ &gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2</span><br><span class="line">ï® AI ì›Œì‹±ì˜ ì£¼ìš” íŠ¹ì§•ê³¼ ë¶€ì‘ìš©</span><br><span class="line">â—‹ AI ì›Œì‹±(AI Washing)ì€ ê¸°ì—…ì´ë‚˜ ì œí’ˆì´ ì‹¤ì œë¡œëŠ” ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ì„œ ê´‘ë²”</span><br><span class="line">ìœ„í•˜ê²Œ í™œìš©í•˜ê³  ìˆëŠ” ê²ƒì²˜ëŸ¼ í™ë³´í•˜ëŠ” í–‰ìœ„ë¡œ AI ì—´í’ì„ ì´ìš©í•˜ëŠ” ë§ˆì¼€íŒ… ì „ëµì˜ ì¼í™˜</span><br><span class="line">ï‚– í™˜ê²½ì¹œí™”ì ì´ì§€ ì•Šì€ ê¸°ì—…ì´ ì¹œí™˜ê²½ì ì¸ ê²ƒì²˜ëŸ¼ í™ë³´í•˜ëŠ” ê·¸ë¦°ì›Œì‹±(Greenwashing) í–‰íƒœì™€ ìœ ì‚¬</span><br><span class="line">ï‚– AI ì— ëŒ€í•œ ì •ì˜ê°€ ê´‘ë²”ìœ„í•˜ê³  ëŠìŠ¨í•˜ê²Œ ì‚¬ìš©ë˜ê³  ìˆë‹¤ëŠ” ì ë„ AI ì›Œì‹±ì˜ ë“±ì¥ ë°°ê²½ ì¤‘ í•˜ë‚˜</span><br><span class="line">ã€AI ì›Œì‹±ì„ í•˜ëŠ” ì´ìœ ã€‘</span><br><span class="line">â—‹ ê¸°ì—…ì€ AI ì‚¬ìš©ì„ ê°•ì¡°í•¨ìœ¼ë¡œì¨ íˆ¬ììì—ê²Œ ë§¤ë ¥ì ì¸ íˆ¬ì ëŒ€ìƒìœ¼ë¡œ ì¸ì‹ë˜ì–´ ë” ë§ì€ ìë³¸</span><br><span class="line">ì„ ìœ ì¹˜í•  ìˆ˜ ìˆìœ¼ë©°, ë³´ë‹¤ í˜ì‹ ì ì´ê³  ê¸°ìˆ  ì„ ë„ì ì¸ ì´ë¯¸ì§€ë¥¼ êµ¬ì¶•í•˜ì—¬ ë¹„ì¦ˆë‹ˆìŠ¤ì—ì„œ ê²½</span><br><span class="line">ìŸì‚¬ë³´ë‹¤ ìœ ë¦¬í•œ ìœ„ì¹˜ë¥¼ ì í•  ìˆ˜ ìˆìŒ</span><br><span class="line">ï‚– [íˆ¬ì ìœ ì¹˜] ã€Ší¬ë¸ŒìŠ¤(Forbes)ã€‹ì— ë”°ë¥´ë©´ íˆ¬ì ìœ ì¹˜ ì‹œ AIë¥¼ ì–¸ê¸‰í•œ ìŠ¤íƒ€íŠ¸ì—…ì´ ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ë³´ë‹¤</span><br><span class="line">ì ê²ŒëŠ” 15%, ë§ê²ŒëŠ” 50% ë” ë§ì€ ìê¸ˆì„ í™•ë³´í•˜ë©´ì„œ AI ì—­ëŸ‰ì„ ê³¼ëŒ€ ê´‘ê³ í•˜ëŠ” ê¸°ì—…ì´ ëŠ˜ê³  ìˆìŒ1</span><br><span class="line">- ì¼ë¶€ ê¸°ì—…ì´ ìê¸ˆ ì¡°ë‹¬ì„ ìœ„í•´ AI ì—­ëŸ‰ì„ ê³¼ì¥í•˜ê³ , ì°½ì—…ìëŠ” íˆ¬ì ìœ ì¹˜ ì‹œ AIë¥¼ ì–¸ê¸‰</span><br><span class="line">í•˜ì§€ ì•Šìœ¼ë©´ ë¶ˆë¦¬í•´ì§ˆ ìˆ˜ ìˆë‹¤ê³  ìƒê°í•˜ëŠ” ê²½í–¥ì´ ìˆìŒ2</span><br><span class="line">. ì´ ë•Œë¬¸ì— AI ì—­ëŸ‰ì„ ì£¼ì¥í•˜</span><br><span class="line">ëŠ” ê¸°ì—…ê³¼ ì‹¤ì§ˆì ì¸ AI ê¸°ì—… ì‚¬ì´ì—ëŠ” ìƒë‹¹í•œ ê°„ê·¹ì´ ì¡´ì¬</span><br><span class="line">- ì˜êµ­ ë²¤ì²˜ìºí”¼í„¸ í€ë“œ MMCë²¤ì²˜ìŠ¤(MMC Ventures)ì˜ 2019ë…„ ì¡°ì‚¬ì— ë”°ë¥´ë©´ ìœ ëŸ½</span><br><span class="line">AI ìŠ¤íƒ€íŠ¸ì—…ì˜ 40%ëŠ” ì‚¬ì‹¤ìƒ AIë¥¼ ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¨</span><br><span class="line">ï‚– [ì‹œì¥ ê²½ìŸë ¥ í™•ë³´] ê°€ì „ì œí’ˆ ì œì¡°ì—…ì²´ê°€ ìì‚¬ ì œí’ˆì— AI ê¸°ìˆ ì´ ì ìš©ë˜ì—ˆë‹¤ê³  ì£¼ì¥í•˜ë©´, ì†Œë¹„ì</span><br><span class="line">ëŠ” ê·¸ ì œí’ˆì´ ë” ìŠ¤ë§ˆíŠ¸í•˜ê³  í˜ì‹ ì ì´ë¼ ìƒê°í•˜ì—¬ ì ê·¹ì ì¸ êµ¬ë§¤ ì˜ì‚¬ë¥¼ ë‚˜íƒ€ë‚¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ</span><br><span class="line">- ê¸°ì—…ì€ AI ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ìˆ ì  ìš°ì›”ì„±ì„ ê°•ì¡°í•˜ê³  ì‹œì¥ì—ì„œ ë¦¬ë”ë¡œ ìë¦¬ë§¤ê¹€í•˜ë ¤</span><br><span class="line">ì˜ë„í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ì¸í„°ë„·ì„ í†µí•´ ì‘ë™í•˜ëŠ” ê²ƒì„ AI ì‹œìŠ¤í…œì„ êµ¬ì¶•í–ˆë‹¤ê³  í™ë³´í•˜ê±°</span><br><span class="line">ë‚˜ ìƒë‹´ ì±—ë´‡ì„ ì¶”ê°€í•˜ëŠ” ì •ë„ì— ê·¸ì¹˜ëŠ” ê²½ìš°ê°€ ë§ìŒ</span><br><span class="line">ã€AI ì›Œì‹±ì˜ ë¬¸ì œì ã€‘</span><br><span class="line">â—‹ AI ì›Œì‹±ì€ í”í•œ ë§ˆì¼€íŒ… ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ, íˆ¬ì ìì›ì˜ ì ì ˆí•œ ë°°ë¶„ì„ ì €í•´í•˜ê³  ì†Œë¹„</span><br><span class="line">ìì˜ ê³¼ë„í•œ ê¸°ëŒ€ê°ì„ ìœ ë„í•´ ê²°êµ­ ì‹ ë¢° ì €í•˜ë¡œ ì´ì–´ì§€ê²Œ í•˜ëŠ” ë“± ë¶€ì •ì ì¸ ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŒ</span><br><span class="line"></span><br><span class="line">1 Bernard Marr, Apr. 25, 2024, â€œSpotting AI Washing: How Companies Overhype Artificial Intelligenceâ€, Forbes</span><br><span class="line">2 ì˜êµ­ê³¼ í•€ë€ë“œì— ë³¸ì‚¬ë¥¼ ë‘” ì‹ ê¸°ìˆ  íˆ¬ìíšŒì‚¬ ì˜¤í”ˆì˜¤ì…˜(OpenOcean)ì— ë”°ë¥´ë©´ 2022ë…„ ê¸°ìˆ  ìŠ¤íƒ€íŠ¸ì—…ì˜ 10%ë§Œì´ íˆ¬ì ìœ ì¹˜ ì‹œ AI</span><br><span class="line">ë¥¼ ì‚¬ìš©í•œë‹¤ê³  ì–¸ê¸‰í–ˆìœ¼ë‚˜ 2023ë…„ì—ëŠ” ì´ ë¹„ìœ¨ì´ 4ë¶„ì˜ 1 ì´ìƒìœ¼ë¡œ ì¦ê°€í–ˆê³  ì˜¬í•´ëŠ” 3ë¶„ì˜ 1 ì´ìƒìœ¼ë¡œ ì¦ê°€í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒ</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3</span><br><span class="line">â—‹ [ì ì ˆí•œ ìì› ë°°ë¶„ ì €í•´] í˜ì‹ ì ì¸ ê¸°ìˆ ì„ ë³´ìœ í•˜ê³  ìˆë‹¤ê³  í¬ì¥í•˜ëŠ” ê¸°ì—…ì— ì†Œë¹„ìì™€ íˆ¬ì</span><br><span class="line">ìì˜ ì´ëª©ê³¼ íˆ¬ìê°€ ì§‘ì¤‘ë  ìˆ˜ ìˆìŒ</span><br><span class="line">ï‚– AI ì›Œì‹± ê¸°ì—…ì— íˆ¬ì ì¬ì›ì´ ëª°ë¦´ ê²½ìš° ì‹¤ì œ í˜ì‹  ê¸°ìˆ ì„ ë³´ìœ í•œ ê¸°ì—…ì´ë‚˜ í”„ë¡œì íŠ¸ì—ëŠ” ì›í™œí•œ</span><br><span class="line">ìê¸ˆ ê³µê¸‰ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ</span><br><span class="line">â—‹ [ì†Œë¹„ì ì‹ ë¢° ì €í•˜] AI ì›Œì‹±ì„ ë§ˆì¼€íŒ… ìˆ˜ë‹¨ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê¸°ì—…ì´ ì¦ê°€í•˜ì—¬ ëŒ€ë¶€ë¶„ì˜ ê¸°ì—…ì´</span><br><span class="line">AIì— ëŒ€í•´ ì–¸ê¸‰í•œë‹¤ë©´ ì†Œë¹„ìëŠ” AIì˜ ì„±ëŠ¥ì— ì˜ë¬¸ì„ ê°–ê²Œ ë  ê²ƒì´ê³ , ì‹¤ì œ ì‚¬ìš©ì„ í†µí•´ ë¶€</span><br><span class="line">ì •ì  ê²½í—˜ì´ ëˆ„ì ë  ê²½ìš° AI ê¸°ìˆ  ìì²´ì— ëŒ€í•œ ì‹ ë¢°ë„ê°€ ì €í•˜ë  ìˆ˜ ìˆìŒ</span><br><span class="line">ï‚– ì†Œë¹„ìëŠ” ê³¼ì¥ëœ ì£¼ì¥ì„ ë°˜ë³µì ìœ¼ë¡œ ì ‘í•˜ë©´ì„œ AI ê¸°ìˆ ì„ ë¶ˆì‹ í•˜ê²Œ ë˜ë©°, ì´ëŠ” ê²°ê³¼ì ìœ¼ë¡œ AI ì œí’ˆ</span><br><span class="line">ê³¼ ì„œë¹„ìŠ¤ì˜ êµ¬ë§¤ ê¸°í”¼ë¡œ ì´ì–´ì ¸ ì§„ì •í•œ AI ê¸°ìˆ ì„ ì œê³µí•˜ëŠ” ê¸°ì—…ì— ë¶€ì •ì  ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŒ</span><br><span class="line">â—‹ [ê¸°ì—…ì˜ ìì› ë‚­ë¹„] AI ì›Œì‹± ê¸°ì—…ì˜ ê³¼ì¥ëœ ì£¼ì¥ì„ ì˜ì‹í•˜ì—¬ ì •ìƒì ì¸ ê¸°ì—…ë§ˆì € í˜„ì‹¤ì„±ì´ ë–¨</span><br><span class="line">ì–´ì§€ëŠ” ëª©í‘œë¥¼ ì„¤ì •í•˜ê³  ì¶”ì§„í•´ ë‚˜ê°ˆ ê²½ìš° í”„ë¡œì íŠ¸ì˜ ì‹¤íŒ¨ë¡œ ì¸í•œ ì†ì‹¤ë¿ ì•„ë‹ˆë¼ ì„ íƒê³¼</span><br><span class="line">ì§‘ì¤‘ì˜ ì‹¤íŒ¨ë¡œ ì¸í•´ í˜ì‹ ì´ ì§€ì²´ë˜ëŠ” ë“±ì˜ ë¶€ì •ì  ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŒ</span><br><span class="line">ï‚– ê¸°ì—…ì´ ì˜ë¯¸ ìˆëŠ” AI ì—­ëŸ‰ì„ ê°œë°œí•˜ëŠ” ëŒ€ì‹  í”¼ìƒì ì¸ ê°œì„ ì— íˆ¬ìë¥¼ ë‚¨ë°œí•  ê²½ìš° ê¸°ìˆ ì˜ ì§„ì „ì„ ëŠ¦ì¶œ</span><br><span class="line">ìˆ˜ ìˆìŒ</span><br><span class="line">ï‚– ì§„ì • ê°€ì¹˜ ìˆëŠ” AI ì†”ë£¨ì…˜ì„ ëª¨ìƒ‰í•˜ëŠ” ê¸°ì—…ì˜ ì˜ì‚¬ ê²°ì •ì„ ë³µì¡í•˜ê²Œ ë§Œë“¤ì–´ ë””ì§€í„¸ ì „í™˜ ë…¸ë ¥ì„</span><br><span class="line">ë°©í•´í•˜ê³  í˜ì‹ ì„ ì €í•´í•˜ë©° ì„±ê³¼ë¥¼ ì•…í™”ì‹œí‚¬ ìˆ˜ ìˆìŒ</span><br><span class="line">ï® AI ì›Œì‹± ì‚¬ë¡€</span><br><span class="line">â—‹ [ì•„ë§ˆì¡´ê³ ] ë¬´ì¸ ë§¤ì¥ ì•„ë§ˆì¡´ê³ (Amazon Go)ì˜ â€˜ì €ìŠ¤íŠ¸ ì›Œí¬ ì•„ì›ƒ(Just Walk Out)â€™ì€ ê³ ê°</span><br><span class="line">í‡´ì  ì‹œ ìë™ìœ¼ë¡œ ê²°ì œê°€ ì²­êµ¬ë˜ëŠ” ì‹œìŠ¤í…œìœ¼ë¡œ ì•Œë ¤ì¡Œìœ¼ë‚˜, ì‚¬ì‹¤ì€ ì¸ë„ ì§€ì‚¬ ì§ì›ë“¤ì´ ìˆ˜</span><br><span class="line">ë™ìœ¼ë¡œ ê²€í† í•˜ëŠ” ê²ƒìœ¼ë¡œ ë°í˜€ì§3</span><br><span class="line">ï‚– ë§¤ì¥ ë‚´ ê°œë³„ ì œí’ˆì— ì„¼ì„œë‚˜ ì¹©ì„ ë¶€ì°©í•˜ì§€ ì•Šê³ , ì²œì¥ì— ë‹¬ë¦° ì„¼ì„œê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ê³ ê°ê³¼ ì œí’ˆì„ ì¶”ì </span><br><span class="line">í•˜ëŠ” ë”¥ëŸ¬ë‹ ê¸°ìˆ ì„ ì ìš©í–ˆë‹¤ê³  ë°í˜”ìœ¼ë‚˜, ì‹œìŠ¤í…œì˜ ìƒë‹¹ ë¶€ë¶„ì´ 1ì²œì—¬ ëª…ì˜ ì¸ë„ ì§€ì‚¬ ì§ì›ì´ ê° ë§¤</span><br><span class="line">ì¥ ì¹´ë©”ë¼ë¥¼ í†µí•´ ìˆ˜ë™ìœ¼ë¡œ ì²´í¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìš´ì˜ëœë‹¤ëŠ” ì‚¬ì‹¤ì´ ë“œëŸ¬ë‚˜ë©´ì„œ AI ì›Œì‹± ë…¼ë€ì´ ì œê¸°ë¨</span><br><span class="line">- ì•„ë§ˆì¡´ê³ ëŠ” ì¸ê°„ì´ ê²°ì œ ì²­êµ¬ ê³¼ì •ì— ê´€ì—¬í•œë‹¤ëŠ” ì‚¬ì‹¤ì€ ë¶€ì¸í•˜ì§€ ì•Šì•˜ì§€ë§Œ, ì§ì›ë“¤ì´</span><br><span class="line">â€˜ì €ìŠ¤íŠ¸ ì›Œí¬ ì•„ì›ƒâ€™ ì‹œìŠ¤í…œì„ ê°œì„ í•˜ê¸° ìœ„í•´ AIê°€ ìƒì„±í•œ ì‹¤ì œ ë°ì´í„°ì— ì£¼ì„ì„ ë‹¬ ë¿ì´</span><br><span class="line">ë©° ì „ì²´ ìš´ì˜ì— ê´€ì—¬í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë¼ê³  ì£¼ì¥4</span><br><span class="line"></span><br><span class="line">3 Alex Bitter, Apr. 4, 2024, â€œAmazonâ€™s just walk out actually uses 1,000 people in Indiaâ€, Business Insider</span><br><span class="line">4 Emma Roth, Apr. 18, 2024, â€œAmazon insists Just Walk Out isnâ€™t secretly run by workers watching you shopâ€, The Verge</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">4</span><br><span class="line">[ê·¸ë¦¼ 1] ì•„ë§ˆì¡´ê³  êµ¬ì¡°ë„ [ê·¸ë¦¼ 2] ì•„ë§ˆì¡´ê³  ì™¸ë¶€ ì „ê²½</span><br><span class="line">ìë£Œ: ìœ„í‚¤í”¼ë””ì•„ ìë£Œ: ìœ„í‚¤í”¼ë””ì•„</span><br><span class="line">â—‹ [ì¤€ì½”] ì±„ìš© ìŠ¤íƒ€íŠ¸ì—… ì¤€ì½”(Joonko)ëŠ” AIë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ì—…ì— ì í•©í•œ ì§€ì›ìë¥¼ ì¶”ì²œí•œë‹¤ëŠ”</span><br><span class="line">í—ˆìœ„ ì •ë³´ë¥¼ ìœ í¬í•´ íˆ¬ì ìê¸ˆì„ ìœ ì¹˜í–ˆë‹¤ëŠ” ì´ìœ ë¡œ ë¯¸êµ­ ì¦ê¶Œê±°ë˜ìœ„ì›íšŒ(SEC)ì™€ ë²•ë¬´ë¶€ë¡œ</span><br><span class="line">ë¶€í„° ê¸°ì†Œë¨</span><br><span class="line">ï‚– ì¦ê¶Œê±°ë˜ìœ„ì›íšŒëŠ” ì§€ë‚œ 6ì›” AIë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§€ì›ìë¥¼ ì„ ë°œ</span><br><span class="line">í•œë‹¤ê³  ì†ì´ê³  ê³ ê° ì •ë³´ì™€ ì§€ì›ì ìˆ˜, ê¸°ì—… ìˆ˜ìµì— ê´€í•œ</span><br><span class="line">í—ˆìœ„ ë° ì˜¤í•´ì˜ ì†Œì§€ê°€ ìˆëŠ” ì§„ìˆ ì„ í•˜ì—¬ ìµœì†Œ 2ì²œ100ë§Œ</span><br><span class="line">ë‹¬ëŸ¬ì˜ íˆ¬ìë¥¼ ìœ ì¹˜í•œ í˜ì˜ë¡œ AI ê¸°ë°˜ ì±„ìš© ìŠ¤íƒ€íŠ¸ì—… ì¤€ì½”</span><br><span class="line">ì˜ CEOì´ì ì°½ì—…ìì¸ ì¼ë¦¿ ë¼ì¦ˆ(llit Raz)ë¥¼ ê¸°ì†Œ</span><br><span class="line">- ì¤€ì½”ëŠ” AIë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì—…ì´ ë‹¤ì–‘ì„±, í˜•í‰ì„±, í¬</span><br><span class="line">ìš©ì„±ì´ ì¡´ì¤‘ë˜ëŠ” ì¸ë ¥ ì¡°ì§ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë„ë¡</span><br><span class="line">ë‹¤ì–‘í•œ ì§€ì›ìë¥¼ ì„ ë°œí•˜ëŠ” ë° ë„ì›€ì„ ì¤€ë‹¤ê³  ì£¼ì¥</span><br><span class="line">í–ˆìœ¼ë‚˜ ì¡°ì‚¬ ê²°ê³¼ ê±°ì§“ìœ¼ë¡œ ë°í˜€ì§</span><br><span class="line">- ë¼ì¦ˆëŠ” íˆ¬ì ìœ ì¹˜ ì‹œ ì¤€ì½”ê°€ ã€Ší¬ì²œ(Fortune)ã€‹ 500ëŒ€ ê¸°ì—…ì„ í¬í•¨í•˜ì—¬ 100ê°œ ì´ìƒì˜ ê³ </span><br><span class="line">ê°ì‚¬ë¥¼ ë³´ìœ í•˜ê³  ìˆìœ¼ë©°, 100ë§Œ ë‹¬ëŸ¬ ì´ìƒì˜ ìˆ˜ìµì„ ì˜¬ë ¸ì„ ë¿ ì•„ë‹ˆë¼ 10ë§Œ ëª… ì´ìƒ</span><br><span class="line">ì˜ í™œì„± êµ¬ì§ìì™€ í˜‘ë ¥í•˜ê³  ìˆë‹¤ëŠ” ë“± ê²½ì˜ ì„±ê³¼ì— ëŒ€í•´ì„œë„ í—ˆìœ„ ì •ë³´ë¥¼ ì œê³µ</span><br><span class="line">- ë¼ì¦ˆì˜ ì£¼ì¥ì— ì˜ì‹¬ì„ í’ˆëŠ” íˆ¬ììì—ê²Œ í—ˆìœ„ ì •ë³´ ìœ í¬ ì‚¬ì‹¤ì„ ì€íí•˜ê¸° ìœ„í•´ ìœ„ì¡°ëœ</span><br><span class="line">ì€í–‰ ê±°ë˜ ë‚´ì—­ì„œì™€ ê³„ì•½ì„œë„ ì œê³µ</span><br><span class="line">â—‹ [ë¸í”¼ì•„ ë° ê¸€ë¡œë²Œí”„ë ˆë”•ì…˜ìŠ¤] ë¯¸êµ­ ì¦ê¶Œê±°ë˜ìœ„ì›íšŒëŠ” AI ì‚¬ìš© ë²”ìœ„ì— ëŒ€í•´ í—ˆìœ„ ë° ì˜¤í•´ì˜ ì†Œì§€ê°€</span><br><span class="line">ìˆëŠ” ì§„ìˆ ì„ í•œ í˜ì˜ë¡œ íˆ¬ììë¬¸ì‚¬ ë¸í”¼ì•„(Delphia)ì™€ ê¸€ë¡œë²Œí”„ë ˆë”•ì…˜ìŠ¤(Global Predictions)ë¥¼ ê¸°</span><br><span class="line">ì†Œ. ì–‘ì‚¬ëŠ” ì œê¸°ëœ í˜ì˜ë¥¼ í•´ê²°í•˜ê³  ê°ê° 22ë§Œ 5ì²œ ë‹¬ëŸ¬ì™€ 17ë§Œ 5ì²œ ë‹¬ëŸ¬ì˜ ë²Œê¸ˆì„ ì§€ë¶ˆí•˜ëŠ”</span><br><span class="line">ì„ ì—ì„œ í•©ì˜</span><br><span class="line">[ê·¸ë¦¼ 3] ì¤€ì½” í™ë³´ ì´ë¯¸ì§€</span><br><span class="line">ìë£Œ: ì¤€ì½” ë§í¬ë“œì¸ ê³„ì •</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">5</span><br><span class="line">ï‚– [ë¸í”¼ì•„] AI ë¥¼ í™œìš©í•˜ì—¬ ì–´ë–¤ ê¸°ì—…ê³¼ íŠ¸ë Œë“œê°€ í¬ê²Œ</span><br><span class="line">ì„±ì¥í•  ê²ƒì¸ì§€ ì˜ˆì¸¡í•˜ì—¬ ê³ ê°ì´ ì„ ì œì ìœ¼ë¡œ íˆ¬ìí• </span><br><span class="line">ìˆ˜ ìˆë„ë¡ ì •ë³´ë¥¼ ì œê³µí–ˆë‹¤ê³  ì£¼ì¥í–ˆì§€ë§Œ, ì¦ê¶Œê±°ë˜</span><br><span class="line">ìœ„ì›íšŒëŠ” ë¸í”¼ì•„ê°€ ì‹¤ì œë¡œëŠ” AI ë° ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ ì„</span><br><span class="line">ê°–ì¶”ì§€ ì•Šì•˜ë‹¤ê³  íŒë‹¨</span><br><span class="line">- ë¸í”¼ì•„ëŠ” ë“±ë¡ íˆ¬ììë¬¸ì‚¬ê°€ ì¤‘ìš” ì‚¬ì‹¤ì— ëŒ€</span><br><span class="line">í•œ í—ˆìœ„ ì§„ìˆ ì´ í¬í•¨ëœ ê´‘ê³ ë¥¼ ìœ í¬í•˜ëŠ” ê²ƒ</span><br><span class="line">ì„ ê¸ˆì§€í•œ ë§ˆì¼€íŒ… ê·œì¹™ì„ ìœ„ë°˜í•œ í˜ì˜ë¡œ ì¶”</span><br><span class="line">ê°€ ê¸°ì†Œë¨</span><br><span class="line">ï‚– [ê¸€ë¡œë²Œí”„ë ˆë”•ì…˜ìŠ¤] í™ˆí˜ì´ì§€, ì†Œì…œë¯¸ë””ì–´, ì´ë©”ì¼ì—</span><br><span class="line">ì„œ ìì‹ ë“¤ì´ ìµœì´ˆì˜ AI ê¸ˆìœµ ìë¬¸ì‚¬ë¡œ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ AI ê¸°ë°˜ ì˜ˆì¸¡(Expert AI-driven Forecasts)</span><br><span class="line">ì„ ì œê³µí•œë‹¤ê³  ì£¼ì¥í–ˆì§€ë§Œ, ì´ì— ëŒ€í•œ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì§€ ëª»í–ˆê³  ì‹¤ì œë¡œë„ AI ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ</span><br><span class="line">- í™ˆí˜ì´ì§€ì™€ ìœ íŠœë¸Œ ì±„ë„ì— ì‹¤ì œ ê³ ê° ë°ì´í„°</span><br><span class="line">ê°€ ì•„ë‹Œ ê°€ìƒì˜ ì„±ê³¼ ë°ì´í„°ë¥¼ ê³µê°œí•˜ë©´ì„œ ì´</span><br><span class="line">ì‚¬ì‹¤ì„ ëª…ì‹œí•˜ì§€ ì•Šì•„ ì¦ê¶Œê±°ë˜ìœ„ì›íšŒ ê·œì •ì„</span><br><span class="line">ìœ„ë°˜. ë˜í•œ AI ê¸°ë°˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œì˜ ìš°ìˆ˜ì„±ì„</span><br><span class="line">í™ë³´í–ˆì§€ë§Œ ì‹¤ì œë¡œëŠ” AIë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì€ ê²ƒ</span><br><span class="line">ìœ¼ë¡œ ë“œëŸ¬ë‚¨</span><br><span class="line">- ì´ëŸ¬í•œ ê·œì • ìœ„ë°˜ì— ëŒ€í•œ ì‹œì • ì¡°ì¹˜ë¡œ ë§ˆì¼€íŒ…</span><br><span class="line">ë° êµìœ¡ ìë£Œ ê²€í† ë¥¼ ìœ„í•œ ê·œì • ì¤€ìˆ˜ ì»¨ì„¤í„´</span><br><span class="line">íŠ¸ë¥¼ ê³ ìš©í•˜ê¸°ë¡œ ì¦ê¶Œê±°ë˜ìœ„ì›íšŒì™€ í•©ì˜</span><br><span class="line">â—‹ [ì˜¤í† ë©”ì´í„°ìŠ¤ AI] ì˜¤í† ë©”ì´í„°ìŠ¤ AI(Automators AI)ëŠ” AI ê¸°ìˆ ì„ í™œìš©í•´ ì˜¨ë¼ì¸ ìŠ¤í† ì–´ì˜ íŒë§¤</span><br><span class="line">ëŸ‰ì„ ìë™ìœ¼ë¡œ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” í—ˆìœ„ ì •ë³´ë¥¼ ìœ í¬í•˜ì—¬ íˆ¬ìë¥¼ ìœ ì¹˜í•œ í˜ì˜ë¡œ ì—°ë°©ê±°ë˜ìœ„</span><br><span class="line">ì›íšŒ(FTC)ì— ì˜í•´ ê¸°ì†Œë¨5</span><br><span class="line">ï‚– ì•„ë§ˆì¡´ì´ë‚˜ ì›”ë§ˆíŠ¸ì™€ ê°™ì€ í”Œë«í¼ì—ì„œ ìš´ì˜ë˜ëŠ” ì˜¨ë¼ì¸ ìŠ¤í† ì–´ì— ì…ì í•œ ì‚¬ì—…ìë“¤ì—ê²Œ ìì‹ ë“¤ì˜</span><br><span class="line">AI ê¸°ìˆ ì„ í™œìš©í•˜ë©´ ë§¤ì›” 4 ì²œ ë‹¬ëŸ¬ì—ì„œ 6 ì²œ ë‹¬ëŸ¬ì˜ ìˆœì´ìµì„ ì–»ê³ , 8 ê°œì›” í›„ 100% íˆ¬ììˆ˜ìµë¥ </span><br><span class="line">ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤ê³  í™ë³´í•˜ì—¬ 2 ì²œ 200 ë§Œ ë‹¬ëŸ¬ì˜ íˆ¬ìë¥¼ ìœ ì¹˜í–ˆì§€ë§Œ í—ˆìœ„ ì •ë³´ë¡œ ë“œëŸ¬ë‚¨</span><br><span class="line"></span><br><span class="line">5 2023ë…„ 8ì›” ë¡œë§Œ í¬ë ˆìŠ¤í† (Roman Cresto), ì¡´ í¬ë ˆìŠ¤í† (John Cresto), ì•¤ë“œë£¨ ì±„í”„ë¨¼(Andrew Chapman)ì€ ì˜¤í† ë©”ì´í„°ìŠ¤</span><br><span class="line">AIë¥¼ í¬í•¨í•´ ì—¬ëŸ¬ íšŒì‚¬ë¥¼ ìš´ì˜í•˜ë©´ì„œ ì•„ë§ˆì¡´ ë° ì›”ë§ˆíŠ¸ì˜ ì´ì»¤ë¨¸ìŠ¤ì— AIë¥¼ ì ìš©í•˜ë©´ ë¶ˆë¡œì†Œë“(Passive income)ì„ ì–»ì„ ìˆ˜ ìˆë‹¤</span><br><span class="line">ê³  í™ë³´í•˜ë©° íˆ¬ììë¥¼ ìœ ì¹˜í–ˆìœ¼ë‚˜ AI ê¸°ìˆ ì´ ì‚¬ìš©ë˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ë“œëŸ¬ë‚¨(Federal Trade Commission, Feb. 27, 2024, â€œFTC</span><br><span class="line">Action Leads to Ban for Owners of Automators AI E-Commerce Money-Making Schemeâ€)</span><br><span class="line">[ê·¸ë¦¼ 4] ë¸í”¼ì•„ í™ë³´ ì´ë¯¸ì§€</span><br><span class="line">ìë£Œ: ë¸í”¼ì•„ í™ˆí˜ì´ì§€</span><br><span class="line">[ê·¸ë¦¼ 5] ê¸€ë¡œë²Œí”„ë ˆë”•ì…˜ìŠ¤ í™ë³´ ì´ë¯¸ì§€</span><br><span class="line">ìë£Œ: ê¸€ë¡œë²Œí”„ë ˆë”•ì…˜ìŠ¤ í™ˆí˜ì´ì§€</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">6</span><br><span class="line">- ê·¸ë“¤ì˜ ì£¼ì¥ê³¼ëŠ” ë‹¬ë¦¬ ì‹¤ì œë¡œëŠ” AI ê¸°ìˆ ì„ í™œìš©í•˜ì§€ ì•Šì•˜ê³ , ëŒ€ë¶€ë¶„ì˜ ê³ ê°ì´ ìˆ˜ìµì€ì»¤ë…•</span><br><span class="line">íˆ¬ìê¸ˆë§ˆì € íšŒìˆ˜í•˜ì§€ ëª»í•œ ê²ƒìœ¼ë¡œ ë“œëŸ¬ë‚¨. ì´ì— ì—°ë°©ë²•ì›ì€ íšŒì‚¬ ìš´ì˜ì„ ì¤‘ë‹¨ì‹œí‚¤ê³ , ê´€</span><br><span class="line">ë ¨ìë“¤ì—ê²Œ ì˜êµ¬ì ì¸ ì‚¬ì—… ê¸°íšŒ ë°•íƒˆê³¼ 2ì²œ200ë§Œ ë‹¬ëŸ¬ ë°°ìƒì„ ëª…ë ¹</span><br><span class="line">â—‹ [ì˜¤ë„ë¹„] P&amp;Gì˜ êµ¬ê°• ê´€ë¦¬ ë¸Œëœë“œ ì˜¤ë„ë¹„(Oral-B)ëŠ” ê³ ê°€ì˜ ì „ë™ì¹«ì†”ì„ íŒë§¤í•˜ë©´ì„œ AIê°€ ì¹˜ì•„ ìœ„</span><br><span class="line">ì¹˜ì™€ ë°ê¸° ë“±ì„ íŒŒì•…í•´ ì´ê°€ ì˜ ë‹¦ì˜€ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆë‹¤ê³  ê´‘ê³ . ê·¸ëŸ¬ë‚˜ ã€Šì›Œì‹±í„´í¬ìŠ¤íŠ¸ã€‹ëŠ” â€œì´ ì¹«</span><br><span class="line">ì†”ì— AI ê¸°ëŠ¥ì´ ì •í™•íˆ ì–´ë–»ê²Œ ì ìš©ë˜ëŠ”ì§€ ë¬¼ì—ˆì§€ë§Œ íšŒì‚¬ëŠ”</span><br><span class="line">ëŒ€ë‹µí•˜ì§€ ëª»í–ˆë‹¤â€ê³  ë³´ë„6</span><br><span class="line">ï‚– ì˜¤ë„ë¹„ëŠ” í•´ë‹¹ ì¹«ì†”ì´ AI ê¸°ìˆ ë¡œ ì‚¬ìš©ìì˜ ì¹«ì†”ì§ˆ ìŠµê´€</span><br><span class="line">ì„ ì‹¤ì‹œê°„ ë¶„ì„í•˜ì—¬ ë§ì¶¤í˜• í”¼ë“œë°±ì„ ì œê³µí•œë‹¤ê³  ì£¼ì¥í–ˆ</span><br><span class="line">ìœ¼ë‚˜, ì†Œë¹„ìì™€ ì „ë¬¸ê°€ë“¤ì€ ì´ëŸ¬í•œ ê¸°ëŠ¥ì´ ì–¼ë§ˆë‚˜ íš¨ê³¼</span><br><span class="line">ì ì¸ì§€ì— ëŒ€í•´ ì˜ë¬¸ì„ ì œê¸°</span><br><span class="line">- ì¼ë¶€ ì‚¬ìš©ìëŠ” ì¹«ì†”ì´ ì œê³µí•˜ëŠ” í”¼ë“œë°±ì´ ê°œì¸í™”ë˜ì—ˆë‹¤</span><br><span class="line">ê³  ë³´ê¸° ì–´ë µê±°ë‚˜, ì˜ˆìƒë³´ë‹¤ ë‹¨ìˆœí•˜ë‹¤ë©° ë¶ˆë§Œì„ í‘œì‹œ</span><br><span class="line">â—‹ [ì½”ì¹´ì½œë¼] ì½”ì¹´ì½œë¼ëŠ” AIë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ìŒë£Œ</span><br><span class="line">ë¥¼ ë§Œë“¤ì—ˆë‹¤ê³  í™ë³´í–ˆì§€ë§Œ AI ì›Œì‹±ì„ì´ ë“œëŸ¬ë‚˜ ì—…</span><br><span class="line">ê³„ì˜ ë¹„ë‚œì„ ë°›ìŒ</span><br><span class="line">ï‚– 3000 ë…„ëŒ€ë¥¼ ìƒìƒí•˜ë©° ë§Œë“¤ì—ˆë‹¤ê³  í™ë³´í•œ ì½”ì¹´</span><br><span class="line">ì½œë¼ Y3000 ì€ AI ì™€ ê³µë™ìœ¼ë¡œ ì œí’ˆì„ ê°œë°œí–ˆë‹¤</span><br><span class="line">ê³  ë°í˜”ì§€ë§Œ, AI ê°€ ê°œë°œ ê³¼ì •ì—ì„œ ì–´ë–»ê²Œ ê´€ì—¬í–ˆ</span><br><span class="line">ëŠ”ì§€ì— ëŒ€í•´ì„œëŠ” ì„¤ëª…í•˜ì§€ ì•ŠìŒ</span><br><span class="line">ï® AI ì›Œì‹±ì„ í”¼í•˜ë ¤ë©´?</span><br><span class="line">â—‹ AI ê¸°ìˆ ì´ ì§€ì†ì ìœ¼ë¡œ ë°œì „í•¨ì— ë”°ë¼ AI ì›Œì‹±ì— ëŒ€í•œ ê·œì œì™€ ì†Œë¹„ì ë³´í˜¸ì˜ ì¤‘ìš”ì„±ì´ ë”ìš± ê°•ì¡°ë  ê²ƒì´ë©°,</span><br><span class="line">ê¸°ì—…ì€ íˆ¬ëª…í•œ AI ì‚¬ìš©ì„ í†µí•´ ì†Œë¹„ìì™€ íˆ¬ììì˜ ì‹ ë¢°ë¥¼ í™•ë³´í•¨ìœ¼ë¡œì¨ ì§„ì •í•œ í˜ì‹ ì„ ë„ëª¨í•  í•„ìš”</span><br><span class="line">ã€ì •ë¶€ ë° ê¸°ì—…ã€‘</span><br><span class="line">â—‹ [ê·œì œ ê°•í™”] ì •ë¶€ëŠ” AI ì›Œì‹±ì— ëŒ€í•œ ê·œì œë¥¼ ê°•í™”í•¨ìœ¼ë¡œì¨ ê¸°ì—…ì´ AI ì‚¬ìš©ì— ëŒ€í•œ íˆ¬ëª…ì„±ì„ ë†’</span><br><span class="line">ì´ê³  í—ˆìœ„ ì£¼ì¥ì„ ë‚¨ë°œí•˜ì§€ ì•Šë„ë¡ ìœ ë„í•  í•„ìš”</span><br><span class="line"></span><br><span class="line">6 Shira Ovide, Apr. 5, 2024, â€œThis $400 toothbrush is peak AI maniaâ€, The Washington Post</span><br><span class="line">[ê·¸ë¦¼ 6] ì˜¤ë„ë¹„ iOì‹œë¦¬ì¦ˆ í™ë³´ë¬¸êµ¬</span><br><span class="line">ìë£Œ: ì˜¤ë„ë¹„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´</span><br><span class="line">[ê·¸ë¦¼ 7] ì½”ì¹´ì½œë¼ Y3000</span><br><span class="line">ìë£Œ: ì½”ì¹´ì½œë¼ í™ˆí˜ì´ì§€</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">7</span><br><span class="line">ï‚– ë¯¸êµ­ ì¦ê¶Œê±°ë˜ìœ„ì›íšŒì™€ ë²•ë¬´ë¶€, ì—°ë°©ê±°ë˜ìœ„ì›íšŒëŠ” AI ì›Œì‹± ê¸°ì—…ì„ ëŒ€ìƒìœ¼ë¡œ ê¸°ì†Œí•˜ê±°ë‚˜ ê±°ì•¡ì˜</span><br><span class="line">ë²Œê¸ˆì„ ë¶€ê³¼í•˜ëŠ” ë“±ì˜ ì¡°ì¹˜ë¥¼ ì·¨í•¨ìœ¼ë¡œì¨ AI ì™€ ê´€ë ¨ëœ í—ˆìœ„ ë˜ëŠ” ì˜¤í•´ì˜ ì†Œì§€ê°€ ìˆëŠ” ì£¼ì¥ì„ í• </span><br><span class="line">ê²½ìš° ë‹¨ì† ëŒ€ìƒì´ ë  ìˆ˜ ìˆë‹¤ëŠ” ê²½ê³  ë©”ì‹œì§€ë¥¼ ì „ë‹¬</span><br><span class="line">- ì¦ê¶Œê±°ë˜ìœ„ì›íšŒëŠ” ì§€ë‚œí•´ ë§ JPëª¨ê±´ì²´ì´ìŠ¤ ë“± ëŒ€í˜• ê¸ˆìœµì‚¬ì˜ AI ì‚¬ìš© ì‹¤íƒœì— ëŒ€í•œ ì „ìˆ˜</span><br><span class="line">ì¡°ì‚¬ì— ì°©ìˆ˜. ê³ ê° í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬ì— í™œìš©ë˜ëŠ” AI ì•Œê³ ë¦¬ì¦˜ ëª¨ë¸ ë° ê´€ë ¨ ë§ˆì¼€íŒ… ì„œë¥˜,</span><br><span class="line">ë°ì´í„°ì— ëŒ€í•œ ì œ3ì ì œê³µ í˜„í™©, ì»´í”Œë¼ì´ì–¸ìŠ¤ êµìœ¡ ì‚¬í•­ ë“±ì´ ìš”ì²­ ë‚´ì—­ì— í¬í•¨ë¨7</span><br><span class="line">- ì—°ë°©ê±°ë˜ìœ„ì›íšŒëŠ” ê¸°ì—…ì´ â€œAI ì œí’ˆì˜ ê¸°ëŠ¥ì„ ê³¼ì¥í•˜ê³  ìˆì§€ ì•Šë‚˜ìš”? AI ì œí’ˆì´ ë¹„AI ì œ</span><br><span class="line">í’ˆë³´ë‹¤ ë” ë‚˜ì€ ê²ƒì²˜ëŸ¼ í™ë³´í•˜ê³  ìˆë‚˜ìš”? ì œí’ˆì´ ì‹¤ì œë¡œ AIë¥¼ ì‚¬ìš©í•˜ê³  ìˆë‚˜ìš”?â€ ë“±ì˜</span><br><span class="line">ì§ˆë¬¸ì„ í†µí•´ ìê¸° ê²€ì—´ì„ ê°•í™”í•  ê²ƒì„ ê¶Œê³ </span><br><span class="line">ï‚– ì˜êµ­ ê´‘ê³ í‘œì¤€ìœ„ì›íšŒ(Advertising Standards Authority)ëŠ” AI ì›Œì‹±ì— ëŒ€í•œ ê·œì¹™ê³¼ ë²•ë¥ ì„ ì œì •.</span><br><span class="line">ì—¬ê¸°ì—ëŠ” AI ì™€ ê´€ë ¨í•˜ì—¬ ì‹¤ì§ˆì ìœ¼ë¡œ ì˜¤í•´ì˜ ì†Œì§€ê°€ ìˆëŠ” ì§„ìˆ ì„ ê¸ˆì§€í•˜ëŠ” ë‚´ìš©ì´ í¬í•¨ë¨</span><br><span class="line">â—‹ [íˆ¬ëª…í•œ ê¸°ìˆ  ì‚¬ìš© ë° ì •ë³´ ì œê³µ] ê¸°ì—…ì€ ì‹¤ì œ ì‚¬ìš©í•œ AI ê¸°ìˆ ê³¼ ì¼ì¹˜í•˜ëŠ” íˆ¬ëª…í•œ ì •ë³´ë¥¼ ì œê³µ</span><br><span class="line">í•˜ê³ , ê·œì œ ê¸°ê´€ ë“±ì˜ ê°ì‚¬ë‚˜ ì™¸ë¶€ ì˜í˜¹ì— ëŒ€ë¹„í•˜ì—¬ ê¸°ë¡í•˜ê³  ë¬¸ì„œí™”í•  í•„ìš”</span><br><span class="line">ï‚– AI ê¸°ìˆ ì„ í™ë³´í•  ë•Œ í—ˆìœ„ ì£¼ì¥ì„ í”¼í•˜ê³  ê´€ë ¨ ì •ë³´ë¥¼ íˆ¬ëª…í•˜ê²Œ ì œê³µí•¨ìœ¼ë¡œì¨ ì†Œë¹„ìì™€ íˆ¬ììì—</span><br><span class="line">ê²Œ ì‹ ë¢°ë¥¼ ì–»ì–´ì•¼ í•¨</span><br><span class="line">- ì¶”ê°€ì ìœ¼ë¡œ ê¸°ì—… ë‚´ë¶€ì˜ ê° ë¶€ì„œê°€ í˜‘ë ¥í•˜ì—¬ AI ê¸°ìˆ  ì‚¬ìš©ì— ëŒ€í•œ ì •í™•í•œ ì •ë³´ë¥¼ ê³µìœ í•´</span><br><span class="line">ì•¼ í•˜ë©°, AI ê¸°ìˆ ì´ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ì— ëŒ€í•œ ìƒì„¸í•œ ê¸°ë¡ì„ ë‚¨ê¸°ê³  ì •ê¸°ì ìœ¼ë¡œ</span><br><span class="line">ì—…ë°ì´íŠ¸ë¥¼ í•´ì•¼ í•¨</span><br><span class="line">â—‹ [ì†Œë¹„ì êµìœ¡] ì •ë¶€ë‚˜ ê¸°ì—…ì€ ì†Œë¹„ìê°€ AI ê¸°ìˆ ì— ëŒ€í•œ ì´í•´ë„ë¥¼ ë†’ì´ê³  AIì™€ ê´€ë ¨ëœ ê³¼ì¥ëœ</span><br><span class="line">ì£¼ì¥ì— í˜„í˜¹ë˜ì§€ ì•Šë„ë¡ ì†Œë¹„ì ëŒ€ìƒ êµìœ¡ í”„ë¡œê·¸ë¨ì„ ìš´ì˜í•  í•„ìš”</span><br><span class="line">ï‚– êµìœ¡ í”„ë¡œê·¸ë¨ ë“±ì„ ì œê³µí•˜ì—¬ ì†Œë¹„ìê°€ AI ê¸°ìˆ ì˜ í•œê³„ì™€ ì‹¤ì œ í™œìš© ê°€ëŠ¥ì„±ì— ëŒ€í•´ ëª…í™•í•˜ê²Œ ì´</span><br><span class="line">í•´í•¨ìœ¼ë¡œì¨ ê³¼ëŒ€ ê´‘ê³  ë° í—ˆìœ„ íˆ¬ì ìœ ì¹˜ ë“±ì— í˜„í˜¹ë˜ì§€ ì•ŠëŠ” ì—­ëŸ‰ì„ í•¨ì–‘ì‹œí‚¤ëŠ” ê²ƒì´ ì¤‘ìš”</span><br><span class="line">ã€ì†Œë¹„ì ì…ì¥ã€‘</span><br><span class="line">â—‹ [ë¹„íŒì  íƒœë„ ê°–ê¸°] ì†Œë¹„ìì™€ íˆ¬ììëŠ” ê¸°ì—…ì˜ AI í™œìš© ì£¼ì¥ì— ëŒ€í•´ ë¹„íŒì  íƒœë„ë¥¼ ê°€ì ¸ì•¼ í•¨.</span><br><span class="line">ì‚¬ìš©ëœ AI ëª¨ë¸ï‚ì•Œê³ ë¦¬ì¦˜ê³¼ ê°™ì€ ê¸°ìˆ ì ì¸ ë¶€ë¶„ì— ëŒ€í•œ êµ¬ì²´ì  ì–¸ê¸‰ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ê¸°ì—…</span><br><span class="line">ì´ ê´€ë ¨ ë°ì´í„°ì™€ ì•Œê³ ë¦¬ì¦˜ ìœ í˜•ì— ëŒ€í•œ íˆ¬ëª…ì„±ì„ ìœ ì§€í•˜ëŠ”ì§€ ì ê²€í•  í•„ìš”</span><br><span class="line"></span><br><span class="line">7 ê¹€í˜„ìˆ˜ï‚ì‹ ì•„í˜•, 2023.12.12, â€œç¾ SEC â€˜íˆ¬ììˆ˜ìµ ê³¼ì¥í•˜ëŠ” AIì›Œì‹± ìœ„í—˜â€™â€¦ ì›”è¡— ì‹¤íƒœ ì „ìˆ˜ì¡°ì‚¬â€, ã€Šë™ì•„ì¼ë³´ã€‹</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">8</span><br><span class="line">ï‚– ì†Œë¹„ìì™€ íˆ¬ììê°€ ì£¼ì²´ì ìœ¼ë¡œ ì •ë³´ë¥¼ ê²€í† í•˜ê³  ê²€ì¦í•˜ëŠ” íƒœë„ë¥¼ ê°–ëŠ” ê²ƒì´ ì¤‘ìš”. ê¸°ì—…ì´ AI ê¸°ìˆ </span><br><span class="line">ì— ëŒ€í•´ ë‹¨ìˆœíˆ â€˜í˜ì‹ ì â€™ ë˜ëŠ” â€˜ì§€ëŠ¥ì â€™ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ë‚¨ë°œí•˜ëŠ” ê²ƒì´ ì•„ë‹Œì§€ ë¹„íŒì ì¸ ì‹œê°ìœ¼ë¡œ ë°”</span><br><span class="line">ë¼ë³¼ í•„ìš”</span><br><span class="line">â—‹ [íˆ¬ëª…í•œ ì„¤ëª… ìš”êµ¬í•˜ê¸°] ì†Œë¹„ìëŠ” AI ì‘ë™ ë°©ì‹ì— ëŒ€í•œ íˆ¬ëª…í•œ ì„¤ëª…ì„ ìš”êµ¬í•˜ì—¬ ê¸°ì—…ì´ ì±…ì„ê°ì„</span><br><span class="line">ê°–ê³  ê¸°ìˆ ì˜ ì‹¤ì œ ì„±ëŠ¥ê³¼ í•œê³„ ë“±ì„ í¬í•¨í•´ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ë„ë¡ ìœ ë„í•´ì•¼ í•¨</span><br><span class="line">ï‚– AI ì‹œìŠ¤í…œì˜ ë°ì´í„° ì¶œì²˜ë‚˜ ì•Œê³ ë¦¬ì¦˜ ì‘ë™ ë°©ì‹, ì •í™•ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì–´ë–¤ ì¡°ì¹˜ë¥¼ ì·¨í–ˆëŠ”ì§€ ë“±</span><br><span class="line">ì— ëŒ€í•œ ì„¤ëª…ì„ ìš”êµ¬í•´ì•¼ í•¨. ë°ì´í„°ì™€ ì•Œê³ ë¦¬ì¦˜ì˜ í¸í–¥ì´ë‚˜ AI í™˜ê°ì„ í”¼í•˜ê¸° ìœ„í•œ í•´ê²°ì±…ì´ ì—†</span><br><span class="line">ëŠ” ê²½ìš° ì‹¤ì œë¡œ AI ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ì„ ê°€ëŠ¥ì„±ì´ ì¡´ì¬</span><br><span class="line">&lt;ì±…ì„ì—°êµ¬ì› ì†¡ì›í˜¸(wonho.song@kbfg.com) â˜02)2073-5730&gt;</span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-08-21</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/08/21/2024-8-26-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 8ì›” 26ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/08/21/2024-8-21-AI-NEWS/" title="2024ë…„ 8ì›” 21ì¼ AI ì†Œì‹">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/08/16/2024-8-16-AI-NEWS/" title="2024ë…„ 8ì›” 16ì¼ AI ì†Œì‹">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>