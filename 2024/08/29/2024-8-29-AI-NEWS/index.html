<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 8월 29일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="NAVER에서는 HyperCLOVA X Vision을 발표하며, 텍스트와 이미지 처리가 가능한 대규모 비전-언어 모델을 소개하였습니다. 또한, 오토브라우징 기술을 통해 LLM의 실시간 정보 수집과 처리 능력을 강화하는 방법도 제시하였습니다. Google은 세 가지 새로"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 8월 29일 AI 소식</a></h3></div><div class="post-content"><p>NAVER에서는 HyperCLOVA X Vision을 발표하며, 텍스트와 이미지 처리가 가능한 대규모 비전-언어 모델을 소개하였습니다. 또한, 오토브라우징 기술을 통해 LLM의 실시간 정보 수집과 처리 능력을 강화하는 방법도 제시하였습니다. Google은 세 가지 새로운 Gemini 모델을 출시하여 고성능과 효율성을 동시에 갖춘 AI 모델을 선보였습니다. NVIDIA는 NIM Agent Blueprints를 통해 기업들이 맞춤형 AI 애플리케이션을 손쉽게 구축할 수 있는 기반을 마련했습니다. Cartesia는 SSM 기반의 소형 언어 모델 Rene를 발표하여 온디바이스 AI의 효율성을 극대화하였으며, Tsinghua 대학은 CogVideoX라는 텍스트-비디오 생성 모델을 공개하였습니다. Meta는 LLM의 사이버 보안 능력을 평가하기 위한 CYBERSECEVAL 3 벤치마크를 발표하였고, LLM-as-Judge 접근법 및 Anchored Preference Optimization(APO) 기법을 포함한 새로운 AI 정렬 및 평가 방법에 대한 종합적인 분석도 소개되었습니다.</p>
<h3 id="NAVER-HyperCLOVA-X-Vision-발표"><a href="#NAVER-HyperCLOVA-X-Vision-발표" class="headerlink" title="NAVER, HyperCLOVA X Vision 발표"></a>NAVER, HyperCLOVA X Vision 발표</h3><p><a target="_blank" rel="noopener" href="https://clova.ai/en/hyperclova">링크</a>, 2024년 8월 19일</p>
<ul>
<li><strong>HyperCLOVA X Vision</strong>은 기존의 대규모 언어 모델(LLM)에서 확장된 대규모 비전-언어 모델(LVLM)로, 텍스트와 이미지를 동시에 이해할 수 있는 능력을 갖춤.</li>
<li>이 모델은 한국어에 특화된 데이터셋으로 학습되었으며, NAVER의 우수한 OCR 기술을 통해 한국어와 손글씨 텍스트 인식에서 높은 정확성을 보임.</li>
<li><strong>주요 기능</strong>: 문서 인식, 이미지 내 텍스트 이해, 차트 및 표 이해, 문화적 맥락 및 유머 이해, 수식 이해, 창의적 글쓰기 등.</li>
<li>성능 평가에서는 SEEDv1, TextVQA, DocVQA, ChartQA 등 30개 이상의 벤치마크에서 GPT-4V와 비교하여 대부분의 지표에서 우위를 점함.</li>
<li>특히 한국어 일반 교육 개발(K-GED) 테스트에서 GPT-4o보다 높은 83.8%의 정답률을 기록, 한국어 처리 능력에서 탁월한 성능을 입증함.</li>
<li>HyperCLOVA X Vision은 앞으로 영상 스트림 처리와 고해상도 처리 능력을 추가하는 등 지속적인 업데이트가 예정되어 있으며, 향후 로봇 및 독립형 에이전트에 활용될 가능성도 있음.</li>
</ul>
<h3 id="NAVER-오토브라우징-기술로-LLM의-한계를-극복"><a href="#NAVER-오토브라우징-기술로-LLM의-한계를-극복" class="headerlink" title="NAVER, 오토브라우징 기술로 LLM의 한계를 극복"></a>NAVER, 오토브라우징 기술로 LLM의 한계를 극복</h3><p><a target="_blank" rel="noopener" href="https://clova.ai/tech-blog/%EC%98%A4%ED%86%A0%EB%B8%8C%EB%9D%BC%EC%9A%B0%EC%A7%95-llm%EC%9D%98-%ED%95%9C%EA%B3%84%EB%A5%BC-%EA%B7%B9%EB%B3%B5%ED%95%98%EB%8B%A4">링크</a>, 2024년 8월 26일</p>
<ul>
<li><strong>오토브라우징</strong>은 웹 탐색 기술과 AI를 결합한 형태로, AI가 웹을 자율적으로 탐색하여 실시간으로 정보를 수집하고 처리할 수 있게 함.</li>
<li>이 기술은 동적 콘텐츠와 로그인 필요한 페이지 접근이 가능하며, 기존 웹 탐색 기술의 한계를 극복함.</li>
<li>오토브라우징은 AI가 실시간으로 정보를 탐색하여 수집된 데이터를 바탕으로 사용자에게 최적화된 응답을 제공할 수 있게 함.</li>
<li>NAVER의 <strong>Project CONNECT X</strong>에서는 이 기술을 활용해 기업의 내부 데이터와 시스템을 통합하여 업무 자동화와 효율성을 높임.</li>
<li><strong>RPA</strong>(Robotic Process Automation)와 <strong>RAG</strong>(Retrieval-Augmented Generation) 기술과 비교하며, 오토브라우징이 어떻게 실시간 정보 반영 문제를 해결하고, 비정형 데이터에 대한 처리 능력을 확장하는지 설명함.</li>
<li><strong>소버린 AI</strong>와 결합하여 특정 국가나 문화권에 맞춘 AI 시스템 구축이 가능해질 것으로 기대됨.</li>
</ul>
<h3 id="Google-새로운-Gemini-모델-세-가지-출시"><a href="#Google-새로운-Gemini-모델-세-가지-출시" class="headerlink" title="Google, 새로운 Gemini 모델 세 가지 출시"></a>Google, 새로운 Gemini 모델 세 가지 출시</h3><p><a target="_blank" rel="noopener" href="https://www.unite.ai/google-releases-three-new-experimental-gemini-models/">링크</a>, 2024년 8월 27일</p>
<ul>
<li><strong>Gemini 1.5 Flash 8B</strong>: 8억 매개변수를 가진 모델로, 멀티모달 작업과 장문 요약에 최적화된 성능을 제공. 특히 대량의 데이터를 빠르게 처리할 수 있는 효율적인 구조로 설계됨.</li>
<li><strong>Enhanced Gemini 1.5 Pro</strong>: 복잡한 프롬프트 처리와 코딩 작업에 강점을 가진 모델로, 이전 버전보다 모든 측면에서 성능이 개선됨. 특히 고난이도 작업에서 뛰어난 성능을 발휘.</li>
<li><strong>Improved Gemini 1.5 Flash</strong>: 속도와 효율성에 중점을 둔 모델로, Google의 내부 벤치마크에서 성능이 크게 향상된 것으로 보고됨. 이 모델은 빠른 처리와 고품질 결과를 동시에 제공.</li>
<li>이들 모델은 Google AI Studio와 Gemini API를 통해 무료로 제공되며, 개발자들이 실험적으로 사용해볼 수 있음. Google은 이 모델들을 통해 실제 환경에서의 피드백을 받아 향후 개선에 반영할 계획.</li>
</ul>
<h3 id="NVIDIA-NIM-Agent-Blueprints-발표"><a href="#NVIDIA-NIM-Agent-Blueprints-발표" class="headerlink" title="NVIDIA, NIM Agent Blueprints 발표"></a>NVIDIA, NIM Agent Blueprints 발표</h3><p><a target="_blank" rel="noopener" href="https://nvidianews.nvidia.com/news/nvidia-and-global-partners-launch-nim-agent-blueprints-for-enterprises-to-make-their-own-ai?ncid=so-link-468011">링크</a>, 2024년 8월 27일</p>
<ul>
<li><strong>NIM Agent Blueprints</strong>는 사전 학습된 AI 워크플로우 카탈로그로, 고객 서비스 아바타, 약물 발견, PDF 데이터 추출 등 다양한 AI 애플리케이션을 위한 블루프린트를 제공.</li>
<li>이 블루프린트는 NVIDIA NeMo와 NIM 마이크로서비스를 통합하여 사용자 데이터를 바탕으로 맞춤형 AI 애플리케이션을 개발하고 운영할 수 있게 함.</li>
<li><strong>디지털 휴먼 NIM Agent Blueprint</strong>는 고객 서비스에 사용될 수 있는 3D 애니메이션 아바타 인터페이스를 제공하여, 기존 고객 서비스 옵션보다 더 매력적인 사용자 경험을 제공함.</li>
<li><strong>멀티모달 PDF 데이터 추출 워크플로우</strong>는 대규모 기업 PDF 데이터를 분석하여, 정확하고 포괄적인 응답을 생성할 수 있게 함.</li>
<li><strong>약물 발견 가상 스크리닝 워크플로우</strong>는 3D 단백질 구조 예측, 분자 생성 및 도킹을 가속화하여 신약 후보 물질의 발굴을 지원함.</li>
<li>NVIDIA의 글로벌 파트너들과 함께 기업들이 이 블루프린트를 통해 AI 솔루션을 신속하게 구축하고 배포할 수 있도록 지원하고 있음.</li>
<li>추가적인 블루프린트는 매월 출시될 예정이며, 고객 경험, 콘텐츠 생성, 소프트웨어 엔지니어링 및 제품 연구개발 등 다양한 분야에 적용될 수 있음.</li>
</ul>
<h3 id="Cartesia-Rene-1-3B-모델-발표"><a href="#Cartesia-Rene-1-3B-모델-발표" class="headerlink" title="Cartesia, Rene 1.3B 모델 발표"></a>Cartesia, Rene 1.3B 모델 발표</h3><p><a target="_blank" rel="noopener" href="https://cartesia.ai/blog/2024-08-27-on-device">링크</a>, 2024년 8월 27일</p>
<ul>
<li><strong>Rene 1.3B</strong>는 1.3억 매개변수를 가진 소형 언어 모델로, 온디바이스에서 효율적으로 실행될 수 있도록 설계된 SSM(State Space Model) 기반의 모델.</li>
<li>Mamba-2와 MLP 레이어를 결합한 하이브리드 구조로, Sliding Window Attention(SWA) 레이어를 통해 고정된 메모리 풋프린트를 유지하며, 리소스 제약이 있는 환경에서 안정적으로 실행 가능.</li>
<li>1.5T 토큰으로 훈련되었으며, Apple의 OpenELM과 Google의 Gemma 2B와 같은 유사 크기의 모델들에 비해 뛰어난 성능을 발휘.</li>
<li>Cartesia는 또한 <strong>Sonic On-Device</strong>라는 초저지연 음성 생성 모델을 공개, 이 모델은 실시간 스트리밍과 음성 복제를 지원하며 다양한 디바이스에서 실행 가능.</li>
<li><strong>Edge 라이브러리</strong>를 통해 SSM 기반 모델의 연구와 배포를 지원하며, Apple 하드웨어에서 최적화된 Metal 커널을 제공.</li>
<li>Cartesia는 향후 SSM 기술이 에지 컴퓨팅과 온디바이스 AI의 주요 요소로 자리 잡을 것으로 기대하며, 이를 통해 개인 비서, 로봇, 게임, 보안, 의료 등 다양한 응용 분야에서 혁신을 추구하고 있음.</li>
</ul>
<h3 id="Tsinghua-대학-CogVideoX-5B-모델-발표"><a href="#Tsinghua-대학-CogVideoX-5B-모델-발표" class="headerlink" title="Tsinghua 대학, CogVideoX 5B 모델 발표"></a>Tsinghua 대학, CogVideoX 5B 모델 발표</h3><p>[링크</p>
<p>](<a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/CogVideoX-5b">https://huggingface.co/THUDM/CogVideoX-5b</a>), 2024년 8월 23일</p>
<ul>
<li><strong>CogVideoX 5B</strong>는 텍스트에서 비디오로 변환하는 대규모 디퓨전 모델로, 3D 변량 오토인코더(VAE)를 활용하여 비디오 데이터를 효율적으로 압축.</li>
<li>텍스트와 비디오 간의 깊은 융합을 위해 전문가 변환기와 전문가 적응형 LayerNorm을 도입, 텍스트-비디오 정렬 성능을 개선.</li>
<li>프로그레시브 학습 기법을 통해 긴 영상에서 일관성 있는 움직임과 높은 질의 비디오 생성 가능.</li>
<li>이 모델은 Luma, Runway, Pika와 같은 모델들과 비교하여 최첨단 성능을 보이며, 기계 및 인간 평가에서 우수한 성적을 기록.</li>
<li>CogVideoX는 비디오 생성 품질과 의미적 정렬 모두에서 뛰어난 성과를 보여줌.</li>
</ul>
<h3 id="Meta-CYBERSECEVAL-3-벤치마크-발표"><a href="#Meta-CYBERSECEVAL-3-벤치마크-발표" class="headerlink" title="Meta, CYBERSECEVAL 3 벤치마크 발표"></a>Meta, CYBERSECEVAL 3 벤치마크 발표</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.01605">링크</a>, 2024년 8월 2일</p>
<ul>
<li>Meta는 LLM의 사이버 보안 능력을 평가하기 위한 새로운 벤치마크 <strong>CYBERSECEVAL 3</strong>을 발표.</li>
<li>이 벤치마크는 LLM의 사이버 보안 위험과 능력을 평가하는 8가지 다른 위험 요소를 포함, 특히 자동화된 사회공학, 사이버 공격 확장, 자율적 사이버 공격 등 새로운 영역을 다룸.</li>
<li>CYBERSECEVAL 3은 Llama 3 모델과 다양한 최신 LLM에 적용되어, 완화 조치가 있는 상태와 없는 상태에서의 위험을 비교하여 평가.</li>
<li>Meta는 이 연구를 통해 LLM의 사이버 보안 위험 평가에 대한 논의를 이어가고자 함.</li>
</ul>
<h3 id="LLM-as-Judge-접근법-종합-분석"><a href="#LLM-as-Judge-접근법-종합-분석" class="headerlink" title="LLM-as-Judge 접근법 종합 분석"></a>LLM-as-Judge 접근법 종합 분석</h3><p><a target="_blank" rel="noopener" href="https://eugeneyan.com/writing/llm-evaluators/">링크</a>, 2024년 8월 14일</p>
<ul>
<li>LLM-evaluators, 또는 <strong>LLM-as-Judge</strong>는 다른 LLM의 응답을 평가하는 모델로, 최근 복잡하고 개방형 작업에서 사용이 증가하고 있음.</li>
<li><strong>평가 접근법</strong>: 직접 평가(Direct Scoring), 쌍비교(Pairwise Comparison), 참조 기반 평가(Reference-based Evaluation) 등 세 가지 주요 접근법이 있음.<ul>
<li><strong>직접 평가</strong>: 단일 응답을 평가하며, 객관적인 평가에 적합.</li>
<li><strong>쌍비교</strong>: 두 개의 응답을 비교하여 더 나은 것을 선택, 주관적인 평가에 적합.</li>
<li><strong>참조 기반 평가</strong>: 생성된 응답을 골드 표준과 비교하여 평가.</li>
</ul>
</li>
<li><strong>평가 메트릭</strong>: Cohen’s kappa, Kendall’s tau, Spearman’s rho 등 다양한 상관 메트릭이 사용되며, 각각의 데이터 유형에 맞는 메트릭 선택이 중요함.</li>
<li>LLM-as-Judge는 특히 사람 평가자와의 상관성을 높이기 위해 여러 가지 기법을 사용하며, 성능과 신뢰성을 향상시키는 다양한 방법들이 연구되고 있음.</li>
</ul>
<h3 id="Anchored-Preference-Optimization-및-Contrastive-Learning-from-AI-Revisions-CLAIR-기법-발표"><a href="#Anchored-Preference-Optimization-및-Contrastive-Learning-from-AI-Revisions-CLAIR-기법-발표" class="headerlink" title="Anchored Preference Optimization 및 Contrastive Learning from AI Revisions (CLAIR) 기법 발표"></a>Anchored Preference Optimization 및 Contrastive Learning from AI Revisions (CLAIR) 기법 발표</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2408.06266">링크</a>, 2024년 8월 13일</p>
<ul>
<li>**Anchored Preference Optimization (APO)**는 대조적 선호 최적화를 통해 모델의 정렬 성능을 향상시키는 새로운 RLHF(보상 강화 학습) 기법.<ul>
<li><strong>APO-zero</strong>: 선호하는 출력이 더 나을 경우, 그 출력을 강화하고 부정적 출력을 억제하는 방식.</li>
<li><strong>APO-down</strong>: 모델이 선호하는 출력보다 일반적으로 더 나은 경우, 전체적으로 억제하되 부정적 출력을 더 강하게 억제하는 방식.</li>
</ul>
</li>
<li>**Contrastive Learning from AI Revisions (CLAIR)**은 GPT-4 터보 모델을 사용하여 각 프롬프트&#x2F;응답 쌍에 대해 최소한의 수정된 출력을 생성, 대조적 데이터셋을 만들어 성능을 향상시키는 기법.</li>
<li>CLAIR와 APO-zero를 결합하여 Llama 3-8B-Instruct 모델의 성능을 7.65% 향상시켰으며, GPT-4 터보와의 성능 격차를 45% 줄임.</li>
<li>이 기법들은 MixEval-Hard에서 성능이 검증되었으며, 추후 더 다양한 데이터셋과 모델에서의 평가가 기대됨.</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://clova.ai/en/hyperclova</span><br><span class="line">Aug 19, 2024</span><br><span class="line">NAVER</span><br><span class="line"></span><br><span class="line">Introducing HyperCLOVA X Vision</span><br><span class="line">Image for Introducing HyperCLOVA X Vision</span><br><span class="line">Introduction</span><br><span class="line">They say a picture is worth a thousand words to stress the power of vision over text. They also say eyes are windows to the soul, emphasizing our ability to take in visual information. Think of what it means for HyperCLOVA X (HCX) to understand images on top of text: we could automate document and image processing. Or, in the more distant future, we could use it to power the brains of robots, and these independent agents could perform tasks that require vision capabilities.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">For language models to process visual information, we have to train them with large amounts of text and image data. To create our AI model, we first obtained high-quality training data, some of which came from our NAVER services. We then tried to make the best of these data so that they would not contradict each other but create synergy. All these efforts culminated in HyperCLOVA X Vision, which can now understand documents and more. We added image capabilities without compromising its existing text capabilities while also ensuring safety. In this post, we’ll walk you through our HyperCLOVA X Vision use cases to help you get an idea of its technology and performance.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Technology behind HyperCLOVA X Vision</span><br><span class="line">HCX Vision advanced from a large language model (LLM) to a large vision-language model (LVLM). Trained on wide-ranging visual and language data, it can now support text and image modalities and perform tasks in different scenarios, such as recognizing documents and understanding text within images.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Drawing on our world-class OCR technology of extracting Korean and handwritten text, HCX Vision delivers accurate and secure services through enhanced document processing.</span><br><span class="line">HCX Vision is powered by HCX, an LLM trained on vast amounts of Korean data. Its knowledge of the Korean language and culture is unmatched, and its abilities have been successfully translated into HCX Vision to understand both documents and images.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Quantitative evaluations</span><br><span class="line"></span><br><span class="line">Public Benchmarks</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SEEDv1</span><br><span class="line"></span><br><span class="line">(image)</span><br><span class="line"></span><br><span class="line">MMMU</span><br><span class="line"></span><br><span class="line">(val)</span><br><span class="line"></span><br><span class="line">TextVQA</span><br><span class="line"></span><br><span class="line">(val)</span><br><span class="line"></span><br><span class="line">DocVQA</span><br><span class="line"></span><br><span class="line">(test)</span><br><span class="line"></span><br><span class="line">ChartQA</span><br><span class="line"></span><br><span class="line">(test)</span><br><span class="line"></span><br><span class="line">InfographicVQA</span><br><span class="line"></span><br><span class="line">(test)</span><br><span class="line"></span><br><span class="line">MathVista</span><br><span class="line"></span><br><span class="line">(testmini)</span><br><span class="line"></span><br><span class="line">VQAv2	Average</span><br><span class="line">GPT-4V	69.1	56.8	78	88.4	78.5	75.1	49.9	77.2	71.63</span><br><span class="line">HCX-VLM	75.6	45.1	77.9	89.8	81	65.2	57.1	81	71.59</span><br><span class="line">Accuracy (%)	99.94</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">We use over 30 benchmarks to track the performance of HCX Vision. Here, we examine some of them to compare our model against GPT-4V. We’ll continue to bring updates in the second half of 2024, including enhancements to reasoning and vision alignment tasks, changes to the encoder structure for high-resolution processing, and more.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">K-GED performance</span><br><span class="line"></span><br><span class="line">Model	Correct Answers</span><br><span class="line">GPT-4o	1152/1480 (77.8%)</span><br><span class="line">HCX-VLM	1240/1480 (83.8%)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">We thought that one benchmark that could showcase a model’s Korean capabilities was the Korean General Educational Development (K-GED) tests, which are primary and secondary education equivalency diplomas. The benchmark consists of 1,480 four-option multiple-choice questions. Even when testing with image inputs, HCX Vision answered correctly 83.8% of the time, surpassing the 60% pass threshold and the 77.8% scored by GPT-4o.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Capabilities and use cases</span><br><span class="line">Here, we’ll show you how you can leverage HyperCLOVA X Vision across multiple use cases. Our development is still in progress, and our model has its limitations. But we feel we’ve come a long way since the days when we had to create a separate model for each individual task.</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">Detailed image captioning</span><br><span class="line">Image captioning allows you to generate image descriptions. HCX Vision can accurately identify and describe small details in an image without using a separate object detection model.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Reasoning</span><br><span class="line">HCX Vision can reason and predict the next step based on images you have uploaded. It leverages the unique capabilities of an LLM but with added visual features.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Entity recognition</span><br><span class="line">An entity can be a person, place, product, or anything that holds meaning in and of itself. HCX Vision can name historical figures, landmarks, products, and food with just image inputs. Even without using the Retrieval Augmented Generation (RAG) technique, our model can easily detect entities that were part of the training data.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Chart understanding</span><br><span class="line">Putting numerical data into a chart helps people understand data intuitively. But this seemingly easy task demands high performance on the part of machines because it requires making sense of something abstract.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Table understanding</span><br><span class="line">Tables are often used to compare numerical data. If the data is in an Excel or CSV file, it can be processed immediately by LLMs that support text modalities. If the data is a screenshot of an image, getting responses for your prompts is more complicated because the model must first recognize text and understand how the numbers are related.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Document understanding</span><br><span class="line">HCX Vision supports documents in Korean, English, Japanese, and Chinese.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Culture and humor (meme understanding)</span><br><span class="line">Understanding humor and cultural norms is a highly complex ability. HCX Vision is trained on large amounts of image and text pairs and can perform well in understanding memes.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Equation understanding</span><br><span class="line">Reading rendered math equations is easy, but rendering them into a digital format is difficult and could take a long time if you’re not used to writing math formulas using TeX. HCX Vision can help you save time manually inputting data.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">Code generation</span><br><span class="line">You can show HCX Vision a shape, chart, or graph and ask HCX Vision to generate code that resembles your picture.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Math problem-solving</span><br><span class="line">Get answers and explanations to math problems that include shapes.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">Creative writing (with image grounding)</span><br><span class="line">This feature is already available in the HCX language model, but HCX Vision gets even more creative by drawing inspiration from an image.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">What’s next in HyperCLOVA X Vision and sovereign AI?</span><br><span class="line">Right now, HyperCLOVA X Vision can understand one image at a time. But soon, with context length support in the millions, we expect HCX Vision to understand hours-long movies and video streams. Add to that real-time processing capabilities, and AI could become an integral part of our lives, just like robots in the movies who behave like humans.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">As with LLMs built to understand textual information, LVLMs must be able to understand and express different social values and norms, whatever the users’ geography and cultural backgrounds. AI sovereignty will play a vital role in interpreting visual information, making access to data aligned with our social values essential in building safe and useful AI. As Korea’s leading platform, NAVER holds an advantage in securing these kinds of quality data.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Conclusion</span><br><span class="line">We expect our LVLM technology to take us beyond the limits of text-based communications and closer to how people interact in the real world. At NAVER, we’re committed to making HCX Vision beneficial to people from diverse backgrounds in many domains, and we envision a future where HCX Vision will become a part of many people’s lives.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://clova.ai/tech-blog/%EC%98%A4%ED%86%A0%EB%B8%8C%EB%9D%BC%EC%9A%B0%EC%A7%95-llm%EC%9D%98-%ED%95%9C%EA%B3%84%EB%A5%BC-%EA%B7%B9%EB%B3%B5%ED%95%98%EB%8B%A4</span><br><span class="line">Aug 26, 2024</span><br><span class="line">NAVER</span><br><span class="line"></span><br><span class="line">오토브라우징, LLM의 한계를 극복하다</span><br><span class="line">Image for 오토브라우징, LLM의 한계를 극복하다</span><br><span class="line">오토브라우징, LLM의 한계를 극복하다</span><br><span class="line"></span><br><span class="line">인간은 본능적으로 효율을 추구합니다. 이는 단순히 편의를 위한 것이 아니라, 생존과 번영을 위한 진화적 전략으로 이해할 수 있습니다. 효율을 추구하는 본능은 발명과 기술 발전의 핵심 동력이 되어 왔습니다. 우리는 기술을 통해 단순 반복 작업에서 벗어나 한정된 자원을 더 높은 차원의 문제 해결과 창조적 활동에 활용하고 싶어 합니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">생성형 AI(Generative AI)가 처음 등장했을 때, 복잡하고 창의적인 작업이나 개인화된 업무 영역도 AI로 자동화할 수 있을 것이라는 기대가 컸습니다. 하지만 실시간 정보에 대한 제한이나 할루시네이션과 같은 한계가 존재했고, 여전히 인간의 검증이 필요하다는 사실이 드러났습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">현재 생성형 AI는 ‘현실적 적용의 단계‘로 진입하고 있다고 볼 수 있습니다. (Gartner, 2024) 초기의 과장된 기대는 사그라들고 기존 기술들과의 융합을 통해 한계를 극복해 나가는 단계를 의미합니다. 이러한 기술 간 융합은 AI의 능력을 확장하고 실제 비즈니스에 AI가 더욱 폭넓게 활용될 수 있게 도울 것입니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">이번 콘텐츠에서는 최근 각광받는 오토브라우징이 AI와 만나 어떤 시너지를 가져오고 비즈니스에 어떻게 활용될 수 있는지 알아보도록 하겠습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">생성형 AI, 잠재력과 한계 사이</span><br><span class="line"></span><br><span class="line">생성형 AI의 잠재력과 시장의 기대에 비해 기업들의 AI 활용 수준은 아직 제한적입니다. 이는 소버린 AI를 소개하는 콘텐츠에서 언급했던 국가 및 기업 간 기술 격차 외에도 여러가지 이유가 있습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">가장 주요한 원인은 정보의 신뢰성 문제입니다. 종종 AI는 사실과 다른 정보를 생성합니다. ‘할루시네이션‘이라고 불리우는 이 현상은 특정 주제에 관한 학습 데이터가 부족하거나 모델이 문맥을 정확히 파악하지 못하고 부적절한 추론을 할 때 발생합니다. 즉 잘 알려지지 않은 과학 이론에 대해 묻거나 복잡한 질문을 했을 때 나타날 가능성이 높습니다. 또한 AI는 사전에 학습한 정보를 기반으로 답변을 생성하기 때문에 상시 업데이트되는 실시간성 정보나 학습 이후의 데이터를 반영하지 못합니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">그래서 생성형 AI 사용 시에는 결과를 한 번 더 검토할 것이 권장되며, 중요한 의사결정에는 반드시 인간의 판단을 거칠 것을 명시하고 있습니다. 이러한 한계점은 우리가 상상하는 AI 시대로 나아가기 위해 반드시 극복해야 할 과제로 손꼽혀 왔습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">오토브라우징</span><br><span class="line"></span><br><span class="line">오토브라우징은 웹 탐색 기술과 AI를 결합한 형태로, AI 모델이 자동으로 웹을 탐색하여 필요한 정보를 수집, 처리, 제공하는 기술입니다. AI의 자율성을 활용하여 마치 사람이 마우스를 클릭하고 키보드를 입력하듯 자동으로 웹 브라우저를 조작하고 필요한 정보를 수집합니다. 이는 실제 사용자의 행동을 모방한다는 점에서 웹 스크래핑이나 웹 크롤링과 차별화됩니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">오토브라우징의 주요 장점은 클릭, 스크롤, 입력 등의 상호작용이 가능하다는 것입니다. 로그인이 필요한 페이지에 접근과 동적 콘텐츠 처리가 가능해 기존 웹 탐색 기술의 한계를 넘어섰다는 평가를 받고 있습니다. AI와 자연어 처리 기술의 발전으로 오토브라우징은 더욱 주목받고 있으며, 특히 실시간 정보 수집과 자동화된 데이터 처리의 필요성의 증가로 그 중요성이 커지고 있습니다. 오토브라우징을 활용하면 정보 수집과 처리의 효율성을 높이고 사용자 경험을 크게 향상시킬 수 있어, 다양한 서비스 분야에 적용될 것으로 기대됩니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">LLM과 오토브라우징</span><br><span class="line"></span><br><span class="line">채팅형 AI 서비스를 사용해 보셨다면 웹 검색이 필요한 정보는 제공할 수 없다는 답변을 받아 보셨을지도 모릅니다. 누군가 나를 대신해서 실시간으로 정보를 검색하고 취합하여 필요한 형태로 출력해 준다면 얼마나 편리할까요. LLM과 오토브라우징의 결합은 AI에 실시간으로 웹을 탐색할 수 있는 능력을 부여합니다. 이는 AI가 실시간으로 발생하는 정보를 반영한 답변을 생성할 수 있게 함으로써 잘못된 정보를 생성할 위험을 크게 줄여줍니다. 또한 다양한 웹페이지를 통합 분석하거나, 특정 웹페이지의 구조와 내용을 종합적으로 고려할 수 있기 때문에 보다 구체적이고 구조화된 답변을 생성할 수 있습니다. 두 기술의 융합은 생성형 AI의 한계를 보완하고 신뢰할 수 있는 도구로 발전시키는 핵심 요소로 손꼽히고 있습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">LLM과 오토브라우징으로 할 수 있는 일</span><br><span class="line"></span><br><span class="line">사용자의 질문에 대한 이해 및 분석 : 오토브라우징은 CoT라는 과정을 통해서, 사용자의 복합적인 의도와 질의를 분석하고, 검색 순서와 방식을 결정하여 Plan을 세웁니다. 이를 통해 병렬 검색이나 복합 추론 등 고차원적인 정보 탐색이 가능합니다.</span><br><span class="line">실시간 정보 검색 및 분석 : AI는 사용자의 질문에 대해 웹에서 최신 정보를 실시간으로 검색하고 분석합니다. 이는 경쟁사 동향을 모니터링하거나 마켓 트렌드를 탐색하는 데 유용합니다.</span><br><span class="line">데이터 수집 및 통합 분석 : 다양한 출처에서 수집된 데이터를 통합 분석하여 정확하고 포괄적인 정보를 제공할 수 있습니다. 예를 들어, 여러 여행 리뷰 사이트에서 특정 관광지에 대한 평가를 수집하고 분석하여 장단점을 종합적으로 제시할 수 있습니다. 이는 제품 개선, 시장 분석 등 다양한 의사결정 상황에 활용될 수 있습니다.</span><br><span class="line">구조화된 정보 제공 : 웹페이지의 구조와 내용을 종합적으로 분석하여 보다 유용한 답변을 생성할 수 있습니다. 예를 들어 Apple 제품 최신 정보를 요청했을 때, 공식 홈페이지의 구조를 분석하여 ‘특징, ‘사양’, ‘가격’ 등의 섹션을 식별하고 관련 정보를 추출할 수 있으며 ‘출시 일정’ 등의 헤드라인을 인식하여 중요 정보로 제공할 수 있습니다.</span><br><span class="line">동적 웹 환경 대응 : JavaScript로 렌더링되는 동적 웹페이지나 로그인이 필요한 페이지의 정보도 접근하여 처리할 수 있습니다. 이는 더 광범위하고 다양한 정보 소스에 접근할 수 있게 해줍니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">RPA, RAG 그리고 오토브라우징</span><br><span class="line"></span><br><span class="line">인간은 항상 단순 반복 작업을 자동화하려는 욕구를 가져 왔습니다. 이를 위해 활용되었던 기술의 발전 과정을 살펴보면서 오토브라우징에 대해 자세히 알아보도록 하겠습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">RPA : 생성형 AI 등장 이전에 RPA(Robotic Process Automation)가 있었습니다. RPA는 규칙 기반의 소프트웨어 ‘로봇‘을 사용하여 반복적인 업무 프로세스를 자동화합니다. 예를 들어, 은행에서는 RPA를 활용하여 온라인 대출 신청서의 정보를 자동으로 추출하고 은행의 내부 시스템에 입력하고 미리 설정된 기준에 따라 대출의 초기 심사를 자동으로 수행합니다. 하지만 RPA는 미리 정의된 규칙과 시나리오에 따라 작동하기 때문에 예외 상황이나 비정형적 데이터에 대응할 수 없으며, 자연어를 이해하고 처리할 수 없습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">RAG : 생성형 AI 등장 이후, AI가 잘못된 정보를 사실인 것처럼 생성하는 ‘할루시네이션’ 현상을 보완하기 위한 대안으로 RAG(Retrieval-Augmented Generation) 기술이 주목받기 시작했습니다. RAG는 AI가 응답을 생성할 때 관련 문서나 데이터베이스에서 정보를 검색하여 참조하는 방식입니다. 예를 들어, 법률 분야에 RAG를 활용하면 AI가 답변을 생성함에 있어 법령과 판례 데이터를 참조하여 더 정확한 법률 자문을 제공할 수 있습니다. RAG는 LLM에 정보 검색 능력을 결합한 모델로, 사용자의 질문이 들어오면 시스템은 답변을 출력하기 전에 사전에 정의된 데이터베이스에 정보를 검색하고 이를 참조하여 응답을 생성합니다. 정보 검색과 생성이 별도로 이루어져, 검색된 정보를 다시 가공하여 응답을 만드는 메커니즘입니다. RAG에 활용하는 데이터베이스는 기업 내부에 보유하고 있는 매뉴얼이나 보고서를 시스템에 업로드하거나, 학술 논문과 같이 신뢰할 수 있는 외부 소스를 엄선하여 사전 구축합니다. 따라서 정보의 신뢰성과 일관성을 보장할 수 있지만 데이터베이스의 업데이트 주기에 따라 최신 정보를 반영하지 못할 수 있으며, 사전에 준비된 데이터의 범위 내에서만 대응이 가능합니다. 이러한 특성으로 인해 RAG는 특정 도메인에 대한 깊이 있는 지식이 필요한 경우에 유용합니다. 예를 들어, 법률 자문이나 의학 정보 제공과 같이 정확성과 중요한 분야 또는 기업 내부의 특수한 정책이나 기밀 사항이 포함된 AI 서비스 구축에 활용될 수 있습니다. 또한 기존 LLM에 검색 기능을 추가하는 형태로 구현되므로, 이미 학습된 LLM을 그대로 활용하면서 기능을 확장하는 데 용이합니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">오토브라우징 : 최근에는 RAG에서 오토브라우징으로 트렌드가 이동하고 있습니다. 오토브라우징은 LLM과 정보 검색, 처리 기능을 결합한 형태로 RAG의 기능을 포함하면서도 실시간성 정보 반영 문제까지 해결할 수 있습니다. 실시간으로 웹을 탐색하여 정보를 수집하고, 그와 동시에 응답을 생성하기 때문입니다. 웹에서 검색을 수행하기 때문에 사전에 정의하지 않은 정보에도 대응할 수 있다는 점이 장점이지만 웹에 있는 모든 정보가 신뢰할 수 있는 정보는 아니기 때문에 추가 검증이 필요할 수 있습니다. 개인정보나 저작권 관련 이슈 또한 고려해야 할 요소입니다. 오토브라우징과 LLM을 통합하여 설계하는 것은 초기에 많은 리소스가 필요할 수 있지만 일단 구축하고 나면 더 높은 자동화와 실시간 처리가 가능합니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">오토브라우징과 RAG의 활용</span><br><span class="line"></span><br><span class="line">LLM은 광범위하지만 정적인 백과사전식 지식을 가지고 있는 반면, 오토브라우징이 결합된 AI는 백과사전을 실시간으로 업데이트하고 확장할 수 있는 능력을 갖추게 됩니다. RAG는 특정 분야에 대해 깊이있고 신뢰할 수 있는 지식을 제공한다는 점에서 전공 심화 교재에 비유할 수 있습니다.</span><br><span class="line">LLM은 넓은 지식을 제공하지만 깊이와 일관성, 최신성에 한계가 있습니다.</span><br><span class="line">RAG는 특정 분야에 대해 깊이 있고 신뢰할 수 있는 정보를 제공하지만, 범위가 제한적일 수 있습니다.</span><br><span class="line">오토브라우징은 실시간으로 정보를 탐색할 수 있지만, 신뢰성 검증이 필요할 수 있습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">오토브라우징과 RAG는 비슷한 듯 보이지만 각기 다른 장점을 가지고 있습니다. 그래서 현실에서는 두 기술을 상호 보완적으로 사용하는 경우가 많습니다. 예를 들어 지식은 RAG를 통해 얻고, 최신 동향이나 실시간 데이터는 오토브라우징을 통해 보완하는 방식입니다. 두 기술의 장단점을 비교하기보다는 정보의 성격이나 신뢰성의 중요도, 업데이트 주기 등을 고려하여 적절한 기술을 선택하거나 조합하는 것을 추천합니다. 두 기술 모두 LLM을 안전하고 효율적으로 활용하는 데 큰 도움을 줄 수 있습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[사례] 네이버의 Project Connect X</span><br><span class="line"></span><br><span class="line">직장인들은 평균적으로 업무시간의 약 61%를 본업이 아닌 커뮤니케이션에 할애한다고 합니다. 사내 시스템이나 이메일에서 정보를 찾고 각종 문의에 대응하는 활동에 절반 이상의 시간을 쓰고 있는 것입니다. 네이버는 HyperCLOVA X를 기반으로 기업의 업무 생산성 향상에 도움을 주는 솔루션을 만들고 있습니다. Project CONNECT X라는 이 프로젝트의 목표는 업무에 필요한 정보와 데이터를 기반으로 업무 프로세스를 돕거나 자동화하는 것입니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">사내 업무 규정이나 사내 게시판에 아카이빙 된 데이터는 일반적인 LLM 모델로 처리할 수 없습니다. 이러한 기업 특화 정보를 AI 시스템에 통합하기 위해서는 RAG를 활용할 수 있습니다. RAG를 통해 LLM이 기업의 내부 데이터를 검색하고 활용할 수 있게 되면 기업 특성에 맞는 정확하고 관련성 높은 응답을 제공할 수 있기 때문입니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Project CONNECT X 팀은 사내 다양한 시스템에 분산되어 있는 정보들을 모으고, 이를 쉽게 탐색할 수 있는 검색시스템과 어시스턴트 기능을 만들었습니다. CONNECT X의 업무 어시스턴트는 주고받은 메일이나 메시지를 기반으로 ‘할 일’을 추천하거나, 메일 초안을 작성해 주고, 이전 대화의 히스토리도 요약해 주기도 합니다. 지난 11월 출시 후, 많은 임직원들의 업무에 도움을 주고 있습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">하지만 현재 RAG 기술은 정형화된 텍스트 데이터만 이해하는 등 학습 가능한 데이터 커버리지가 낮고 단순 검색향의 답변만 제공한다는 아쉬움이 있습니다. 이에 오토브라우징을 접목하여 답변의 커버리지를 넓히고, 비정형 데이터를 RAG에서 이해할 수 있는 포맷으로 변환하는 특화 모델을 활용하여 실제 업무생산성 향상에 도움이 되는 Auto-Task 기능을 고려하고 있습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">오토브라우징은 사용자의 요청을 어떤 프로세스로 수행해야 할지 스스로 설계하고, 마치 사람처럼 여러 단계의 브라우징을 자동으로 수행하며 필요한 정보를 수집할 수 있습니다. 복합 정보 탐색, 탐색한 정보 비교, 추론, 분석이 가능하기 때문에 현업에 적용되었을 때 실질적인 생산성 향상에 크게 기여할 수 있을 것으로 기대됩니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Project CONNECT X 사례에서처럼 RAG는 지속적으로 업데이트되는 특정 데이터베이스를 참조할 수 있어 LLM의 할루시네이션이나 최신성 문제를 해결하는데 도움을 주고, 이용자의 상황에 최적화된 정확한 정보를 생성할 수 있다는 장점이 있습니다. 반면 특정 데이터베이스에 의존하므로 외부 데이터에 대한 확장성이 부족합니다. 그래서 오토브라우징 기술을 함께 적용하여 활용가능한 데이터 소스 풀을 넓히고 AI의 커버리지를 높일 수 있습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">소버린 AI와 오토브라우징</span><br><span class="line"></span><br><span class="line">소버린 AI의 기본적 정의는 각 기업이나 국가가 독립적인 통제권을 가진 AI를 개발하고 운영하는 것을 뜻합니다. 그러나 보다 포괄적이고 현실적인 관점에서는 자국의 사회문화적 특수성을 반영한 AI 모델까지 포함하여 이해하는 것이 합리적입니다.</span><br><span class="line"></span><br><span class="line">타국에서 개발된 LLM을 기초 모델(베이스 모델)로 채택하여 소버린 AI를 구축하는 과정에서, 자국의 사회문화적 특수성을 LLM에 충분히 반영하지 못하면 문화적 편향과 언어적 한계가 발생할 수 있습니다. 자국의 사회문화적 특수성을 LLM이 제대로 학습하지 못하는 경우는 다음과 같습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">소규모이거나 디지털 전환이 늦은 국가의 경우, LLM 학습에 필요한 방대한 양의 고품질 데이터를 확보하기 어려울 수 있습니다.</span><br><span class="line">특수하거나 폐쇄적인 국가의 경우, 해당 국가의 데이터만으로 AI를 학습시켰을 때 글로벌 맥락에서의 이해와 소통 능력이 제한될 수 있습니다.</span><br><span class="line">기술적 자원의 한계입니다. LLM을 학습시키는 것은 엄청난 컴퓨팅 파워와 전문 인력을 필요로 합니다. 이는 개발도상국에 상당한 경제적 부담이 될 수 있습니다.</span><br><span class="line">위와 같은 경우, 오토브라우징이 효율적인 접근 방식이 될 수 있습니다. 오토브라우징은 AI가 실시간으로 웹을 탐색하고 정보를 수집할 수 있게 해주는 기술입니다. 소버린 AI와 결합하면, 해당 문화권에 대한 LLM의 이해도가 낮더라도 자국의 최신 정보와 문화적 맥락을 AI에 지속적으로 반영할 수 있습니다. 이는 단순히 정보를 축적하는 것을 넘어, 장기적인 관점에서 AI가 현지 맥락에 맞는 응답을 점점 더 정확하게 생성할 수 있게 해줍니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">결론적으로 오토브라우징을 활용한 소버린 AI는, 타국의 모델을 활용하더라도 AI 개발이 어려운 국가들이 기술 격차를 좁히는 것을 넘어 각국의 문화적 정체성을 유지하며 글로벌 AI 생태계에 참여할 수 있는 기회를 제공할 수 있습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">새로운 기술은 그 자체로도 놀라운 가능성을 열어주지만 기술을 통한 혁신적인 도약은 기존 기술과의 융합을 통해 이루어져 왔습니다. 기술 간 융합은 각각의 기술이 가진 한계를 극복하고 그 이상의 가치를 창출할 수 있는 기회를 제공하기 때문입니다. 이번 콘텐츠에서는 LLM의 한계를 보완할 수 있는 기술 중 하나로 오토브라우징에 대해 소개했습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">LLM의 베이스가 되는 것도, RAG와 오토브라우징에 활용되는 것도 우리가 오래 전부터 지금 이 순간까지 쌓고 있는 데이터입니다. 이는 시공간을 초월한 인류의 집단 지성이 현재 우리가 누리는 자유롭고 편리한 삶을 가능케 한다는 것을 의미합니다. 이러한 관점에서, 현대의 지식과 기술을 당연히 여기기보다는 올바른 방향으로 사용하고 인간에게 도움이 되도록 발전시키는 것이 우리의 역할입니다. 콘텐츠에서 소개한 Project CONNECT X 사례나 소버린 AI의 개선을 위해 오토브라우징 기술을 활용하는 것은 이러한 노력의 좋은 예가 될 수 있습니다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">우리는 AI를 그 자체로 발전시기기도 하지만 다양한 기술의 융합을 통해 더욱 효과적이고 유용하게 활용할 수 있습니다. 앞으로도 AI를 인류의 이익을 위해 사용할 수 있도록 하는 것이 네이버와 같은 기술 기업이 해야 할 일이라고 생각합니다.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.unite.ai/google-releases-three-new-experimental-gemini-models/</span><br><span class="line">Artificial IntelligenceGoogle Releases Three New Experimental Gemini ModelsUpdated on August 27, 2024By Alex McFarland</span><br><span class="line"></span><br><span class="line">Google has just announced the release of three new experimental AI models, showcasing its ongoing innovation in the field while also highlighting the rapid pace at which AI capabilities are progressing.</span><br><span class="line"></span><br><span class="line">At the forefront of Google&#x27;s new offerings is the Gemini 1.5 Flash 8B, a compact powerhouse designed to handle a wide range of multimodal tasks. This 8-billion-parameter model represents a significant achievement in AI efficiency, demonstrating that smaller models pack a considerable punch when it comes to performance.</span><br><span class="line"></span><br><span class="line">The Flash 8B variant is particularly noteworthy for its ability to tackle high-volume tasks and long-context summarization. This capability makes it an attractive option for applications that require quick processing of large amounts of data or the ability to understand and synthesize information from lengthy documents.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Enhanced Gemini 1.5 Pro: Pushing the boundaries</span><br><span class="line">Building on the success of its predecessor, the updated Gemini 1.5 Pro model is an enhanced version that boasts superior performance across various internal benchmarks, with particular strengths in handling complex prompts and tackling coding tasks.</span><br><span class="line"></span><br><span class="line">The advancements in Gemini 1.5 Pro are not mere incremental improvements. According to Google, this new iteration outperforms its predecessor in all aspects, signaling a significant leap forward in AI capabilities. This progress is particularly relevant for developers and businesses working on sophisticated AI applications that require nuanced understanding and generation of language.</span><br><span class="line"></span><br><span class="line">Improved Gemini 1.5 Flash: Speed and efficiency</span><br><span class="line">Completing the trio of new releases is the updated Gemini 1.5 Flash model. While specific details about its improvements are less extensive in the announcement, Google reports that this model has shown significant performance gains across many internal benchmarks.</span><br><span class="line"></span><br><span class="line">The focus on enhancing the Flash model underscores the importance of speed and efficiency in AI applications. As businesses and developers seek to implement AI solutions at scale, models that can deliver rapid results without compromising on quality become increasingly valuable.</span><br><span class="line"></span><br><span class="line">These three models, each with its unique strengths and capabilities, represent Google&#x27;s multifaceted approach to advancing AI technology. By offering a range of options tailored to different needs and use cases, Google is positioning itself to meet the diverse demands of the AI market while pushing the boundaries of what&#x27;s possible in language processing.</span><br><span class="line"></span><br><span class="line">Implications for Developers and AI Applications</span><br><span class="line">Google has made these experimental models available through Google AI Studio and the Gemini API. The Gemini 1.5 Flash 8B model is accessible for free under the name “gemini-1.5-flash-8b-exp-0827“. The updated Gemini 1.5 Pro and Flash versions are available as “gemini-1.5-pro-exp-0827” and “gemini-1.5-flash-exp-0827” respectively.</span><br><span class="line"></span><br><span class="line">These models open up new possibilities for developers working on:</span><br><span class="line"></span><br><span class="line">High-volume data processing</span><br><span class="line">Long-context summarization</span><br><span class="line">Complex prompt handling</span><br><span class="line">Advanced coding tasks</span><br><span class="line">Google&#x27;s release of these experimental models mainly serves a dual purpose:</span><br><span class="line"></span><br><span class="line">Providing developers with cutting-edge tools</span><br><span class="line">Gathering real-world feedback to inform future improvements</span><br><span class="line">The company plans to use insights from these experimental launches to refine the models before their broader release.</span><br><span class="line"></span><br><span class="line">Google&#x27;s AI Strategy is Getting Clearer</span><br><span class="line">Google&#x27;s strategy is becoming more clear, with the company focusing on developing both high-capacity models and more efficient, task-specific variants. This approach aims to cater to a wide range of AI applications, from resource-intensive tasks to those requiring quick, lightweight processing.</span><br><span class="line"></span><br><span class="line">The release of these experimental models, closely following previous versions, demonstrates Google&#x27;s commitment to rapid development cycles in AI. This agile approach allows for quick incorporation of improvements and adaptations based on user feedback.</span><br><span class="line"></span><br><span class="line">By continually updating and expanding its AI model offerings, Google maintains its position as a key player in the AI space. This strategy directly competes with other major tech companies developing large language models and AI tools.</span><br><span class="line"></span><br><span class="line">These releases also highlight the importance of real-world testing in AI development. By making experimental models available to developers, Google accelerates the feedback loop and practical application of AI technologies.</span><br><span class="line"></span><br><span class="line">The Bottom Line</span><br><span class="line">Google&#x27;s release of the three experimental AI models—Gemini 1.5 Flash 8B, enhanced Gemini 1.5 Pro, and improved Gemini 1.5 Flash—marks a significant advancement in language processing technology. These models, balancing power and efficiency, cater to diverse AI applications from high-volume data processing to complex coding tasks. By making these tools accessible to developers and prioritizing real-world feedback, Google not only strengthens its position in the competitive AI landscape but also accelerates the evolution of AI capabilities.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://nvidianews.nvidia.com/news/nvidia-and-global-partners-launch-nim-agent-blueprints-for-enterprises-to-make-their-own-ai?ncid=so-link-468011</span><br><span class="line">NVIDIA</span><br><span class="line"></span><br><span class="line">NVIDIA and Global Partners Launch NIM Agent Blueprints for Enterprises to Make Their Own AI</span><br><span class="line">August 27, 2024</span><br><span class="line">NVIDIA and Global Partners Launch NIM Agent Blueprints for Enterprises to Make Their Own AI</span><br><span class="line">Catalog of Customizable Workflows Speeds Deployments of Core Generative AI Use Cases, Starting With Customer Service, Drug Discovery and Data Extraction for PDFs, With More to Come</span><br><span class="line">Companies Can Build and Operationalize Their AI Applications — Creating Data-Driven AI Flywheels — Using NIM Agent Blueprints Along With NIM Microservices and NeMo Framework, All Part of the NVIDIA AI Enterprise Platform</span><br><span class="line">Accenture, Cisco, Dell Technologies, Deloitte, Hewlett Packard Enterprise, Lenovo, SoftServe, World Wide Technology Among First Partners Delivering NIM Agent Blueprints to World’s Enterprises</span><br><span class="line">NVIDIA today announced NVIDIA NIM™ Agent Blueprints, a catalog of pretrained, customizable AI workflows that equip millions of enterprise developers with a full suite of software for building and deploying generative AI applications for canonical use cases, such as customer service avatars, retrieval-augmented generation and drug discovery virtual screening.</span><br><span class="line"></span><br><span class="line">NIM Agent Blueprints provide a jump-start for developers creating AI applications that use one or more AI agents. They include sample applications built with NVIDIA NeMo™, NVIDIA NIM and partner microservices, reference code, customization documentation and a Helm chart for deployment.</span><br><span class="line"></span><br><span class="line">Enterprises can modify NIM Agent Blueprints using their business data and run their generative AI applications across accelerated data centers and clouds. With NIM Agent Blueprints, enterprises can continually refine their AI applications based on user feedback, creating a data-driven AI flywheel.</span><br><span class="line"></span><br><span class="line">The first NIM Agent Blueprints now available include a digital human workflow for customer service, a generative virtual screening workflow for computer-aided drug discovery and a multimodal PDF data extraction workflow for enterprise retrieval-augmented generation (RAG) that can use vast quantities of business data for more accurate responses. NIM Agent Blueprints are free for developers to experience and download and can be deployed in production with the NVIDIA AI Enterprise software platform.</span><br><span class="line"></span><br><span class="line">Global system integrators and technology solutions providers Accenture, Deloitte, SoftServe and World Wide Technology (WWT) are bringing NVIDIA NIM Agent Blueprints to enterprises worldwide. Cisco, Dell Technologies, Hewlett Packard Enterprise and Lenovo are offering full-stack NVIDIA-accelerated infrastructure and solutions to speed NIM Agent Blueprints deployments.</span><br><span class="line"></span><br><span class="line">“Generative AI is advancing at lightspeed. Frontier model capabilities are growing exponentially with a continuous stream of new applications,” said Jensen Huang, founder and CEO of NVIDIA. “The enterprise AI wave is here. With the NVIDIA AI Enterprise toolkit — including NeMo, NIM microservices and the latest NIM Agent Blueprints — our expansive partner ecosystem is poised to help enterprises customize open-source models, build bespoke AI applications and deploy them seamlessly across any cloud, on premises or at the edge.”</span><br><span class="line"></span><br><span class="line">Digital Human NIM Agent Blueprint Advances Customer Service</span><br><span class="line">Gartner® reports that 80% of conversational offerings will embed generative AI by 2025, up from 20% in 2024(1). The digital human NIM Agent Blueprint for customer service helps enterprises rapidly prepare for this coming shift, bringing enterprise applications to life with a 3D animated avatar interface.</span><br><span class="line"></span><br><span class="line">With approachable, humanlike interactions, customer service applications can provide more engaging user experiences compared to traditional customer service options. Powered by NVIDIA Tokkio technologies, the digital human workflow features NVIDIA software including NVIDIA ACE, NVIDIA Omniverse RTX™, NVIDIA Audio2Face™ and Llama 3.1 NIM microservices, and is designed to integrate with existing enterprise generative AI applications built using RAG.</span><br><span class="line"></span><br><span class="line">Multimodal PDF Data Extraction NIM Agent Blueprint Taps Business Data</span><br><span class="line">The multimodal PDF data extraction workflow for enterprise RAG uses NVIDIA NeMo Retriever NIM microservices to unlock insights from massive volumes of enterprise PDF data. With this workflow, developers can create digital humans, AI agents or customer service chatbots that can quickly become experts on any topic captured within their corpus of PDF data.</span><br><span class="line"></span><br><span class="line">Using the workflow, enterprises can combine NeMo Retriever NIM microservices with community or custom models to build high-accuracy, multimodal retrieval pipelines that can be deployed wherever enterprise data resides.</span><br><span class="line"></span><br><span class="line">Generative Virtual Screening NIM Agent Blueprint Accelerates Drug Discovery</span><br><span class="line">The generative virtual screening NVIDIA NIM Agent Blueprint for drug discovery accelerates the identification and optimization of promising drug-like molecules, significantly reducing time and cost by generating molecules with favorable properties and higher probabilities of success.</span><br><span class="line"></span><br><span class="line">Researchers and application developers can quickly customize and deploy AI models for 3D protein structure prediction, small molecule generation and molecular docking. This blueprint incorporates NVIDIA NIM microservices — including AlphaFold2, MolMIM and DiffDock — to accelerate the virtual screening of small molecules using generative models.</span><br><span class="line"></span><br><span class="line">In combination with other tools available in NVIDIA BioNeMo™, enterprises can easily connect multiple NIM Agent Blueprints to build increasingly sophisticated AI applications and accelerate their drug discovery work.</span><br><span class="line"></span><br><span class="line">Additional blueprints will be released monthly for workflows to build generative AI applications for customer experience, content generation, software engineering, and product research and development.</span><br><span class="line"></span><br><span class="line">NVIDIA Partner Ecosystem Amplifies Enterprise Generative AI Success</span><br><span class="line">NVIDIA partners are readying to help the world’s enterprises rapidly build and deploy their own generative AI applications using NIM Agent Blueprints.</span><br><span class="line"></span><br><span class="line">Global professional services company Accenture will add NVIDIA NIM Agent Blueprints to its Accenture AI Refinery™, unveiled last month.</span><br><span class="line"></span><br><span class="line">“Across industries, generative AI is acting as a catalyst for companies looking to reinvent with tech, data and AI,” said Julie Sweet, chair and CEO of Accenture. “By integrating NVIDIA’s catalog of workflows into Accenture’s AI Refinery, we can help our clients develop custom AI systems at speed and reimagine how they do business and serve their customers to drive stronger business outcomes and create new value.”</span><br><span class="line"></span><br><span class="line">Global consulting firm Deloitte will integrate NVIDIA NIM Agent Blueprints into its deep portfolio of NVIDIA-powered solutions.</span><br><span class="line"></span><br><span class="line">“While many organizations are still working to fully harness the potential of GenAI, its implementation is steadily enhancing efficiencies and productivity,” said Jason Girzadas, CEO of Deloitte US. “By embedding NVIDIA&#x27;s NIM Agent Blueprints into enterprise solutions that are built on NVIDIA NIM microservices, Deloitte is engaging with our clients to innovate faster, unlock new growth opportunities and define AI-competitive advantage.”</span><br><span class="line"></span><br><span class="line">IT consulting and digital services provider SoftServe is integrating NIM Agent Blueprints into its generative AI portfolio to speed enterprise adoption.</span><br><span class="line"></span><br><span class="line">“Every enterprise knows generative AI is central to modernizing their operations, but not every enterprise knows where to begin their generative AI journey,” said Harry Propper, CEO of SoftServe. “Adding NVIDIA NIM Agent Blueprints into the SoftServe Gen AI Solutions portfolio gives clients proven frameworks for developing AI applications that put their own data to work.”</span><br><span class="line"></span><br><span class="line">A solution provider for the majority of Fortune 100 companies, WWT will assist enterprises in building NIM Agent Blueprints that leverage their business data.</span><br><span class="line"></span><br><span class="line">“WWT is committed to helping enterprises harness the power of AI as a catalyst for business transformation,” said Jim Kavanaugh, cofounder and CEO of World Wide Technology. “WWT’s AI Proving Ground, equipped with NVIDIA NIM Agent Blueprints and coupled with our data scientists, consultants and high-performance architecture engineers, offers a comprehensive resource for our clients to experiment with, validate and scale AI solutions.”</span><br><span class="line"></span><br><span class="line">Enterprises can develop and deploy NIM Agent Blueprints on NVIDIA AI platforms with compute, networking and software provided by NVIDIA’s global server manufacturing partners.</span><br><span class="line"></span><br><span class="line">These include Cisco Nexus HyperFabric AI clusters with NVIDIA, the Dell AI Factory with NVIDIA, NVIDIA AI Computing by HPE and HPE Private Cloud AI, as well as Lenovo Hybrid AI solutions powered by NVIDIA.</span><br><span class="line"></span><br><span class="line">“Cisco, together with NVIDIA, created a revolutionary, flexible and simple-to-deploy AI infrastructure with Nexus HyperFabric,” said Chuck Robbins, chair and CEO of Cisco. “Combining Cisco innovation with NVIDIA NIM Agent Blueprints offers customers a simple and secure way to deploy generative AI fast and efficiently, with the adaptability they need to build and customize new applications at scale.”</span><br><span class="line"></span><br><span class="line">“Dell Technologies and NVIDIA are making it easy for enterprises to unlock the power of AI-enabled applications and agents,” said Michael Dell, founder and CEO of Dell Technologies. “Incorporating NVIDIA NIM Agent Blueprints into the Dell AI Factory with NVIDIA provides an express lane for the transformative value of AI.”</span><br><span class="line"></span><br><span class="line">“HPE and NVIDIA are expanding on our recent blockbuster collaboration to deliver NVIDIA AI Computing by HPE,” said Antonio Neri, president and CEO of Hewlett Packard Enterprise. “By integrating NVIDIA NIM Agent Blueprints into our co-developed turnkey HPE Private Cloud AI solution, we will enable enterprises to focus resources on developing new AI use cases that boost productivity and unlock new revenue streams.&quot;</span><br><span class="line"></span><br><span class="line">“Generative AI is a full-stack challenge that requires accelerated infrastructure, specialized software and services, and powerful AI-ready devices that can maximize the capabilities of Hybrid AI,” said Yuanqing Yang, chairman and CEO of Lenovo. “NVIDIA NIM Agent Blueprints, combined with Lenovo’s comprehensive, end-to-end portfolio, give enterprises a head start for building generative AI applications that they can run everywhere on Lenovo Hybrid AI.”</span><br><span class="line"></span><br><span class="line">Enterprises can experience NVIDIA NIM Agent Blueprints today.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/THUDM/CogVideoX-5b</span><br><span class="line">The Knowledge Engineering Group (KEG) &amp; Data Mining (THUDM) at Tsinghua University.</span><br><span class="line">8/23/24</span><br><span class="line">CogVideoX 5B - Open weights Text to Video AI model is out, matching the likes of luma/ runway/ pika! 🔥</span><br><span class="line">Powered by diffusers - requires less than 10GB VRAM to run inference! ⚡</span><br><span class="line">Checkout the free demo below to play with it!</span><br><span class="line">We introduce CogVideoX, large-scale diffusion transformer models designed for</span><br><span class="line">generating videos based on text prompts. To efficently model video data, we propose to levearge a 3D Variational Autoencoder (VAE) to compress videos along</span><br><span class="line">both spatial and temporal dimensions. To improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep</span><br><span class="line">fusion between the two modalities. By employing a progressive training technique,</span><br><span class="line">CogVideoX is adept at producing coherent, long-duration videos characterized by</span><br><span class="line">significant motions. In addition, we develop an effective text-video data processing</span><br><span class="line">pipeline that includes various data preprocessing strategies and a video captioning</span><br><span class="line">method. It significantly helps enhance the performance of CogVideoX, improving</span><br><span class="line">both generation quality and semantic alignment. Results show that CogVideoX</span><br><span class="line">demonstrates state-of-the-art performance across both multiple machine metrics</span><br><span class="line">and human evaluations. The model weights of both the 3D Causal VAE and</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://cartesia.ai/blog/2024-08-27-on-device</span><br><span class="line">The On‑Device Intelligence Update</span><br><span class="line">August 27, 2024</span><br><span class="line">Cartesia</span><br><span class="line"></span><br><span class="line">Cartesia AI just released their Smol LM - Rene 1.3B, State Space Model with Apache 2.0 license, running ~80-120 tok/sec 🔥</span><br><span class="line">&gt; Alternating Mamba 2 &amp; MLP layers w/ Sliding window attention</span><br><span class="line">&gt; SoTA in &lt; 2B SLM (beats apple Open ELM, Recurrent Gemma 2B)</span><br><span class="line">&gt; Inference in MLX &amp; PyTorch</span><br><span class="line">&gt; Trained on 1.5B trillion tokens</span><br><span class="line">&gt; Custom SSM kernels in MLX</span><br><span class="line">&gt; Supports 8-bit &amp; 4-bit inference</span><br><span class="line">&gt; Models on the Hugging Face hub 🤗</span><br><span class="line"></span><br><span class="line">At Cartesia, our mission is to build the next generation of AI: ubiquitous, interactive intelligence that runs wherever you are. Our models and platform enable developers to build real-time, multi-modal AI systems, starting with our advances in audio and the world’s fastest generative voice API Sonic. Today, we’re announcing the first milestone in our journey to bring intelligence to every device.</span><br><span class="line"></span><br><span class="line">We believe that artificial intelligence needs to be so incredibly efficient that it can eventually be untethered from the data center. This puts capable models on every device, and makes it possible to build applications that respect our privacy and rapidly interact with the world at volumes and speeds that seem impossible today. Applications such as agents, personal assistants, robotics, gaming, healthcare, transportation, defense, and security are just a few areas that will transform as this technology emerges.</span><br><span class="line"></span><br><span class="line">We’ve spent the last few years pioneering a new architecture for AI called state space models (SSMs). Compared to widely used attention architectures, SSMs are highly efficient, with near-linear scaling costs in sequence length as opposed to quadratic. We believe SSMs will play a critical role in the future of AI progress and enable the creation of real-time foundation models with long-term memory and low latency that can run on every device. We’re committed to nurturing this early technology over the next few years as we progress towards our goals.</span><br><span class="line"></span><br><span class="line">We’re now excited to bring state space models to your device. As part of this release, we’re announcing:</span><br><span class="line"></span><br><span class="line">Edge, an open-source (Apache 2.0) library to support the research and release of highly efficient SSMs for on-device applications across accelerators and environments. Edge brings together open-weights SSM language models, including Rene, in one place and makes them available for Apple hardware through our new SSM optimized Metal kernels.</span><br><span class="line">Rene, an open-source 1.3B parameter language model pretrained from scratch with a hybrid Mamba-2 SSM backbone, and tailored to on-device inference.</span><br><span class="line">Sonic On-Device, in private beta. Sonic On-Device is the first ultra-realistic generative voice model that supports low latency real-time streaming on device, with unlimited voices and instant voice cloning.</span><br><span class="line">Collectively, these three releases represent a milestone in our goal to create radically new model architectures and platforms that break the compute requirements of today’s systems. If you’re interested in partnering with us to build the next generation of on-device AI products with Rene and Sonic On-Device, reach out to us through our outreach form.</span><br><span class="line"></span><br><span class="line">On-Device Intelligence</span><br><span class="line">The prevailing trend in AI is the creation of ever-larger foundation models, an area that will continue to grow with more investment and R&amp;D. These large models are built for compute and energy intensive data-centers, far away from where they’re used.</span><br><span class="line"></span><br><span class="line">What about the opposite of this? There are a huge number of use cases that will become possible as models that are “large enough” are deployed at the edge, or on-device.</span><br><span class="line"></span><br><span class="line">This approach has differentiated advantages over the large-model-that-runs-on-cloud-via-API approach,</span><br><span class="line"></span><br><span class="line">Data transfer is minimized, enabling applications to continuously stream high-resolution multimodal data into the models e.g. for audio and video understanding.</span><br><span class="line">Network latency is removed, enabling application latency and reliability that would otherwise be impossible to achieve.</span><br><span class="line">Network connectivity is no longer needed, and model capabilities can be available in environments with poor connectivity, high security requirements, or with intolerance for network drops.</span><br><span class="line">Deployments are fully private and secure, never leaving the physical constraints of the hardware.</span><br><span class="line">On-device AI enables new applications that can continuously process and respond to its environment in real-time. A small sample of on-device use cases we’re excited about are,</span><br><span class="line"></span><br><span class="line">Assistants: Turn any device into a personal assistant that proactively helps users and makes suggestions.</span><br><span class="line">Communication: Translate between languages instantly in the style of Babelfish, wherever you are.</span><br><span class="line">Security: Identify anomalies and events of interest on video cameras and take action.</span><br><span class="line">Healthcare: Privately communicate with patients in healthcare interactions.</span><br><span class="line">Education: Generate content on an iPad to safely and privately create personalized educational content for students.</span><br><span class="line">Robots: Perceive and take action with high speed and reliability in response to rapid environmental changes.</span><br><span class="line">Gaming: Generate and control video on a PC to create fully generative games.</span><br><span class="line">While use cases such as those outlined above might start from prototypes that run in the cloud, the best user experience will require a shift towards edge or on-device computing.</span><br><span class="line"></span><br><span class="line">New model architectures will be the key to unlocking faster, lower latency and more efficient deployments at the edge. We think state space models are going to be a key technology to unlock power and compute efficient foundation models that run on the edge, and we’re excited to share our first releases along this journey.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Rene: An Open-Source 1.3B SSM Language Model</span><br><span class="line">Rene is a small language model (SLM) designed for highly efficient on-device use. At 1.3B parameters, Rene is a hybrid SLM composed of alternating Mamba-2 and MLP layers, with some sliding-window attention (SWA) layers interspersed. Rene was trained on 1.5T tokens of the publicly available Dolma-1.7 dataset (h/t AllenAI).</span><br><span class="line"></span><br><span class="line">Efficiency-first. With the use of Mamba-2 and SWA, Rene has a fixed memory footprint at inference time, critical for reliably running it in resource-constrained environments such as personal computing devices. The use of Mamba-2 layers enables faster prefill times compared to other pure-SSM based architectures.</span><br><span class="line"></span><br><span class="line">Evaluation. We compared Rene to a suite of recent open-source SLMs on a variety of standard language modeling benchmarks, using the LM Evaluation Harness. These benchmarks span several categories, including common-sense reasoning (COPA, WinoGrande, ARC-Easy, ARC-Challenge) and language understanding (PIQA, HellaSwag, OpenBookQA, MMLU). Rene also outperforms SLMs such as Apple&#x27;s OpenELM (1.1B) and Google&#x27;s recurrent Gemma (2B) (Figure 1).1 Rene is the first SLM based on a recurrent architecture to achieve similar performance to similarly-sized (≤2B parameters) state-of-the-art SLMs across these benchmarks.</span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2408.01605</span><br><span class="line">[Submitted on 2 Aug 2024]</span><br><span class="line">META</span><br><span class="line">CYBERSECEVAL 3: Advancing the Evaluation of Cybersecurity Risks and Capabilities in Large Language Models</span><br><span class="line">Shengye Wan, Cyrus Nikolaidis, Daniel Song, David Molnar, James Crnkovich, Jayson Grace, Manish Bhatt, Sahana Chennabasappa, Spencer Whitman, Stephanie Ding, Vlad Ionescu, Yue Li, Joshua Saxe</span><br><span class="line">We are releasing a new suite of security benchmarks for LLMs, CYBERSECEVAL 3, to continue the conversation on empirically measuring LLM cybersecurity risks and capabilities. CYBERSECEVAL 3 assesses 8 different risks across two broad categories: risk to third parties, and risk to application developers and end users. Compared to previous work, we add new areas focused on offensive security capabilities: automated social engineering, scaling manual offensive cyber operations, and autonomous offensive cyber operations. In this paper we discuss applying these benchmarks to the Llama 3 models and a suite of contemporaneous state-of-the-art LLMs, enabling us to contextualize risks both with and without mitigations in place.</span><br><span class="line">As part of the release of Llama 3.1, we also released new trust and safety artifacts including CyberSecEval 3. We&#x27;ve published a new research paper on this work to continue the conversation with researchers and developers on empirically measuring LLM cybersecurity risks and capabilities.</span><br><span class="line">CyberSecEval 3 research paper</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2408.07199</span><br><span class="line">Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents</span><br><span class="line">Published on Aug 14</span><br><span class="line">Authors:</span><br><span class="line">Pranav Putta</span><br><span class="line">,</span><br><span class="line">Edmund Mills</span><br><span class="line">,</span><br><span class="line">Naman Garg</span><br><span class="line">,</span><br><span class="line">Sumeet Motwani</span><br><span class="line">,</span><br><span class="line">Chelsea Finn</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Divyansh Garg</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Rafael Rafailov</span><br><span class="line">Abstract</span><br><span class="line">Large Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex reasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a difficult challenge. Traditional supervised pre-training on static datasets falls short in enabling autonomous agent capabilities needed to perform complex decision-making in dynamic settings like web navigation. Previous attempts to bridge this ga-through supervised fine-tuning on curated expert demonstrations-often suffer from compounding errors and limited exploration data, resulting in sub-optimal policy outcomes. To overcome these challenges, we propose a framework that combines guided Monte Carlo Tree Search (MCTS) search with a self-critique mechanism and iterative fine-tuning on agent interactions using an off-policy variant of the Direct Preference Optimization (DPO) algorithm. Our method allows LLM agents to learn effectively from both successful and unsuccessful trajectories, thereby improving their generalization in complex, multi-step reasoning tasks. We validate our approach in the WebShop environment-a simulated e-commerce platform where it consistently outperforms behavior cloning and reinforced fine-tuning baseline, and beats average human performance when equipped with the capability to do online search. In real-world booking scenarios, our methodology boosts Llama-3 70B model&#x27;s zero-shot performance from 18.6% to 81.7% success rate (a 340% relative increase) after a single day of data collection and further to 95.4% with online search. We believe this represents a substantial leap forward in the capabilities of autonomous agents, paving the way for more sophisticated and reliable decision-making in real-world settings.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2408.06266</span><br><span class="line">Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment</span><br><span class="line">Published on Aug 13</span><br><span class="line"></span><br><span class="line">New promising RLHF and synthetic data generation method! 👀 APO and CLAIR improve the performance of Llama 3.1 8B by 7.45% on MixEval Hard, outperforming OpenAI GPT-4o mini! 🤯</span><br><span class="line">𝗖𝗼𝗻𝘁𝗿𝗮𝘀𝘁𝗶𝘃𝗲 𝗟𝗲𝗮𝗿𝗻𝗶𝗻𝗴 𝗳𝗿𝗼𝗺 𝗔𝗜 𝗥𝗲𝘃𝗶𝘀𝗶𝗼𝗻 (𝗖𝗟𝗔𝗜𝗥):</span><br><span class="line">CLAIR is a synthetic preference dataset method where you create for each prompt/response pair a minimally revised output using a stronger model (GPT4-turbo), ensuring the dataset has high contrastiveness.</span><br><span class="line">𝗔𝗻𝗰𝗵𝗼𝗿𝗲𝗱 𝗣𝗿𝗲𝗳𝗲𝗿𝗲𝗻𝗰𝗲 𝗢𝗽𝘁𝗶𝗺𝗶𝘇𝗮𝘁𝗶𝗼𝗻 (𝗔𝗣𝗢):</span><br><span class="line">APO is an RLHF method that offers fine-grained control over the likelihood of winning and losing outputs during training.</span><br><span class="line">- APO-zero: If the winning outputs are better, this variant increases the likelihood of winning outputs and decreases the likelihood of losing outputs.</span><br><span class="line">- APO-down: If the model is generally better than the winning outputs, this variant decreases both, but decreases the losing likelihood more.</span><br><span class="line">Insights:</span><br><span class="line">🚀 CLAIR + APO-zero achieves +7.65% improvement on MixEval-Hard</span><br><span class="line">🏆 SFT &gt; RLHF (DPO) when you only have GPT-4 outputs and no Reward model or Reviser.</span><br><span class="line">🤏 Tested datasets only included 32K preference pairs.</span><br><span class="line">⚙️ Used RMSProp optimizer with a learning rate of 2 × 10⁻⁷ and trained for 18 epochs</span><br><span class="line">👍 on-policy data +4% compared to off-policy for DPO and APO</span><br><span class="line">💪 APO outperforms DPO across different datasets</span><br><span class="line">🗝️ Contrastiveness of preference pairs is a major driver of improvements</span><br><span class="line">👀 Only evaluated on MixEval, looking forward to more results once the model is released</span><br><span class="line">🤗 APO is available in Hugging Face TRL</span><br><span class="line"></span><br><span class="line">Authors:</span><br><span class="line">Karel D&#x27;Oosterlinck</span><br><span class="line">,</span><br><span class="line">Winnie Xu</span><br><span class="line">,</span><br><span class="line">Chris Develder</span><br><span class="line">,</span><br><span class="line">Thomas Demeester</span><br><span class="line">,</span><br><span class="line">Amanpreet Singh</span><br><span class="line">,</span><br><span class="line">Christopher Potts</span><br><span class="line">,</span><br><span class="line">Douwe Kiela</span><br><span class="line">,</span><br><span class="line">Shikib Mehri</span><br><span class="line">Abstract</span><br><span class="line">Large Language Models (LLMs) are often aligned using contrastive alignment objectives and preference pair datasets. The interaction between model, paired data, and objective makes alignment a complicated procedure, sometimes producing subpar results. We study this and find that (i) preference data gives a better learning signal when the underlying responses are contrastive, and (ii) alignment objectives lead to better performance when they specify more control over the model during training. Based on these insights, we introduce Contrastive Learning from AI Revisions (CLAIR), a data-creation method which leads to more contrastive preference pairs, and Anchored Preference Optimization (APO), a controllable and more stable alignment objective. We align Llama-3-8B-Instruct using various comparable datasets and alignment objectives and measure MixEval-Hard scores, which correlate highly with human judgments. The CLAIR preferences lead to the strongest performance out of all datasets, and APO consistently outperforms less controllable objectives. Our best model, trained on 32K CLAIR preferences with APO, improves Llama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code is available at https://github.com/ContextualAI/CLAIR_and_APO.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://eugeneyan.com/writing/llm-evaluators/</span><br><span class="line">Evaluating the Effectiveness of LLM-Evaluators (aka LLM-as-Judge)</span><br><span class="line">The most comprehensive overview of LLM-as-a-Judge! READ IT! 🚨 &quot;Evaluating the Effectiveness of LLM-Evaluators (aka LLM-as-Judge)” summarizes and analyzes two dozen papers on different LLM as Judge approaches.  🤯</span><br><span class="line">TL;DR;</span><br><span class="line">⚖️ Direct scoring is suitable for objective evaluations, while pairwise comparisons are better for subjective ones.</span><br><span class="line">👍 Simplify evaluation to binary (true/false) when possible.</span><br><span class="line">📝 Prompting like cross-examination, form-filling with Chain-of-Thought, generating multiple sample improves performance and robustness</span><br><span class="line">🛠️ Interactive tools like EvalLM and ConstraintMaker can help refine prompts and apply constraints.</span><br><span class="line">🧠 Align LLM with human preferences using EvalGen, which suggests criteria based on input-output pairs.</span><br><span class="line">🐛 Finetuned LLM can improve specialized evaluation capabilities.</span><br><span class="line">Eugene&#x27;s Blog:</span><br><span class="line">https://lnkd.in/dMr8ByEm</span><br><span class="line"></span><br><span class="line">If you are interested in a practical example, read my &quot;LLM Evaluation doesn&#x27;t need to be complicated” and learn how to apply those techniques.</span><br><span class="line">LLM-evaluators, also known as “LLM-as-a-Judge”, are large language models (LLMs) that evaluate the quality of another LLM’s response to an instruction or query.</span><br><span class="line"></span><br><span class="line">Their growing adoption is partly driven by necessity. LLMs can now solve increasingly complex and open-ended tasks such as long-form summarization, translation, and multi-turn dialogue. As a result, conventional evals that rely on n-grams, semantic similarity, or a gold reference have become less effective at distinguishing good responses from the bad. And while we can rely on human evaluation or finetuned task-specific evaluators, they require significant effort and high-quality labeled data, making them difficult to scale.</span><br><span class="line"></span><br><span class="line">Thus, LLM-evaluators offer a promising alternative. If you’re considering using an LLM-evaluator, this is written for you. Drawing from two dozen papers, we’ll discuss:</span><br><span class="line"></span><br><span class="line">Key considerations when using LLM-evaluators</span><br><span class="line">Use cases for LLM-evaluators</span><br><span class="line">Techniques for prompting LLM-evaluators</span><br><span class="line">Aligning LLM-evaluators to our criteria</span><br><span class="line">Finetuning LLM-evaluator models</span><br><span class="line">Critiques against and support for LLM-evaluators</span><br><span class="line">After reading this, you’ll gain an intuition on how to apply, evaluate, and operate LLM-evaluators. We’ll learn when to apply (i) direct scoring vs. pairwise comparisons, (ii) correlation vs. classification metrics, and (iii) LLM APIs vs. finetuned evaluator models.</span><br><span class="line"></span><br><span class="line">Key considerations before adopting an LLM-evaluator</span><br><span class="line">Before reviewing the literature on LLM-evaluators, let’s first discuss a few questions which will help us interpret the findings as well as figure out how to use an LLM-evaluator.</span><br><span class="line"></span><br><span class="line">First, what baseline are we comparing an LLM-evaluator against? For example, if we’re prompting an LLM API, are we comparing it to human annotators or a smaller, finetuned evaluator model? It’s easier to match the former than the latter on accuracy and speed.</span><br><span class="line"></span><br><span class="line">Most folks have human annotators as the baseline. Here, we aim for the LLM-human correlation to match human-human correlation. Compared to human annotators, LLM-evaluators can be orders of magnitude faster and cheaper, as well as more reliable.</span><br><span class="line"></span><br><span class="line">On the other hand, if your baseline is a finetuned classifier or reward model, then the goal is for the LLM-evaluator to achieve similar recall and precision as a finetuned classifier. This is a more challenging baseline. Furthermore, LLM-evaluators are unlikely to match the millisecond-level latency of a small finetuned evaluator, especially if the former requires Chain-of-Thought (CoT). LLM-evaluators likely also cost more per inference.</span><br><span class="line"></span><br><span class="line">Second, how will we score responses via LLM-evaluators? There are at least three approaches that provide varying levels of accuracy, reliablity, and flexibility.</span><br><span class="line"></span><br><span class="line">Direct scoring evaluates a single response without needing an alternative for comparison. This makes it more versatile than pairwise comparison. Because it scores output directly, it’s more suitable for objective assessments such as measuring faithfulness to a source text or detecting policy violations such as toxicity.</span><br><span class="line"></span><br><span class="line">Pairwise comparison chooses the better of two responses or declares a tie. It’s typically used—and more reliable—for subjective evals such as persuasiveness, tone, coherence, etc. Studies show that pairwise comparisons lead to more stable results and smaller differences between LLM judgments and human annotations relative to direct scoring.</span><br><span class="line"></span><br><span class="line">Reference-based evaluation involves comparing the response being evaluated to a gold reference. The reference contains the information that should be included in the generated response. The LLM-evaluator evaluates how close the generated response matches the reference, essentially doing a more sophisticated form of fuzzy-matching.</span><br><span class="line"></span><br><span class="line">These three approaches are not interchangeable. Some evaluation tasks, such as assessing faithfulness or instruction-following, don’t fit the pairwise comparison paradigm. For example, a response is either faithful to the provided context or it is not—evaluating a response as more faithful than the alternative address the eval criteria. Similarly, reference-based evaluations require annotated references, while direct scoring and pairwise comparisons do not.</span><br><span class="line"></span><br><span class="line">Finally, what metrics will we use to evaluate LLM-evaluators? Classification and correlation metrics are typically adopted in the literature and industry.</span><br><span class="line"></span><br><span class="line">Classification metrics are more straightforward to apply and interpret. For example, we can evaluate the recall and precision of an LLM-evaluator at the task of evaluating the factual inconsistency or toxicity of responses. Or we could assess the LLM-evaluator’s ability to pick the more preferred response via pairwise comparison. Either way, we can frame it as a binary task and rely on good ol’ classification metrics.</span><br><span class="line"></span><br><span class="line">Diagnostic plots for classification tasks</span><br><span class="line"></span><br><span class="line">Diagnostic plots for classification tasks (source)</span><br><span class="line"></span><br><span class="line">Correlation metrics are trickier to interpret. Some commonly used correlation metrics include Cohen’s</span><br><span class="line"> (kappa), Kendall’s</span><br><span class="line"> (tau), and Spearman’s</span><br><span class="line"> (rho).</span><br><span class="line"></span><br><span class="line">Cohen’s</span><br><span class="line"> measures the agreement between two raters on categorical data, taking into account the probability of agreement occurring due to chance. It ranges from -1 to 1, with 0 indicating no agreement beyond chance and 1 indicating perfect agreement. It is generally more conservative compared to other correlation metrics. Values of 0.21 - 0.40 can be interpreted as fair agreement while 0.41 - 0.60 suggest moderate agreement.</span><br><span class="line"></span><br><span class="line">Kendall’s</span><br><span class="line"> and Spearman’s</span><br><span class="line"> measures the strength and direction of the association between two rankings. It ranges from -1 to 1. -1 indicates perfect negative correlation, 1 indicates perfect positive correlation, and 0 suggests no correlation. Kendall’s</span><br><span class="line"> is more robust to outliers due to its focus on the relative ordering of pairs while Spearman’s</span><br><span class="line"> is more sensitive to the magnitude of differences between ranks. They typically have higher values compared to Cohen’s</span><br><span class="line"> since they don’t adjust for chance agreement.</span><br><span class="line"></span><br><span class="line">When choosing a metric, consider the type of data you’re working with. Cohen’s</span><br><span class="line"> is more suitable for binary or categorical data when you want to assess the agreement between raters while adjusting for chance agreement. However, it may over-penalize ordinal data, such as a Likert scale. If your data is ordinal, consider Kendall’s</span><br><span class="line"> or Spearman’s</span><br><span class="line"> instead.</span><br><span class="line"></span><br><span class="line">I tend to be skeptical of correlation metrics. They don’t account for chance agreement and thus could be overoptimistic (though Cohen’s</span><br><span class="line"> is an exception). Furthermore, compared to classification metrics, it’s less straightforward to translate correlation metrics to performance in production. (What’s the evaluator’s recall on bad responses? What about false positive rate?) Thus, where possible, I have my evaluators return binary outputs. This improves model performance while making it easier to apply classification metrics.</span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-08-29</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/08/29/2024-8-29-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 8월 29일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/08/26/2024-8-26-AI-NEWS/" title="2024년 8월 26일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>