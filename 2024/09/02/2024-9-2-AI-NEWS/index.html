<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 9월 2일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="Cohere에서는 새로운 모델 C4AI Command R+ 08-2024를 발표하여 멀티스텝 도구 사용과 다국어 지원을 포함한 고급 기능을 제공합니다. Qwen에서는 Qwen2-VL을 출시하여 복잡한 이미지 및 비디오 이해, 다국어 지원, 그리고 실시간 대화 기능을 강"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 9월 2일 AI 소식</a></h3></div><div class="post-content"><p>Cohere에서는 새로운 모델 C4AI Command R+ 08-2024를 발표하여 멀티스텝 도구 사용과 다국어 지원을 포함한 고급 기능을 제공합니다. Qwen에서는 Qwen2-VL을 출시하여 복잡한 이미지 및 비디오 이해, 다국어 지원, 그리고 실시간 대화 기능을 강화했습니다. Salesforce는 대규모 행동 모델인 xLAM 시리즈를 공개하며, NVIDIA는 NV-Embed-v2로 MTEB 리더보드 1위를 탈환했습니다. 여러 연구 논문이 대형 언어 모델의 평가 방법과 성능에 대한 중요한 기술적 인사이트를 제공했으며, Gartner는 판매 혁신을 이끌 주요 기술들을 분석하여 발표했습니다.</p>
<h3 id="Cohere-Model-Card-for-C4AI-Command-R-08-2024"><a href="#Cohere-Model-Card-for-C4AI-Command-R-08-2024" class="headerlink" title="Cohere, Model Card for C4AI Command R+ 08-2024"></a>Cohere, Model Card for C4AI Command R+ 08-2024</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024">링크</a>, 8&#x2F;30&#x2F;24</p>
<ul>
<li>C4AI Command R+ 08-2024는 1040억 개의 파라미터를 갖춘 대형 언어 모델로, Retrieval Augmented Generation(RAG)과 다단계 도구 사용을 지원하여 복잡한 작업을 자동화하는 데 최적화됨.</li>
<li>이 모델은 Grouped Query Attention(GQA) 방식을 사용하여 최대 2배 높은 처리량과 낮은 지연 시간을 제공함.</li>
<li>23개 언어로 학습되고, 한국어를 포함한 10개 언어로 평가되어 다양한 다국어 환경에서 활용 가능.</li>
<li>최대 128K 길이의 컨텍스트를 지원하여 장문의 텍스트 처리에서도 높은 성능을 유지함.</li>
<li>모델은 Transformer 라이브러리와 호환되어 손쉽게 사용 가능하며, 허깅 페이스 허브에서 모델 체크포인트를 제공함.</li>
</ul>
<h3 id="Qwen-Qwen2-VL-To-See-the-World-More-Clearly"><a href="#Qwen-Qwen2-VL-To-See-the-World-More-Clearly" class="headerlink" title="Qwen, Qwen2-VL: To See the World More Clearly"></a>Qwen, Qwen2-VL: To See the World More Clearly</h3><p><a target="_blank" rel="noopener" href="https://qwenlm.github.io/blog/qwen2-vl/">링크</a>, 8&#x2F;29&#x2F;24</p>
<ul>
<li>Qwen2-VL은 비전 언어 모델 Qwen2를 기반으로 한 최신 버전으로, 다양한 해상도와 비율의 이미지를 효과적으로 처리할 수 있는 SoTA(State-of-the-Art) 성능을 자랑.</li>
<li>비디오 이해 기능이 대폭 향상되어 최대 20분 이상의 비디오를 분석, 질문에 답하거나 실시간 대화를 유지할 수 있음.</li>
<li>Qwen2-VL은 Naive Dynamic Resolution 지원을 통해 다양한 해상도의 이미지를 동적으로 처리하며, Multimodal Rotary Position Embedding(M-ROPE) 기법을 도입하여 1D 텍스트, 2D 이미지, 3D 비디오 정보의 위치 정보를 동시에 캡처하고 통합함.</li>
<li>7B 모델은 비용 효율성을 유지하면서도 이미지, 다중 이미지, 비디오 입력을 지원하여 다양한 작업에서 우수한 성능을 발휘함.</li>
<li>작은 크기의 2B 모델도 출시되어 모바일 배포를 염두에 두고 최적화되었으며, 비디오 관련 작업과 문서 이해, 일반적인 시나리오 질문 응답에서 우수한 성능을 보임.</li>
</ul>
<h3 id="Salesforce-Large-Action-Models-xLAM-7B"><a href="#Salesforce-Large-Action-Models-xLAM-7B" class="headerlink" title="Salesforce, Large Action Models xLAM-7B"></a>Salesforce, Large Action Models xLAM-7B</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/Salesforce/xLAM-7b-r">링크</a>, 8&#x2F;29&#x2F;24</p>
<ul>
<li>Salesforce의 xLAM 시리즈는 AI 에이전트의 의사 결정과 사용자 의도를 실행 가능한 행동으로 변환하는 대규모 행동 모델로, 7B, 8x7B, 8x22B 등 다양한 파라미터 크기로 제공됨.</li>
<li>이 모델들은 사용자의 복잡한 요구를 충족시키기 위해 자율적으로 계획하고 작업을 실행할 수 있는 능력을 갖춤.</li>
<li>최대 64K의 컨텍스트 길이를 지원하여 장문의 대화나 복잡한 작업에서도 높은 성능을 유지.</li>
<li>이 모델은 Transformers 라이브러리와 통합되어 AI 에이전트 구축에 용이하게 사용 가능.</li>
</ul>
<h3 id="NVIDIA-NV-Embed-v2"><a href="#NVIDIA-NV-Embed-v2" class="headerlink" title="NVIDIA, NV-Embed-v2"></a>NVIDIA, NV-Embed-v2</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/nvidia/NV-Embed-v2">링크</a>, 9&#x2F;1&#x2F;24</p>
<ul>
<li>NV-Embed-v2는 Massive Text Embedding Benchmark(MTEB)에서 72.31점이라는 기록적인 점수를 달성하며 56개의 텍스트 임베딩&#x2F;검색 작업에서 1위를 차지.</li>
<li>모델은 Latent-Attention Pooling 기법을 사용하여 임베딩 출력을 개선하고, 하드 네거티브 마이닝을 통해 잘못된 네거티브 샘플을 제거하여 성능을 향상시킴.</li>
<li>NV-Embed-v2는 Mistral-7B-v0.1 기반의 디코더-온리 LLM을 사용하여 4096 차원의 임베딩을 생성하며, 특히 RAG 기술 개발에 필수적인 검색 작업에서 탁월한 성과를 보임.</li>
</ul>
<h3 id="Gartner-Gartner-Hype-Cycle-Reveals-Top-Technologies-That-Will-Transform-Sales-In-the-Next-Decade"><a href="#Gartner-Gartner-Hype-Cycle-Reveals-Top-Technologies-That-Will-Transform-Sales-In-the-Next-Decade" class="headerlink" title="Gartner, Gartner Hype Cycle Reveals Top Technologies That Will Transform Sales In the Next Decade"></a>Gartner, Gartner Hype Cycle Reveals Top Technologies That Will Transform Sales In the Next Decade</h3><p><a target="_blank" rel="noopener" href="https://www.gartner.com/en/newsroom/press-releases/2024-08-27-gartner-hype-cycle-reveals-top-technologies-that-will-transform-sales-in-the-next-decade">링크</a>, 8&#x2F;28&#x2F;24</p>
<ul>
<li>Gartner는 2024년 판매 기술 하이프 사이클을 통해 향후 10년간 판매를 혁신할 25가지의 주요 기술을 발표, 이들 기술은 자율형 AI, 개발자 생산성 향상, 총체적 경험, 인간 중심의 보안 및 개인정보 보호 등 네 가지 트렌드로 분류됨.</li>
<li><strong>자율형 AI</strong>:<ul>
<li>AI 시스템이 인간의 감독 없이 스스로 학습하고 복잡한 환경에서 효과적으로 의사 결정을 내릴 수 있도록 개발 가속화.</li>
<li>이 기술에는 다중 에이전트 시스템, 대규모 행동 모델, 자율 에이전트 등이 포함됨.</li>
</ul>
</li>
<li><strong>개발자 생산성 향상</strong>:<ul>
<li>AI 증강 소프트웨어 엔지니어링, 클라우드 네이티브, 프롬프트 엔지니어링 등의 기술이 개발자의 생산성을 극대화함.</li>
</ul>
</li>
<li><strong>총체적 경험</strong>:<ul>
<li>고객 경험, 직원 경험 등을 통합하여 우수한 공유 경험을 창출하고, 이를 통해 신뢰도, 만족도, 충성도를 향상.</li>
</ul>
</li>
<li><strong>인간 중심의 보안 및 개인정보 보호</strong>:<ul>
<li>AI 트리즘, 디지털 면역 시스템 등 신뢰 기반의 보안 기술을 통해 조직의 보안 구조를 강화하고, 프라이버시 문제를 해결할 수 있는 방안을 제시.</li>
</ul>
</li>
<li><strong>Emotion AI</strong>:<ul>
<li>감정 AI는 사용자의 감정 상태를 분석하여, 판매 팀이 고객과 더 깊이 공감할 수 있도록 도와줌.</li>
<li>이 기술은 프라이버시 및 윤리적 문제 해결이 필요한 과제를 포함.</li>
</ul>
</li>
<li><strong>Digital Twin of a Customer (DToC)</strong>:<ul>
<li>고객의 행동을 시뮬레이션하고 예측할 수 있는 가상 모델로, 맞춤형 서비스 제공 및 고객 경험 개선을 지원.</li>
<li>이러한 기술 도입을 위해서는 고도의 머신러닝 알고리즘과 데이터 과학 인력이 필요.</li>
</ul>
</li>
<li><strong>Machine Sellers</strong>:<ul>
<li>비인간 에이전트가 판매를 자동화하여 효율성을 크게 증가시키며, 특히 반복적인 수익 창출 모델에서 강력한 효과를 발휘.</li>
<li>산업별, 지역별, 비즈니스 모델별로 도입 효과 차이가 있을 것으로 예상.</li>
</ul>
</li>
</ul>
<h3 id="연구-논문-Let-Me-Speak-Freely-A-Study-on-the-Impact-of-Format-Restrictions-on-Performance-of-Large-Language-Models"><a href="#연구-논문-Let-Me-Speak-Freely-A-Study-on-the-Impact-of-Format-Restrictions-on-Performance-of-Large-Language-Models" class="headerlink" title="연구 논문, Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models"></a>연구 논문, Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2408.02442">링크</a>, 8&#x2F;5&#x2F;24</p>
<ul>
<li>연구는 대형 언어 모델(LLM)에서 구조화된 형식(JSON, XML 등)으로 출력할 때 성능 저하를 초래할 수 있음을 확인.</li>
<li>형식 제한은 특히 추론 능력에 부정적인 영향을 미치며, 형식 제약이 클수록 성능 저하가 두드러짐.</li>
<li>Gemini 1.5 Flash 모델이 형식 간 일관성에서 가장 우수한 성능을 보임.</li>
<li>연구는 다양한 데이터셋(GSM8K, Last Letter, DDXPlus 등)을 사용하여 분석 수행.</li>
</ul>
<h3 id="연구-논문-Aligning-with-Human-Judgement-The-Role-of-Pairwise-Preference-in-Large-Language-Model-Evaluators"><a href="#연구-논문-Aligning-with-Human-Judgement-The-Role-of-Pairwise-Preference-in-Large-Language-Model-Evaluators" class="headerlink" title="연구 논문, Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators"></a>연구 논문, Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2403.16950">링크</a>, 3&#x2F;26&#x2F;24</p>
<ul>
<li>Pairwise-preference Search(PairS) 방법을 도입하여 LLM 평가에서 인간 판단과의 정렬 문제를 해결하고, 평가의 정확성과 일관성을 크게 향상시킴.</li>
<li>PairS는 여러 텍스트 간 쌍별 비교를 통해 평가 대상을 순위화하는 방법으로, 기존의 Win-loss 비율이나 ELO 레이팅 시스템보다 효율적이며, 약 30%의 비교만으로 유사한 성능을 달성 가능.</li>
<li>연구는 Spearman 상관계수에서 G-Eval, Win-loss rate, ELO rating보다 높은 성</li>
</ul>
<p>능을 기록하며, 코드와 예제를 공개하여 연구 재현성을 보장.</p>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024</span><br><span class="line">8/30/24</span><br><span class="line">Cohere</span><br><span class="line"></span><br><span class="line">Model Card for C4AI Command R+ 08-2024</span><br><span class="line">Model Summary</span><br><span class="line">C4AI Command R+ 08-2024 is an open weights research release of a 104B billion parameter model with highly advanced capabilities, this includes Retrieval Augmented Generation (RAG) and tool use to automate sophisticated tasks. The tool use in this model generation enables multi-step tool use which allows the model to combine multiple tools over multiple steps to accomplish difficult tasks. C4AI Command R+ 08-2024 is a multilingual model trained on 23 languages and evaluated in 10 languages. Command R+ 08-2024 is optimized for a variety of use cases including reasoning, summarization, and question answering.</span><br><span class="line"></span><br><span class="line">C4AI Command R+ 08-2024 is part of a family of open weight releases from Cohere For AI and Cohere. Our smaller companion model is C4AI Command R 08-2024.</span><br><span class="line"></span><br><span class="line">Point of Contact: Cohere For AI: cohere.for.ai</span><br><span class="line">License: CC-BY-NC, requires also adhering to C4AI&#x27;s Acceptable Use Policy</span><br><span class="line">Model: c4ai-command-r-plus-08-2024</span><br><span class="line">Model Size: 104 billion parameters</span><br><span class="line">Context length: 128K</span><br><span class="line"></span><br><span class="line">Cohere just dropped updated Command R plus &amp; Command R - built for RAG and tool use, multilingual (23 languages), grounded generation, 128K content and much more! 🔥</span><br><span class="line">&gt; Command R Plus - 104B param, Command R - 35B param</span><br><span class="line">&gt; up-to 2x higher throughput &amp; 2x lower latency</span><br><span class="line">&gt; Uses Grouped Query Attention (GQA)</span><br><span class="line">&gt; SFT + preference tuned model</span><br><span class="line">&gt; Massive 128K context window</span><br><span class="line">&gt; Trained in 23 languages, evaluated on 10</span><br><span class="line">&gt; Capable of code rewrites, explanations and snippets.</span><br><span class="line">&gt; Supports citation, tool execution, and structured outputs for grounded agentic use</span><br><span class="line">&gt; Model checkpoint available on the hub</span><br><span class="line">&gt; Works out of the box with transformers 🤗</span><br><span class="line">This is a massive improvement from the last iteration of Command R+ (which is still one of my favourites on Hugging Chat).</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://qwenlm.github.io/blog/qwen2-vl/</span><br><span class="line">Qwen2-VL: To See the World More Clearly</span><br><span class="line">August 29, 2024</span><br><span class="line"> · 17 min · 3569 words · Qwen Team | Translations:</span><br><span class="line">简体中文</span><br><span class="line"></span><br><span class="line">DEMO GITHUB HUGGING FACE MODELSCOPE API DISCORD</span><br><span class="line"></span><br><span class="line">After a year’s relentless efforts, today we are thrilled to release Qwen2-VL! Qwen2-VL is the latest version of the vision language models based on Qwen2 in the Qwen model familities. Compared with Qwen-VL, Qwen2-VL has the capabilities of:</span><br><span class="line"></span><br><span class="line">SoTA understanding of images of various resolution &amp; ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.</span><br><span class="line"></span><br><span class="line">Understanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.</span><br><span class="line"></span><br><span class="line">Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.</span><br><span class="line"></span><br><span class="line">Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">We opensource Qwen2-VL-2B and Qwen2-VL-7B with Apache 2.0 license, and we release the API of Qwen2-VL-72B! The opensource is integrated to Hugging Face Transformers, vLLM, and other third-party frameworks. Hope you enjoy!</span><br><span class="line"></span><br><span class="line">Performance</span><br><span class="line">We evaluate our model’s visual capabilities across six key dimensions: complex college-level problem-solving, mathematical abilities, document and table comprehension, multilingual text-image understanding, general scenario question-answering, video comprehension, and agent-based interactions. Overall, our 72B model showcases top-tier performance across most metrics, often surpassing even closed-source models like GPT-4o and Claude 3.5-Sonnet. Notably, it demonstrates a significant edge in document understanding.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">At the 7B scale, we’ve managed to retain support for image, multi-image, and video inputs, delivering competitive performance in a more cost-effective model size. Specifically, our model excels in document understanding tasks such as DocVQA and in multilingual text understanding from images, as assessed by MTVQA, establishing state-of-the-art performance.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Additionally, we’re excited to introduce a smaller 2B model, optimized for potential mobile deployment. Despite its compact size, this model boasts strong performance in image, video, and multilingual comprehension. It particularly shines in video-related tasks, document understanding, and general scenario question-answering when compared to other models of similar scale.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Model Capabilities</span><br><span class="line">1. Enhanced Recognition Capabilities</span><br><span class="line">Qwen2-VL now boasts improved object recognition, extending beyond plants and landmarks to comprehend complex relationships between multiple objects in a scene. We’ve also significantly boosted the model’s ability to recognize handwritten text and multiple languages within images, making it more accessible to users worldwide.</span><br><span class="line">2. Visual Reasoning: Solving Real-World Problems</span><br><span class="line">In this iteration, we have significantly enhanced Qwen2-VL’s mathematical and coding proficiencies. The model is not only capable of solving problems by analyzing pictures but can also interpret and solve complex mathematical problems through chart analysis. Extremely aspect-ratio-distorted images can also be correctly interpreted. Additionally, we have reinforced the model’s capability to extract information from real-world images and charts and improved its instruction-following skills. This fusion of visual perception and logical reasoning empowers the model to tackle practical issues, bridging the gap between abstract concepts and tangible solutions.</span><br><span class="line"></span><br><span class="line">3. Video Understanding and Live Chat</span><br><span class="line">Beyond static images, Qwen2-VL extends its prowess to video content analysis. It can summarize video content, answer questions related to it, and maintain a continuous flow of conversation in real-time, offering live chat support. This functionality allows it to act as a personal assistant, helping users by providing insights and information drawn directly from video content.</span><br><span class="line">4. Visual Agent Capabilities: Function Calling and Visual Interactions.</span><br><span class="line">Qwen2-VL demonstrates strong potential as a visual agent, facilitating interactions similar to human perceptions of the world.</span><br><span class="line"></span><br><span class="line">The model facilitates Function Calling, enabling it to harness external tools for real-time data retrieval – be it flight statuses, weather forecasts, or package tracking – by deciphering visual cues. This integration of visual interpretation with functional execution elevates its utility, making it a powerful tool for information management and decision-making.</span><br><span class="line"></span><br><span class="line">Visual Interactions represent a significant stride towards mimicking human perception. By allowing the model to engage with visual stimuli akin to human senses, we’re pushing the boundaries of AI’s ability to perceive and respond to its environment. This capability paves the way for more intuitive and immersive interactions, where Qwen2-VL acts not just as an observer, but an active participant in our visual experiences.</span><br><span class="line">model is unable to extract audio from videos, and its knowledge is only up to date as of June 2023. Additionally, the model cannot guarantee complete accuracy when processing complex instructions or scenarios, and it is relatively weak in tasks involving counting, character recognition, and 3D spatial awareness.</span><br><span class="line"></span><br><span class="line">Model Architecture</span><br><span class="line">Overall, we’ve continued with the Qwen-VL architecture, which leverages a Vision Transformer (ViT) model and Qwen2 language models. For all these variants, we utilized a ViT with approximately 600M parameters, designed to handle both image and video inputs seamlessly. To further enhance the model’s ability to effectively perceive and comprehend visual information in videos, we introduced several key upgrades:</span><br><span class="line"></span><br><span class="line">A key architectural improvement in Qwen2-VL is the implementation of Naive Dynamic Resolution support. Unlike its predecessor, Qwen2-VL can handle arbitrary image resolutions, mapping them into a dynamic number of visual tokens, thereby ensuring consistency between the model input and the inherent information in images. This approach more closely mimics human visual perception, allowing the model to process images of any clarity or size.</span><br><span class="line"></span><br><span class="line">Another key architectural enhancement is the innovation of Multimodal Rotary Position Embedding (M-ROPE). By deconstructing the original rotary embedding into three parts representing temporal and spatial (height and width) information，M-ROPE enables LLM to concurrently capture and integrate 1D textual, 2D visual, and 3D video positional information.</span><br><span class="line"></span><br><span class="line">License</span><br><span class="line">Both the opensource Qwen2-VL-2B and Qwen2-VL-7B are under Apache 2.0.</span><br><span class="line"></span><br><span class="line">Qwen 2VL 7B &amp; 2B are here - Apache 2.0 licensed smol Vision Language Models competitive with GPT 4o mini - w/ video understanding, function calling and more! 🔥</span><br><span class="line">&gt; 72B (to be released later) beats 3.5 Sonnet &amp; GPT 4o</span><br><span class="line">&gt; Can understand up to 20 min of video</span><br><span class="line">&gt; Handles arbitrary image resolutions</span><br><span class="line">&gt; Multimodal RoPE to capture 1D, 2D &amp; 3D information</span><br><span class="line">&gt; Enhanced Recognition capabilities - can understand complex relationships b/w objects</span><br><span class="line">&gt; Better Visual Reasoning &amp; video understanding w/ live chat</span><br><span class="line">&gt; Function calling for tools + data access</span><br><span class="line">&gt; Integrated with Transformers! 🤗</span><br><span class="line">&gt; Model checkpoints on the Hub</span><br><span class="line">Kudos to the Alibaba Qwen group; I&#x27;m a huge fan of the Qwen series, especially their multilingual capabilities! Looking forward to the 72B ⚡</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/Salesforce/xLAM-7b-r</span><br><span class="line">8/29/24</span><br><span class="line">Salseforce</span><br><span class="line"></span><br><span class="line">Let&#x27;s go.. Salesforce released Large Action Models xLAM - 7B, 8x7B, 8x22B, up to 64K context length primed for AI agents use-cases! 🔥</span><br><span class="line">LAMs are designed to enhance decision-making and translate user intentions into executable actions.</span><br><span class="line">Integrated with Transformers 🤗</span><br><span class="line">Welcome to the xLAM model family! Large Action Models (LAMs) are advanced large language models designed to enhance decision-making and translate user intentions into executable actions that interact with the world. LAMs autonomously plan and execute tasks to achieve specific goals, serving as the brains of AI agents. They have the potential to automate workflow processes across various domains, making them invaluable for a wide range of applications. The model release is exclusively for research purposes. A new and enhanced version of xLAM will soon be available exclusively to customers on our Platform.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/nvidia/NV-Embed-v2</span><br><span class="line">NVIDIA</span><br><span class="line">9/1/24</span><br><span class="line">We have reclaimed #1 on the MTEB Leaderboard 🏆 Our NV-Embed-v2, has achieved a record-breaking score of 72.31 across 56 text embedding/retrieval tasks, reclaiming the top spot on the Massive Text Embedding Benchmark (MTEB) leaderboard. It also holds the No. 1 in the retrieval sub-category (15 tasks) in the leaderboard, which is essential to the development of RAG technology.</span><br><span class="line">We present NV-Embed-v2, a generalist embedding model that ranks No. 1 on the Massive Text Embedding Benchmark (MTEB benchmark)(as of Aug 30, 2024) with a score of 72.31 across 56 text embedding tasks. It also holds the No. 1 in the retrieval sub-category (a score of 62.65 across 15 tasks) in the leaderboard, which is essential to the development of RAG technology.</span><br><span class="line"></span><br><span class="line">NV-Embed-v2 presents several new designs, including having the LLM attend to latent vectors for better pooled embedding output, and demonstrating a two-staged instruction tuning method to enhance the accuracy of both retrieval and non-retrieval tasks. Additionally, NV-Embed-v2 incorporates a novel hard-negative mining methods that take into account the positive relevance score for better false negatives removal.</span><br><span class="line"></span><br><span class="line">For more technical details, refer to our paper: NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models.</span><br><span class="line"></span><br><span class="line">Model Details</span><br><span class="line">Base Decoder-only LLM: Mistral-7B-v0.1</span><br><span class="line">Pooling Type: Latent-Attention</span><br><span class="line">Embedding Dimension: 4096</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2408.02442</span><br><span class="line">Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models</span><br><span class="line">Published on Aug 5</span><br><span class="line">Authors:</span><br><span class="line">Zhi Rui Tam</span><br><span class="line">,</span><br><span class="line">Cheng-Kuang Wu</span><br><span class="line">,</span><br><span class="line">Yi-Lin Tsai</span><br><span class="line">,</span><br><span class="line">Chieh-Yen Lin</span><br><span class="line">,</span><br><span class="line">Hung-yi Lee</span><br><span class="line">,</span><br><span class="line">Yun-Nung Chen</span><br><span class="line">Abstract</span><br><span class="line">Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs). This study investigates whether such constraints on generation space impact LLMs&#x27; abilities, including reasoning and domain knowledge comprehension. Specifically, we evaluate LLMs&#x27; performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks. Surprisingly, we observe a significant decline in LLMs&#x27; reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks.</span><br><span class="line">Structured Prompting is a key requirement for building real-world LLM applications or agents, but does it harm the performance and ability to reason? 🤔 ”Let Me Speak Freely” studies the impact of structured formats (JSON, XML, YAML) versus generating free-form responses across various common tasks. 👀</span><br><span class="line">Methods: Constraint Decoding (JSON-Mode), Format-Restricting Instructions (FRI, ”respond in JSON”), NL-to-Format (2 step, first NL answer → covert to JSON)</span><br><span class="line">Datasets: GSM8K, Last Letter, Shuffled Objects, DDXPlus, Sports, Task280, and MultiFin</span><br><span class="line">Models: Gemini 1.5 Flash, Claude 3.5 Haiku, GPT-3.5-Turbo, Llama 3 8B or Gemma 2 9B</span><br><span class="line">Insights</span><br><span class="line">🧠 Reasoning abilities can decline under format restrictions</span><br><span class="line">🚀 Gemini 1.5 Flash achieved the highest consistency across formats</span><br><span class="line">📄 Gemini, Llama 3, Gemma 2 work best with JSON format</span><br><span class="line">📉 Claude 3 Haiku showed a big performance drop for JSON but not for XML</span><br><span class="line">⚠️ Adding concrete schema constraints can decrease performance</span><br><span class="line">🔍 Classification: JSON-Mode performs equally, if not better</span><br><span class="line">📊 Classification: JSON-Mode performs better than FRI (instructed) for open models</span><br><span class="line">⬇️ Reasoning: JSON-Mode performs worse than other methods</span><br><span class="line">💪 Reasoning: NL-to-Format has the strongest results after NL.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2403.16950</span><br><span class="line">Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators</span><br><span class="line">Published on Mar 26</span><br><span class="line">Authors:</span><br><span class="line">Yinhong Liu</span><br><span class="line">,</span><br><span class="line">Han Zhou</span><br><span class="line">,</span><br><span class="line">Zhijiang Guo</span><br><span class="line">,</span><br><span class="line">Ehsan Shareghi</span><br><span class="line">,</span><br><span class="line">Ivan Vulić</span><br><span class="line">,</span><br><span class="line">Anna Korhonen</span><br><span class="line">,</span><br><span class="line">Nigel Collier</span><br><span class="line">Abstract</span><br><span class="line">Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PairS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PairS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthermore, we provide insights into the role of pairwise preference in quantifying the transitivity of LLMs and demonstrate how PairS benefits from calibration.</span><br><span class="line">How can we improve the reliability of LLM Evaluators? “The Role of Pairwise Preference in Large Language Model Evaluators” compares existing evaluation approaches and shows how to perform robustness evaluations using Pairwise-preference Search (PAIRS)! 👀</span><br><span class="line">PAIRS compares multiple preference pairs of generated texts and then ranks candidates globally, like a tournament or tree, to find the best one. It improves the robustness of evaluation and is also more efficient than win-loss or ELO rating systems. “PAIRS-greedy typically requires only about 30% of the comparisons to achieve performance similar to ELO rating.”</span><br><span class="line">⚖️ PAIRS trades compute/cost for more precision and robustness</span><br><span class="line">🧮 Example: 16 candidates per data point lead to 16 * 15 / 2 = 120 comparisons, or if greedy is used, then 36.</span><br><span class="line">🚀 PAIRS outperforms G-Eval, Win-loss rate, and ELO rating on Spearman correlations</span><br><span class="line">🧑🏻‍💻 Code and examples available</span><br><span class="line">📝 Used simple generic prompts for comparison.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.gartner.com/en/newsroom/press-releases/2024-08-27-gartner-hype-cycle-reveals-top-technologies-that-will-transform-sales-in-the-next-decade</span><br><span class="line">STAMFORD, Conn, August 28, 2024</span><br><span class="line">Gartner</span><br><span class="line"></span><br><span class="line">Gartner Hype Cycle Reveals Top Technologies That Will Transform Sales In the Next Decade</span><br><span class="line">Emotion AI, Digital Twin of a Customer and Machine Sellers Will Have the Biggest Impact on Sales Organizations</span><br><span class="line">Emotion AI, machine sellers and digital twin of a customer will have a transformative impact on the sales function in the next decade, according to Gartner, Inc. However, sales leaders must closely navigate the hype to evaluate when these technologies will be appropriate for their organization to implement.</span><br><span class="line"></span><br><span class="line">The Gartner Hype Cycle for Revenue and Sales Technology, 2024 distills key insights that Gartner profiles each year into a succinct set of “must-know” emerging technologies. These technologies have potential to deliver transformational benefits over the next two to 10 years (see Figure 1).</span><br><span class="line"></span><br><span class="line">“The common theme of these three technologies are their ability to predict, interpret and serve buyers’ needs and behaviors and to streamline and automate sales fulfillment, releasing sellers to focus on developing high value client relationships,” said Guy Wood, Senior Director Analyst in the Gartner Sales practice.“In order to gain a competitive advantage, sales operations leaders need to be looking on the horizon for technologies not currently in the mainstream.”</span><br><span class="line"></span><br><span class="line">Figure 1: Hype Cycle for Revenue and Sales Technology, 2024</span><br><span class="line"></span><br><span class="line">Source: (Gartner, August 2024)</span><br><span class="line"></span><br><span class="line">Emotion AI</span><br><span class="line"></span><br><span class="line">Emotion AI, which is at the Peak of Inflated Expectations, uses AI and software techniques to analyze the emotional state of a user via computer vision, audio/voice input, sensors and/or software logic. Emotion AI turns human behavioral attributes into data. By enabling sales teams to utilize data to actively learn from and empathize with the customer, emotion AI is poised to dramatically change the sales function.</span><br><span class="line"></span><br><span class="line">“Emotion AI has already been widely adopted in contact centers, but the sales function has yet to fully realize the technology’s potential,” said Wood. “However, CSOs must navigate privacy concerns and bias, which may be a barrier to successful adoption. For example, privacy and ethics challenges surround psychological profiling, especially when applied to consumers, recruitment prospects or protected individuals like minors. ”</span><br><span class="line"></span><br><span class="line">Digital Twin of a Customer</span><br><span class="line"></span><br><span class="line">A digital twin of a customer (DToC) is a dynamic virtual mirror representation of a customer that organizations can use to simulate, emulate and anticipate behavior. DToCs, currently at the Innovation Trigger,  help organizations better understand their customers and provide a personalized, empathetic service to customers, many of whose buying habits repeatedly change.</span><br><span class="line"></span><br><span class="line">“DToCs can transform the way organizations sell products or services and provide customers with better experiences, which will result in increased revenue and lasting customer relationships,&quot; said Wood. “DToC can be an engine of transformation and disruption. Organizations need competency in machine learning algorithms and staff with data science skills to build or manage DToCs”</span><br><span class="line"></span><br><span class="line">Machine Sellers</span><br><span class="line"></span><br><span class="line">Machine sellers, at the early stage of the Innovation Trigger, are nonhuman agents that automate end-to-end selling actions on behalf of human sellers, or a sales organization, to sell products and services in exchange for payment. Currently, machine sellers can be used to facilitate simple and transactional sales.</span><br><span class="line"></span><br><span class="line">“Sales organizations deploying machine sellers will gain a competitive advantage by satisfying buyer preferences for seamless purchases and ‘locking-in’ recurring revenue. Organizations that do not adopt machine sellers will risk wasting resources, decreasing efficiency and missing revenue goals,” said Wood. “However, the impact of machine sellers will not be evenly distributed; it will vary by vertical industry, geography and business model”</span><br><span class="line"></span><br><span class="line">Gartner clients can read more in “Hype Cycle for Revenue and Sales Technology, 2024”</span><br><span class="line"></span><br><span class="line">About Gartner for Sales Leaders</span><br><span class="line"></span><br><span class="line">Gartner for Sales Leaders provides heads of sales and their teams with the insights, advice and tools they need to address mission-critical priorities amid mounting pressures to drive growth through new and existing customers. With extensive qualitative and quantitative research, Gartner for Sales Leaders helps sales teams combat commoditization and price-based purchasing, develop critical manager and seller skills, elevate the value of sales interactions, unlock existing growth potential, and optimize sales force enablement. Follow news and update from the Gartner Sales practice on X and LinkedIn using #GartnerSales. Members of the media can find additional information and insights in the Gartner Sales Newsroom.</span><br><span class="line">2024 신기술 하이프 사이클 발표...”자율형 AI 등장 가속화”</span><br><span class="line"></span><br><span class="line">가트너가 2024년 신기술 하이프 사이클(Hype Cycle for Emerging Technologies) 보고서를 통해 주목해야 할 25가지의 혁신 기술을 발표했다. 이 기술들은 ▲자율형 AI ▲개발자 생산성 ▲총체적 경험 ▲인간 중심의 보안 및 개인정보 보호 프로그램 등 네 가지 주요 트렌드로 분류된다.</span><br><span class="line"></span><br><span class="line">아룬 찬드라세카란 가트너 수석 VP 애널리스트는 “기반 모델에 대한 기대감에서 ROI를 창출하는 사용 사례로 비즈니스 초점이 이동하고 있다. 생성형 AI는 부풀려진 기대의 정점을 넘어섰으며 자율형 AI의 등장을 가속하고 있다”며 “현재 AI 모델에는 에이전트 기능이 부족하다. AI 연구소들은 목표 달성을 위해 상호 작용할 수 있는 AI 에이전트를 신속하게 출시하고 있으나 개발 과정은 점진적으로 진행될 것”이라고 예측했다.</span><br><span class="line"></span><br><span class="line">찬드라세카란 수석 VP 애널리스트는 “AI가 계속해서 주목받고 있는 가운데 CIO와 IT 경영진은 개발, 보안, 고객 및 직원 경험에 혁신적인 잠재력을 가진 신기술을 검토해야 한다”며 “또한 검증되지 않은 기술에 대한 관리, 활용법을 조직의 능력에 맞춰 전략적으로 수립해야 한다”고 조언했다.</span><br><span class="line"></span><br><span class="line">신기술 하이프 사이클은 가트너가 발표하는 여러 하이프 사이클 중에서도 독보적인 인사이트를 제공한다. 가트너가 매년 프로파일링하는 2000개 이상의 기술 및 응용 프레임워크에서 핵심적인 인사이트를 도출해 반드시 알아야 할 떠오르는 기술들을 간결하게 정리해 제시하기 때문이다. 해당 기술들은 향후 2년에서 10년간 혁신적인 이점을 제공할 잠재력을 갖춘 것으로 평가된다.</span><br><span class="line"></span><br><span class="line">2024년 신기술의 4가지 트렌드는 다음과 같다.</span><br><span class="line"></span><br><span class="line">자율형 AI :</span><br><span class="line"></span><br><span class="line">AI의 빠른 발전으로 인해 인간의 감독을 최소화하면서, 스스로 작동하고 개선하며 복잡한 환경에서도 효과적인 의사 결정을 내릴 수 있는 자율형 AI 시스템이 탄생하고 있다. 인간이 할 수 있는 모든 작업을 수행할 수 있는 이러한 첨단 AI 시스템은 공상 과학에서 현실로 서서히 다가오고 있다.</span><br><span class="line"></span><br><span class="line">이 기술에는 ▲다중 에이전트 시스템 ▲대규모 행동 모델 ▲기계 고객 ▲휴머노이드 작업 로봇 ▲자율 에이전트 ▲강화 학습 등이 포함된다.</span><br><span class="line"></span><br><span class="line">개발자 생산성 향상:</span><br><span class="line"></span><br><span class="line">개발자 생산성은 코드를 빠르게 작성하는 것 이상의 의미를 갖는다. 이는 개발자의 효과적인 커뮤니케이션과 협업, 집중력, 만족도 등에 영향을 받는다. 개발자의 생산성을 향상시키는 신기술로는 ▲AI 증강 소프트웨어 엔지니어링 ▲클라우드 네이티브 ▲깃옵스 ▲내부 개발자 포털 ▲프롬프트 엔지니어링 ▲웹어셈블리 등이 있다.</span><br><span class="line"></span><br><span class="line">찬드라세카란 수석 VP 애널리스트는 “기술은 개발자가 소프트웨어를 설계하고 제공하는 방식을 혁신해 그 어느 때보다 높은 생산성을 나타내고 있다”며 “개발자의 만족도, 협업, 플로우 개선을 통해 이익을 극대화하면서 고품질의 제품을 신속하게 제공할 수 있게 됐다”고 말했다.</span><br><span class="line"></span><br><span class="line">총체적 경험을 통한 역량 강화:</span><br><span class="line"></span><br><span class="line">총체적 경험은 고객 경험, 직원 경험, 다중 경험, 사용자 경험을 서로 연결해 우수한 공유 경험을 창출하는 전략이다. 신뢰도, 만족도, 충성도, 지지도 향상을 목표로 기술을 사용해 중요한 상호 작용을 해결하고 고객과 직원 모두의 역량을 강화한다. 평가 대상 기술에는 ▲디지털 트윈 ▲공간 컴퓨팅 ▲슈퍼앱 ▲6G 등이 포함된다.</span><br><span class="line"></span><br><span class="line">인간 중심의 보안 및 개인정보 보호 제공:</span><br><span class="line"></span><br><span class="line">기업은 상호 신뢰의 문화를 조성하고 팀 간에 공유된 위험을 인식하는 보안 및 개인정보 보호 기술을 사용해 더욱 탄력적인 조직으로 거듭날 수 있다. 인간 중심 보안 및 개인정보 보호를 지원하는 떠오르는 기술로는 ▲AI 트리즘 ▲사이버 보안 메시 아키텍처 ▲디지털 면역 시스템 ▲허위 정보 보안 ▲연합 머신러닝 ▲동형 암호가 있다.</span><br><span class="line"></span><br><span class="line">찬드라세카란 수석 VP 애널리스트는 “보안 관행이 충분히 안전하고 확실한 방식으로 작동할 것이라는 전제에 의존하는 경우가 많다. 하지만 많은 조직이 보안과 비즈니스 제공 중 하나를 선택해야 할 때 비즈니스 제공을 우선시해 지나치게 엄격한 보안 조치를 우회하는 선택을 종종 한다”며 “인간 중심의 보안 및 개인정보 보호는 조직의 디지털 설계에 긴밀한 보안 및 개인정보 보호 구조를 엮어준다”고 말했다.</span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-09-02</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/09/02/2024-9-2-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 9월 2일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/09/06/2024-9-6-AI-NEWS/" title="2024년 9월 6일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/08/29/2024-8-29-AI-NEWS/" title="2024년 8월 29일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>