<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 9ì›” 2ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="Cohereì—ì„œëŠ” ìƒˆë¡œìš´ ëª¨ë¸ C4AI Command R+ 08-2024ë¥¼ ë°œí‘œí•˜ì—¬ ë©€í‹°ìŠ¤í… ë„êµ¬ ì‚¬ìš©ê³¼ ë‹¤êµ­ì–´ ì§€ì›ì„ í¬í•¨í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. Qwenì—ì„œëŠ” Qwen2-VLì„ ì¶œì‹œí•˜ì—¬ ë³µì¡í•œ ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ì´í•´, ë‹¤êµ­ì–´ ì§€ì›, ê·¸ë¦¬ê³  ì‹¤ì‹œê°„ ëŒ€í™” ê¸°ëŠ¥ì„ ê°•"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 9ì›” 2ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><p>Cohereì—ì„œëŠ” ìƒˆë¡œìš´ ëª¨ë¸ C4AI Command R+ 08-2024ë¥¼ ë°œí‘œí•˜ì—¬ ë©€í‹°ìŠ¤í… ë„êµ¬ ì‚¬ìš©ê³¼ ë‹¤êµ­ì–´ ì§€ì›ì„ í¬í•¨í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. Qwenì—ì„œëŠ” Qwen2-VLì„ ì¶œì‹œí•˜ì—¬ ë³µì¡í•œ ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ì´í•´, ë‹¤êµ­ì–´ ì§€ì›, ê·¸ë¦¬ê³  ì‹¤ì‹œê°„ ëŒ€í™” ê¸°ëŠ¥ì„ ê°•í™”í–ˆìŠµë‹ˆë‹¤. SalesforceëŠ” ëŒ€ê·œëª¨ í–‰ë™ ëª¨ë¸ì¸ xLAM ì‹œë¦¬ì¦ˆë¥¼ ê³µê°œí•˜ë©°, NVIDIAëŠ” NV-Embed-v2ë¡œ MTEB ë¦¬ë”ë³´ë“œ 1ìœ„ë¥¼ íƒˆí™˜í–ˆìŠµë‹ˆë‹¤. ì—¬ëŸ¬ ì—°êµ¬ ë…¼ë¬¸ì´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ í‰ê°€ ë°©ë²•ê³¼ ì„±ëŠ¥ì— ëŒ€í•œ ì¤‘ìš”í•œ ê¸°ìˆ ì  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí–ˆìœ¼ë©°, GartnerëŠ” íŒë§¤ í˜ì‹ ì„ ì´ëŒ ì£¼ìš” ê¸°ìˆ ë“¤ì„ ë¶„ì„í•˜ì—¬ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.</p>
<h3 id="Cohere-Model-Card-for-C4AI-Command-R-08-2024"><a href="#Cohere-Model-Card-for-C4AI-Command-R-08-2024" class="headerlink" title="Cohere, Model Card for C4AI Command R+ 08-2024"></a>Cohere, Model Card for C4AI Command R+ 08-2024</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024">ë§í¬</a>, 8&#x2F;30&#x2F;24</p>
<ul>
<li>C4AI Command R+ 08-2024ëŠ” 1040ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°–ì¶˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ë¡œ, Retrieval Augmented Generation(RAG)ê³¼ ë‹¤ë‹¨ê³„ ë„êµ¬ ì‚¬ìš©ì„ ì§€ì›í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìë™í™”í•˜ëŠ” ë° ìµœì í™”ë¨.</li>
<li>ì´ ëª¨ë¸ì€ Grouped Query Attention(GQA) ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ìµœëŒ€ 2ë°° ë†’ì€ ì²˜ë¦¬ëŸ‰ê³¼ ë‚®ì€ ì§€ì—° ì‹œê°„ì„ ì œê³µí•¨.</li>
<li>23ê°œ ì–¸ì–´ë¡œ í•™ìŠµë˜ê³ , í•œêµ­ì–´ë¥¼ í¬í•¨í•œ 10ê°œ ì–¸ì–´ë¡œ í‰ê°€ë˜ì–´ ë‹¤ì–‘í•œ ë‹¤êµ­ì–´ í™˜ê²½ì—ì„œ í™œìš© ê°€ëŠ¥.</li>
<li>ìµœëŒ€ 128K ê¸¸ì´ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì§€ì›í•˜ì—¬ ì¥ë¬¸ì˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬ì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•¨.</li>
<li>ëª¨ë¸ì€ Transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í˜¸í™˜ë˜ì–´ ì†ì‰½ê²Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë©°, í—ˆê¹… í˜ì´ìŠ¤ í—ˆë¸Œì—ì„œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì œê³µí•¨.</li>
</ul>
<h3 id="Qwen-Qwen2-VL-To-See-the-World-More-Clearly"><a href="#Qwen-Qwen2-VL-To-See-the-World-More-Clearly" class="headerlink" title="Qwen, Qwen2-VL: To See the World More Clearly"></a>Qwen, Qwen2-VL: To See the World More Clearly</h3><p><a target="_blank" rel="noopener" href="https://qwenlm.github.io/blog/qwen2-vl/">ë§í¬</a>, 8&#x2F;29&#x2F;24</p>
<ul>
<li>Qwen2-VLì€ ë¹„ì „ ì–¸ì–´ ëª¨ë¸ Qwen2ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ìµœì‹  ë²„ì „ìœ¼ë¡œ, ë‹¤ì–‘í•œ í•´ìƒë„ì™€ ë¹„ìœ¨ì˜ ì´ë¯¸ì§€ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” SoTA(State-of-the-Art) ì„±ëŠ¥ì„ ìë‘.</li>
<li>ë¹„ë””ì˜¤ ì´í•´ ê¸°ëŠ¥ì´ ëŒ€í­ í–¥ìƒë˜ì–´ ìµœëŒ€ 20ë¶„ ì´ìƒì˜ ë¹„ë””ì˜¤ë¥¼ ë¶„ì„, ì§ˆë¬¸ì— ë‹µí•˜ê±°ë‚˜ ì‹¤ì‹œê°„ ëŒ€í™”ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆìŒ.</li>
<li>Qwen2-VLì€ Naive Dynamic Resolution ì§€ì›ì„ í†µí•´ ë‹¤ì–‘í•œ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ë™ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©°, Multimodal Rotary Position Embedding(M-ROPE) ê¸°ë²•ì„ ë„ì…í•˜ì—¬ 1D í…ìŠ¤íŠ¸, 2D ì´ë¯¸ì§€, 3D ë¹„ë””ì˜¤ ì •ë³´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ë™ì‹œì— ìº¡ì²˜í•˜ê³  í†µí•©í•¨.</li>
<li>7B ëª¨ë¸ì€ ë¹„ìš© íš¨ìœ¨ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ ì´ë¯¸ì§€, ë‹¤ì¤‘ ì´ë¯¸ì§€, ë¹„ë””ì˜¤ ì…ë ¥ì„ ì§€ì›í•˜ì—¬ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•¨.</li>
<li>ì‘ì€ í¬ê¸°ì˜ 2B ëª¨ë¸ë„ ì¶œì‹œë˜ì–´ ëª¨ë°”ì¼ ë°°í¬ë¥¼ ì—¼ë‘ì— ë‘ê³  ìµœì í™”ë˜ì—ˆìœ¼ë©°, ë¹„ë””ì˜¤ ê´€ë ¨ ì‘ì—…ê³¼ ë¬¸ì„œ ì´í•´, ì¼ë°˜ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ ì§ˆë¬¸ ì‘ë‹µì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„.</li>
</ul>
<h3 id="Salesforce-Large-Action-Models-xLAM-7B"><a href="#Salesforce-Large-Action-Models-xLAM-7B" class="headerlink" title="Salesforce, Large Action Models xLAM-7B"></a>Salesforce, Large Action Models xLAM-7B</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/Salesforce/xLAM-7b-r">ë§í¬</a>, 8&#x2F;29&#x2F;24</p>
<ul>
<li>Salesforceì˜ xLAM ì‹œë¦¬ì¦ˆëŠ” AI ì—ì´ì „íŠ¸ì˜ ì˜ì‚¬ ê²°ì •ê³¼ ì‚¬ìš©ì ì˜ë„ë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ í–‰ë™ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ëŒ€ê·œëª¨ í–‰ë™ ëª¨ë¸ë¡œ, 7B, 8x7B, 8x22B ë“± ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° í¬ê¸°ë¡œ ì œê³µë¨.</li>
<li>ì´ ëª¨ë¸ë“¤ì€ ì‚¬ìš©ìì˜ ë³µì¡í•œ ìš”êµ¬ë¥¼ ì¶©ì¡±ì‹œí‚¤ê¸° ìœ„í•´ ììœ¨ì ìœ¼ë¡œ ê³„íší•˜ê³  ì‘ì—…ì„ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°–ì¶¤.</li>
<li>ìµœëŒ€ 64Kì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì§€ì›í•˜ì—¬ ì¥ë¬¸ì˜ ëŒ€í™”ë‚˜ ë³µì¡í•œ ì‘ì—…ì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€.</li>
<li>ì´ ëª¨ë¸ì€ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í†µí•©ë˜ì–´ AI ì—ì´ì „íŠ¸ êµ¬ì¶•ì— ìš©ì´í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥.</li>
</ul>
<h3 id="NVIDIA-NV-Embed-v2"><a href="#NVIDIA-NV-Embed-v2" class="headerlink" title="NVIDIA, NV-Embed-v2"></a>NVIDIA, NV-Embed-v2</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/nvidia/NV-Embed-v2">ë§í¬</a>, 9&#x2F;1&#x2F;24</p>
<ul>
<li>NV-Embed-v2ëŠ” Massive Text Embedding Benchmark(MTEB)ì—ì„œ 72.31ì ì´ë¼ëŠ” ê¸°ë¡ì ì¸ ì ìˆ˜ë¥¼ ë‹¬ì„±í•˜ë©° 56ê°œì˜ í…ìŠ¤íŠ¸ ì„ë² ë”©&#x2F;ê²€ìƒ‰ ì‘ì—…ì—ì„œ 1ìœ„ë¥¼ ì°¨ì§€.</li>
<li>ëª¨ë¸ì€ Latent-Attention Pooling ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ì¶œë ¥ì„ ê°œì„ í•˜ê³ , í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ë§ˆì´ë‹ì„ í†µí•´ ì˜ëª»ëœ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œì„ ì œê±°í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚´.</li>
<li>NV-Embed-v2ëŠ” Mistral-7B-v0.1 ê¸°ë°˜ì˜ ë””ì½”ë”-ì˜¨ë¦¬ LLMì„ ì‚¬ìš©í•˜ì—¬ 4096 ì°¨ì›ì˜ ì„ë² ë”©ì„ ìƒì„±í•˜ë©°, íŠ¹íˆ RAG ê¸°ìˆ  ê°œë°œì— í•„ìˆ˜ì ì¸ ê²€ìƒ‰ ì‘ì—…ì—ì„œ íƒì›”í•œ ì„±ê³¼ë¥¼ ë³´ì„.</li>
</ul>
<h3 id="Gartner-Gartner-Hype-Cycle-Reveals-Top-Technologies-That-Will-Transform-Sales-In-the-Next-Decade"><a href="#Gartner-Gartner-Hype-Cycle-Reveals-Top-Technologies-That-Will-Transform-Sales-In-the-Next-Decade" class="headerlink" title="Gartner, Gartner Hype Cycle Reveals Top Technologies That Will Transform Sales In the Next Decade"></a>Gartner, Gartner Hype Cycle Reveals Top Technologies That Will Transform Sales In the Next Decade</h3><p><a target="_blank" rel="noopener" href="https://www.gartner.com/en/newsroom/press-releases/2024-08-27-gartner-hype-cycle-reveals-top-technologies-that-will-transform-sales-in-the-next-decade">ë§í¬</a>, 8&#x2F;28&#x2F;24</p>
<ul>
<li>GartnerëŠ” 2024ë…„ íŒë§¤ ê¸°ìˆ  í•˜ì´í”„ ì‚¬ì´í´ì„ í†µí•´ í–¥í›„ 10ë…„ê°„ íŒë§¤ë¥¼ í˜ì‹ í•  25ê°€ì§€ì˜ ì£¼ìš” ê¸°ìˆ ì„ ë°œí‘œ, ì´ë“¤ ê¸°ìˆ ì€ ììœ¨í˜• AI, ê°œë°œì ìƒì‚°ì„± í–¥ìƒ, ì´ì²´ì  ê²½í—˜, ì¸ê°„ ì¤‘ì‹¬ì˜ ë³´ì•ˆ ë° ê°œì¸ì •ë³´ ë³´í˜¸ ë“± ë„¤ ê°€ì§€ íŠ¸ë Œë“œë¡œ ë¶„ë¥˜ë¨.</li>
<li><strong>ììœ¨í˜• AI</strong>:<ul>
<li>AI ì‹œìŠ¤í…œì´ ì¸ê°„ì˜ ê°ë… ì—†ì´ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ê³  ë³µì¡í•œ í™˜ê²½ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì˜ì‚¬ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆë„ë¡ ê°œë°œ ê°€ì†í™”.</li>
<li>ì´ ê¸°ìˆ ì—ëŠ” ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ, ëŒ€ê·œëª¨ í–‰ë™ ëª¨ë¸, ììœ¨ ì—ì´ì „íŠ¸ ë“±ì´ í¬í•¨ë¨.</li>
</ul>
</li>
<li><strong>ê°œë°œì ìƒì‚°ì„± í–¥ìƒ</strong>:<ul>
<li>AI ì¦ê°• ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ë§, í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë“±ì˜ ê¸°ìˆ ì´ ê°œë°œìì˜ ìƒì‚°ì„±ì„ ê·¹ëŒ€í™”í•¨.</li>
</ul>
</li>
<li><strong>ì´ì²´ì  ê²½í—˜</strong>:<ul>
<li>ê³ ê° ê²½í—˜, ì§ì› ê²½í—˜ ë“±ì„ í†µí•©í•˜ì—¬ ìš°ìˆ˜í•œ ê³µìœ  ê²½í—˜ì„ ì°½ì¶œí•˜ê³ , ì´ë¥¼ í†µí•´ ì‹ ë¢°ë„, ë§Œì¡±ë„, ì¶©ì„±ë„ë¥¼ í–¥ìƒ.</li>
</ul>
</li>
<li><strong>ì¸ê°„ ì¤‘ì‹¬ì˜ ë³´ì•ˆ ë° ê°œì¸ì •ë³´ ë³´í˜¸</strong>:<ul>
<li>AI íŠ¸ë¦¬ì¦˜, ë””ì§€í„¸ ë©´ì—­ ì‹œìŠ¤í…œ ë“± ì‹ ë¢° ê¸°ë°˜ì˜ ë³´ì•ˆ ê¸°ìˆ ì„ í†µí•´ ì¡°ì§ì˜ ë³´ì•ˆ êµ¬ì¡°ë¥¼ ê°•í™”í•˜ê³ , í”„ë¼ì´ë²„ì‹œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë°©ì•ˆì„ ì œì‹œ.</li>
</ul>
</li>
<li><strong>Emotion AI</strong>:<ul>
<li>ê°ì • AIëŠ” ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœë¥¼ ë¶„ì„í•˜ì—¬, íŒë§¤ íŒ€ì´ ê³ ê°ê³¼ ë” ê¹Šì´ ê³µê°í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤Œ.</li>
<li>ì´ ê¸°ìˆ ì€ í”„ë¼ì´ë²„ì‹œ ë° ìœ¤ë¦¬ì  ë¬¸ì œ í•´ê²°ì´ í•„ìš”í•œ ê³¼ì œë¥¼ í¬í•¨.</li>
</ul>
</li>
<li><strong>Digital Twin of a Customer (DToC)</strong>:<ul>
<li>ê³ ê°ì˜ í–‰ë™ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê³  ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ê°€ìƒ ëª¨ë¸ë¡œ, ë§ì¶¤í˜• ì„œë¹„ìŠ¤ ì œê³µ ë° ê³ ê° ê²½í—˜ ê°œì„ ì„ ì§€ì›.</li>
<li>ì´ëŸ¬í•œ ê¸°ìˆ  ë„ì…ì„ ìœ„í•´ì„œëŠ” ê³ ë„ì˜ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ê³¼ ë°ì´í„° ê³¼í•™ ì¸ë ¥ì´ í•„ìš”.</li>
</ul>
</li>
<li><strong>Machine Sellers</strong>:<ul>
<li>ë¹„ì¸ê°„ ì—ì´ì „íŠ¸ê°€ íŒë§¤ë¥¼ ìë™í™”í•˜ì—¬ íš¨ìœ¨ì„±ì„ í¬ê²Œ ì¦ê°€ì‹œí‚¤ë©°, íŠ¹íˆ ë°˜ë³µì ì¸ ìˆ˜ìµ ì°½ì¶œ ëª¨ë¸ì—ì„œ ê°•ë ¥í•œ íš¨ê³¼ë¥¼ ë°œíœ˜.</li>
<li>ì‚°ì—…ë³„, ì§€ì—­ë³„, ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ë³„ë¡œ ë„ì… íš¨ê³¼ ì°¨ì´ê°€ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒ.</li>
</ul>
</li>
</ul>
<h3 id="ì—°êµ¬-ë…¼ë¬¸-Let-Me-Speak-Freely-A-Study-on-the-Impact-of-Format-Restrictions-on-Performance-of-Large-Language-Models"><a href="#ì—°êµ¬-ë…¼ë¬¸-Let-Me-Speak-Freely-A-Study-on-the-Impact-of-Format-Restrictions-on-Performance-of-Large-Language-Models" class="headerlink" title="ì—°êµ¬ ë…¼ë¬¸, Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models"></a>ì—°êµ¬ ë…¼ë¬¸, Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2408.02442">ë§í¬</a>, 8&#x2F;5&#x2F;24</p>
<ul>
<li>ì—°êµ¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ì„œ êµ¬ì¡°í™”ëœ í˜•ì‹(JSON, XML ë“±)ìœ¼ë¡œ ì¶œë ¥í•  ë•Œ ì„±ëŠ¥ ì €í•˜ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŒì„ í™•ì¸.</li>
<li>í˜•ì‹ ì œí•œì€ íŠ¹íˆ ì¶”ë¡  ëŠ¥ë ¥ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ë©°, í˜•ì‹ ì œì•½ì´ í´ìˆ˜ë¡ ì„±ëŠ¥ ì €í•˜ê°€ ë‘ë“œëŸ¬ì§.</li>
<li>Gemini 1.5 Flash ëª¨ë¸ì´ í˜•ì‹ ê°„ ì¼ê´€ì„±ì—ì„œ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„.</li>
<li>ì—°êµ¬ëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ì…‹(GSM8K, Last Letter, DDXPlus ë“±)ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ ìˆ˜í–‰.</li>
</ul>
<h3 id="ì—°êµ¬-ë…¼ë¬¸-Aligning-with-Human-Judgement-The-Role-of-Pairwise-Preference-in-Large-Language-Model-Evaluators"><a href="#ì—°êµ¬-ë…¼ë¬¸-Aligning-with-Human-Judgement-The-Role-of-Pairwise-Preference-in-Large-Language-Model-Evaluators" class="headerlink" title="ì—°êµ¬ ë…¼ë¬¸, Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators"></a>ì—°êµ¬ ë…¼ë¬¸, Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2403.16950">ë§í¬</a>, 3&#x2F;26&#x2F;24</p>
<ul>
<li>Pairwise-preference Search(PairS) ë°©ë²•ì„ ë„ì…í•˜ì—¬ LLM í‰ê°€ì—ì„œ ì¸ê°„ íŒë‹¨ê³¼ì˜ ì •ë ¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , í‰ê°€ì˜ ì •í™•ì„±ê³¼ ì¼ê´€ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚´.</li>
<li>PairSëŠ” ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ê°„ ìŒë³„ ë¹„êµë¥¼ í†µí•´ í‰ê°€ ëŒ€ìƒì„ ìˆœìœ„í™”í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ê¸°ì¡´ì˜ Win-loss ë¹„ìœ¨ì´ë‚˜ ELO ë ˆì´íŒ… ì‹œìŠ¤í…œë³´ë‹¤ íš¨ìœ¨ì ì´ë©°, ì•½ 30%ì˜ ë¹„êµë§Œìœ¼ë¡œ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë‹¬ì„± ê°€ëŠ¥.</li>
<li>ì—°êµ¬ëŠ” Spearman ìƒê´€ê³„ìˆ˜ì—ì„œ G-Eval, Win-loss rate, ELO ratingë³´ë‹¤ ë†’ì€ ì„±</li>
</ul>
<p>ëŠ¥ì„ ê¸°ë¡í•˜ë©°, ì½”ë“œì™€ ì˜ˆì œë¥¼ ê³µê°œí•˜ì—¬ ì—°êµ¬ ì¬í˜„ì„±ì„ ë³´ì¥.</p>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹"><a href="#todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹" class="headerlink" title="(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,"></a>(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAIì—ì„œëŠ” ~~~ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<p><a href="link">ë§í¬</a>, date,</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024</span><br><span class="line">8/30/24</span><br><span class="line">Cohere</span><br><span class="line"></span><br><span class="line">Model Card for C4AI Command R+ 08-2024</span><br><span class="line">Model Summary</span><br><span class="line">C4AI Command R+ 08-2024 is an open weights research release of a 104B billion parameter model with highly advanced capabilities, this includes Retrieval Augmented Generation (RAG) and tool use to automate sophisticated tasks. The tool use in this model generation enables multi-step tool use which allows the model to combine multiple tools over multiple steps to accomplish difficult tasks. C4AI Command R+ 08-2024 is a multilingual model trained on 23 languages and evaluated in 10 languages. Command R+ 08-2024 is optimized for a variety of use cases including reasoning, summarization, and question answering.</span><br><span class="line"></span><br><span class="line">C4AI Command R+ 08-2024 is part of a family of open weight releases from Cohere For AI and Cohere. Our smaller companion model is C4AI Command R 08-2024.</span><br><span class="line"></span><br><span class="line">Point of Contact: Cohere For AI: cohere.for.ai</span><br><span class="line">License: CC-BY-NC, requires also adhering to C4AI&#x27;s Acceptable Use Policy</span><br><span class="line">Model: c4ai-command-r-plus-08-2024</span><br><span class="line">Model Size: 104 billion parameters</span><br><span class="line">Context length: 128K</span><br><span class="line"></span><br><span class="line">Cohere just dropped updated Command R plus &amp; Command R - built for RAG and tool use, multilingual (23 languages), grounded generation, 128K content and much more! ğŸ”¥</span><br><span class="line">&gt; Command R Plus - 104B param, Command R - 35B param</span><br><span class="line">&gt; up-to 2x higher throughput &amp; 2x lower latency</span><br><span class="line">&gt; Uses Grouped Query Attention (GQA)</span><br><span class="line">&gt; SFT + preference tuned model</span><br><span class="line">&gt; Massive 128K context window</span><br><span class="line">&gt; Trained in 23 languages, evaluated on 10</span><br><span class="line">&gt; Capable of code rewrites, explanations and snippets.</span><br><span class="line">&gt; Supports citation, tool execution, and structured outputs for grounded agentic use</span><br><span class="line">&gt; Model checkpoint available on the hub</span><br><span class="line">&gt; Works out of the box with transformers ğŸ¤—</span><br><span class="line">This is a massive improvement from the last iteration of Command R+ (which is still one of my favourites on Hugging Chat).</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://qwenlm.github.io/blog/qwen2-vl/</span><br><span class="line">Qwen2-VL: To See the World More Clearly</span><br><span class="line">August 29, 2024</span><br><span class="line"> Â· 17 min Â· 3569 words Â· Qwen Team | Translations:</span><br><span class="line">ç®€ä½“ä¸­æ–‡</span><br><span class="line"></span><br><span class="line">DEMO GITHUB HUGGING FACE MODELSCOPE API DISCORD</span><br><span class="line"></span><br><span class="line">After a yearâ€™s relentless efforts, today we are thrilled to release Qwen2-VL! Qwen2-VL is the latest version of the vision language models based on Qwen2 in the Qwen model familities. Compared with Qwen-VL, Qwen2-VL has the capabilities of:</span><br><span class="line"></span><br><span class="line">SoTA understanding of images of various resolution &amp; ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.</span><br><span class="line"></span><br><span class="line">Understanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.</span><br><span class="line"></span><br><span class="line">Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.</span><br><span class="line"></span><br><span class="line">Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">We opensource Qwen2-VL-2B and Qwen2-VL-7B with Apache 2.0 license, and we release the API of Qwen2-VL-72B! The opensource is integrated to Hugging Face Transformers, vLLM, and other third-party frameworks. Hope you enjoy!</span><br><span class="line"></span><br><span class="line">Performance</span><br><span class="line">We evaluate our modelâ€™s visual capabilities across six key dimensions: complex college-level problem-solving, mathematical abilities, document and table comprehension, multilingual text-image understanding, general scenario question-answering, video comprehension, and agent-based interactions. Overall, our 72B model showcases top-tier performance across most metrics, often surpassing even closed-source models like GPT-4o and Claude 3.5-Sonnet. Notably, it demonstrates a significant edge in document understanding.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">At the 7B scale, weâ€™ve managed to retain support for image, multi-image, and video inputs, delivering competitive performance in a more cost-effective model size. Specifically, our model excels in document understanding tasks such as DocVQA and in multilingual text understanding from images, as assessed by MTVQA, establishing state-of-the-art performance.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Additionally, weâ€™re excited to introduce a smaller 2B model, optimized for potential mobile deployment. Despite its compact size, this model boasts strong performance in image, video, and multilingual comprehension. It particularly shines in video-related tasks, document understanding, and general scenario question-answering when compared to other models of similar scale.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Model Capabilities</span><br><span class="line">1. Enhanced Recognition Capabilities</span><br><span class="line">Qwen2-VL now boasts improved object recognition, extending beyond plants and landmarks to comprehend complex relationships between multiple objects in a scene. Weâ€™ve also significantly boosted the modelâ€™s ability to recognize handwritten text and multiple languages within images, making it more accessible to users worldwide.</span><br><span class="line">2. Visual Reasoning: Solving Real-World Problems</span><br><span class="line">In this iteration, we have significantly enhanced Qwen2-VLâ€™s mathematical and coding proficiencies. The model is not only capable of solving problems by analyzing pictures but can also interpret and solve complex mathematical problems through chart analysis. Extremely aspect-ratio-distorted images can also be correctly interpreted. Additionally, we have reinforced the modelâ€™s capability to extract information from real-world images and charts and improved its instruction-following skills. This fusion of visual perception and logical reasoning empowers the model to tackle practical issues, bridging the gap between abstract concepts and tangible solutions.</span><br><span class="line"></span><br><span class="line">3. Video Understanding and Live Chat</span><br><span class="line">Beyond static images, Qwen2-VL extends its prowess to video content analysis. It can summarize video content, answer questions related to it, and maintain a continuous flow of conversation in real-time, offering live chat support. This functionality allows it to act as a personal assistant, helping users by providing insights and information drawn directly from video content.</span><br><span class="line">4. Visual Agent Capabilities: Function Calling and Visual Interactions.</span><br><span class="line">Qwen2-VL demonstrates strong potential as a visual agent, facilitating interactions similar to human perceptions of the world.</span><br><span class="line"></span><br><span class="line">The model facilitates Function Calling, enabling it to harness external tools for real-time data retrieval â€“ be it flight statuses, weather forecasts, or package tracking â€“ by deciphering visual cues. This integration of visual interpretation with functional execution elevates its utility, making it a powerful tool for information management and decision-making.</span><br><span class="line"></span><br><span class="line">Visual Interactions represent a significant stride towards mimicking human perception. By allowing the model to engage with visual stimuli akin to human senses, weâ€™re pushing the boundaries of AIâ€™s ability to perceive and respond to its environment. This capability paves the way for more intuitive and immersive interactions, where Qwen2-VL acts not just as an observer, but an active participant in our visual experiences.</span><br><span class="line">model is unable to extract audio from videos, and its knowledge is only up to date as of June 2023. Additionally, the model cannot guarantee complete accuracy when processing complex instructions or scenarios, and it is relatively weak in tasks involving counting, character recognition, and 3D spatial awareness.</span><br><span class="line"></span><br><span class="line">Model Architecture</span><br><span class="line">Overall, weâ€™ve continued with the Qwen-VL architecture, which leverages a Vision Transformer (ViT) model and Qwen2 language models. For all these variants, we utilized a ViT with approximately 600M parameters, designed to handle both image and video inputs seamlessly. To further enhance the modelâ€™s ability to effectively perceive and comprehend visual information in videos, we introduced several key upgrades:</span><br><span class="line"></span><br><span class="line">A key architectural improvement in Qwen2-VL is the implementation of Naive Dynamic Resolution support. Unlike its predecessor, Qwen2-VL can handle arbitrary image resolutions, mapping them into a dynamic number of visual tokens, thereby ensuring consistency between the model input and the inherent information in images. This approach more closely mimics human visual perception, allowing the model to process images of any clarity or size.</span><br><span class="line"></span><br><span class="line">Another key architectural enhancement is the innovation of Multimodal Rotary Position Embedding (M-ROPE). By deconstructing the original rotary embedding into three parts representing temporal and spatial (height and width) informationï¼ŒM-ROPE enables LLM to concurrently capture and integrate 1D textual, 2D visual, and 3D video positional information.</span><br><span class="line"></span><br><span class="line">License</span><br><span class="line">Both the opensource Qwen2-VL-2B and Qwen2-VL-7B are under Apache 2.0.</span><br><span class="line"></span><br><span class="line">Qwen 2VL 7B &amp; 2B are here - Apache 2.0 licensed smol Vision Language Models competitive with GPT 4o mini - w/ video understanding, function calling and more! ğŸ”¥</span><br><span class="line">&gt; 72B (to be released later) beats 3.5 Sonnet &amp; GPT 4o</span><br><span class="line">&gt; Can understand up to 20 min of video</span><br><span class="line">&gt; Handles arbitrary image resolutions</span><br><span class="line">&gt; Multimodal RoPE to capture 1D, 2D &amp; 3D information</span><br><span class="line">&gt; Enhanced Recognition capabilities - can understand complex relationships b/w objects</span><br><span class="line">&gt; Better Visual Reasoning &amp; video understanding w/ live chat</span><br><span class="line">&gt; Function calling for tools + data access</span><br><span class="line">&gt; Integrated with Transformers! ğŸ¤—</span><br><span class="line">&gt; Model checkpoints on the Hub</span><br><span class="line">Kudos to the Alibaba Qwen group; I&#x27;m a huge fan of the Qwen series, especially their multilingual capabilities! Looking forward to the 72B âš¡</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/Salesforce/xLAM-7b-r</span><br><span class="line">8/29/24</span><br><span class="line">Salseforce</span><br><span class="line"></span><br><span class="line">Let&#x27;s go.. Salesforce released Large Action Models xLAM - 7B, 8x7B, 8x22B, up to 64K context length primed for AI agents use-cases! ğŸ”¥</span><br><span class="line">LAMs are designed to enhance decision-making and translate user intentions into executable actions.</span><br><span class="line">Integrated with Transformers ğŸ¤—</span><br><span class="line">Welcome to the xLAM model family! Large Action Models (LAMs) are advanced large language models designed to enhance decision-making and translate user intentions into executable actions that interact with the world. LAMs autonomously plan and execute tasks to achieve specific goals, serving as the brains of AI agents. They have the potential to automate workflow processes across various domains, making them invaluable for a wide range of applications. The model release is exclusively for research purposes. A new and enhanced version of xLAM will soon be available exclusively to customers on our Platform.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/nvidia/NV-Embed-v2</span><br><span class="line">NVIDIA</span><br><span class="line">9/1/24</span><br><span class="line">We have reclaimed #1 on the MTEB Leaderboard ğŸ† Our NV-Embed-v2, has achieved a record-breaking score of 72.31 across 56 text embedding/retrieval tasks, reclaiming the top spot on the Massive Text Embedding Benchmark (MTEB) leaderboard. It also holds the No. 1 in the retrieval sub-category (15 tasks) in the leaderboard, which is essential to the development of RAG technology.</span><br><span class="line">We present NV-Embed-v2, a generalist embedding model that ranks No. 1 on the Massive Text Embedding Benchmark (MTEB benchmark)(as of Aug 30, 2024) with a score of 72.31 across 56 text embedding tasks. It also holds the No. 1 in the retrieval sub-category (a score of 62.65 across 15 tasks) in the leaderboard, which is essential to the development of RAG technology.</span><br><span class="line"></span><br><span class="line">NV-Embed-v2 presents several new designs, including having the LLM attend to latent vectors for better pooled embedding output, and demonstrating a two-staged instruction tuning method to enhance the accuracy of both retrieval and non-retrieval tasks. Additionally, NV-Embed-v2 incorporates a novel hard-negative mining methods that take into account the positive relevance score for better false negatives removal.</span><br><span class="line"></span><br><span class="line">For more technical details, refer to our paper: NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models.</span><br><span class="line"></span><br><span class="line">Model Details</span><br><span class="line">Base Decoder-only LLM: Mistral-7B-v0.1</span><br><span class="line">Pooling Type: Latent-Attention</span><br><span class="line">Embedding Dimension: 4096</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2408.02442</span><br><span class="line">Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models</span><br><span class="line">Published on Aug 5</span><br><span class="line">Authors:</span><br><span class="line">Zhi Rui Tam</span><br><span class="line">,</span><br><span class="line">Cheng-Kuang Wu</span><br><span class="line">,</span><br><span class="line">Yi-Lin Tsai</span><br><span class="line">,</span><br><span class="line">Chieh-Yen Lin</span><br><span class="line">,</span><br><span class="line">Hung-yi Lee</span><br><span class="line">,</span><br><span class="line">Yun-Nung Chen</span><br><span class="line">Abstract</span><br><span class="line">Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs). This study investigates whether such constraints on generation space impact LLMs&#x27; abilities, including reasoning and domain knowledge comprehension. Specifically, we evaluate LLMs&#x27; performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks. Surprisingly, we observe a significant decline in LLMs&#x27; reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks.</span><br><span class="line">Structured Prompting is a key requirement for building real-world LLM applications or agents, but does it harm the performance and ability to reason? ğŸ¤” â€Let Me Speak Freelyâ€ studies the impact of structured formats (JSON, XML, YAML) versus generating free-form responses across various common tasks. ğŸ‘€</span><br><span class="line">Methods: Constraint Decoding (JSON-Mode), Format-Restricting Instructions (FRI, â€respond in JSONâ€), NL-to-Format (2 step, first NL answer â†’ covert to JSON)</span><br><span class="line">Datasets: GSM8K, Last Letter, Shuffled Objects, DDXPlus, Sports, Task280, and MultiFin</span><br><span class="line">Models: Gemini 1.5 Flash, Claude 3.5 Haiku, GPT-3.5-Turbo, Llama 3 8B or Gemma 2 9B</span><br><span class="line">Insights</span><br><span class="line">ğŸ§  Reasoning abilities can decline under format restrictions</span><br><span class="line">ğŸš€ Gemini 1.5 Flash achieved the highest consistency across formats</span><br><span class="line">ğŸ“„ Gemini, Llama 3, Gemma 2 work best with JSON format</span><br><span class="line">ğŸ“‰ Claude 3 Haiku showed a big performance drop for JSON but not for XML</span><br><span class="line">âš ï¸ Adding concrete schema constraints can decrease performance</span><br><span class="line">ğŸ” Classification: JSON-Mode performs equally, if not better</span><br><span class="line">ğŸ“Š Classification: JSON-Mode performs better than FRI (instructed) for open models</span><br><span class="line">â¬‡ï¸ Reasoning: JSON-Mode performs worse than other methods</span><br><span class="line">ğŸ’ª Reasoning: NL-to-Format has the strongest results after NL.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2403.16950</span><br><span class="line">Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators</span><br><span class="line">Published on Mar 26</span><br><span class="line">Authors:</span><br><span class="line">Yinhong Liu</span><br><span class="line">,</span><br><span class="line">Han Zhou</span><br><span class="line">,</span><br><span class="line">Zhijiang Guo</span><br><span class="line">,</span><br><span class="line">Ehsan Shareghi</span><br><span class="line">,</span><br><span class="line">Ivan VuliÄ‡</span><br><span class="line">,</span><br><span class="line">Anna Korhonen</span><br><span class="line">,</span><br><span class="line">Nigel Collier</span><br><span class="line">Abstract</span><br><span class="line">Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PairS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PairS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthermore, we provide insights into the role of pairwise preference in quantifying the transitivity of LLMs and demonstrate how PairS benefits from calibration.</span><br><span class="line">How can we improve the reliability of LLM Evaluators? â€œThe Role of Pairwise Preference in Large Language Model Evaluatorsâ€ compares existing evaluation approaches and shows how to perform robustness evaluations using Pairwise-preference Search (PAIRS)! ğŸ‘€</span><br><span class="line">PAIRS compares multiple preference pairs of generated texts and then ranks candidates globally, like a tournament or tree, to find the best one. It improves the robustness of evaluation and is also more efficient than win-loss or ELO rating systems. â€œPAIRS-greedy typically requires only about 30% of the comparisons to achieve performance similar to ELO rating.â€</span><br><span class="line">âš–ï¸ PAIRS trades compute/cost for more precision and robustness</span><br><span class="line">ğŸ§® Example: 16 candidates per data point lead to 16 * 15 / 2 = 120 comparisons, or if greedy is used, then 36.</span><br><span class="line">ğŸš€ PAIRS outperforms G-Eval, Win-loss rate, and ELO rating on Spearman correlations</span><br><span class="line">ğŸ§‘ğŸ»â€ğŸ’» Code and examples available</span><br><span class="line">ğŸ“ Used simple generic prompts for comparison.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.gartner.com/en/newsroom/press-releases/2024-08-27-gartner-hype-cycle-reveals-top-technologies-that-will-transform-sales-in-the-next-decade</span><br><span class="line">STAMFORD, Conn, August 28, 2024</span><br><span class="line">Gartner</span><br><span class="line"></span><br><span class="line">Gartner Hype Cycle Reveals Top Technologies That Will Transform Sales In the Next Decade</span><br><span class="line">Emotion AI, Digital Twin of a Customer and Machine Sellers Will Have the Biggest Impact on Sales Organizations</span><br><span class="line">Emotion AI, machine sellers and digital twin of a customer will have a transformative impact on the sales function in the next decade, according to Gartner, Inc. However, sales leaders must closely navigate the hype to evaluate when these technologies will be appropriate for their organization to implement.</span><br><span class="line"></span><br><span class="line">The Gartner Hype Cycle for Revenue and Sales Technology, 2024 distills key insights that Gartner profiles each year into a succinct set of â€œmust-knowâ€ emerging technologies. These technologies have potential to deliver transformational benefits over the next two to 10 years (see Figure 1).</span><br><span class="line"></span><br><span class="line">â€œThe common theme of these three technologies are their ability to predict, interpret and serve buyersâ€™ needs and behaviors and to streamline and automate sales fulfillment, releasing sellers to focus on developing high value client relationships,â€ said Guy Wood, Senior Director Analyst in the Gartner Sales practice.â€œIn order to gain a competitive advantage, sales operations leaders need to be looking on the horizon for technologies not currently in the mainstream.â€</span><br><span class="line"></span><br><span class="line">Figure 1: Hype Cycle for Revenue and Sales Technology, 2024</span><br><span class="line"></span><br><span class="line">Source: (Gartner, August 2024)</span><br><span class="line"></span><br><span class="line">Emotion AI</span><br><span class="line"></span><br><span class="line">Emotion AI, which is at the Peak of Inflated Expectations, uses AI and software techniques to analyze the emotional state of a user via computer vision, audio/voice input, sensors and/or software logic. Emotion AI turns human behavioral attributes into data. By enabling sales teams to utilize data to actively learn from and empathize with the customer, emotion AI is poised to dramatically change the sales function.</span><br><span class="line"></span><br><span class="line">â€œEmotion AI has already been widely adopted in contact centers, but the sales function has yet to fully realize the technologyâ€™s potential,â€ said Wood. â€œHowever, CSOs must navigate privacy concerns and bias, which may be a barrier to successful adoption. For example, privacy and ethics challenges surround psychological profiling, especially when applied to consumers, recruitment prospects or protected individuals like minors. â€</span><br><span class="line"></span><br><span class="line">Digital Twin of a Customer</span><br><span class="line"></span><br><span class="line">A digital twin of a customer (DToC) is a dynamic virtual mirror representation of a customer that organizations can use to simulate, emulate and anticipate behavior. DToCs, currently at the Innovation Trigger,  help organizations better understand their customers and provide a personalized, empathetic service to customers, many of whose buying habits repeatedly change.</span><br><span class="line"></span><br><span class="line">â€œDToCs can transform the way organizations sell products or services and provide customers with better experiences, which will result in increased revenue and lasting customer relationships,&quot; said Wood. â€œDToC can be an engine of transformation and disruption. Organizations need competency in machine learning algorithms and staff with data science skills to build or manage DToCsâ€</span><br><span class="line"></span><br><span class="line">Machine Sellers</span><br><span class="line"></span><br><span class="line">Machine sellers, at the early stage of the Innovation Trigger, are nonhuman agents that automate end-to-end selling actions on behalf of human sellers, or a sales organization, to sell products and services in exchange for payment. Currently, machine sellers can be used to facilitate simple and transactional sales.</span><br><span class="line"></span><br><span class="line">â€œSales organizations deploying machine sellers will gain a competitive advantage by satisfying buyer preferences for seamless purchases and â€˜locking-inâ€™ recurring revenue. Organizations that do not adopt machine sellers will risk wasting resources, decreasing efficiency and missing revenue goals,â€ said Wood. â€œHowever, the impact of machine sellers will not be evenly distributed; it will vary by vertical industry, geography and business modelâ€</span><br><span class="line"></span><br><span class="line">Gartner clients can read more in â€œHype Cycle for Revenue and Sales Technology, 2024â€</span><br><span class="line"></span><br><span class="line">About Gartner for Sales Leaders</span><br><span class="line"></span><br><span class="line">Gartner for Sales Leaders provides heads of sales and their teams with the insights, advice and tools they need to address mission-critical priorities amid mounting pressures to drive growth through new and existing customers. With extensive qualitative and quantitative research, Gartner for Sales Leaders helps sales teams combat commoditization and price-based purchasing, develop critical manager and seller skills, elevate the value of sales interactions, unlock existing growth potential, and optimize sales force enablement. Follow news and update from the Gartner Sales practice on X and LinkedIn using #GartnerSales. Members of the media can find additional information and insights in the Gartner Sales Newsroom.</span><br><span class="line">2024 ì‹ ê¸°ìˆ  í•˜ì´í”„ ì‚¬ì´í´ ë°œí‘œ...â€ììœ¨í˜• AI ë“±ì¥ ê°€ì†í™”â€</span><br><span class="line"></span><br><span class="line">ê°€íŠ¸ë„ˆê°€ 2024ë…„ ì‹ ê¸°ìˆ  í•˜ì´í”„ ì‚¬ì´í´(Hype Cycle for Emerging Technologies) ë³´ê³ ì„œë¥¼ í†µí•´ ì£¼ëª©í•´ì•¼ í•  25ê°€ì§€ì˜ í˜ì‹  ê¸°ìˆ ì„ ë°œí‘œí–ˆë‹¤. ì´ ê¸°ìˆ ë“¤ì€ â–²ììœ¨í˜• AI â–²ê°œë°œì ìƒì‚°ì„± â–²ì´ì²´ì  ê²½í—˜ â–²ì¸ê°„ ì¤‘ì‹¬ì˜ ë³´ì•ˆ ë° ê°œì¸ì •ë³´ ë³´í˜¸ í”„ë¡œê·¸ë¨ ë“± ë„¤ ê°€ì§€ ì£¼ìš” íŠ¸ë Œë“œë¡œ ë¶„ë¥˜ëœë‹¤.</span><br><span class="line"></span><br><span class="line">ì•„ë£¬ ì°¬ë“œë¼ì„¸ì¹´ë€ ê°€íŠ¸ë„ˆ ìˆ˜ì„ VP ì• ë„ë¦¬ìŠ¤íŠ¸ëŠ” â€œê¸°ë°˜ ëª¨ë¸ì— ëŒ€í•œ ê¸°ëŒ€ê°ì—ì„œ ROIë¥¼ ì°½ì¶œí•˜ëŠ” ì‚¬ìš© ì‚¬ë¡€ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ì´ˆì ì´ ì´ë™í•˜ê³  ìˆë‹¤. ìƒì„±í˜• AIëŠ” ë¶€í’€ë ¤ì§„ ê¸°ëŒ€ì˜ ì •ì ì„ ë„˜ì–´ì„°ìœ¼ë©° ììœ¨í˜• AIì˜ ë“±ì¥ì„ ê°€ì†í•˜ê³  ìˆë‹¤â€ë©° â€œí˜„ì¬ AI ëª¨ë¸ì—ëŠ” ì—ì´ì „íŠ¸ ê¸°ëŠ¥ì´ ë¶€ì¡±í•˜ë‹¤. AI ì—°êµ¬ì†Œë“¤ì€ ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•´ ìƒí˜¸ ì‘ìš©í•  ìˆ˜ ìˆëŠ” AI ì—ì´ì „íŠ¸ë¥¼ ì‹ ì†í•˜ê²Œ ì¶œì‹œí•˜ê³  ìˆìœ¼ë‚˜ ê°œë°œ ê³¼ì •ì€ ì ì§„ì ìœ¼ë¡œ ì§„í–‰ë  ê²ƒâ€ì´ë¼ê³  ì˜ˆì¸¡í–ˆë‹¤.</span><br><span class="line"></span><br><span class="line">ì°¬ë“œë¼ì„¸ì¹´ë€ ìˆ˜ì„ VP ì• ë„ë¦¬ìŠ¤íŠ¸ëŠ” â€œAIê°€ ê³„ì†í•´ì„œ ì£¼ëª©ë°›ê³  ìˆëŠ” ê°€ìš´ë° CIOì™€ IT ê²½ì˜ì§„ì€ ê°œë°œ, ë³´ì•ˆ, ê³ ê° ë° ì§ì› ê²½í—˜ì— í˜ì‹ ì ì¸ ì ì¬ë ¥ì„ ê°€ì§„ ì‹ ê¸°ìˆ ì„ ê²€í† í•´ì•¼ í•œë‹¤â€ë©° â€œë˜í•œ ê²€ì¦ë˜ì§€ ì•Šì€ ê¸°ìˆ ì— ëŒ€í•œ ê´€ë¦¬, í™œìš©ë²•ì„ ì¡°ì§ì˜ ëŠ¥ë ¥ì— ë§ì¶° ì „ëµì ìœ¼ë¡œ ìˆ˜ë¦½í•´ì•¼ í•œë‹¤â€ê³  ì¡°ì–¸í–ˆë‹¤.</span><br><span class="line"></span><br><span class="line">ì‹ ê¸°ìˆ  í•˜ì´í”„ ì‚¬ì´í´ì€ ê°€íŠ¸ë„ˆê°€ ë°œí‘œí•˜ëŠ” ì—¬ëŸ¬ í•˜ì´í”„ ì‚¬ì´í´ ì¤‘ì—ì„œë„ ë…ë³´ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•œë‹¤. ê°€íŠ¸ë„ˆê°€ ë§¤ë…„ í”„ë¡œíŒŒì¼ë§í•˜ëŠ” 2000ê°œ ì´ìƒì˜ ê¸°ìˆ  ë° ì‘ìš© í”„ë ˆì„ì›Œí¬ì—ì„œ í•µì‹¬ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  ë– ì˜¤ë¥´ëŠ” ê¸°ìˆ ë“¤ì„ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ ì œì‹œí•˜ê¸° ë•Œë¬¸ì´ë‹¤. í•´ë‹¹ ê¸°ìˆ ë“¤ì€ í–¥í›„ 2ë…„ì—ì„œ 10ë…„ê°„ í˜ì‹ ì ì¸ ì´ì ì„ ì œê³µí•  ì ì¬ë ¥ì„ ê°–ì¶˜ ê²ƒìœ¼ë¡œ í‰ê°€ëœë‹¤.</span><br><span class="line"></span><br><span class="line">2024ë…„ ì‹ ê¸°ìˆ ì˜ 4ê°€ì§€ íŠ¸ë Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</span><br><span class="line"></span><br><span class="line">ììœ¨í˜• AI :</span><br><span class="line"></span><br><span class="line">AIì˜ ë¹ ë¥¸ ë°œì „ìœ¼ë¡œ ì¸í•´ ì¸ê°„ì˜ ê°ë…ì„ ìµœì†Œí™”í•˜ë©´ì„œ, ìŠ¤ìŠ¤ë¡œ ì‘ë™í•˜ê³  ê°œì„ í•˜ë©° ë³µì¡í•œ í™˜ê²½ì—ì„œë„ íš¨ê³¼ì ì¸ ì˜ì‚¬ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆëŠ” ììœ¨í˜• AI ì‹œìŠ¤í…œì´ íƒ„ìƒí•˜ê³  ìˆë‹¤. ì¸ê°„ì´ í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì´ëŸ¬í•œ ì²¨ë‹¨ AI ì‹œìŠ¤í…œì€ ê³µìƒ ê³¼í•™ì—ì„œ í˜„ì‹¤ë¡œ ì„œì„œíˆ ë‹¤ê°€ì˜¤ê³  ìˆë‹¤.</span><br><span class="line"></span><br><span class="line">ì´ ê¸°ìˆ ì—ëŠ” â–²ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ â–²ëŒ€ê·œëª¨ í–‰ë™ ëª¨ë¸ â–²ê¸°ê³„ ê³ ê° â–²íœ´ë¨¸ë…¸ì´ë“œ ì‘ì—… ë¡œë´‡ â–²ììœ¨ ì—ì´ì „íŠ¸ â–²ê°•í™” í•™ìŠµ ë“±ì´ í¬í•¨ëœë‹¤.</span><br><span class="line"></span><br><span class="line">ê°œë°œì ìƒì‚°ì„± í–¥ìƒ:</span><br><span class="line"></span><br><span class="line">ê°œë°œì ìƒì‚°ì„±ì€ ì½”ë“œë¥¼ ë¹ ë¥´ê²Œ ì‘ì„±í•˜ëŠ” ê²ƒ ì´ìƒì˜ ì˜ë¯¸ë¥¼ ê°–ëŠ”ë‹¤. ì´ëŠ” ê°œë°œìì˜ íš¨ê³¼ì ì¸ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ê³¼ í˜‘ì—…, ì§‘ì¤‘ë ¥, ë§Œì¡±ë„ ë“±ì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤. ê°œë°œìì˜ ìƒì‚°ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ì‹ ê¸°ìˆ ë¡œëŠ” â–²AI ì¦ê°• ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ë§ â–²í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ â–²ê¹ƒì˜µìŠ¤ â–²ë‚´ë¶€ ê°œë°œì í¬í„¸ â–²í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ â–²ì›¹ì–´ì…ˆë¸”ë¦¬ ë“±ì´ ìˆë‹¤.</span><br><span class="line"></span><br><span class="line">ì°¬ë“œë¼ì„¸ì¹´ë€ ìˆ˜ì„ VP ì• ë„ë¦¬ìŠ¤íŠ¸ëŠ” â€œê¸°ìˆ ì€ ê°œë°œìê°€ ì†Œí”„íŠ¸ì›¨ì–´ë¥¼ ì„¤ê³„í•˜ê³  ì œê³µí•˜ëŠ” ë°©ì‹ì„ í˜ì‹ í•´ ê·¸ ì–´ëŠ ë•Œë³´ë‹¤ ë†’ì€ ìƒì‚°ì„±ì„ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤â€ë©° â€œê°œë°œìì˜ ë§Œì¡±ë„, í˜‘ì—…, í”Œë¡œìš° ê°œì„ ì„ í†µí•´ ì´ìµì„ ê·¹ëŒ€í™”í•˜ë©´ì„œ ê³ í’ˆì§ˆì˜ ì œí’ˆì„ ì‹ ì†í•˜ê²Œ ì œê³µí•  ìˆ˜ ìˆê²Œ ëë‹¤â€ê³  ë§í–ˆë‹¤.</span><br><span class="line"></span><br><span class="line">ì´ì²´ì  ê²½í—˜ì„ í†µí•œ ì—­ëŸ‰ ê°•í™”:</span><br><span class="line"></span><br><span class="line">ì´ì²´ì  ê²½í—˜ì€ ê³ ê° ê²½í—˜, ì§ì› ê²½í—˜, ë‹¤ì¤‘ ê²½í—˜, ì‚¬ìš©ì ê²½í—˜ì„ ì„œë¡œ ì—°ê²°í•´ ìš°ìˆ˜í•œ ê³µìœ  ê²½í—˜ì„ ì°½ì¶œí•˜ëŠ” ì „ëµì´ë‹¤. ì‹ ë¢°ë„, ë§Œì¡±ë„, ì¶©ì„±ë„, ì§€ì§€ë„ í–¥ìƒì„ ëª©í‘œë¡œ ê¸°ìˆ ì„ ì‚¬ìš©í•´ ì¤‘ìš”í•œ ìƒí˜¸ ì‘ìš©ì„ í•´ê²°í•˜ê³  ê³ ê°ê³¼ ì§ì› ëª¨ë‘ì˜ ì—­ëŸ‰ì„ ê°•í™”í•œë‹¤. í‰ê°€ ëŒ€ìƒ ê¸°ìˆ ì—ëŠ” â–²ë””ì§€í„¸ íŠ¸ìœˆ â–²ê³µê°„ ì»´í“¨íŒ… â–²ìŠˆí¼ì•± â–²6G ë“±ì´ í¬í•¨ëœë‹¤.</span><br><span class="line"></span><br><span class="line">ì¸ê°„ ì¤‘ì‹¬ì˜ ë³´ì•ˆ ë° ê°œì¸ì •ë³´ ë³´í˜¸ ì œê³µ:</span><br><span class="line"></span><br><span class="line">ê¸°ì—…ì€ ìƒí˜¸ ì‹ ë¢°ì˜ ë¬¸í™”ë¥¼ ì¡°ì„±í•˜ê³  íŒ€ ê°„ì— ê³µìœ ëœ ìœ„í—˜ì„ ì¸ì‹í•˜ëŠ” ë³´ì•ˆ ë° ê°œì¸ì •ë³´ ë³´í˜¸ ê¸°ìˆ ì„ ì‚¬ìš©í•´ ë”ìš± íƒ„ë ¥ì ì¸ ì¡°ì§ìœ¼ë¡œ ê±°ë“­ë‚  ìˆ˜ ìˆë‹¤. ì¸ê°„ ì¤‘ì‹¬ ë³´ì•ˆ ë° ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ì§€ì›í•˜ëŠ” ë– ì˜¤ë¥´ëŠ” ê¸°ìˆ ë¡œëŠ” â–²AI íŠ¸ë¦¬ì¦˜ â–²ì‚¬ì´ë²„ ë³´ì•ˆ ë©”ì‹œ ì•„í‚¤í…ì²˜ â–²ë””ì§€í„¸ ë©´ì—­ ì‹œìŠ¤í…œ â–²í—ˆìœ„ ì •ë³´ ë³´ì•ˆ â–²ì—°í•© ë¨¸ì‹ ëŸ¬ë‹ â–²ë™í˜• ì•”í˜¸ê°€ ìˆë‹¤.</span><br><span class="line"></span><br><span class="line">ì°¬ë“œë¼ì„¸ì¹´ë€ ìˆ˜ì„ VP ì• ë„ë¦¬ìŠ¤íŠ¸ëŠ” â€œë³´ì•ˆ ê´€í–‰ì´ ì¶©ë¶„íˆ ì•ˆì „í•˜ê³  í™•ì‹¤í•œ ë°©ì‹ìœ¼ë¡œ ì‘ë™í•  ê²ƒì´ë¼ëŠ” ì „ì œì— ì˜ì¡´í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. í•˜ì§€ë§Œ ë§ì€ ì¡°ì§ì´ ë³´ì•ˆê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì œê³µ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì•¼ í•  ë•Œ ë¹„ì¦ˆë‹ˆìŠ¤ ì œê³µì„ ìš°ì„ ì‹œí•´ ì§€ë‚˜ì¹˜ê²Œ ì—„ê²©í•œ ë³´ì•ˆ ì¡°ì¹˜ë¥¼ ìš°íšŒí•˜ëŠ” ì„ íƒì„ ì¢…ì¢… í•œë‹¤â€ë©° â€œì¸ê°„ ì¤‘ì‹¬ì˜ ë³´ì•ˆ ë° ê°œì¸ì •ë³´ ë³´í˜¸ëŠ” ì¡°ì§ì˜ ë””ì§€í„¸ ì„¤ê³„ì— ê¸´ë°€í•œ ë³´ì•ˆ ë° ê°œì¸ì •ë³´ ë³´í˜¸ êµ¬ì¡°ë¥¼ ì—®ì–´ì¤€ë‹¤â€ê³  ë§í–ˆë‹¤.</span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-09-02</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/09/02/2024-9-2-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 9ì›” 2ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/09/06/2024-9-6-AI-NEWS/" title="2024ë…„ 9ì›” 6ì¼ AI ì†Œì‹">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/08/29/2024-8-29-AI-NEWS/" title="2024ë…„ 8ì›” 29ì¼ AI ì†Œì‹">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>