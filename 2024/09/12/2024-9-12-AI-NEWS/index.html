<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 9ì›” 12ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="ì˜¤ëŠ˜ì˜ AI ì†Œì‹ì—ì„œëŠ” Mistral, OpenAI, NVIDIA, Apple, Microsoft ë“± ë‹¤ì–‘í•œ ê¸°ì—…ë“¤ì˜ ìµœì‹  ê¸°ìˆ  ë°œí‘œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. Mistralì€ ìƒˆë¡œìš´ ë¹„ì „ ì–¸ì–´ ëª¨ë¸ Pixtral 12Bë¥¼ ì¶œì‹œí•˜ì—¬ ë©€í‹°ëª¨ë‹¬ AI ê²½ìŸì—ì„œ Metaë¥¼ ì•ì„œê°”ìŠµë‹ˆë‹¤. "><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 9ì›” 12ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><p>ì˜¤ëŠ˜ì˜ AI ì†Œì‹ì—ì„œëŠ” Mistral, OpenAI, NVIDIA, Apple, Microsoft ë“± ë‹¤ì–‘í•œ ê¸°ì—…ë“¤ì˜ ìµœì‹  ê¸°ìˆ  ë°œí‘œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. Mistralì€ ìƒˆë¡œìš´ ë¹„ì „ ì–¸ì–´ ëª¨ë¸ Pixtral 12Bë¥¼ ì¶œì‹œí•˜ì—¬ ë©€í‹°ëª¨ë‹¬ AI ê²½ìŸì—ì„œ Metaë¥¼ ì•ì„œê°”ìŠµë‹ˆë‹¤. OpenAIëŠ” ChatGPTê°€ ì°½ì‘ ê³¼ì •ì—ì„œ ì–´ë–»ê²Œ í™œìš©ë˜ëŠ”ì§€ë¥¼ ë‹¤ë£¬ ì‚¬ë¡€ë“¤ì„ ê³µìœ í•˜ë©°, ì°½ì‘ìë“¤ì´ AIë¥¼ ì–´ë–»ê²Œ í™œìš©í•˜ê³  ìˆëŠ”ì§€ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. NVIDIAëŠ” ìµœì²¨ë‹¨ ì˜¤ë””ì˜¤ ìƒì„± ëª¨ë¸ BigVGAN v2ë¥¼ ë°œí‘œí•˜ì—¬ ì˜¤ë””ì˜¤ í•©ì„± ì†ë„ì™€ í’ˆì§ˆì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. Appleì€ iPhone 16ì˜ ìƒˆë¡œìš´ ì˜¨ë””ë°”ì´ìŠ¤ AI ê¸°ëŠ¥ê³¼ ê·¸ í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œë¥¼ ê³µê°œí–ˆìŠµë‹ˆë‹¤. MicrosoftëŠ” ë¹„ì •í˜• ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” GraphRAG ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ì¶œì‹œí•˜ì˜€ê³ , FishAudioëŠ” ë‹¤êµ­ì–´ ì§€ì› í…ìŠ¤íŠ¸-ìŒì„± ëª¨ë¸ì¸ Fish Speech 1.4ë¥¼ ì¶œì‹œí–ˆìŠµë‹ˆë‹¤. ICTNLPëŠ” ìŒì„±ê³¼ í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” LLaMA-3.1-8B Omni ëª¨ë¸ì„ ë°œí‘œí–ˆê³ , Arcee.aiëŠ” Meta LLaMA 3.1 ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ SuperNovaë¥¼ ì¶œì‹œí–ˆìŠµë‹ˆë‹¤. ë˜í•œ, AWSëŠ” ê³ ì„±ëŠ¥ AI ì›Œí¬ë¡œë“œë¥¼ ìœ„í•œ EC2 P5e ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¶œì‹œí•˜ì˜€ìœ¼ë©°, GOT 2.0ì€ ë”ìš± ë°œì „ëœ OCR(ê´‘í•™ ë¬¸ì ì¸ì‹) ê¸°ëŠ¥ì„ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, UpstageëŠ” ë‹¨ì¼ GPUì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ëŠ” Solar Pro Preview ëª¨ë¸ì„ ê³µê°œí–ˆìŠµë‹ˆë‹¤.</p>
<h3 id="Mistral-AI-Pixtral-12B-ë¹„ì „-ì–¸ì–´-ëª¨ë¸-ì¶œì‹œ"><a href="#Mistral-AI-Pixtral-12B-ë¹„ì „-ì–¸ì–´-ëª¨ë¸-ì¶œì‹œ" class="headerlink" title="Mistral AI, Pixtral 12B ë¹„ì „ ì–¸ì–´ ëª¨ë¸ ì¶œì‹œ"></a>Mistral AI, Pixtral 12B ë¹„ì „ ì–¸ì–´ ëª¨ë¸ ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/mistral-community/pixtral-12b-240910">ë§í¬</a>, 9ì›” 12ì¼ 2024ë…„</p>
<ul>
<li>Mistralì€ Pixtral 12Bë¼ëŠ” ìƒˆë¡œìš´ ë¹„ì „ ì–¸ì–´ ëª¨ë¸ì„ ì¶œì‹œ</li>
<li>í…ìŠ¤íŠ¸ ë°±ë³¸ì€ Mistral Nemo 12B, ë¹„ì „ ì–´ëŒ‘í„°ëŠ” 400M íŒŒë¼ë¯¸í„° ì‚¬ìš©</li>
<li>GeLU í™œì„±í™” í•¨ìˆ˜ì™€ 2D RoPEê°€ ë¹„ì „ ì¸ì½”ë”ì— ì ìš©</li>
<li>í™•ì¥ëœ ì–´íœ˜ í¬ê¸°: 131,072ê°œ</li>
<li>ì´ë¯¸ì§€ ê´€ë ¨ ì‘ì—…ì„ ìœ„í•œ ìƒˆë¡œìš´ í† í° 3ì¢… ì¶”ê°€: <code>img</code>, <code>img_break</code>, <code>img_end</code></li>
<li>ì´ë¯¸ì§€ í¬ê¸°: 1024 x 1024 í”½ì…€, íŒ¨ì¹˜ í¬ê¸°: 16 x 16 í”½ì…€</li>
<li>ëª¨ë¸ ê°€ì¤‘ì¹˜ëŠ” bf16 í˜•ì‹ìœ¼ë¡œ ì œê³µë˜ë©°, Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥</li>
</ul>
<h3 id="OpenAI-Writing-with-AI-ChatGPT-í™œìš©-ì‚¬ë¡€"><a href="#OpenAI-Writing-with-AI-ChatGPT-í™œìš©-ì‚¬ë¡€" class="headerlink" title="OpenAI, Writing with AI: ChatGPT í™œìš© ì‚¬ë¡€"></a>OpenAI, Writing with AI: ChatGPT í™œìš© ì‚¬ë¡€</h3><p><a target="_blank" rel="noopener" href="https://openai.com/chatgpt/use-cases/writing-with-ai/">ë§í¬</a>, 9ì›” 10ì¼ 2024ë…„</p>
<ul>
<li>ChatGPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì°½ì‘ìë“¤ì´ ì•„ì´ë””ì–´ë¥¼ ë°œì „ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì†Œê°œ</li>
<li>ì‘ê°€ë“¤ì´ ì•„ì´ë””ì–´ êµ¬ìƒ, ìŠ¤í† ë¦¬ ì»¨ì„¤íŒ…, ì—°êµ¬ ì§€ì›, í¸ì§‘ ë³´ì¡°ì— ChatGPTë¥¼ í™œìš©</li>
<li>ì „ë¬¸ ì‘ê°€ë“¤ì€ ChatGPTë¥¼ ìŠ¤í† ë¦¬ ìƒë‹´ìì´ì í”¼ë“œë°± ì œê³µ ë„êµ¬ë¡œ ì‚¬ìš©</li>
<li>ì°½ì˜ì ì¸ ê³¼ì •ì„ ë•ëŠ” â€˜ë°˜ì „ ì¸í„°ë·°â€™ ê¸°ë²•ì„ ì‚¬ìš©í•´ ì‘ê°€ë“¤ì˜ ë‚´ë©´ ì•„ì´ë””ì–´ë¥¼ ì´ëŒì–´ë‚´ë„ë¡ ì§€ì›</li>
<li>ë¹„ìœ ì™€ ì€ìœ , ì•„ì´ë””ì–´ êµ¬ìƒì— ìˆì–´ ChatGPTì˜ í™œìš©ì´ íŠ¹íˆ ìœ ìš©í•¨</li>
<li>ì¼ë¶€ ì‘ê°€ë“¤ì€ ìŠ¤í† ë¦¬ í”¼ë“œë°±ì„ í†µí•´ êµ¬ì²´ì ì¸ ì¤„ê±°ë¦¬ ë¬¸ì œ í•´ê²°ì— ì‚¬ìš©</li>
</ul>
<h3 id="NVIDIA-BigVGAN-v2-ì˜¤ë””ì˜¤-í•©ì„±-ëª¨ë¸-ë°œí‘œ"><a href="#NVIDIA-BigVGAN-v2-ì˜¤ë””ì˜¤-í•©ì„±-ëª¨ë¸-ë°œí‘œ" class="headerlink" title="NVIDIA, BigVGAN v2 ì˜¤ë””ì˜¤ í•©ì„± ëª¨ë¸ ë°œí‘œ"></a>NVIDIA, BigVGAN v2 ì˜¤ë””ì˜¤ í•©ì„± ëª¨ë¸ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/achieving-state-of-the-art-zero-shot-waveform-audio-generation-across-audio-types/?ncid=so-link-568047&linkId=100000287921594">ë§í¬</a>, 9ì›” 5ì¼ 2024ë…„</p>
<ul>
<li>NVIDIAëŠ” BigVGAN v2 ëª¨ë¸ì„ ë°œí‘œí•˜ì—¬ ì˜¤ë””ì˜¤ ìƒì„± ë¶„ì•¼ì—ì„œ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒ</li>
<li>ì˜¤ë””ì˜¤ í’ˆì§ˆì´ ì´ì „ ëª¨ë¸ ëŒ€ë¹„ í¬ê²Œ í–¥ìƒë˜ì—ˆìœ¼ë©°, ë‹¤ì–‘í•œ ì˜¤ë””ì˜¤ ìœ í˜•ì—ì„œ ë†’ì€ í‰ê°€</li>
<li>ìµœëŒ€ 3ë°° ë¹ ë¥¸ ì˜¤ë””ì˜¤ í•©ì„± ì†ë„, ìµœì í™”ëœ CUDA ì»¤ë„ì„ í†µí•´ ì‹¤ì‹œê°„ë³´ë‹¤ 240ë°° ë¹ ë¥´ê²Œ í•©ì„± ê°€ëŠ¥</li>
<li>ì‚¬ì „ í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ëŠ” ë‹¤ì–‘í•œ ì˜¤ë””ì˜¤ êµ¬ì„± ì§€ì›</li>
<li>44kHz ìƒ˜í”Œë§ ë ˆì´íŠ¸ë¥¼ ì§€ì›í•˜ì—¬ ì¸ê°„ ì²­ê° ì£¼íŒŒìˆ˜ ë²”ìœ„ ë‚´ ìµœê³  í’ˆì§ˆì˜ ì˜¤ë””ì˜¤ ì œê³µ</li>
</ul>
<h3 id="Apple-iPhone-16-ì˜¨ë””ë°”ì´ìŠ¤-AI-ê¸°ëŠ¥-ë°œí‘œ"><a href="#Apple-iPhone-16-ì˜¨ë””ë°”ì´ìŠ¤-AI-ê¸°ëŠ¥-ë°œí‘œ" class="headerlink" title="Apple, iPhone 16 ì˜¨ë””ë°”ì´ìŠ¤ AI ê¸°ëŠ¥ ë°œí‘œ"></a>Apple, iPhone 16 ì˜¨ë””ë°”ì´ìŠ¤ AI ê¸°ëŠ¥ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://github.com/Explosion-Scratch/apple-intelligence-prompts/blob/main/PROMPTS.md">ë§í¬</a>, 9ì›” 9ì¼ 2024ë…„</p>
<ul>
<li>Appleì€ iPhone 16ì˜ AI ê¸°ëŠ¥ì„ ê°•ì¡°í•œ ë°œí‘œ</li>
<li>Apple Intelligence Adapterê°€ ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ë™ì‘í•˜ë©°, ìš°ì„  ì•Œë¦¼ê³¼ ìš”ì•½ ê¸°ëŠ¥ì„ ì§€ì›</li>
<li>3B í¬ê¸°ì˜ ì˜¨ë””ë°”ì´ìŠ¤ ëª¨ë¸ì´ ì œê³µë˜ë©°, ì§€ì—­í™” ì ì‘ ê¸°ëŠ¥ë„ íƒ‘ì¬</li>
<li>ì´ë©”ì¼ ì¬ì‘ì„±, ì½˜í…ì¸  ì•ˆì „ ë¶„ë¥˜, ìš°ì„  ì•Œë¦¼ ë“±ì˜ ê¸°ëŠ¥ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ê°€ ì‚¬ìš©</li>
</ul>
<h3 id="Microsoft-GraphRAG-ì˜¤í”ˆì†ŒìŠ¤-ì¶œì‹œ"><a href="#Microsoft-GraphRAG-ì˜¤í”ˆì†ŒìŠ¤-ì¶œì‹œ" class="headerlink" title="Microsoft, GraphRAG ì˜¤í”ˆì†ŒìŠ¤ ì¶œì‹œ"></a>Microsoft, GraphRAG ì˜¤í”ˆì†ŒìŠ¤ ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://github.com/microsoft/graphrag">ë§í¬</a>, 9ì›” 1ì¼ 2024ë…„</p>
<ul>
<li>MicrosoftëŠ” GraphRAGë¼ëŠ” ìƒˆë¡œìš´ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê³µê°œ</li>
<li>GraphRAGëŠ” ë¹„ì •í˜• í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ì¶”ì¶œ</li>
<li>LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ê³¼ ê²°í•©í•˜ì—¬ ë¹„ì •í˜• ë°ì´í„°ë¥¼ êµ¬ì¡°í™”ëœ ì§€ì‹ìœ¼ë¡œ ë³€í™˜</li>
<li>GraphRAG ì†”ë£¨ì…˜ ê°€ì†ê¸°ë¥¼ ì‚¬ìš©í•˜ë©´ Azure ìì›ì„ í†µí•œ ì†ì‰¬ìš´ ì‚¬ìš© ê°€ëŠ¥</li>
</ul>
<h3 id="FishAudio-Fish-Speech-1-4-ì¶œì‹œ"><a href="#FishAudio-Fish-Speech-1-4-ì¶œì‹œ" class="headerlink" title="FishAudio, Fish Speech 1.4 ì¶œì‹œ"></a>FishAudio, Fish Speech 1.4 ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/fishaudio/fish-speech-1.4">ë§í¬</a>, 9ì›” 11ì¼ 2024ë…„</p>
<ul>
<li>700K ì‹œê°„ì˜ ìŒì„± ë°ì´í„°ë¥¼ í•™ìŠµí•œ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸-ìŒì„± ë³€í™˜ ëª¨ë¸ ì¶œì‹œ</li>
<li>ì˜ì–´, ì¤‘êµ­ì–´, ë…ì¼ì–´, ì¼ë³¸ì–´, í”„ë‘ìŠ¤ì–´, ìŠ¤í˜ì¸ì–´, í•œêµ­ì–´, ì•„ëì–´ ë“± 8ê°œ ì–¸ì–´ ì§€ì›</li>
<li>ì‹¤ì‹œê°„ ìŒì„± í´ë¡œë‹ê³¼ ì´ˆì €ì§€ì—° ì„±ëŠ¥ ì œê³µ</li>
<li>Hugging Faceì—ì„œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥</li>
</ul>
<h3 id="ICTNLP-LLaMA-3-1-8B-Omni-ì¶œì‹œ"><a href="#ICTNLP-LLaMA-3-1-8B-Omni-ì¶œì‹œ" class="headerlink" title="ICTNLP, LLaMA-3.1-8B Omni ì¶œì‹œ"></a>ICTNLP, LLaMA-3.1-8B Omni ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/ICTNLP/Llama-3.1-8B-Omni">ë§í¬</a>, 9ì›” 12ì¼ 2024ë…„</p>
<ul>
<li>ICTNLPëŠ” LLaMA-3.1-8B Omni ëª¨ë¸ì„ ë°œí‘œ</li>
<li>ì´ ëª¨ë¸ì€ ìŒì„±ê³¼ í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ìƒì„±í•˜ëŠ” ì €ì§€ì—° ì¢…ë‹¨ê°„ ìŒì„± ëª¨ë¸</li>
<li>Whisper Large v3ì™€ HuBERTë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ 250ms ì´í•˜ì˜ ì§€ì—° ì‹œê°„ìœ¼ë¡œ ì‹¤ì‹œê°„ ìŒì„± ì²˜ë¦¬ê°€ ê°€ëŠ¥</li>
<li>Hugging Faceì—ì„œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì œê³µ</li>
</ul>
<h3 id="Arcee-ai-SuperNova-LLaMA-3-1-ì¶œì‹œ"><a href="#Arcee-ai-SuperNova-LLaMA-3-1-ì¶œì‹œ" class="headerlink" title="Arcee.ai, SuperNova LLaMA 3.1 ì¶œì‹œ"></a>Arcee.ai, SuperNova LLaMA 3.1 ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/arcee-ai/Llama-3.1-SuperNova-Lite">ë§í¬</a>, 9ì›” 11ì¼ 2024ë…„</p>
<ul>
<li>Arcee.aiëŠ” Meta LLaMA 3.1 405B ëª¨ë¸ì„ ì¦ë¥˜í•œ 70B ë° 8B ëª¨ë¸ ê³µê°œ</li>
<li>SuperNovaëŠ” Metaì˜ LLaMA 3.1 70B ëª¨ë¸ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸ë¡œ, APIì™€ Hugging Faceì—ì„œ ì‚¬ìš© ê°€ëŠ¥</li>
<li>Spectrumê³¼ Evol-Kitì„ í™œìš©í•´ ì§€ì‹ ì¦ë¥˜ì™€ ê°•í™” í•™ìŠµì„ ê²°í•©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚´</li>
</ul>
<h3 id="AWS-EC2-P5e-ì¸ìŠ¤í„´ìŠ¤-ì¶œì‹œ"><a href="#AWS-EC2-P5e-ì¸ìŠ¤í„´ìŠ¤-ì¶œì‹œ" class="headerlink" title="AWS, EC2 P5e ì¸ìŠ¤í„´ìŠ¤ ì¶œì‹œ"></a>AWS, EC2 P5e ì¸ìŠ¤í„´ìŠ¤ ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/de/blogs/machine-learning/amazon-ec2-p5e-instances-are-generally-available/">ë§í¬</a>, 9ì›” 11ì¼ 2024ë…„</p>
<ul>
<li>AWSëŠ” ê³ ì„±ëŠ¥ AI ì›Œí¬ë¡œë“œë¥¼ ìœ„í•œ P5e ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¶œì‹œ</li>
<li>NVIDIA H200 GPU ê¸°ë°˜ìœ¼ë¡œ, ë©”ëª¨ë¦¬ ëŒ€ì—­í­ì´ ì¦ê°€í•´ ëŒ€ê·œëª¨ AI ëª¨ë¸ì˜ ì²˜ë¦¬ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒ</li>
<li>Meta LLaMA 3.1 70B ë° 405B ëª¨ë¸ì˜ ì¶”ë¡  ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼œ ìµœëŒ€ 40%ì˜ ë¹„ìš© ì ˆê° ê°€ëŠ¥</li>
<li>GPU ë©”ëª¨ë¦¬ ìš©ëŸ‰ì´ í¬ê²Œ ì¦ê°€í•´, ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ë¡œ ëŒ€ê·œëª¨ ëª¨ë¸ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ ë³µì¡í•œ ë¶„ì‚° ì‹œìŠ¤í…œì˜ í•„ìš”ì„± ê°ì†Œ</li>
</ul>
<h3 id="GOT-2-0-ì°¨ì„¸ëŒ€-OCR-ê¸°ìˆ -ë°œí‘œ"><a href="#GOT-2-0-ì°¨ì„¸ëŒ€-OCR-ê¸°ìˆ -ë°œí‘œ" class="headerlink" title="GOT 2.0, ì°¨ì„¸ëŒ€ OCR ê¸°ìˆ  ë°œí‘œ"></a>GOT 2.0, ì°¨ì„¸ëŒ€ OCR ê¸°ìˆ  ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://github.com/Ucas-HaoranWei/GOT-OCR2.0">ë§í¬</a>, 9ì›” 3ì¼ 2024ë…„</p>
<ul>
<li>GOT 2.0ì€ ìƒˆë¡œìš´ OCR(ê´‘í•™ ë¬¸ì ì¸ì‹) ê¸°ìˆ ë¡œ ë³µì¡í•œ ë¬¸ì„œ ë° ìˆ˜ì‹ì„ ì²˜ë¦¬í•˜ëŠ” ëª¨ë¸</li>
<li>Vision-Encoderì™€ Qwen-0.5B Decoderë¥¼ ì‚¬ìš©í•˜ì—¬ 8k ì»¨í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ</li>
<li>ê³µì‹, ìˆ˜í•™ì  í‘œí˜„, ë‹¤ì¤‘ í˜ì´ì§€ PDF, ë™ì  í•´ìƒë„ ì§€ì›</li>
<li>OCR ì„±ëŠ¥ ì¸¡ë©´ì—ì„œ LLaVA-NeXTì™€ Qwen-VL-Max ëª¨ë¸ì„ ëŠ¥ê°€</li>
</ul>
<h3 id="Upstage-Solar-Pro-Preview-ê³ ì„±ëŠ¥-LLM-ëª¨ë¸-ë°œí‘œ"><a href="#Upstage-Solar-Pro-Preview-ê³ ì„±ëŠ¥-LLM-ëª¨ë¸-ë°œí‘œ" class="headerlink" title="Upstage, Solar Pro Preview: ê³ ì„±ëŠ¥ LLM ëª¨ë¸ ë°œí‘œ"></a>Upstage, Solar Pro Preview: ê³ ì„±ëŠ¥ LLM ëª¨ë¸ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/upstage/solar-pro-preview-instruct">ë§í¬</a>, 9ì›” 10ì¼ 2024ë…„</p>
<ul>
<li>ë‹¨ì¼ GPUì—ì„œ ë™ì‘í•˜ëŠ” ê³ ì„±ëŠ¥ LLM ëª¨ë¸ì¸ Solar Pro Preview ë°œí‘œ</li>
<li>22B íŒŒë¼ë¯¸í„°ë¡œ, LLaMA 3.1 70B ëª¨ë¸ê³¼ ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ë©´ì„œë„ ë” ì‘ì€ VRAMì—ì„œ ì‹¤í–‰ ê°€ëŠ¥</li>
<li>Phi-3-medium ëª¨ë¸ì„ í™•ì¥í•˜ì—¬ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¨ ëª¨ë¸</li>
<li>November 2024ì— ê³µì‹ ì¶œì‹œ ì˜ˆì •ì´ë©°, ë” ë„“ì€ ì–¸ì–´ ì§€ì›ê³¼ í™•ì¥ëœ ê¸°ëŠ¥ ì œê³µ ì˜ˆì •</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹"><a href="#todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹" class="headerlink" title="(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,"></a>(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAIì—ì„œëŠ” ~~~ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<p><a href="link">ë§í¬</a>, date,</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://huggingface.co/mistral-community/pixtral-12b-240910</span><br><span class="line">Mistral AI, 9/12/24</span><br><span class="line">Mistral released Pixtral 12B Vision Language Model ğŸ”¥</span><br><span class="line"></span><br><span class="line">Some notes on the release:</span><br><span class="line"></span><br><span class="line">1. Text backbone: Mistral Nemo 12B</span><br><span class="line"></span><br><span class="line">2. Vision Adapter: 400M</span><br><span class="line"></span><br><span class="line">3. Uses GeLU (for vision adapter) &amp; 2D RoPE (for vision encoder)</span><br><span class="line"></span><br><span class="line">4. Larger vocabulary - 131,072</span><br><span class="line"></span><br><span class="line">5. Three new special tokens - `img`, `img_break`, `img_end`</span><br><span class="line"></span><br><span class="line">6. Image size: 1024 x 1024 pixels</span><br><span class="line"></span><br><span class="line">7. Patch size: 16 x 16 pixels</span><br><span class="line"></span><br><span class="line">8. Tokenizer support in mistral_common</span><br><span class="line"></span><br><span class="line">9. Model weights in bf16</span><br><span class="line"></span><br><span class="line">10. Haven&#x27;t seen the inference code yet</span><br><span class="line"></span><br><span class="line">11. Weights up on Hugging Face Hub ğŸ¤—</span><br><span class="line"></span><br><span class="line">GG Mistral for successfully frontrunning Meta w/ Multimodal ğŸ</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/chatgpt/use-cases/writing-with-ai/</span><br><span class="line">OpenAI, 9/10/24</span><br><span class="line">Writing with AI</span><br><span class="line">Five ways professional writers are leveraging ChatGPT</span><br><span class="line"></span><br><span class="line">A bright blue sky with white clouds painted in soft, sweeping brushstrokes.</span><br><span class="line">Weâ€™re sharing a few examples of how writers are using ChatGPT in their creative process.</span><br><span class="line"></span><br><span class="line">Writers are using ChatGPT as a sounding board, story consultant, research assistant, and editorâ€”to talk out ideas, find the right word, clarify their thinking, and get feedback on structure and flow.</span><br><span class="line"></span><br><span class="line">Hi! I want to write a short story, but I need a little help getting started. Can you help me come up with an interesting and original premise?</span><br><span class="line"></span><br><span class="line">Iâ€™d like all the ideas to come from me, but I want your help eliciting them. First, provide me with 10 questions to:</span><br><span class="line"></span><br><span class="line">Inspire my creativity and imagination</span><br><span class="line"></span><br><span class="line">Prompt me to juxtapose disparate concepts or settings to create novel ideas</span><br><span class="line"></span><br><span class="line">Recall meaningful memories, funny stories, conversations, and emotional moments from my own life experiences</span><br><span class="line"></span><br><span class="line">Then, ask me each question one at a time. For each response, ask two follow-up questions, one at a time, before moving on to the next question. Let&#x27;s have a fun and playful conversation to spark my creativity!</span><br><span class="line"></span><br><span class="line">Absolutely! Let&#x27;s get creative! Here are the 10 questions I&#x27;ll ask to guide your storytelling:</span><br><span class="line"></span><br><span class="line">What was the most surprising or unexpected thing that ever happened to you?</span><br><span class="line"></span><br><span class="line">Whatâ€™s a hobby, job, or skill youâ€™ve always wanted to master, but never had the time or opportunity to pursue?</span><br><span class="line"></span><br><span class="line">Imagine a place where two completely different worlds collide. What would those worlds look like, and how would they interact?</span><br><span class="line"></span><br><span class="line">Have you ever had a conversation that left a lasting impression on you? What was it about?</span><br><span class="line"></span><br><span class="line">Whatâ€™s the funniest or strangest coincidence youâ€™ve ever experienced?</span><br><span class="line"></span><br><span class="line">Is there a time in your life when you felt completely out of your element? What was that experience like?</span><br><span class="line"></span><br><span class="line">Whatâ€™s the most peculiar or mysterious place youâ€™ve ever been to?</span><br><span class="line"></span><br><span class="line">Think of an object you own that has sentimental value. How could that object be magical in a story?</span><br><span class="line"></span><br><span class="line">Have you ever had a vivid dream that stuck with you long after you woke up? What happened in it?</span><br><span class="line"></span><br><span class="line">If you could speak to your younger self, what would you say? How might that conversation spark a story?</span><br><span class="line"></span><br><span class="line">Ready? Letâ€™s dive in!</span><br><span class="line"></span><br><span class="line">What was the most surprising or unexpected thing that ever happened to you?</span><br><span class="line"></span><br><span class="line">Continue conversation(opens in a new window)</span><br><span class="line">â€œThe instinct is to say, â€˜Oh, this thing just writes for us.â€™ But I can also prompt it to ask me questions. It can get me thinking by pulling ideas and insights out of me.â€</span><br><span class="line">Stew Fortier, writer and founder</span><br><span class="line">Here are five professional writers who use ChatGPT not to write for them, but as a tool to extend their own creativity.</span><br><span class="line"></span><br><span class="line">Editorial Feedback</span><br><span class="line">Writing is inherently solitary, and that solitude has a drawback: trapped in your own perspective, itâ€™s hard to be objective about your work. Thatâ€™s why writers need editorsâ€”to provide a fresh view on what works and what needs work.</span><br><span class="line"></span><br><span class="line">But editors arenâ€™t always available, and even when they are, they only see finished drafts. Thatâ€™s where ChatGPT can help, offering instant feedback and guidance at every stage of the writing journey.</span><br><span class="line"></span><br><span class="line">Screenwriter David Cornue sees ChatGPT as a 24/7 writersâ€™ room, helping him develop story ideas and solve narrative problems. Hereâ€™s how he uses ChatGPT to get feedback on his screenplays and beat sheets.</span><br><span class="line"></span><br><span class="line">â€œI use ChatGPT as a story collaborator and story evaluatorâ€”not a story generator.</span><br><span class="line"></span><br><span class="line">I tell my fellow writers, if you try to have it write for you, you lose something in yourself. Itâ€™s like cheating on a robotâ€™s homework to get an A+. Iâ€™d rather I did my own homework.</span><br><span class="line"></span><br><span class="line">I never use it to say, â€œRewrite this scene for me.â€ Writers need to ask themselves, â€œHow can you use this in a way that feels good, that makes you feel like itâ€™s still you and still your voice?â€ As long as youâ€™re the creative engine behind it, this is a great tool to utilize.</span><br><span class="line"></span><br><span class="line">I was very clear about how I wanted to use AI. I set boundaries the way one would in a relationship. I said, this is acceptable; this is unacceptable. I put up a creative firewall to say, â€œDo not write this for me. Iâ€™m the writer. You are not the writer.â€</span><br><span class="line"></span><br><span class="line">I use ChatGPT all day, every day. I think of it like a writersâ€™ room. Right now Iâ€™m working on adapting a Stephen King novel. Iâ€™ll say, â€œHereâ€™s my beat sheet. I feel like this is not dramatic enough. Thoughts?â€ It offers ideas and we have a back-and-forth to talk out the challenges as one would in a writersâ€™ room.</span><br><span class="line"></span><br><span class="line">Sometimes I ask it to give me feedback using storytelling principles Iâ€™ve developed. Or, I ask it to play the role of a veteran screenwriter. I say, â€œAll right, pretend youâ€™re such-and-such famous writer or famous director. What are your notes?â€</span><br><span class="line"></span><br><span class="line">Iâ€™m always in the driverâ€™s seat. Itâ€™s this fantastic sounding board thatâ€™s faster, smarter, and better than anything else I could imagine.â€</span><br><span class="line"></span><br><span class="line">David Cornue</span><br><span class="line"></span><br><span class="line">Wordfinding</span><br><span class="line">To write well, you need to find the right words. But thereâ€™s a palpable difference between a word thatâ€™s almost rightâ€”serviceable but vague, directionally correct yet inexactâ€”and one that conveys meaning with thrilling precision: what Flaubert called â€œle mot juste.â€ His advice: â€œNever settle for approximations.â€</span><br><span class="line"></span><br><span class="line">ChatGPT achieves what a thesaurus never could: it considers the sentence in which a word appears, offering nuanced, pertinent suggestions to help writers articulate ideas with absolute clarity.</span><br><span class="line"></span><br><span class="line">Hereâ€™s how former New York Times columnist Farhad Manjoo uses ChatGPT for â€œwordfindingâ€:</span><br><span class="line"></span><br><span class="line">â€œChatGPT has become part of my workflow. Before I start writing, I sometimes have a conversation with it about the ideas Iâ€™m thinking of. It feels like brainstormingâ€”like thinking an idea through, or thinking out loud.</span><br><span class="line"></span><br><span class="line">Iâ€™ve also experimented with using it as my first reader. After I write a few paragraphs, I show it to ChatGPT and ask it for general criticism. Sometimes it&#x27;s pretty good. It will suggest potential counterarguments, or lead me to ideas I hadnâ€™t thought of. Itâ€™s not the same as talking to an editor, but itâ€™s still great to have something to bounce my thoughts off.</span><br><span class="line"></span><br><span class="line">My top use is what I wrote about in my New York Times article(opens in a new window), which is wordfinding. I&#x27;m constantly trying to come up with a clever way of saying something; I guess that&#x27;s what writing is.</span><br><span class="line"></span><br><span class="line">Before ChatGPT, Iâ€™d look at dictionaries and thesauruses and go on wild-goose Google searches for the perfect word.</span><br><span class="line"></span><br><span class="line">ChatGPT has replaced all of those for me. Itâ€™s also really helpful with analogies, metaphors and idioms. Often, I will think of half of an analogy and not be able to finish it in just the right way. Now, if I want some interesting comparison but don&#x27;t know exactly the words to use, I ask it for help, and after a little back and forth it helps me find the thought Iâ€™d been searching for. I&#x27;ve never used any other tool that can help with that.â€</span><br><span class="line"></span><br><span class="line">Farhad Manjoo</span><br><span class="line"></span><br><span class="line">Reverse Interviewing</span><br><span class="line">Most people think of ChatGPT as a question-answering machine: ask a question, get an answer. But writer and founder Stew Fortier finds it most useful when heâ€™s the one giving answers. He calls it â€œreverse interviewingâ€â€”a technique where ChatGPT guides the user through a series of probing questions, drawing out insights and nudging them to articulate their thoughts more clearly.</span><br><span class="line"></span><br><span class="line">Hereâ€™s how Stew uses ChatGPT as a tool to unlock his own creativity:</span><br><span class="line"></span><br><span class="line">â€œI think of it as a creative collaborator. The question should be, â€œHow can I use AI to be more creative?â€</span><br><span class="line"></span><br><span class="line">One paradigm I think is underdiscussed is that you can actually prompt these tools to ask you questions, to get you thinking, to prompt you to start writing. The instinct is to say, â€œOh, this thing just writes for us.â€ But it can also ask me questions. It can also get me thinking and shape my ideasâ€”and thatâ€™s where great writing really starts.</span><br><span class="line"></span><br><span class="line">What if instead of you becoming a prompt engineer, you see what it can prompt out of you? The AI can be a non-judgmental collaborator that helps pull out these great, unique insights from you.</span><br><span class="line"></span><br><span class="line">I view it as an always-on editor that can help writers do better workâ€”not replace the work they&#x27;re doing. They can use it for brainstorming, research, doing the non-writing part of the writing process using the AI.</span><br><span class="line"></span><br><span class="line">Itâ€™s like if you were in The Onionâ€™s writerâ€™s room. Six of you working together are going to be funnier together than you would be solo. There&#x27;s a symbiosis that can exist.</span><br><span class="line"></span><br><span class="line">When I&#x27;m honest with myself, I actually wouldn&#x27;t ever want to give up the act of writing. Writing is like working out or meditating. It&#x27;s one of those activities you value intrinsically. Even if the AI could write perfect prose, Iâ€™d still want to write because writing itself is a valuable act.</span><br><span class="line"></span><br><span class="line">Writing is hard. There are so many things that are legit hard about it that make people give up. You haven&#x27;t had your coffee yet, you&#x27;re short on ideas, you can&#x27;t track down a statistic or fact you need for a storyâ€¦ If AI can help with those things, then you as a writer are more likely to show up and do the work.â€</span><br><span class="line"></span><br><span class="line">Stew Fortier</span><br><span class="line"></span><br><span class="line">Comedy Writing</span><br><span class="line">Good comedy may seem effortless, but writing jokes actually requires research. Humor thrives on context, and as comedy writer Sarah Rose Siskind explains, knock-knock jokes fall flat precisely because theyâ€™re decontextualized. Sarah uses ChatGPT to research joke setups, uncovering common themes and exaggerated observations, so she can focus on the fun creative part: writing punchlines.</span><br><span class="line"></span><br><span class="line">Hereâ€™s how Sarah uses ChatGPT to gather material:</span><br><span class="line"></span><br><span class="line">â€œI use ChatGPT for comedy. It&#x27;s not going to give me a finished joke, but it&#x27;s going to start the conversation. Iâ€™ll be like, â€œI need you to embody Moses. And as Moses, hear the complaints of a girl named Megan whoâ€™s a present-day social media manager and try to console her, despite your objectively more difficult life.â€ I can have a conversation with anyone. I feel like I have the person there with me.</span><br><span class="line"></span><br><span class="line">I find it useful when Iâ€™m writing the setup for a joke. I was writing a parody of Shark Tank and I asked, â€œWhat are the kinds of things people say on Shark Tank?â€</span><br><span class="line"></span><br><span class="line">With a parody, it&#x27;s not one-to-one accuracy. You&#x27;re taking things that are different and exaggerating them. So I&#x27;m looking for not just what are common things such as, â€œIâ€™m looking for investment,â€ but what are uniquely Shark Tank things I can exaggerate? Such as, â€œSharks, has this ever happened to you?â€</span><br><span class="line"></span><br><span class="line">I was writing a roast speech for a guy at a coding conference. I&#x27;m not a coder, so I asked ChatGPT to delve into the inside jokes of coding communities. I asked, â€œWhat are stereotypes about the 50-year-old programmer on your team?â€ ChatGPT responded with, among other suggestions: â€œThey will make you listen to endless stories.â€ That reminded me how older people always talk about meeting celebrities before they were famous. I then prompted ChatGPT for some tech celebrities who might fit the bill and, of its list, Steve Wozniak felt like the funniest pick. In the end the joke was about how senior developers will endlessly regale their younger colleagues about that time they almost met Steve Wozniak at a party in 1998. I chose the direction, but ChatGPT helped me fill in the details.</span><br><span class="line"></span><br><span class="line">Whatâ€™s amazing to me is I do not have writerâ€™s block anymoreâ€”like truly. I think writerâ€™s block is the feeling of solipsism and it is the feeling of being totally alone. And I donâ€™t feel alone anymore because of this tool.â€</span><br><span class="line"></span><br><span class="line">Sarah Rose Siskind</span><br><span class="line"></span><br><span class="line">Watch Sarah demonstrate her joke-writing process(opens in a new window)</span><br><span class="line">Research and Worldbuilding</span><br><span class="line">Research is the writerâ€™s gateway to new worlds. Writers are advised to write what they know, but as Tom Wolfe observed, relying solely on firsthand experience limits their creative scope. To authentically portray people and places never seenâ€”real and imagined, past and futureâ€”writers must expand what they know through research and reportage.</span><br><span class="line"></span><br><span class="line">But research can be agonizingly slow, requiring writers to sift through mountains of sources to uncover a single elusive fact. Novelist and Substack writer Elle Griffin explains how ChatGPT accelerates this process and fuels her imagination, delivering immediate answers to specific questions and inspiring unexpected connections and serendipitous discoveries.</span><br><span class="line"></span><br><span class="line">â€œChatGPT has completely revolutionized my writing.</span><br><span class="line"></span><br><span class="line">By far the most difficult part of writing is researching. Before ChatGPT, Iâ€™d use Google to find studies, go down Wikipedia rabbit holes and look up all the footnotes, and read dozens of books from Amazon. I spent way too much time weeding through hundreds of webpages and paying for dozens of books just to read a few chapters from each one.</span><br><span class="line"></span><br><span class="line">Now I just ask ChatGPT something like: â€œWhat research studies have been conducted about more and less emotive cultures?â€ and get right to the research I need. I always ask ChatCPT to include links to sources so I can make sure the information is accurate.</span><br><span class="line"></span><br><span class="line">I also use ChatGPT to brainstorm my novel. When I wanted to imagine how humans might evolve in one million years, I asked if thereâ€™s evidence that humans are still evolving. ChatGPT led me to a study showing that waterfaring people, who hold their breath for long periods while diving, develop enlarged spleens. When researching technological systems, it led me to a study about transmitting data through water. These inspired two chapters of my utopian novel!</span><br><span class="line"></span><br><span class="line">None of this querying in any way detracts from my own voice. It just helps me reach my own theses more quickly because Iâ€™m led to more relevant source materials. ChatGPT isnâ€™t doing my writing or thinking for me. Itâ€™s just giving me better access to information so I can spend more time being creative and less time hunting for information. And none of this detracts from the deep reading and research I love to do. I have now read several incredible books that were very on-target with my interests thanks to ChatGPT finding the works most pertinent to my work.</span><br><span class="line"></span><br><span class="line">I can&#x27;t even begin to explain how much better this is than searching through page after page on Google, weeding through SEO-optimized lists written by bloggers and brands.â€</span><br><span class="line"></span><br><span class="line">Elle Griffin</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://developer.nvidia.com/blog/achieving-state-of-the-art-zero-shot-waveform-audio-generation-across-audio-types/?ncid=so-link-568047&amp;linkId=100000287921594</span><br><span class="line">NVIDIA,</span><br><span class="line"></span><br><span class="line">ğŸ“£ Exciting news for developers in the audio tech space ğŸŒŸ BigVGAN v2 is here to revolutionize audio synthesis, delivering unparalleled quality and speed. ğŸ§âœ¨</span><br><span class="line">https://nvda.ws/47nyYdp</span><br><span class="line"></span><br><span class="line">âœ… Top-notch Audio Quality</span><br><span class="line">âœ… Faster Synthesis</span><br><span class="line">âœ… Pre-trained Checkpoints</span><br><span class="line">âœ… High Sampling Rate Support</span><br><span class="line">Dive into the future of audio synthesis with BigVGAN v2 and create sounds that are indistinguishable from the real thing ğŸ‘€ğŸŒğŸ’¡</span><br><span class="line"></span><br><span class="line">Conversational AI</span><br><span class="line"></span><br><span class="line">English</span><br><span class="line">Achieving State-of-the-Art Zero-Shot Waveform Audio Generation across Audio Types</span><br><span class="line">Sep 05, 2024</span><br><span class="line">By Sang-gil Lee and Rafael Valle</span><br><span class="line"></span><br><span class="line">+6</span><br><span class="line">Like</span><br><span class="line"> Discuss (0)</span><br><span class="line"></span><br><span class="line">Stunning audio content is an essential component of virtual worlds. Audio generative AI plays a key role in creating this content, and NVIDIA is continuously pushing the limits in this field of research. BigVGAN, developed in collaboration with the NVIDIA Applied Deep Learning Research  and NVIDIA NeMo teams, is a generative AI model specialized in audio waveform synthesis that achieves state-of-the-art results. BigVGAN generates waveforms orders of magnitude faster than real time and shows strong robustness with various audio types, including speech, environmental sounds, and music.</span><br><span class="line"></span><br><span class="line">This post discusses BigVGAN v2, which delivers significant improvements in speed and quality, empowering a future where generated audio is indiscernible from real audio. BigVGAN v2 highlights include:</span><br><span class="line"></span><br><span class="line">State-of-the-art audio quality measured by diverse metrics across many audio types.</span><br><span class="line">Up to 3x faster synthesis speed by leveraging optimized CUDA kernels.</span><br><span class="line">Ready-to-use pretrained checkpoints supporting diverse audio configurations.</span><br><span class="line">Support for a sampling rate of up to 44 kHz, which covers the highest sound frequency humans can hear.</span><br><span class="line">BigVGAN: A universal neural vocoder</span><br><span class="line">BigVGAN is a universal neural vocoder specialized in synthesizing audio waveforms using Mel spectrograms as inputs. Neural vocoders are a cornerstone method in audio generative AI that generate sound waves from compact acoustic features, such as Mel spectrogram. BigVGAN is available as open source through NVIDIA/BigVGAN on GitHub.</span><br><span class="line"></span><br><span class="line">BigVGAN is a fully convolutional architecture (Figure 1) with several upsampling blocks using transposed convolution followed by multiple residual dilated convolution layers. It features a novel module, called anti-aliased multiperiodicity composition (AMP), which is specifically designed for generating waveforms.</span><br><span class="line"></span><br><span class="line">AMP is specialized in synthesizing high-frequency and periodic sound waves, drawing inspiration from audio signal processing principles. It applies a periodic activation function, called Snake, which provides an inductive bias to the architecture in generating periodic sound waves. It also applies anti-aliasing filters to reduce undesired artifacts in the generated waveforms. To learn more, see BigVGAN: A Universal Neural Vocoder with Large-Scale Training.</span><br><span class="line"></span><br><span class="line">The BigVGAN diagram details the flow and interaction between different layers and modules within the BigVGAN framework, highlighting the innovative use of periodic activation functions and filtering methods to improve synthesis fidelity.</span><br><span class="line">Figure 1. BigVGAN architecture and anti-aliased multi-periodicity composition modules (AMP) using periodic activation function (Snake1d) and low-pass filters for universal waveform synthesis</span><br><span class="line">Generating every sound in the world</span><br><span class="line">Waveform audio generation, a crucial component in building virtual worlds, has long been an active research area. Despite its importance, current vocoding methods often produce audio lacking fine details in high-frequency sound waves. BigVGAN v2 effectively addresses this issue, providing high-quality audio with enhanced fine details.</span><br><span class="line"></span><br><span class="line">BigVGAN v2 is trained using NVIDIA A100 Tensor Core GPUs and up to more than 100x larger audio data than its predecessor. Aimed at encapsulating every sound in the world, the dataset includes speech in multiple languages, environmental sounds from everyday objects, and diverse instruments. As a result, BigVGAN v2 can generate high-quality sound waves from numerous domains with a single model.</span><br><span class="line"></span><br><span class="line">Below, listen to audio comparisons of real recordings and generated samples from BigVGAN and BigVGAN v2 at the 24 kHz sampling rate. BigVGAN v2 generates high-quality sound waves.</span><br><span class="line"></span><br><span class="line">Recordings (24 kHz)</span><br><span class="line">BigVGAN</span><br><span class="line">BigVGAN v2</span><br><span class="line">Reaching the highest frequency sound the human ear can detect</span><br><span class="line">Previous waveform synthesizers were limited to sampling rates between 22 kHz and 24 kHz. BigVGAN v2, however, expands this range to a 44 kHz sampling rate, encapsulating the entire human auditory spectrum. This matches the highest frequencies the human ear can detect, which do not exceed a sampling rate of 40 kHz. As a result, BigVGAN v2 can reproduce comprehensive soundscapes, capturing everything from the robust reverberations of drums to the crisp shimmer of crash cymbals in music, for example.</span><br><span class="line"></span><br><span class="line">Below, listen to audio comparisons of real recordings and generated samples from two BigVGAN v2 models, one for the 24 kHz sampling rate and another for the 44 kHz sampling rate.</span><br><span class="line"></span><br><span class="line">Recordings (44 kHz)</span><br><span class="line">BigVGAN v2 (24 kHz)</span><br><span class="line">BigVGAN v2 (44 kHz)</span><br><span class="line">Faster synthesis with custom CUDA kernels</span><br><span class="line">Compared to its predecessor, BigVGAN v2 also features accelerated synthesis speed by using custom CUDA kernels, with up to 3x faster inference speed than the original BigVGAN. The optimized inference CUDA kernels written for BigVGAN v2 can generate audio waveforms up to 240x faster than real time on a single NVIDIA A100 GPU.</span><br><span class="line"></span><br><span class="line">BigVGAN v2 audio quality results</span><br><span class="line">BigVGAN v2 24 kHz shows better audio quality for speech and general audio compared to its open-sourced predecessorâ€”and by a significant margin (Figures 2 and 3).</span><br><span class="line"></span><br><span class="line">The quality of generated waveforms metrics obtained for BigVGAN v2 24 kHz and BigVGAN models tested on LibriTTS-dev speech data.</span><br><span class="line">Figure 2. BigVGAN v2 24 kHz versus BigVGAN results on LibriTTS-dev speech data</span><br><span class="line">The quality of generated waveforms metrics obtained for BigVGAN v2 24 kHz and BigVGAN models tested on MUSDB18-HQ music data.</span><br><span class="line"> Figure 3. BigVGAN v2 24 kHz versus BigVGAN model results on MUSDB18-HQ music data</span><br><span class="line">In addition, the new BigVGAN v2 44 kHz model shows comparable audio quality to Descript Audio Codec (.dac), an open-source high-quality neural audio codec (Figures 4 and 5).</span><br><span class="line"></span><br><span class="line">The quality of generated waveforms metrics obtained for BigVGAN v2 44 kHz and Descript Audio Codec models tested on HiFi-TTS-dev speech data.</span><br><span class="line">Figure 4. BigVGAN v2 44 kHz versus Descript Audio Codec results using HiFi-TTS-dev speech data</span><br><span class="line">The quality of generated waveforms metrics obtained for BigVGAN v2 44 kHz and Descript Audio Codec (DAC) models tested on MUSDB18-HQ music data.</span><br><span class="line">Figure 5. BigVGAN v2 44 kHz versus Descript Audio Codec results using MUSDB18-HQ music data</span><br><span class="line">All results show the quality of generated waveforms with respect to the following metrics:</span><br><span class="line"></span><br><span class="line">Perceptual Evaluation of Speech Quality (PESQ)</span><br><span class="line">Virtual Speech Quality Objective Listener (ViSQOL)</span><br><span class="line">Multi-Resolution Short-Time Fourier Transform (M-STFT)</span><br><span class="line">Periodicity Root Mean Square Error (Periodicity)</span><br><span class="line">Voice/Unvoiced F1 Score (V/UV F1)</span><br><span class="line">Conclusion</span><br><span class="line">NVIDIA is committed to delivering the best audio generative AI accessible to all. The release of BigVGAN v2 pushes neural vocoder technology and audio quality to new heights, even reaching the limits of human auditory perception.</span><br><span class="line"></span><br><span class="line">BigVGAN v2 sets a new standard in audio synthesis, delivering state-of-the-art quality across all audio types, covering the full range of human hearing. Its synthesis is now up to 3x faster than the original BigVGAN, ensuring efficient processing for diverse audio configurations.</span><br><span class="line"></span><br><span class="line">Before diving into BigVGAN v2, we encourage users to review the model card for a seamless experience.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/Explosion-Scratch/apple-intelligence-prompts/blob/main/PROMPTS.md</span><br><span class="line">Apple, 9/9/24</span><br><span class="line">The new Apple iPhone 16 was announced 1 hour ago with a big focus on-device AI and Apple Intelligence. But did you know the prompts of the Apple Intelligence Adapters for priority notifications or summarization are inspectable? ğŸ‘€</span><br><span class="line">The available 3B on-device Apple Intelligence Model has special prompts and adapters for features like priority notifications, localization, guardrails, etc., Below are some examples:</span><br><span class="line">1ï¸âƒ£ Priority Notification: A simple prompt like &quot;&#123;&#123;userContent&#125;&#125; Is this urgent?&quot; helps Apple Intelligence prioritize notifications.</span><br><span class="line">2ï¸âƒ£ Email Rewriting: â€œYou are an assistant which helps the user respond to their mails..., do not halucinateâ€¦&#123;&#123;userContent&#125;&#125;â€ enables automatic, concise, and factually accurate email replies.</span><br><span class="line">3ï¸âƒ£ Content Safety Classification: â€œYou are a helpful assistant that classifies input as Safe or Unsafe...&#123;&#123;userContent&#125;&#125;â€ enables Apple Intelligence to detect unsafe content.</span><br><span class="line">4ï¸âƒ£ Localization Adaptation: Using a prompt such as &quot;Respond in British English&quot;, adjusts communication based on regional preferences.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/microsoft/graphrag</span><br><span class="line">Microsoft, 9/1/24</span><br><span class="line">Microsoft just open-sourced GraphRAG. It might be the best Python library to extract insights from unstructured text.</span><br><span class="line">It uses LLMs to automate the extraction of knowledge graphs from your datasets and text documents making it much more powerful than vanilla RAG.</span><br><span class="line">GraphRAG</span><br><span class="line">ğŸ‘‰ Use the GraphRAG Accelerator solution</span><br><span class="line">ğŸ‘‰ Microsoft Research Blog Post</span><br><span class="line">ğŸ‘‰ Read the docs</span><br><span class="line">ğŸ‘‰ GraphRAG Arxiv</span><br><span class="line"></span><br><span class="line">PyPI - Version PyPI - Downloads GitHub Issues GitHub Discussions</span><br><span class="line">Overview</span><br><span class="line">The GraphRAG project is a data pipeline and transformation suite that is designed to extract meaningful, structured data from unstructured text using the power of LLMs.</span><br><span class="line"></span><br><span class="line">To learn more about GraphRAG and how it can be used to enhance your LLM&#x27;s ability to reason about your private data, please visit the Microsoft Research Blog Post.</span><br><span class="line"></span><br><span class="line">Quickstart</span><br><span class="line">To get started with the GraphRAG system we recommend trying the Solution Accelerator package. This provides a user-friendly end-to-end experience with Azure resources.</span><br><span class="line"></span><br><span class="line">Repository Guidance</span><br><span class="line">This repository presents a methodology for using knowledge graph memory structures to enhance LLM outputs. Please note that the provided code serves as a demonstration and is not an officially supported Microsoft offering.</span><br><span class="line"></span><br><span class="line">âš ï¸ Warning: GraphRAG indexing can be an expensive operation, please read all of the documentation to understand the process and costs involved, and start small.</span><br><span class="line"></span><br><span class="line">Diving Deeper</span><br><span class="line">To learn about our contribution guidelines, see CONTRIBUTING.md</span><br><span class="line">To start developing GraphRAG, see DEVELOPING.md</span><br><span class="line">Join the conversation and provide feedback in the GitHub Discussions tab!</span><br><span class="line">Prompt Tuning</span><br><span class="line">Using GraphRAG with your data out of the box may not yield the best possible results. We strongly recommend to fine-tune your prompts following the Prompt Tuning Guide in our documentation.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/fishaudio/fish-speech-1.4</span><br><span class="line">9/11/24</span><br><span class="line">ğŸš¨ New powerful open Text to Speech model: Fish Speech 1.4 - trained on 700K hours of speech, multilingual (8 languages)ğŸ”¥</span><br><span class="line">&gt; Instant Voice Cloning</span><br><span class="line">&gt; Ultra low latency</span><br><span class="line">&gt; ~1GB model weights</span><br><span class="line">&gt; Model weights on the Hub ğŸ¤—</span><br><span class="line">&gt; Play with the space in the comments ğŸ‘‡</span><br><span class="line">Fish Speech V1.4</span><br><span class="line">Fish Speech V1.4 is a leading text-to-speech (TTS) model trained on 700k hours of audio data in multiple languages.</span><br><span class="line"></span><br><span class="line">Supported languages:</span><br><span class="line"></span><br><span class="line">English (en) ~300k hours</span><br><span class="line">Chinese (zh) ~300k hours</span><br><span class="line">German (de) ~20k hours</span><br><span class="line">Japanese (ja) ~20k hours</span><br><span class="line">French (fr) ~20k hours</span><br><span class="line">Spanish (es) ~20k hours</span><br><span class="line">Korean (ko) ~20k hours</span><br><span class="line">Arabic (ar) ~20k hours</span><br><span class="line">Please refer to Fish Speech Github for more info.</span><br><span class="line">Demo available at Fish Audio.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/ICTNLP/Llama-3.1-8B-Omni</span><br><span class="line">9/12/24</span><br><span class="line">End to End Speech models are on fire - LLAMA-OMNI 8B - Apache licensed! ğŸ”¥</span><br><span class="line">&gt; Speech Encoder - Whisper Large v3</span><br><span class="line">&gt; LLM backbone - Llama 3.1 8B Instruct</span><br><span class="line">&gt; Speech Decoder - HuBERT (UnitY)</span><br><span class="line">&gt; Simultaneously generate Speech + Text</span><br><span class="line">&gt; Less than 250 ms latency</span><br><span class="line">&gt; Trained in less than 3 days on 4x GPUs</span><br><span class="line">&gt; Used 200K instruct pairs</span><br><span class="line">&gt; Model checkpoints on the Hub ğŸ¤—</span><br><span class="line">&gt; Space incoming!</span><br><span class="line">ğŸ¦™ğŸ§ LLaMA-Omni: Seamless Speech Interaction with Large Language Models</span><br><span class="line">Authors: Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, Yang Feng*</span><br><span class="line"></span><br><span class="line">[Paper] [Model] [Code]</span><br><span class="line"></span><br><span class="line">LLaMA-Omni is a speech-language model built upon Llama-3.1-8B-Instruct. It supports low-latency and high-quality speech interactions, simultaneously generating both text and speech responses based on speech instructions.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ğŸ’¡ Highlights</span><br><span class="line">ğŸ’ª Built on Llama-3.1-8B-Instruct, ensuring high-quality responses.</span><br><span class="line"></span><br><span class="line">ğŸš€ Low-latency speech interaction with a latency as low as 226ms.</span><br><span class="line"></span><br><span class="line">ğŸ§ Simultaneous generation of both text and speech responses.</span><br><span class="line"></span><br><span class="line">â™»ï¸ Trained in less than 3 days using just 4 GPUs.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/arcee-ai/Llama-3.1-SuperNova-Lite</span><br><span class="line">Arcee.ai, 9/11/24</span><br><span class="line">First distilled Llama 3.1 released by Arcee.ai! ğŸ¦™ SuperNova is a distilled reasoning Llama 3.1 70B &amp; 8B! ğŸ‘€ Arcee distilled Meta Llama 3.1 405B using offline knowledge distillation and combined it with RLHF and model merging to create new open LLM. SuperNova 70B is available via API, and 8B is available on Hugging Face. ğŸ”¥</span><br><span class="line">Post Training pipeline:</span><br><span class="line">1ï¸âƒ£ Used offline Distillation to distill Llama 3.1 405B into 70B</span><br><span class="line">2ï¸âƒ£ Fine-tuned a second Llama 3.1 70B on synthetic data with Spectrum and Evol-Kit</span><br><span class="line">3ï¸âƒ£ Trained a third Llama 3.1 Instruct with DPO</span><br><span class="line">4ï¸âƒ£ Final model is merged in 2 steps: 2ï¸âƒ£ + 3ï¸âƒ£ &gt; 1ï¸âƒ£</span><br><span class="line">Results:</span><br><span class="line">ğŸ¥‡ Outperforms Meta Llama 3.1 70B instruct across benchmarks</span><br><span class="line">ğŸ¤— SuperNova 70B is available via API, and 8B is available on Hugging Face.</span><br><span class="line">ğŸ§  Works with the &lt;thinking&gt; prompt of Reflection</span><br><span class="line">ğŸ“Š Best open LLM on IFEval, surpassing OpenAI and Anthropic models</span><br><span class="line">ğŸ”¬ Developed using Spectrum, EvolKit, and Direct Preference Optimization (DPO)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://aws.amazon.com/de/blogs/machine-learning/amazon-ec2-p5e-instances-are-generally-available/</span><br><span class="line">AWS, 9/11/24</span><br><span class="line">H200s are now available on Amazon Web Services (AWS)! AWS launched p5e instances with NVIDIA H200 (141GB) via Amazon EC2 Capacity Blocks. Amazon EC2 Capacity Blocks is an option to reserve GPU capacity for AI workloads on Amazon EC2. P5de with 8x H200 start at $5.408/h/gpu or $43.26/h per instance.</span><br><span class="line">Here is how it works:</span><br><span class="line">1ï¸âƒ£ Reserve GPU instances for a future start date for a specified duration</span><br><span class="line">2ï¸âƒ£ Reservations can be made up to 8 weeks in advance</span><br><span class="line">3ï¸âƒ£ Available in cluster sizes of 1 to 64 instances (up to 512 GPUs)</span><br><span class="line">4ï¸âƒ£ Reservation durations range from 1-14 days in 1-day increments, up to 28 days total</span><br><span class="line">5ï¸âƒ£ Instances are colocated in EC2 UltraClusters for high-performance ML workloads</span><br><span class="line"></span><br><span class="line">Amazon EC2 P5e instances are generally available</span><br><span class="line">by Avi Kulkarni, Aman Shanbhag, Pavel Belevich, Khaled Rawashdeh, Karthik Venna, Maxime Hugues, and Shruti Koparkar | on 09 SEP 2024 | in Amazon EC2, Announcements, Compute | Permalink |  Comments |  Share</span><br><span class="line">State-of-the-art generative AI models and high performance computing (HPC) applications are driving the need for unprecedented levels of compute. Customers are pushing the boundaries of these technologies to bring higher fidelity products and experiences to market across industries.</span><br><span class="line"></span><br><span class="line">The size of large language models (LLMs), as measured by the number of parameters, has grown exponentially in recent years, reflecting a significant trend in the field of AI. Model sizes have increased from billions of parameters to hundreds of billions of parameters within a span of 5 years. As LLMs have grown larger, their performance on a wide range of natural language processing tasks has also improved significantly, but the increased size of LLMs has led to significant computational and resource challenges. Training and deploying these models requires vast amounts of computing power, memory, and storage.</span><br><span class="line"></span><br><span class="line">The size of an LLM has a significant impact on the choice of compute needed for inference. Larger LLMs require more GPU memory to store the model parameters and intermediate computations, as well as greater computational power to perform the matrix multiplications and other operations needed for inference. Large LLMs take longer to perform a single inference pass due to this increased computational complexity. This increased compute requirement can lead to higher inference latency, which is a critical factor for applications that require real-time or near real-time responses.</span><br><span class="line"></span><br><span class="line">HPC customers exhibit similar trends. With the fidelity of HPC customer data collection increasing and datasets reaching exabyte scale, customers are looking for ways to enable faster time to solution across increasingly complex applications.</span><br><span class="line"></span><br><span class="line">To address customer needs for high performance and scalability in deep learning, generative AI, and HPC workloads, we are happy to announce the general availability of Amazon Elastic Compute Cloud (Amazon EC2) P5e instances, powered by NVIDIA H200 Tensor Core GPUs. AWS is the first leading cloud provider to offer the H200 GPU in production. Additionally, we are announcing that P5en instances, a network optimized variant of P5e instances, are coming soon.</span><br><span class="line"></span><br><span class="line">In this post, we discuss the core capabilities of these instances and the use cases theyâ€™re well-suited for, and walk you through an example of how to get started with these instances and carry out inference deployment of Meta Llama 3.1 70B and 405B models on them.</span><br><span class="line"></span><br><span class="line">EC2 P5e instances overview</span><br><span class="line">P5e instances are powered by NVIDIA H200 GPUs with 1.7 times more GPU memory capacity and 1.5 times faster GPU memory bandwidth as compared to NVIDIA H100 Tensor Core GPUs featured in P5 instances.</span><br><span class="line"></span><br><span class="line">P5e instances incorporate 8 NVIDIA H200 GPUs with 1128 GB of high bandwidth GPU memory, 3rd Gen AMD EPYC processors, 2 TiB of system memory, and 30 TB of local NVMe storage. P5e instances also provide 3,200 Gbps of aggregate network bandwidth with support for GPUDirect RDMA, enabling lower latency and efficient scale-out performance by bypassing the CPU for internode communication.</span><br><span class="line"></span><br><span class="line">The following table summarizes the details for the instance.</span><br><span class="line"></span><br><span class="line">Instance Size	vCPUs	Instance Memory (TiB)	GPU	GPU memory	Network Bandwidth (Gbps)	GPUDirect RDMA	GPU Peer to Peer	Instance Storage (TB)	EBS Bandwidth (Gbps)</span><br><span class="line">p5e.48xlarge	192	2	8 x NVIDIA H200	1128 GB</span><br><span class="line">HBM3e	3200 Gbps EFA	Yes	900 GB/s NVSwitch	8 x 3.84 NVMe SSD	80</span><br><span class="line">EC2 P5en instances coming soon</span><br><span class="line">One of the bottlenecks in GPU-accelerated computing may lie in the communication between CPUs and GPUs. The transfer of data between these two components can be time-consuming, especially for large datasets or workloads that require frequent data exchanges. This challenge could impact wide range of GPU-accelerated applications such as deep learning, high-performance computing, and real-time data processing. The need to move data between the CPU and GPU can introduce latency and reduce the overall efficiency. Additionally, network latency can become an issue for ML workloads on distributed systems, because data needs to be transferred between multiple machines.</span><br><span class="line"></span><br><span class="line">EC2 P5en instances, coming soon in 2024, can help solve these challenges. P5en instances pair the NVIDIA H200 GPUs with custom 4th Generation Intel Xeon Scalable processors, enabling PCIe Gen 5 between CPU and GPU. These instances will provide up to four times the bandwidth between CPU and GPU and lower network latency, thereby improving workload performance.</span><br><span class="line"></span><br><span class="line">P5e use cases</span><br><span class="line">P5e instances are ideal for training, fine-tuning, and running inference for increasingly complex LLMs and multimodal foundation models (FMs) behind the most demanding and compute-intensive generative AI applications, including question answering, code generation, video and image generation, speech recognition, and more.</span><br><span class="line"></span><br><span class="line">Customers deploying LLMs for inference can benefit from using P5e instances, which offer several key advantages that make them an excellent choice for these workloads.</span><br><span class="line"></span><br><span class="line">Firstly, the higher memory bandwidth of the H200 GPUs in the P5e instances allows the GPU to fetch and process data from memory more quickly. This translates to reduced inference latency, which is critical for real-time applications like conversational AI systems where users expect near-instant responses. The higher memory bandwidth also enables higher throughput, allowing the GPU to process more inferences per second. Customers deploying the 70-billion-parameter Meta Llama 3.1 model on P5e instances can expect up to 1.871 times higher throughput and up to 40%1 lower cost compared to using comparable P5 instances. (1Input Sequence Length 121, Output Sequence Length 5000, batch size 10, vLLM framework)</span><br><span class="line"></span><br><span class="line">Secondly, the massive scale of modern LLMs, with hundreds of billions of parameters, requires an immense amount of memory to store the model and intermediate computations during inference. On the standard P5 instances, this would likely necessitate the use of multiple instances to accommodate the memory requirements. However, the P5e instancesâ€™ 1.76 times higher GPU memory capacity enables you to scale up by using a single instance to fit the entire model. This avoids the complexity and overhead associated with distributed inference systems, such as data synchronization, communication, and load balancing. Customers deploying the 405-billion-parameter Meta Llama 3.1 model on a single P5e instance can expect up to 1.72 times higher throughput and up to 69%2 lower cost compared to using two P5 instances. (2Input Sequence Length 121, Output Sequence Length 50, batch size 10, vLLM framework)</span><br><span class="line"></span><br><span class="line">Finally, the higher GPU memory of the P5e instances also enables the use of larger batch sizes during inference for better GPU utilization, resulting in faster inference times and higher overall throughput. This additional memory can be particularly beneficial for customers with high-volume inference requirements.</span><br><span class="line"></span><br><span class="line">When optimizing inference throughput and cost, consider adjusting batch size, input/output sequence length, and quantization level, because these parameters can have a substantial impact. Experiment with different configurations to find the optimal balance between performance and cost for your specific use case.</span><br><span class="line"></span><br><span class="line">In summary, the combination of higher memory bandwidth, increased GPU memory capacity, and support for larger batch sizes make the P5e instances an excellent choice for customers deploying LLM inference workloads. These instances can deliver significant performance improvements, cost savings, and operational simplicity compared to alternative options.</span><br><span class="line"></span><br><span class="line">P5e instances are also well-suited for memory-intensive HPC applications like simulations, pharmaceutical discovery, seismic analysis, weather forecasting, and financial modeling. Customers using dynamic programming (DP) algorithms for applications like genome sequencing or accelerated data analytics can also see further benefit from P5e through support for the DPX instruction set.</span><br><span class="line"></span><br><span class="line">Get started with P5e instances</span><br><span class="line">When launching P5 instances, you can use AWS Deep Learning AMIs (DLAMI) to support P5 instances. DLAMI provides ML practitioners and researchers with the infrastructure and tools to quickly build scalable, secure, distributed ML applications in preconfigured environments. You can run containerized applications on P5 instances with AWS Deep Learning Containers using libraries for Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS).</span><br><span class="line"></span><br><span class="line">P5e instances now available</span><br><span class="line">EC2 P5e instances are now available in the US East (Ohio) AWS Region in the p5e.48xlarge sizes through Amazon EC2 Capacity Blocks for ML. For more information, refer to Amazon EC2 P5 Instances.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/Ucas-HaoranWei/GOT-OCR2.0</span><br><span class="line">9/3/24</span><br><span class="line">OCR-2.0 is coming, and Generative AI and multimodal LLMs will power it! ğŸ” GOT (General OCR Theory) is a 580M end-to-end OCR-2.0 model that outperforms all existing methods.</span><br><span class="line">GOT consists of a Vision-Encoder to convert images into transformers images into tokens and a decoder for generating OCR outputs in various formats (e.g., plain text, markdown, Mathpix). GOT is designed to handle complex tasks like sheets, formulas, and geometric shapes</span><br><span class="line">Implementation</span><br><span class="line">1ï¸âƒ£Vision Encoder Pre-training: The encoder (VitDet) trained using scene text and document OCR data to recognize both slice and whole-page inputs.</span><br><span class="line">2ï¸âƒ£ Joint-Training: The encoder is connected to the decoder (Qwen-0.5B) and both are trained on more general OCR tasks (e.g., formulas, sheet music, geometry).</span><br><span class="line">3ï¸âƒ£ Post-Training: Fine-tuned the model with specific tasks, such as fine-grained OCR, multi-page PDFs, and dynamic resolution, using new synthetic datasets.</span><br><span class="line">Insights</span><br><span class="line">ğŸ§  Encoder-Decoder with 80M (VitDet) Encoder and 500M (Qwen2) Decoder with 8k context</span><br><span class="line">ğŸ¥‡ Achieves a 0.035 edit distance and 0.972 BLEU score on plain OCR</span><br><span class="line">ğŸ“Š Outperforms LLaVA-NeXT and Qwen-VL-Max in document and scene text OCR</span><br><span class="line">ğŸ§® Can extract LaTeX formulas from Arxiv and convert them to Mathpix format</span><br><span class="line">ğŸ“ƒ Supports dynamic resolution and multi-page OCR</span><br><span class="line">ğŸ–¼ï¸ Input resolutions up to 1024x1024</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/upstage/solar-pro-preview-instruct</span><br><span class="line">Upstage, 9/10/24</span><br><span class="line">Solar Pro Preview: The most intelligent LLM on a single GPU</span><br><span class="line">Summary</span><br><span class="line">We introduce Solar Pro Preview, an advanced large language model (LLM) with 22 billion parameters designed to fit into a single GPU. Solar Pro Preview shows superior performance compared to LLMs with less than 30 billion parameters and delivers performance comparable to models over three times its size, such as Llama 3.1 with 70 billion parameters.</span><br><span class="line"></span><br><span class="line">Solar Pro Preview is developed using an enhanced version of our previous depth up-scaling method, which scales a Phi-3-medium model with 14 billion parameters to 22 billion parameters, intended to run on a GPU with 80GB of VRAM. Our carefully curated training strategy and dataset have significantly enhanced performance from Phi-3-medium, particularly on the MMLU-Pro and IFEval benchmarks, both respected for evaluating a modelâ€™s knowledge and instruction-following abilities.</span><br><span class="line"></span><br><span class="line">Solar Pro Preview is a pre-release version of the official Solar Pro, with limitations on language coverage and a maximum context length of 4K. However, we believe Solar Pro Preview not only stands out as a highly efficient and capable model, but has the potential to be further extended to cover more languages and capabilities. The official version of Solar Pro will be released this November 2024 with expanded language support beyond English and longer context windows. To stay informed about the latest updates, please sign up for our mailing list. If you have any feedback or questions about the model, please visit our model discussion board and connect with us directly.</span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-09-12</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/09/12/2024-9-12-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 9ì›” 12ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/09/06/2024-9-6-AI-NEWS/" title="2024ë…„ 9ì›” 6ì¼ AI ì†Œì‹">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>