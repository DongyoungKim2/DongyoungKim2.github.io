<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 9월 19일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="OpenAI는 복잡한 문제 해결 능력을 갖춘 o1-preview와 o1-mini 모델을 발표했고, Google은 공공 데이터 통계를 AI 응답에 활용하는 DataGemma 모델과 NotebookLM을 선보였습니다. Microsoft는 AI 모델 평가를 위한 Eureka"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 9월 19일 AI 소식</a></h3></div><div class="post-content"><p>OpenAI는 복잡한 문제 해결 능력을 갖춘 o1-preview와 o1-mini 모델을 발표했고, Google은 공공 데이터 통계를 AI 응답에 활용하는 DataGemma 모델과 NotebookLM을 선보였습니다. Microsoft는 AI 모델 평가를 위한 Eureka 프레임워크를 발표하여 다양한 AI 모델의 강점과 약점을 분석했습니다. Mistral AI는 128K 컨텍스트를 지원하는 Small Instruct 22B 모델을 출시하며, Alibaba는 다국어 지원과 구조화된 데이터 처리에 강점이 있는 Qwen 2.5를 공개했습니다. Hugging Face는 Reflective MAGLLAMA 데이터셋과 FineVideo 비디오 이해 데이터셋을 발표하여 AI의 학습 및 비디오 분석 기능을 강화했습니다.소규모 모델(Small Models)의 역할을 분석한 연구가 발표되어, 대형 언어 모델(LLM) 시대에서 소규모 모델의 실용성을 조명하였으며, 또 다른 논문에서는 LLM의 메모리 기능을 심층 분석하며, ‘슈뢰딩거의 메모리’ 개념을 제시해 LLM이 특정 입력에 따라 메모리 기능을 발휘하는 방식을 설명했습니다.</p>
<h3 id="OpenAI-o1-preview-및-o1-mini-발표"><a href="#OpenAI-o1-preview-및-o1-mini-발표" class="headerlink" title="OpenAI, o1-preview 및 o1-mini 발표"></a>OpenAI, o1-preview 및 o1-mini 발표</h3><p><a target="_blank" rel="noopener" href="https://openai.com/o1/">링크</a>, 2024년 9월 13일</p>
<ul>
<li>OpenAI는 <strong>o1-preview</strong>와 <strong>o1-mini</strong>라는 두 가지 주요 모델을 발표하여 복잡한 문제 해결 능력을 강화</li>
<li><strong>o1-preview</strong>는 복잡한 문제를 단계적으로 분석하고 해결하는 능력을 갖추었으며, 과학, 코딩, 수학 분야에서 특히 뛰어난 성능을 보임</li>
<li><strong>사고의 사슬(Chain of Thought)</strong> 방식을 도입하여 모델이 추론 과정에서 논리적 단계를 거쳐 정확한 답변을 생성</li>
<li>**강화 학습(Reinforcement Learning)**을 통해 모델이 스스로 학습하고 추론 과정에서 발생한 오류를 수정하며 성능을 지속적으로 향상</li>
<li>o1-preview는 고성능 문제 해결을 목표로 하며, <strong>o1-mini</strong>는 개발자와 연구자에게 실용적이고 비용 효율적인 솔루션을 제공</li>
<li>향후 <strong>웹 브라우징</strong>, <strong>파일 및 이미지 업로드</strong> 기능 추가 예정</li>
<li>안전성과 정렬 지침 준수로 부적절한 응답 방지</li>
<li>다양한 분야에서 성능 입증: <strong>GPQA 다이아몬드 테스트</strong>에서 73.3% 정확도 기록, AIME 수학 시험에서 83.3% 정확도 도달</li>
<li><strong>코딩 분야</strong>에서는 Codeforces 대회에서 1,673점의 Elo 점수를 기록하며 GPT-4o의 성능을 크게 상회</li>
<li>향후 ChatGPT Plus 및 ChatGPT Enterprise 사용자들이 o1 모델을 사용할 수 있으며, API를 통한 개발자 지원도 포함됨</li>
</ul>
<h3 id="Google-DataGemma-출시"><a href="#Google-DataGemma-출시" class="headerlink" title="Google, DataGemma 출시"></a>Google, DataGemma 출시</h3><p><a target="_blank" rel="noopener" href="https://blog.google/technology/ai/google-datagemma-ai-llm/">링크</a>, 2024년 9월 12일</p>
<ul>
<li>Google DeepMind는 <strong>DataGemma</strong>라는 새로운 AI 모델을 발표, <strong>Data Commons</strong>와 연결하여 공공 데이터에 기반한 통계 정보를 활용해 AI 응답의 정확성을 극대화</li>
<li><strong>DataGemma RIG</strong> 모델은 사용자의 질의에 대해 실시간으로 Data Commons 데이터를 조회하고 응답 생성 시 참조, <strong>RAG(Retrieval-Augmented Generation)</strong> 방법론을 통해 모델이 훈련 데이터 외부의 컨텍스트 정보를 가져와 더욱 정확한 응답을 생성</li>
<li>DataGemma 모델은 LLM의 “환각(hallucination)” 문제를 해결하는 데 중점을 두며, 신뢰할 수 있는 통계 정보를 사용해 응답의 정확성을 크게 향상</li>
<li><strong>TPUv5e</strong>를 사용하여 <strong>JAX</strong>로 훈련되었으며, <strong>Hugging Face</strong>에서도 사용 가능</li>
</ul>
<h3 id="Google-NotebookLM-발표"><a href="#Google-NotebookLM-발표" class="headerlink" title="Google, NotebookLM 발표"></a>Google, NotebookLM 발표</h3><p><a target="_blank" rel="noopener" href="https://notebooklm.google/">링크</a>, 2023년 7월 12일</p>
<ul>
<li>Google Labs는 <strong>NotebookLM</strong>이라는 AI 기반 노트북을 발표, 이 도구는 사용자가 문서에서 더 빠르게 인사이트를 얻을 수 있도록 도와주는 실험적인 노트북 시스템</li>
<li>기존의 문서 기반 노트테이킹 방식과 달리, NotebookLM은 AI를 통해 문서를 요약하고, 복잡한 아이디어를 설명하며, 새로운 연결점을 찾아내는 데 중점을 둠</li>
<li>NotebookLM은 사용자가 직접 선택한 Google Docs 문서에 AI 모델을 ‘기반’으로 하여 작업을 수행, 문서의 요약, 질문 응답, 아이디어 생성 등 다양한 작업을 지원</li>
<li>학생, 연구자, 크리에이터들이 데이터를 쉽게 통합하고 인사이트를 얻을 수 있도록 설계되었으며, 향후 더 많은 문서 형식 지원 예정</li>
</ul>
<h3 id="Mistral-AI-Small-Instruct-22B-모델-출시"><a href="#Mistral-AI-Small-Instruct-22B-모델-출시" class="headerlink" title="Mistral AI, Small Instruct 22B 모델 출시"></a>Mistral AI, Small Instruct 22B 모델 출시</h3><p><a target="_blank" rel="noopener" href="https://mistral.ai/news/september-24-release/">링크</a>, 2024년 9월 17일</p>
<ul>
<li>Mistral AI는 <strong>Small Instruct 22B</strong>라는 새로운 다국어 AI 모델을 발표, 128K 컨텍스트 지원 및 <strong>함수 호출(function calling)</strong> 기능 포함</li>
<li>이 모델은 Mistral NeMo 12B와 Mistral Large 123B 사이의 중간 모델로서 22B 파라미터를 갖추고 있으며, 번역, 요약, 감정 분석 등 다양한 태스크에서 뛰어난 성능을 발휘</li>
<li>비상업적 용도로 사용 가능한 모델 가중치를 <strong>Hugging Face</strong>에서 제공</li>
<li>또한 <strong>Pixtral 12B</strong>라는 비전 모델도 출시하여, 이미지 이해 기능을 제공하며 Apache 2.0 라이선스 하에 배포</li>
</ul>
<h3 id="Alibaba-Qwen-2-5-발표"><a href="#Alibaba-Qwen-2-5-발표" class="headerlink" title="Alibaba, Qwen 2.5 발표"></a>Alibaba, Qwen 2.5 발표</h3><p><a target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen2.5">링크</a>, 2024년 9월 19일</p>
<ul>
<li>Alibaba는 <strong>Qwen 2.5</strong> 모델 시리즈를 발표하며, 최대 <strong>72B 파라미터</strong>를 가진 이 모델은 Llama 3.1 및 Mistral Large 2(123B)를 능가하는 성능을 자랑</li>
<li>Qwen2.5는 18조 개의 토큰을 사용해 훈련되었으며, MMLU 벤치마크에서 85점 이상, HumanEval 및 MATH 벤치마크에서 각각 85점 이상과 80점 이상의 성과를 기록</li>
<li><strong>128K 토큰</strong>까지 처리 가능하며, 다국어 지원 (29개 언어) 및 JSON 생성 등 구조화된 데이터 처리에서 뛰어난 성능 발휘</li>
<li>Qwen2.5-Coder, Qwen2.5-Math 등의 특화 모델도 함께 출시, 특히 코딩 및 수학 관련 작업에서 우수한 성능을 보임</li>
</ul>
<h3 id="Microsoft-Eureka-발표"><a href="#Microsoft-Eureka-발표" class="headerlink" title="Microsoft, Eureka 발표"></a>Microsoft, Eureka 발표</h3><p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/blog/eureka-evaluating-and-understanding-progress-in-ai/">링크</a>, 2024년 9월 18일</p>
<ul>
<li>Microsoft는 AI 모델 성능을 평가하는 <strong>Eureka</strong>라는 오픈소스 프레임워크를 발표, 이 도구는 12개의 최첨단 AI 모델에 대한 심층 분석을 제공</li>
<li><strong>멀티모달</strong> 및 <strong>언어 능력</strong>을 초점으로 한 평가를 통해 AI 모델의 강점과 약점에 대한 통찰을 제공하며, 단일 점수로 모델을 평가하는 것을 넘어 다양한 요소를 분석</li>
<li>모델 간 비교뿐만 아니라 AI의 현실 세계 응용에 중요한 기본 기능들이 여전히 도전적인 과제임을 강조</li>
</ul>
<h3 id="Hugging-Face-Reflective-MAGLLAMA-데이터셋-출시"><a href="#Hugging-Face-Reflective-MAGLLAMA-데이터셋-출시" class="headerlink" title="Hugging Face, Reflective-MAGLLAMA 데이터셋 출시"></a>Hugging Face, Reflective-MAGLLAMA 데이터셋 출시</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/thesven/Reflective-MAGLLAMA-v0.1.1">링크</a>, 2024년 9월 13일</p>
<ul>
<li><strong>Reflective MAGLLAMA</strong> 데이터셋은 <strong>반사적 프롬프팅(reflection prompting)</strong> 기법을 통해 심층적인 사고 및 분석을 유도하는 합성 데이터셋으로, 10,000개의 샘플을 포함</li>
<li>이 데이터셋은 <strong>LLaMa 3.1</strong> 모델을 사용하여 생성된 반사적 응답을 수집하여, 분석적 문제 해결 및 학습 촉진에 적합한 모델 훈련 및 평가에 활용 가능</li>
</ul>
<h3 id="Jina-AI-Reader-LM-출시"><a href="#Jina-AI-Reader-LM-출시" class="headerlink" title="Jina AI, Reader-LM 출시"></a>Jina AI, Reader-LM 출시</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/HuggingFaceFV/finevideo">링크</a>, 2024년 9월 14일</p>
<ul>
<li>Jina AI는 <strong>Reader-LM</strong>이라는 모델을 발표, HTML 웹페이지에서 <strong>Markdown</strong>으로 변환하는 전체 파이프라인을 처리하는 모델</li>
<li><strong>Reader-LM-0.5B</strong>와 <strong>1.5B</strong> 두 가지 모델을 출시하여, 다양한 HTML 데이터를 처리하는 능력을 강화하고, Markdown 추출 작업을 자동화</li>
</ul>
<h3 id="Hugging-Face-FineVideo-데이터셋-출시"><a href="#Hugging-Face-FineVideo-데이터셋-출시" class="headerlink" title="Hugging Face, FineVideo 데이터셋 출시"></a>Hugging Face, FineVideo 데이터셋 출시</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/HuggingFaceFV/finevideo">링크</a>, 2024년 9월 15일</p>
<ul>
<li>Hugging Face는 <strong>FineVideo</strong>라는 비디오 이해를 위한 데이터셋을 발표, 43,751개의 비디오와 122개의 카테고리를 포함</li>
<li>약 3,425시간의 콘텐츠를 제공하며, 감정 분석, 스토리텔링, 미디어 편집 등 다중 모달 작업에 최적화된 데이터셋으로, 비디오 내 장면, 캐릭터, 음향-시각적 상호작용</li>
</ul>
<p>에 대한 자세한 주석을 포함</p>
<h3 id="소규모-모델의-역할에-관한-설문-조사-논문-발표"><a href="#소규모-모델의-역할에-관한-설문-조사-논문-발표" class="headerlink" title="소규모 모델의 역할에 관한 설문 조사 논문 발표"></a>소규모 모델의 역할에 관한 설문 조사 논문 발표</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.06857">링크</a>, 2024년 9월 10일</p>
<ul>
<li>**소규모 모델(Small Models)**이 LLM(대형 언어 모델) 시대에서 가지는 역할을 체계적으로 분석한 설문 조사 논문 발표</li>
<li>소규모 모델(SM)은 실용적이며, 학술 연구나 자원이 제한된 비즈니스 환경에서 특히 유용하다는 점을 강조</li>
<li>SM이 LLM과 협업 또는 경쟁하는 방식에 대해 분석하고, 효율적인 컴퓨팅 자원 사용에 대한 통찰 제공</li>
</ul>
<h3 id="LLM-메모리-연구-논문-발표"><a href="#LLM-메모리-연구-논문-발표" class="headerlink" title="LLM 메모리 연구 논문 발표"></a>LLM 메모리 연구 논문 발표</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.10482">링크</a>, 2024년 9월 16일</p>
<ul>
<li>LLM의 메모리 기능에 대한 심층 연구를 다룬 논문 발표</li>
<li>**슈뢰딩거의 메모리(Schrodinger’s Memory)**라는 개념을 도입하여, LLM의 메모리는 특정 질의가 있을 때만 관찰될 수 있다고 설명</li>
<li>인간의 기억과 LLM의 기억 간 유사점과 차이점에 대해 탐구, Transformer 아키텍처가 <strong>동적 적응성</strong>을 갖춘 모델로 작동하며, 최소한의 입력 정보만으로 전체 내용을 기억할 수 있음</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://openai.com/o1/</span><br><span class="line">OpenAI</span><br><span class="line">9/13/24</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">OpenAI는 혁신적인 AI 모델 시리즈인 **o1-preview**와 **o1-mini**를 발표하여 인공지능 기술의 새로운 지평을 열었습니다. OpenAI의 o1-preview와 o1-mini 모델은 인간의 심층적인 사고 방식을 모방하여 복잡한 문제를 단계별로 분석하고 해결하는 혁신적인 AI 모델로, 과학, 코딩, 수학 분야에서 뛰어난 성능을 보여줍니다. 이 모델들은 &#x27;사고의 사슬&#x27; 방식을 통해 논리적인 추론을 수행하며, 강화 학습을 통해 스스로 학습하고 오류를 수정하여 지속적으로 성능을 향상시킵니다. 또한, 안전성과 정렬 지침을 준수하여 부적절한 응답을 방지하고 사용자에게 신뢰할 수 있는 서비스를 제공합니다. o1-preview는 고도화된 문제 해결 능력을 제공하며, o1-mini는 비용 효율성과 속도를 중시하여 개발자와 연구자들에게 실용적인 솔루션을 제공합니다. 향후 업데이트를 통해 웹 브라우징, 파일 및 이미지 업로드 등의 기능이 추가될 예정이며, 이러한 모델들은 인공지능 분야에서 새로운 가능성을 열어줄 것으로 기대됩니다.</span><br><span class="line"></span><br><span class="line">- 관련 링크: [OpenAI o1](https://openai.com/o1/)</span><br><span class="line"></span><br><span class="line">## 작동 원리</span><br><span class="line"></span><br><span class="line">o1 시리즈는 인간이 복잡한 문제를 해결할 때처럼 **&#x27;심층적인 사고 과정&#x27;**을 거쳐 답변을 생성하도록 설계되었습니다. 이를 위해 다음과 같은 기술과 방법이 적용되었습니다:</span><br><span class="line"></span><br><span class="line">1. **사고의 사슬(Chain of Thought)**: 모델은 내부적으로 긴 추론 과정을 거쳐 최종 답변을 생성합니다. 이 과정에서 문제를 단계별로 분해하고, 각 단계에서 논리적인 결론을 도출합니다. 예를 들어, 복잡한 수학 문제를 풀 때 모델은 공식 적용, 변수 대입, 계산 등 중간 과정을 모두 고려합니다.</span><br><span class="line"></span><br><span class="line">2. **강화 학습(Reinforcement Learning)**: 모델은 강화 학습을 통해 자신의 추론 과정을 지속적으로 개선합니다. 성공적인 문제 해결은 강화되고, 실수나 오류는 교정됩니다. 이를 통해 모델은 다양한 전략을 시도하고 최적의 해결책을 찾는 능력을 갖추게 됩니다.</span><br><span class="line"></span><br><span class="line">3. **오류 인식 및 수정 능력**: 모델은 자신의 추론 과정에서 발생하는 오류를 인식하고, 이를 수정하는 방법을 학습했습니다. 이는 복잡한 문제를 해결하는 데 필수적인 능력으로, 모델의 신뢰성을 높여줍니다.</span><br><span class="line"></span><br><span class="line">## 성능 평가</span><br><span class="line"></span><br><span class="line">o1 시리즈는 다양한 분야에서 탁월한 성능을 입증하였습니다:</span><br><span class="line"></span><br><span class="line">- **과학 분야**:</span><br><span class="line"></span><br><span class="line">  - **GPQA 다이아몬드 테스트**: 과학 분야의 지식을 평가하는 이 테스트에서 o1-preview 모델은 **73.3%**의 정확도를 보였으며, 이는 박사 학위 수준의 인간 전문가를 능가하는 성과입니다.</span><br><span class="line">  - **세부 분야 성과**:</span><br><span class="line">    - **물리학**: 89.5%의 정확도로 GPT-4o의 68.6%를 크게 앞질렀습니다.</span><br><span class="line">    - **화학**: 60.2%의 정확도를 보여 GPT-4o의 43%보다 우수한 성과를 냈습니다.</span><br><span class="line"></span><br><span class="line">- **수학 분야**:</span><br><span class="line"></span><br><span class="line">  - **AIME(미국 수학 경시대회)**: o1 모델은 단일 시도에서 **74.4%**의 문제를 정확히 풀어냈으며, 다중 시도에서는 **83.3%**의 정확도를 보였습니다. 이는 미국 내 상위 500명의 학생에 해당하는 성적입니다.</span><br><span class="line">  - **MATH 벤치마크**: o1 모델은 **94.8%**의 정확도를 달성하여 GPT-4o의 60.3%를 크게 상회했습니다.</span><br><span class="line"></span><br><span class="line">- **코딩 분야**:</span><br><span class="line">  - **Codeforces 대회**: o1 모델은 **1,673점**의 Elo 점수를 기록하여 상위 89%의 성적을 거두었습니다. 이는 GPT-4o의 808점에 비해 두 배 이상의 향상입니다.</span><br><span class="line">  - **HumanEval 벤치마크**: o1-mini 모델은 **92.4%**의 정확도를 보여 코딩 문제 해결 능력에서 탁월한 성능을 입증했습니다.</span><br><span class="line"></span><br><span class="line">## 사용 예시</span><br><span class="line"></span><br><span class="line">- **수학 문제 해결**:</span><br><span class="line"></span><br><span class="line">  - **교육 분야**: o1 모델은 복잡한 수학 문제를 단계별로 풀어내어 학생들에게 학습 자료로 활용될 수 있습니다. 예를 들어, 적분 계산에서 중간 단계의 미분 과정과 함수 분석을 상세히 설명하여 학생들이 개념을 깊이 이해할 수 있도록 돕습니다.</span><br><span class="line"></span><br><span class="line">- **코드 생성 및 디버깅**:</span><br><span class="line"></span><br><span class="line">  - **소프트웨어 개발**: 개발자들은 o1 모델을 활용하여 복잡한 알고리즘 구현이나 버그 수정에 도움을 받을 수 있습니다. 예를 들어, 병렬 프로그래밍이나 분산 시스템에서의 동시성 문제를 해결하는 코드를 생성하고, 잠재적인 동기화 이슈를 식별하여 수정할 수 있습니다.</span><br><span class="line"></span><br><span class="line">- **과학 연구 보조**:</span><br><span class="line"></span><br><span class="line">  - **생물정보학**: 유전자 시퀀싱 데이터의 주석 처리, 단백질 구조 예측 등에서 o1 모델의 고도화된 추론 능력을 활용할 수 있습니다.</span><br><span class="line">  - **화학 반응 예측**: 새로운 화합물의 합성 경로를 예측하고, 반응 메커니즘을 분석하여 연구 시간을 단축할 수 있습니다.</span><br><span class="line"></span><br><span class="line">- **데이터 분석 및 해석**:</span><br><span class="line">  - **빅데이터 처리**: 대규모 데이터셋에서 패턴을 추출하고 해석하여 비즈니스 인사이트를 도출하는 데 활용될 수 있습니다.</span><br><span class="line">  - **통계 모델링**: 복잡한 통계 모델을 구축하고 결과를 해석하여 의사 결정에 도움을 줍니다.</span><br><span class="line"></span><br><span class="line">## 안전성 고려</span><br><span class="line"></span><br><span class="line">OpenAI는 모델의 고도화된 추론 능력을 안전하게 활용하기 위해 다음과 같은 조치를 취했습니다:</span><br><span class="line"></span><br><span class="line">1. **새로운 안전 훈련 접근법 도입**: 모델이 **안전성과 정렬 지침**을 맥락에서 추론하고 적용할 수 있도록 훈련되었습니다. 이는 모델이 사용자 요청을 처리할 때 안전 규칙을 고려하여 부적절한 응답을 방지합니다.</span><br><span class="line"></span><br><span class="line">2. **&#x27;탈옥(jailbreaking)&#x27; 방어 성능 강화**: 모델이 안전 지침을 우회하려는 시도를 방어하는 능력이 크게 향상되었습니다. 예를 들어, 어려운 탈옥 테스트에서 o1-preview 모델은 **84점**을 받아 GPT-4o의 22점을 크게 앞질렀습니다.</span><br><span class="line"></span><br><span class="line">3. **내부 거버넌스 및 외부 협력 강화**: 모델의 안전성을 보장하기 위해 내부적인 평가 프로세스를 강화하고, 미국 및 영국의 AI 안전 연구 기관과 협력하여 모델의 사전 및 사후 검증을 수행하고 있습니다.</span><br><span class="line"></span><br><span class="line">## 사용 대상</span><br><span class="line"></span><br><span class="line">o1 시리즈는 복잡한 문제 해결이 필요한 다양한 분야에서 활용될 수 있습니다:</span><br><span class="line"></span><br><span class="line">- **의료 연구자**:</span><br><span class="line"></span><br><span class="line">  - **유전자 분석**: 유전 질환의 원인 유전자 식별, 유전자 발현 패턴 분석 등에 활용하여 개인 맞춤형 치료법 개발에 기여할 수 있습니다.</span><br><span class="line">  - **약물 개발**: 신약 후보 물질의 효능 예측과 부작용 분석을 통해 연구 효율을 높일 수 있습니다.</span><br><span class="line"></span><br><span class="line">- **물리학자**:</span><br><span class="line"></span><br><span class="line">  - **양자 컴퓨팅**: 복잡한 양자 알고리즘의 시뮬레이션과 최적화에 도움을 줍니다.</span><br><span class="line">  - **천체물리학**: 우주 현상의 모델링과 데이터 해석을 지원하여 새로운 발견을 촉진합니다.</span><br><span class="line"></span><br><span class="line">- **개발자**:</span><br><span class="line"></span><br><span class="line">  - **AI 어시스턴트 개발**: 자연어 처리 능력을 활용하여 사용자와의 대화형 인터페이스를 구현하고, 사용자 요구에 맞는 서비스를 제공합니다.</span><br><span class="line">  - **자동화 도구 개발**: 반복적인 작업을 자동화하는 스크립트나 프로그램을 생성하여 생산성을 향상시킵니다.</span><br><span class="line"></span><br><span class="line">- **교육자 및 학생**:</span><br><span class="line">  - **교육 콘텐츠 생성**: 복잡한 개념을 이해하기 쉬운 언어로 설명하고, 예제와 함께 교육 자료를 작성합니다.</span><br><span class="line">  - **과제 및 연구 보조**: 학생들의 과제 해결을 돕고, 연구 아이디어를 구체화하는 데 도움을 줍니다.</span><br><span class="line"></span><br><span class="line">## OpenAI o1-mini</span><br><span class="line"></span><br><span class="line">**o1-mini**는 o1 시리즈의 경량화된 버전으로, 특정 분야에서 고성능을 발휘하도록 최적화되었습니다.</span><br><span class="line"></span><br><span class="line">- **효율성**:</span><br><span class="line"></span><br><span class="line">  - **비용 절감**: o1-preview에 비해 **80% 저렴**하여 더 많은 사용자들이 접근할 수 있습니다.</span><br><span class="line">  - **속도 향상**: 모델 크기가 작아 응답 시간이 빨라졌으며, 실시간 응답이 중요한 애플리케이션에 적합합니다.</span><br><span class="line"></span><br><span class="line">- **특화 분야**:</span><br><span class="line"></span><br><span class="line">  - **코딩 및 디버깅**: 복잡한 코드 생성과 디버깅에 특화되어 개발자들이 코드 작성 시간을 단축하고 오류를 줄일 수 있습니다.</span><br><span class="line">  - **수학 및 논리 추론**: 고도의 수학적 계산과 논리 문제 해결에 탁월한 성능을 보입니다.</span><br><span class="line"></span><br><span class="line">- **성능 예시**:</span><br><span class="line"></span><br><span class="line">  - **코딩 능력**:</span><br><span class="line">    - **HumanEval 벤치마크**: o1-mini는 **92.4%**의 정확도를 기록하여 복잡한 프로그래밍 문제를 효과적으로 해결할 수 있음을 보여줍니다.</span><br><span class="line">    - **사이버 보안 CTF**: 고등학교 수준의 CTF 챌린지에서 **43%**의 정확도로 우수한 성과를 냈습니다.</span><br><span class="line">  - **수학 능력**:</span><br><span class="line">    - **AIME 시험**: o1-mini는 **70%**의 정확도로 GPT-4o의 13.4%를 크게 상회하였으며, 이는 상위권 성적에 해당합니다.</span><br><span class="line"></span><br><span class="line">- **한계점 및 향후 개선**:</span><br><span class="line">  - **세계 지식 제한**: 광범위한 일반 지식이 필요한 작업에서는 GPT-4o보다 성능이 낮을 수 있습니다.</span><br><span class="line">  - **향후 계획**: 향후 버전에서는 이러한 한계를 극복하고, 다양한 분야로 적용 범위를 넓히기 위한 연구가 진행될 예정입니다.</span><br><span class="line"></span><br><span class="line">## OpenAI o1의 사용 방법</span><br><span class="line"></span><br><span class="line">- **ChatGPT Plus 및 팀 사용자**:</span><br><span class="line"></span><br><span class="line">  - **접근 방법**: ChatGPT 인터페이스에서 모델 선택기(model picker)를 통해 o1-preview와 o1-mini를 선택할 수 있습니다.</span><br><span class="line">  - **메시지 제한**: 초기에는 주간 메시지 제한이 적용됩니다(예: o1-preview는 주당 30개 메시지). 이는 모델의 안정성과 인프라 확장을 위한 조치이며, 추후 확대될 예정입니다.</span><br><span class="line"></span><br><span class="line">- **ChatGPT Enterprise 및 교육 기관 사용자**:</span><br><span class="line"></span><br><span class="line">  - **접근 시기**: 다음 주부터 두 모델 모두에 접근할 수 있으며, 팀 규모에 따라 추가 혜택이 제공될 수 있습니다.</span><br><span class="line"></span><br><span class="line">- **개발자**:</span><br><span class="line"></span><br><span class="line">  - **API 사용**: API 사용 등급 5에 해당하는 개발자는 오늘부터 두 모델을 API에서 프로토타이핑할 수 있습니다.</span><br><span class="line">  - **기능 제한**: 현재 API에서는 함수 호출, 스트리밍, 시스템 메시지 지원 등의 기능은 포함되지 않지만, 개발자들의 피드백을 바탕으로 향후 추가될 예정입니다.</span><br><span class="line">  - **시작 방법**: OpenAI의 [API 문서](http://platform.openai.com/docs/guides/reasoning)를 참고하여 모델 통합과 사용법을 익힐 수 있습니다.</span><br><span class="line"></span><br><span class="line">- **일반 사용자**:</span><br><span class="line">  - **향후 계획**: ChatGPT 무료 사용자들에게도 o1-mini에 대한 접근 권한을 제공할 예정이며, 이는 더 많은 사람들이 최신 AI 기술을 경험할 수 있도록 하기 위한 노력입니다.</span><br><span class="line"></span><br><span class="line">## 향후 계획</span><br><span class="line"></span><br><span class="line">이번에 출시된 모델들은 초기 버전으로, OpenAI는 향후 업데이트를 통해 다음과 같은 기능과 개선을 계획하고 있습니다:</span><br><span class="line"></span><br><span class="line">1. **기능 추가**:</span><br><span class="line"></span><br><span class="line">   - **웹 브라우징**: 모델이 인터넷에서 최신 정보를 검색하여 더 정확하고 시의적절한 답변을 제공할 수 있도록 할 예정입니다.</span><br><span class="line">   - **파일 및 이미지 업로드**: 사용자가 파일이나 이미지를 업로드하여 모델이 이를 분석하고 처리할 수 있는 기능을 추가할 계획입니다.</span><br><span class="line"></span><br><span class="line">2. **모델 성능 향상**:</span><br><span class="line"></span><br><span class="line">   - **강화 학습 개선**: 모델의 추론 능력을 더욱 향상시키기 위해 강화 학습 알고리즘을 지속적으로 개선할 것입니다.</span><br><span class="line">   - **다중 모달 지원**: 텍스트 외에도 이미지, 음성 등 다양한 형태의 데이터를 처리할 수 있도록 연구하고 있습니다.</span><br><span class="line"></span><br><span class="line">3. **GPT 시리즈의 발전**:</span><br><span class="line">   - **새로운 GPT 모델 개발**: o1 시리즈와 함께 GPT 시리즈의 새로운 모델도 계속 개발 및 출시하여 다양한 사용자 요구에 부응할 것입니다.</span><br><span class="line">   - **모델의 통합 및 호환성 강화**: 다양한 모델 간의 호환성을 높여 사용자들이 원하는 모델을 자유롭게 선택하고 활용할 수 있도록 지원할 예정입니다.</span><br><span class="line"></span><br><span class="line">## LangChain과 OpenAI o1의 비교</span><br><span class="line"></span><br><span class="line">| 평가 항목         | LangChain                                              | o1-preview                                                     |</span><br><span class="line">| ----------------- | ------------------------------------------------------ | -------------------------------------------------------------- |</span><br><span class="line">| **주요 기능**     | 다양한 모델과 도구를 연결해 체인을 구성하는 프레임워크 | 복잡한 문제 해결을 위한 추론 모델                              |</span><br><span class="line">| **사용성**        | 사용자가 선택한 모델과 API에 따라 유연성 제공          | 특정 문제 해결을 위해 훈련된 추론 모델, API 기능 일부 제한     |</span><br><span class="line">| **성능**          | 복잡한 연산과 멀티스텝 추론은 제한적                   | 높은 성능의 수학, 코딩, 과학 문제 해결                         |</span><br><span class="line">| **추론 능력**     | 연결된 모델과 도구에 의존하여 추론                     | 체인 오브 싱킹을 사용해 더 높은 수준의 추론 수행               |</span><br><span class="line">| **안정성**        | 연결된 도구나 모델에 따라 안정성 차이                  | 강화 학습을 통한 안전성 높은 모델, 안전성 평가에서 우수한 성과 |</span><br><span class="line">| **확장성**        | 다양한 도구, API와 쉽게 통합 가능                      | 복잡한 추론 문제에 최적화, 범용성은 제한적                     |</span><br><span class="line">| **비용**          | 사용자가 설정한 리소스에 따라 달라짐                   | 고성능 모델 대비 상대적으로 높은 비용                          |</span><br><span class="line">| **코딩 능력**     | 모델에 따라 다르지만 특정 코딩 최적화는 없음           | 복잡한 코딩 문제 해결에 매우 높은 성능 (Codeforces 89% 이상)   |</span><br><span class="line">| **안전성**        | 모델에 따라 다르며, 직접적인 안전성 메커니즘은 없음    | 강화 학습을 통한 안전성 강화, 높은 수준의 안전성 테스트 통과   |</span><br><span class="line">| **적용 분야**     | 광범위한 응용 가능 (상황에 맞게 다양한 모델 선택 가능) | 수학, 과학, 코딩 등 복잡한 문제 해결에 적합                    |</span><br><span class="line">| **웹 브라우징**   | 사용자가 원하는 모델과 브라우징 가능                   | 현재 미지원 (향후 업데이트 예정)                               |</span><br><span class="line">| **파일 업로드**   | 지원 가능 (모델에 따라 다름)                           | 현재 미지원 (향후 업데이트 예정)                               |</span><br><span class="line">| **모델 업데이트** | 다양한 모델의 최신 버전 사용 가능                      | 정기적인 업데이트로 성능 개선 예정                             |</span><br><span class="line"></span><br><span class="line">**종합 비교**:</span><br><span class="line"></span><br><span class="line">- LangChain은 다양한 도구와 모델을 연결하는 유연한 프레임워크로, 다양한 상황에서 응용 가능.</span><br><span class="line">- o1-preview는 수학, 과학, 코딩 등 복잡한 문제 해결에 특화된 모델로, 체인 오브 싱킹을 통해 뛰어난 추론 능력과 안전성을 제공. 다만 일부 일반적인 기능(브라우징, 파일 업로드 등)은 아직 지원되지 않음.</span><br><span class="line"></span><br><span class="line">## 결론</span><br><span class="line"></span><br><span class="line">OpenAI의 o1-preview와 o1-mini 모델은 인공지능 분야에서 중요한 진전을 이뤄냈으며, 복잡한 문제 해결에 새로운 가능성을 열어주고 있습니다. 강화 학습과 &#x27;사고의 사슬&#x27; 방식을 통해 모델의 추론 능력이 크게 향상되었으며, 이는 다양한 분야에서 혁신을 가져올 것으로 기대됩니다.</span><br><span class="line"></span><br><span class="line">향후 업데이트를 통해 더 많은 기능과 개선이 기대되며, 특히 웹 브라우징, 파일 및 이미지 업로드 등의 기능이 추가될 예정입니다. 이는 모델의 활용 범위를 더욱 넓혀줄 것이며, 사용자들은 더욱 다양한 방식으로 AI의 능력을 활용할 수 있게 될 것입니다.</span><br><span class="line"></span><br><span class="line">또한, GPT 시리즈의 새로운 모델도 계속 개발 및 출시될 예정이므로, OpenAI의 AI 기술 발전은 앞으로도 지속될 것으로 보입니다. 사용자들과 개발자들은 이러한 발전을 통해 새로운 기회를 발견하고, 인공지능을 활용한 혁신적인 솔루션을 개발할 수 있을 것입니다.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://blog.google/technology/ai/google-datagemma-ai-llm/</span><br><span class="line">Google</span><br><span class="line"></span><br><span class="line">New Gemma 2 models! 🚀</span><br><span class="line">Google DeepMind</span><br><span class="line">just released 2x Gemma2 fine-tuned optimized Data Commons (DC). DataGemma RAG transfers a user query with context into a DC query. DataGemma RIG includes interleaved DC queries.</span><br><span class="line"></span><br><span class="line">TL;DR;</span><br><span class="line"></span><br><span class="line">🔍 Incorporates public statistical data to answer user queries on data commons.</span><br><span class="line"></span><br><span class="line">📚 Trained on synthetically generated data generated by Gemini 1.5</span><br><span class="line"></span><br><span class="line">🚀 DataGemma RIG improved factual accuracy from 5-17% to 58%.</span><br><span class="line"></span><br><span class="line">🥇 DataGemma RAG 99% for statistical claims when citing directly from the table.</span><br><span class="line"></span><br><span class="line">🔥 DataGemma was trained on TPUv5e, using JAX</span><br><span class="line"></span><br><span class="line">🤗 Available on</span><br><span class="line">Hugging Face</span><br><span class="line">under Gemma license</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Key takeaway: fine-tuning your LLMs improves performance even for Retrieval Augmented Generation (RAG) and tool use.</span><br><span class="line"></span><br><span class="line">DataGemma: Using real-world data to address AI hallucinations</span><br><span class="line">Sep 12, 2024</span><br><span class="line"></span><br><span class="line">DataGemma are the world’s first open models designed to help address the challenges of hallucination by grounding LLMs in the vast, real-world statistical data of Google&#x27;s Data Commons.</span><br><span class="line"></span><br><span class="line">Prem Ramaswami</span><br><span class="line">Prem Ramaswami</span><br><span class="line">Head of Data Commons</span><br><span class="line">2024 Headshot for James Manyika</span><br><span class="line">James Manyika</span><br><span class="line">SVP, Technology &amp; Society</span><br><span class="line">Share</span><br><span class="line">DataGemma Logo</span><br><span class="line">Large language models (LLMs) powering today’s AI innovations are becoming increasingly sophisticated. These models can comb through vast amounts of text and generate summaries, suggest new creative directions and even draft code. However, as impressive as these capabilities are, LLMs sometimes confidently present information that is inaccurate. This phenomenon, known as &quot;hallucination,&quot; is a key challenge in generative AI.</span><br><span class="line"></span><br><span class="line">Today we&#x27;re sharing promising research advancements that tackle this challenge directly, helping reduce hallucination by anchoring LLMs in real-world statistical information. Alongside these research advancements, we are excited to announce DataGemma, the first open models designed to connect LLMs with extensive real-world data drawn from Google&#x27;s Data Commons.</span><br><span class="line"></span><br><span class="line">Data Commons: A vast repository of publicly available, trustworthy data</span><br><span class="line">Data Commons is a publicly available knowledge graph containing over 240 billion rich data points across hundreds of thousands of statistical variables. It sources this public information from trusted organizations like the United Nations (UN), the World Health Organization (WHO), Centers for Disease Control and Prevention (CDC) and Census Bureaus. Combining these datasets into one unified set of tools and AI models empowers policymakers, researchers and organizations seeking accurate insights.</span><br><span class="line"></span><br><span class="line">Think of Data Commons as a vast, constantly expanding database filled with reliable, public information on a wide range of topics, from health and economics to demographics and the environment, which you can interact with in your own words using our AI-powered natural language interface. For example, you can explore which countries in Africa have had the greatest increase in electricity access, how income correlates with diabetes in US counties or your own data-curious query.</span><br><span class="line"></span><br><span class="line">How Data Commons can help tackle hallucination</span><br><span class="line">As generative AI adoption is increasing, we’re aiming to ground those experiences by integrating Data Commons within Gemma, our family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models. These DataGemma models are available to researchers and developers starting now.</span><br><span class="line"></span><br><span class="line">DataGemma will expand the capabilities of Gemma models by harnessing the knowledge of Data Commons to enhance LLM factuality and reasoning using two distinct approaches:</span><br><span class="line"></span><br><span class="line">1. RIG (Retrieval-Interleaved Generation) enhances the capabilities of our language model, Gemma 2, by proactively querying trusted sources and fact-checking against information in Data Commons. When DataGemma is prompted to generate a response, the model is programmed to identify instances of statistical data and retrieve the answer from Data Commons. While the RIG methodology is not new, its specific application within the DataGemma framework is unique.</span><br><span class="line"></span><br><span class="line">Example query: &#x27;&#x27;Has the use of renewables increased in the world?&#x27;&#x27; applying DataGemma RIG methodology leverages Data Commons (DC) for authoritative data.</span><br><span class="line"></span><br><span class="line">2. RAG (Retrieval-Augmented Generation) enables language models to incorporate relevant information beyond their training data, absorb more context, and enable more comprehensive and informative outputs. With DataGemma, this was made possible by leveraging Gemini 1.5 Pro’s long context window. DataGemma retrieves relevant contextual information from Data Commons before the model initiates response generation, thereby minimizing the risk of hallucinations and enhancing the accuracy of responses.</span><br><span class="line"></span><br><span class="line">Example query: &#x27;&#x27;Has the use of renewables increased in the world?&#x27;&#x27; applying DataGemma RAG methodology showcases greater reasoning and inclusion of footnotes.</span><br><span class="line"></span><br><span class="line">Promising results and future directions</span><br><span class="line">Our preliminary findings using RIG and RAG are early, but encouraging. We&#x27;ve observed notable enhancements in the accuracy of our language models when handling numerical facts. This suggests that users will experience fewer hallucinations for use cases across research, decision-making or simply satisfying curiosity. Explore these results in our research paper.</span><br><span class="line"></span><br><span class="line">a black screen reading &quot;What progress has Pakistan made against health goals?&quot; and &quot;Rag answer example&quot;</span><br><span class="line">Illustration of a RAG query and response. Supporting ground truth statistics are referenced as tables served from Data Commons. *Partial response shown for brevity.</span><br><span class="line"></span><br><span class="line">Our research is ongoing, and we’re committed to refining these methodologies further as we scale up this work, subject it to rigorous testing, and ultimately integrate this enhanced functionality into both Gemma and Gemini models, initially through a phased, limited-access approach.</span><br><span class="line"></span><br><span class="line">By sharing our research and making this latest Gemma model variant an “open” model once again, we aspire to facilitate the broader adoption of these Data Commons-led techniques for grounding LLMs in factual data. Making LLMs more reliable and trustworthy is key to ensuring they are indispensable tools for everyone, and building a future where AI empowers people with accurate information, fostering informed decisions, and a deeper understanding of the world around us.</span><br><span class="line"></span><br><span class="line">Researchers and developers can also get started with DataGemma using these quickstart notebooks for both the RIG and RAG approaches. To learn more about how Data Commons and Gemma work together, read our Research post.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://notebooklm.google/</span><br><span class="line">Google</span><br><span class="line">Introducing NotebookLM</span><br><span class="line">Jul 12, 2023</span><br><span class="line"></span><br><span class="line">3 min read</span><br><span class="line"></span><br><span class="line">An AI-first notebook, grounded in your own documents, designed to help you gain insights faster.</span><br><span class="line"></span><br><span class="line">R</span><br><span class="line">Raiza Martin</span><br><span class="line">Product Manager, Google Labs</span><br><span class="line">S</span><br><span class="line">Steven Johnson</span><br><span class="line">Editorial Director, Google Labs</span><br><span class="line">Share</span><br><span class="line">At Google I/O this year we introduced a number of AI-first experiments in development, including Project Tailwind — a new kind of notebook designed to help people learn faster.</span><br><span class="line"></span><br><span class="line">Today we’re beginning to roll out Project Tailwind with its new name: NotebookLM, an experimental offering from Google Labs. It’s our endeavor to reimagine what notetaking software might look like if you designed it from scratch knowing that you would have a powerful language model at its core: hence the LM. It will be immediately available to a small group of users in the U.S. as we continue to refine the product and make it more helpful.</span><br><span class="line"></span><br><span class="line">It’s hard to go from information to insight</span><br><span class="line">We know people are struggling with the rapid growth of information — it&#x27;s everywhere and it’s overwhelming. As we&#x27;ve been talking with students, professors and knowledge workers, one of the biggest challenges is synthesizing facts and ideas from multiple sources. You often have the sources you want, but it&#x27;s time consuming to make the connections.</span><br><span class="line"></span><br><span class="line">We started to explore what we could build that would help people make connections faster in the midst of all this data, especially using sources they care most about.</span><br><span class="line"></span><br><span class="line">an illustration of a screen with NotebookLM, showing boxes and bubbles of synthesized information</span><br><span class="line">NotebookLM automatically generates a document guide to help you get a better understanding of the material</span><br><span class="line"></span><br><span class="line">NotebookLM: an AI notebook for everyone</span><br><span class="line">NotebookLM is an experimental product designed to use the power and promise of language models paired with your existing content to gain critical insights, faster. Think of it as a virtual research assistant that can summarize facts, explain complex ideas, and brainstorm new connections — all based on the sources you select.</span><br><span class="line"></span><br><span class="line">A key difference between NotebookLM and traditional AI chatbots is that NotebookLM lets you “ground” the language model in your notes and sources. Source-grounding effectively creates a personalized AI that’s versed in the information relevant to you. Starting today, you can ground NotebookLM in specific Google Docs that you choose, and we’ll be adding additional formats soon.</span><br><span class="line"></span><br><span class="line">Once you’ve selected your Google Docs, you can do three things:</span><br><span class="line"></span><br><span class="line">Get a summary: When you first add a Google Doc into NotebookLM, it will automatically generate a summary, along with key topics and questions to ask so you get a better understanding of the material.</span><br><span class="line">Ask questions: When you’re ready for a deeper dive, you can ask questions about the documents you’ve uploaded. For example:</span><br><span class="line">A medical student could upload a scientific article about neuroscience and tell NotebookLM to “create a glossary of key terms related to dopamine”</span><br><span class="line">An author working on a biography could upload research notes and ask a question like: “Summarize all the times Houdini and Conan Doyle interacted.”</span><br><span class="line">Generate ideas: NotebookLM isn’t just for Q&amp;A. We’ve found some of its more delightful and useful capabilities are when it’s able to help people come up with creative new ideas. For example:</span><br><span class="line">A content creator could upload their ideas for new videos and ask: “Generate a script for a short video on this topic.”</span><br><span class="line">Or an entrepreneur raising money could upload their pitch and ask: “What questions would potential investors ask?”</span><br><span class="line">While NotebookLM’s source-grounding does seem to reduce the risk of model “hallucinations,” it’s always important to fact-check the AI’s responses against your original source material. When you&#x27;re drawing on multiple sources, we make that fact-checking easy by accompanying each response with citations, showing you the most relevant original quotes from your sources.</span><br><span class="line"></span><br><span class="line">Learning and building, together</span><br><span class="line">NotebookLM is an experimental product, built by a small team in Google Labs.</span><br><span class="line"></span><br><span class="line">Our team has two goals in mind:</span><br><span class="line"></span><br><span class="line">Build a product with our users: We’ll be talking to people and communities often to learn about what’s working well and where the gaps are, with the intent of making NotebookLM a truly useful product.</span><br><span class="line">Roll out this technology responsibly: Getting feedback directly from you is a critical part of developing AI responsibly. We will also use a strict set of safety criteria in alignment with our AI Principles and implement appropriate safeguards before expanding to more users and launching new functionality.</span><br><span class="line">We’ve built NotebookLM such that the model only has access to the source material that you’ve chosen to upload, and your files and dialogue with the AI are not visible to other users. We do not use any of the data collected to train new AI models.</span><br><span class="line"></span><br><span class="line">We hope that in these early days you give NotebookLM a shot. Sign up to the waitlist to try it out!</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://mistral.ai/news/september-24-release/</span><br><span class="line">Mistral AI</span><br><span class="line">9/17/24</span><br><span class="line">Mistral drops improved Small Instruct 22B - Multilingual, 128K context, supports tool use/ function calling! 🔥</span><br><span class="line">Seats comfortably between Mistral NeMo 12B and Mistral Large 123B</span><br><span class="line">&gt; 22B parameters</span><br><span class="line">&gt; Vocabulary to 32768</span><br><span class="line">&gt; Supports function calling</span><br><span class="line">&gt; 128k sequence length</span><br><span class="line">&gt; Model weights (non commercial) on the Hub</span><br><span class="line">&gt; Works out of the box with Transformers 🤗</span><br><span class="line">Congrats Mistral AI for a brilliant open release! GG!</span><br><span class="line">AI in abundance. Big pricing improvements across the board, and a new 12B vision model.</span><br><span class="line"></span><br><span class="line">AI in abundance</span><br><span class="line">Introducing a free API, improved pricing across the board, a new enterprise-grade Mistral Small, and free vision capabilities on le Chat.</span><br><span class="line"></span><br><span class="line">September 17, 2024 Mistral AI team</span><br><span class="line">We’re taking new steps in our mission to bring frontier AI in the hands of everyone. Today, we are releasing:</span><br><span class="line"></span><br><span class="line">A free tier on la Plateforme</span><br><span class="line">A pricing update over our entire family of models</span><br><span class="line">A new, better Mistral Small</span><br><span class="line">Free vision capabilities on le Chat with Pixtral 12B</span><br><span class="line">Free tier on la Plateforme</span><br><span class="line">La Plateforme, the serverless platform to tune and build with Mistral models as API endpoints, now offers a free tier enabling developers to get started with experimentation, evaluation, and prototyping at no cost. Users can seamlessly evolve their endpoints into a commercial tier, and benefit from full data isolation (with a free zero-retention option) and higher rate limits. Users can also choose to deploy our models to different infrastructure: whether using our cloud partners (Azure / AWS / GCP), or choosing to deploy our solutions on their own tenant.</span><br><span class="line"></span><br><span class="line">Reduced prices across the board</span><br><span class="line">We’ve worked hard on making our endpoints faster and more efficient. This enables us to reduce prices across the board, with the following prices</span><br><span class="line"></span><br><span class="line">Model	New price	Old price	Price drop</span><br><span class="line">Mistral Nemo	$0.15 / M input tokens	$0.3 / M tokens	50%</span><br><span class="line">$0.15 / M output tokens	$0.3 / M tokens</span><br><span class="line">Pixtral 12B	$0.15 / M input tokens</span><br><span class="line">$0.15 / M output tokens</span><br><span class="line">Mistral Small	$0.2 / M input tokens	$1 / M input tokens	80%</span><br><span class="line">$0.6 / M output tokens	$3 / M output tokens</span><br><span class="line">Codestral	$0.2 / M input tokens	$1 / M input tokens	80%</span><br><span class="line">$0.6 / M output tokens	$3 / M output tokens</span><br><span class="line">Mistral Large	$2 / M input tokens	$3 / M input tokens	33%</span><br><span class="line">$6 / M output tokens	$9 / M output tokens</span><br><span class="line">This price update makes Mistral Large 2 the most cost-efficient frontier model, make our smaller models extremely cost efficient, and allows customers to realize significantly faster returns on their AI investments. Updated pricing will also reflect on our cloud platform partner offerings (Azure AI Studio, Amazon Bedrock, Google Vertex AI).</span><br><span class="line"></span><br><span class="line">Small gets a big update</span><br><span class="line">We are proud to unveil Mistral Small v24.09, our latest enterprise-grade small model, an upgrade of Mistral Small v24.02. Available under the Mistral Research License, this model offers customers the flexibility to choose a cost-efficient, fast, yet reliable option for use cases such as translation, summarization, sentiment analysis, and other tasks that do not require full-blown general purpose models.</span><br><span class="line"></span><br><span class="line">With 22 billion parameters, Mistral Small v24.09 offers customers a convenient mid-point between Mistral NeMo 12B and Mistral Large 2, providing a cost-effective solution that can be deployed across various platforms and environments. As shown below, the new small model delivers significant improvements in human alignment, reasoning capabilities, and code over the previous model.</span><br><span class="line"></span><br><span class="line">Detailed benchmarks Detailed benchmarks</span><br><span class="line">We’re releasing Mistral Small v24.09 under the MRL license. You may self-deploy it for non-commercial purposes, using e.g. vLLM</span><br><span class="line"></span><br><span class="line">Eye of the Tiger - Pixtral on le Chat</span><br><span class="line">Following our latest Apache model release, Pixtral 12B, a vision-capable model with image understanding capabilities, is now freely available on le Chat. Pixtral 12B is the first open source model to support images of any size without degradation in text-based performance, and you can now use it on le Chat to scan, analyze, search, caption, and better understand your personal or enterprise knowledge files.</span><br><span class="line"></span><br><span class="line">Importantly, the model is available under the Apache 2.0 license, so you can bring visual understanding capabilities to your own environment without having to upload your files to a third-party provider. This is a critical capability for customers that operate with sensitive or proprietary information.</span><br><span class="line"></span><br><span class="line">Do more with less</span><br><span class="line">All the above announcements are now available. Head over to le Chat to try the new image understanding capabilities. To try the free tier of la Plateforme, sign in at console.mistral.ai. To learn more about Mistral Small v24.09, Pixtral 12B, and other Mistral models and pricing, click here.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/QwenLM/Qwen2.5</span><br><span class="line">2024.09.19</span><br><span class="line">Open Source AI/ML is on fire today! 🔥 Multilingual (29) Qwen 2.5 just dropped w/ 128K context too! The 72B rivals Llama 3.1 405B and beats Mistral Large 2 (123B) ⚡</span><br><span class="line">&gt; Trained on an extensive dataset containing up to 18 trillion tokens</span><br><span class="line">&gt; It surpasses its predecessor, Qwen2, with significantly higher scores on MMLU (85+), HumanEval (85+), and MATH (80+) benchmarks</span><br><span class="line">&gt; Excels in instruction following, generating lengthy texts (over 8K tokens), and understanding structured data like tables. It also shows significant progress in generating structured outputs, particularly JSON.</span><br><span class="line">&gt; Supports over 29 languages, including major global languages, and can handle up to 128K tokens, with a text generation capacity of 8K tokens.</span><br><span class="line"></span><br><span class="line">They release specialised models as well:</span><br><span class="line"></span><br><span class="line">1. Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B</span><br><span class="line"></span><br><span class="line">2. Qwen2.5-Coder: 1.5B, 7B, and 32B on the way</span><br><span class="line"></span><br><span class="line">3. Qwen2.5-Math: 1.5B, 7B, and 72B.</span><br><span class="line"></span><br><span class="line">Kudos to Alibaba Qwen team for shipping high quality model</span><br><span class="line"></span><br><span class="line">Qwen2.5: A Party of Foundation Models!</span><br><span class="line">September 19, 2024</span><br><span class="line"> · 9 min · 1739 words · Qwen Team | Translations:</span><br><span class="line">简体中文</span><br><span class="line">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</span><br><span class="line"></span><br><span class="line">Introduction</span><br><span class="line">In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!</span><br><span class="line"></span><br><span class="line">Our latest release features the LLMs Qwen2.5, along with specialized models for coding, Qwen2.5-Coder, and mathematics, Qwen2.5-Math. All open-weight models are dense, decoder-only language models, available in various sizes, including:</span><br><span class="line"></span><br><span class="line">Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B</span><br><span class="line">Qwen2.5-Coder: 1.5B, 7B, and 32B on the way</span><br><span class="line">Qwen2.5-Math: 1.5B, 7B, and 72B.</span><br><span class="line"></span><br><span class="line">All our open-source models, except for the 3B and 72B variants, are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories. In addition to these models, we offer APIs for our flagship language models: Qwen2.5-Plus and Qwen2.5-Turbo through Model Studio, and we encourage you to explore them! Furthermore, we have also open-sourced the Qwen2-VL-72B, which features performance enhancements compared to last month’s release.</span><br><span class="line"></span><br><span class="line">For more details about Qwen2.5, Qwen2.5-Coder, and Qwen2.5-Math, feel free to visit the following links:</span><br><span class="line"></span><br><span class="line">Qwen2.5 LLM Qwen2.5-Coder Qwen2.5-Math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Get ready to unlock a world of possibilities with our extensive lineup of models! We’re excited to share these cutting-edge models with you, and we can’t wait to see the incredible things you’ll achieve with them!</span><br><span class="line"></span><br><span class="line">Takeaways</span><br><span class="line">In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. Qwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. Like Qwen2, the Qwen2.5 language models support up to 128K tokens and can generate up to 8K tokens. They also maintain multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. Below, we provide basic information about the models and details of the supported languages.</span><br><span class="line"></span><br><span class="line">The specialized expert language models, namely Qwen2.5-Coder for coding and Qwen2.5-Math for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on 5.5 trillion tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both Chinese and English and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR).</span><br><span class="line"></span><br><span class="line">Qwen2.5 Specification</span><br><span class="line">Performance</span><br><span class="line">Qwen2.5</span><br><span class="line">To showcase Qwen2.5’s capabilities, we benchmark our largest open-source model, Qwen2.5-72B - a 72B-parameter dense decoder-only language model - against leading open-source models like Llama-3.1-70B, Mistral-Large-V2, and DeepSeek-V2.5. We present comprehensive results from instruction-tuned versions across various benchmarks, evaluating both model capabilities and human preferences.</span><br><span class="line"></span><br><span class="line">Qwen2.5-72B Instruct Performance</span><br><span class="line">Besides the instruction-tuned language models, we figure out that the base language model of our flagship opensource model Qwen2.5-72B reaches top-tier performance even against larger models like Llama-3-405B.</span><br><span class="line"></span><br><span class="line">Qwen2.5-72B Base Model Performance</span><br><span class="line">Furthermore, we benchmark the latest version of our API-based model, Qwen-Plus, against leading proprietary and open-source models, including GPT4-o, Claude-3.5-Sonnet, Llama-3.1-405B, and DeepSeek-V2.5. This comparison showcases Qwen-Plus’s competitive standing in the current landscape of large language models. We show that Qwen-Plus significantly outcompetes DeepSeek-V2.5 and demonstrates competitive performance against Llama-3.1-405B, while still underperforming compared to GPT4-o and Claude-3.5-Sonnet in some aspects. This benchmarking not only highlights Qwen2.5-Plus’s strengths but also identifies areas for future improvement, reinforcing our commitment to continuous enhancement and innovation in the field of large language models.</span><br><span class="line"></span><br><span class="line">Qwen2.5-Plus Instruct Performance</span><br><span class="line">A significant update in Qwen2.5 is the reintroduction of our 14B and 32B models, Qwen2.5-14B and Qwen2.5-32B. These models outperform baseline models of comparable or larger sizes, such as Phi-3.5-MoE-Instruct and Gemma2-27B-IT, across diverse tasks. They achieve an optimal balance between model size and capability, delivering performance that matches or exceeds some larger models. Additionally, our API-based model, Qwen2.5-Turbo, offers highly competitive performance compared to the two open-source models, while providing a cost-effective and rapid service.</span><br><span class="line"></span><br><span class="line">Qwen2.5-32B Instruct Performance</span><br><span class="line">In recent times, there has been a notable shift towards small language models (SLMs). Although SLMs have historically trailed behind their larger counterparts (LLMs), the performance gap is rapidly diminishing. Remarkably, even models with just 3 billion parameters are now delivering highly competitive results. The accompanying figure illustrates a significant trend: newer models achieving scores above 65 in MMLU are increasingly smaller, underscoring the accelerated growth in knowledge density among language models. Notably, our Qwen2.5-3B stands out as a prime example, achieving impressive performance with only around 3 billion parameters, showcasing its efficiency and capability compared to its predecessors.</span><br><span class="line"></span><br><span class="line">Qwen2.5 Small Model</span><br><span class="line">In addition to the notable enhancements in benchmark evaluations, we have refined our post-training methodologies. Our four key updates include support for long text generation of up to 8K tokens, significantly improved comprehension of structured data, more reliable generation of structured outputs, particularly in JSON format, and enhanced performance across diverse system prompts, which facilitates effective role-playing. Check the LLM blog for details about how to leverage these capabilities.</span><br><span class="line"></span><br><span class="line">Qwen2.5-Coder</span><br><span class="line">Since the launch of CodeQwen1.5, we have attracted numerous users who rely on this model for various coding tasks, such as debugging, answering coding-related questions, and providing code suggestions. Our latest iteration, Qwen2.5-Coder, is specifically designed for coding applications. In this section, we present the performance results of Qwen2.5-Coder-7B-Instruct, benchmarked against leading open-source models, including those with significantly larger parameter sizes.</span><br><span class="line"></span><br><span class="line">Qwen2.5-Coder Instruct Performance</span><br><span class="line">We believe that Qwen2.5-Coder is an excellent choice as your personal coding assistant. Despite its smaller size, it outperforms many larger language models across a range of programming languages and tasks, demonstrating its exceptional coding capabilities.</span><br><span class="line"></span><br><span class="line">Qwen2.5-Math</span><br><span class="line">In terms of the math specific language models, we released the first models, Qwen2-Math, last month, and this time, compared to Qwen2-Math, Qwen2.5-Math has been pretrained larger-scale of math related data, including the synthetic data generated by Qwen2-Math. Additionally we extend the support of Chinese this time and we also strengthen its reasoning capabilities by endowing it with the abilities to perform CoT, PoT, and TIR. The general performance of Qwen2.5-Math-72B-Instruct surpasses both Qwen2-Math-72B-Instruct and GPT4-o, and even very small expert model like Qwen2.5-Math-1.5B-Instruct can achieve highly competitive performance against large language models.</span><br><span class="line"></span><br><span class="line">Qwen2.5 Math Performance Across All Sizes</span><br><span class="line">Develop with Qwen2.5</span><br><span class="line">The simplest way to use is through Hugging Face Transfomer as demonstrated in the model card:</span><br><span class="line"></span><br><span class="line">from transformers import AutoModelForCausalLM, AutoTokenizer</span><br><span class="line">model_name = &quot;Qwen/Qwen2.5-7B-Instruct&quot;</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_name,</span><br><span class="line">    torch_dtype=&quot;auto&quot;,</span><br><span class="line">    device_map=&quot;auto&quot;</span><br><span class="line">)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">prompt = &quot;Give me a short introduction to large language model.&quot;</span><br><span class="line">messages = [</span><br><span class="line">    &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt&#125;</span><br><span class="line">]</span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tokenize=False,</span><br><span class="line">    add_generation_prompt=True</span><br><span class="line">)</span><br><span class="line">model_inputs = tokenizer([text], return_tensors=&quot;pt&quot;).to(model.device)</span><br><span class="line">generated_ids = model.generate(</span><br><span class="line">    **model_inputs,</span><br><span class="line">    max_new_tokens=512</span><br><span class="line">)</span><br><span class="line">generated_ids = [</span><br><span class="line">    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)</span><br><span class="line">]</span><br><span class="line">response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]</span><br><span class="line">To use Qwen2.5 with vLLM, running the following command can deploy an OpenAI API compatible service:</span><br><span class="line"></span><br><span class="line">python -m vllm.entrypoints.openai.api_server \</span><br><span class="line">    --model Qwen/Qwen2.5-7B-Instruct</span><br><span class="line">or use vllm serve if you use vllm&gt;=0.5.3. Then you can communicate with Qwen2.5 via curl:</span><br><span class="line"></span><br><span class="line">curl http://localhost:8000/v1/chat/completions -H &quot;Content-Type: application/json&quot; -d &#x27;&#123;</span><br><span class="line">  &quot;model&quot;: &quot;Qwen/Qwen2.5-7B-Instruct&quot;,</span><br><span class="line">  &quot;messages&quot;: [</span><br><span class="line">    &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me something about large language models.&quot;&#125;</span><br><span class="line">  ],</span><br><span class="line">  &quot;temperature&quot;: 0.7,</span><br><span class="line">  &quot;top_p&quot;: 0.8,</span><br><span class="line">  &quot;repetition_penalty&quot;: 1.05,</span><br><span class="line">  &quot;max_tokens&quot;: 512</span><br><span class="line">&#125;&#x27;</span><br><span class="line">Furthermore, Qwen2.5 supports vllm’s built-in tool calling. This functionality requires vllm&gt;=0.6. If you want to enable this functionality, please start vllm’s OpenAI-compatible service with:</span><br><span class="line"></span><br><span class="line">vllm serve Qwen/Qwen2.5-7B-Instruct --enable-auto-tool-choice --tool-call-parser hermes</span><br><span class="line">You can then use it in the same way you use GPT’s tool calling.</span><br><span class="line"></span><br><span class="line">Qwen2.5 also supports Ollama’s tool calling. You can use it by starting Ollama’s OpenAI-compatible service and using it in the same way you use GPT’s tool calling.</span><br><span class="line"></span><br><span class="line">Qwen2.5’s chat template also includes a tool calling template, meaning that you can use Hugging Face transformers’ tool calling support.</span><br><span class="line"></span><br><span class="line">The vllm / Ollama / transformers tool calling support uses a tool calling template inspired by Nous’ Hermes. Historically, Qwen-Agent provided tool calling support using Qwen2’s own tool calling template (which is harder to be integrated with vllm and Ollama), and Qwen2.5 maintains compatibility with Qwen2’s template and Qwen-Agent as well.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Friends of Qwen</span><br><span class="line">💗 Qwen is nothing without its friends! So many thanks to the support of these old buddies and new friends :</span><br><span class="line"></span><br><span class="line">Hugging Face Transformers</span><br><span class="line"></span><br><span class="line">Finetuning: Peft, ChatLearn, Llama-Factory, Axolotl, Firefly, Swift, XTuner, Unsloth, Liger Kernel</span><br><span class="line"></span><br><span class="line">Quantization: AutoGPTQ, AutoAWQ, Neural Compressor</span><br><span class="line"></span><br><span class="line">Deployment: vLLM, SGL, SkyPilot, TensorRT-LLM, OpenVino, TGI, Xinference</span><br><span class="line"></span><br><span class="line">API Platforms: Together, Fireworks, OpenRouter, Sillicon Flow</span><br><span class="line"></span><br><span class="line">Local Run: MLX, Llama.cpp, Ollama, LM Studio, Jan</span><br><span class="line"></span><br><span class="line">Agent and RAG Frameworks: Dify, LlamaIndex, CrewAI</span><br><span class="line"></span><br><span class="line">Evaluation: LMSys, OpenCompass, Open LLM Leaderboard</span><br><span class="line"></span><br><span class="line">Model Training: Arcee AI, Sailor, Dolphin, Openbuddy</span><br><span class="line"></span><br><span class="line">We would like to extend our heartfelt gratitude to the numerous teams and individuals who have contributed to Qwen, even if they haven’t been specifically mentioned. Your support is invaluable, and we warmly invite more friends to join us in this exciting journey. Together, we can enhance collaboration and drive forward the research and development of the open-source AI community, making it stronger and more innovative than ever before.</span><br><span class="line"></span><br><span class="line">What’s Next?</span><br><span class="line">While we are thrilled to launch numerous high-quality models simultaneously, we recognize that significant challenges remain. Our recent releases demonstrate our commitment to developing robust foundation models across language, vision-language, and audio-language domains. However, it is crucial to integrate these different modalities into a single model to enable seamless end-to-end processing of information across all three. Additionally, although we have made strides in enhancing reasoning capabilities through data scaling, we are inspired by the recent advancements in reinforcement learning (e.g., o1) and are dedicated to further improving our models’ reasoning abilities by scaling inference compute. We look forward to introducing you to the next generation of models soon! Stay tuned for more exciting developments!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.microsoft.com/en-us/research/blog/eureka-evaluating-and-understanding-progress-in-ai/</span><br><span class="line">Microsoft</span><br><span class="line">9/18/24</span><br><span class="line">Excited to announce the release of Eureka, an open-source framework for evaluating and understanding large foundation models! 🌟 Eureka offers: 🔍 In-depth analysis of 12 cutting-edge models 🧠 Multimodal &amp; language capability testing beyond single-score reporting and rankings 📈 Insights into model strengths, weaknesses, determinism, and backward compatibility.</span><br><span class="line">Join us in exploring the next AI frontier and contribute to open-source evaluations &amp; insights!</span><br><span class="line">Blog:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In a fast-paced discipline like AI, where every model release promises the next big leap in intelligence, how often have we found ourselves puzzling about questions like:</span><br><span class="line">- Is a fresh model release competitive with the most capable models known to date?</span><br><span class="line">- If most models rank similarly in known leaderboards, are these models comparable, if not the same, in terms of capabilities?</span><br><span class="line">- If they are not the same, what are the strengths and weaknesses of each model?</span><br><span class="line">- Are there capabilities that are fundamental for making AI useful in the real world but also universally challenging for most models?</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">We will be sharing one Eureka insight per day in the following days, but here is a spoiler.</span><br><span class="line">💡 *Eureka Insight Day 0*: In contrast to recent trends in evaluation reports and leaderboards showing absolute rankings and claims for one model or another to be the best, our analysis shows that there is no such best model. Different models have different strengths, but there are models that appear more often than others as best performers for several capabilities. Despite the many observed improvements, it also becomes obvious that current models still struggle with a number of fundamental capabilities including detailed image understanding, benefiting from multimodal input when available rather than fully relying on language, factuality and grounding for information retrieval, and over refusals.</span><br><span class="line">This first version of Eureka is a joint team effort from several amazing AI scientists at #MicrosoftResearch AI Frontiers: Vidhisha Balachandran Jingya Chen Neel J. Hamid Palangi Eduardo Salinas Vibhav Vineet, James Woffinden-Luey, Safoora Yousefi + endless guidance and support from Ahmed Awadallah Ece Kamar Eric Horvitz John Langford Rafah Aboul Hosn Saleema Amershi. Thanks everyone for the shared curiosity and passion for understanding AI in depth and for making this release possible! We welcome contributions, questions, and suggestions from everyone in the community.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/datasets/thesven/Reflective-MAGLLAMA-v0.1.1</span><br><span class="line">thesven</span><br><span class="line">9/13/24</span><br><span class="line"></span><br><span class="line">&lt;thinking&gt;</span><br><span class="line">Magpie is a great method for creating synthetic datasets; we can prompt LLMs with &quot;empty&quot; user inputs.</span><br><span class="line">&lt;thinking&gt;</span><br><span class="line">&lt;reflection&gt;</span><br><span class="line">To enhance reasoning, we add self-reflection tags and scale it using Argilla</span><br><span class="line">distilabel.</span><br><span class="line">&lt;reflection&gt;</span><br><span class="line">&lt;output&gt;</span><br><span class="line">New open dataset with 10k synthetic generated &lt;thinking&gt; samples, available on Hugging Face</span><br><span class="line">https://lnkd.in/eXQq4Zi5</span><br><span class="line">&lt;/output&gt;</span><br><span class="line"></span><br><span class="line">Dataset Card for Reflective-MAGLLAMA-v0.1</span><br><span class="line">Please Use v0.1.1</span><br><span class="line">This dataset has been created with distilabel.</span><br><span class="line"></span><br><span class="line">Overview</span><br><span class="line">Reflective MAGLLAMA is a dataset created using MAGPIE-generated prompts in combination with reflection pattern responses generated by the LLaMa 3.1 70B model. This dataset is tailored specifically to encourage reflection-based analysis through reflection prompting, a technique that enhances deeper thinking and learning. It was curated using the Distilabel framework, and the full pipeline used for building the dataset is available in the associated repository.</span><br><span class="line"></span><br><span class="line">Purpose of the Dataset</span><br><span class="line">The primary goal of the Reflective MAGLLAMA dataset is to provide a robust resource for training, fine-tuning, or evaluating models in the context of reflective thinking and analytical problem-solving. By simulating human-like reflective reasoning, the dataset encourages models to generate more thoughtful and insightful outputs.</span><br><span class="line"></span><br><span class="line">Key Aspects of Reflection Prompting</span><br><span class="line">Purpose and Benefits:</span><br><span class="line"></span><br><span class="line">Reflection prompting is a strategic technique designed to:</span><br><span class="line"></span><br><span class="line">Enhance learning and understanding by encouraging the model (or human) to think beyond the surface level.</span><br><span class="line">Promote critical thinking, fostering an environment where decision-making becomes more thorough and insightful.</span><br><span class="line">Improve the decision-making process by revealing underlying thought patterns and providing space for reflection on multiple perspectives.</span><br><span class="line">Reveal insights into both individual and collective thought processes, offering a clearer understanding of how information is processed and conclusions are reached.</span><br><span class="line">Data Collection Process</span><br><span class="line">This dataset was constructed by feeding MAGPIE-generated prompts to LLaMa 3.1 70B and collecting the reflection-based responses. These responses were subsequently labeled using the Distilabel framework to ensure consistent quality and relevance of the outputs.</span><br><span class="line"></span><br><span class="line">Intended Use</span><br><span class="line">The Reflective MAGLLAMA dataset is intended for:</span><br><span class="line"></span><br><span class="line">Researchers aiming to explore or enhance models in reflective thinking, decision-making, and reasoning tasks.</span><br><span class="line">Developers looking to fine-tune models for applications involving education, critical thinking, or complex problem-solving.</span><br><span class="line">Evaluation purposes, particularly in contexts where a model’s ability to reflect on its own reasoning and improve its decision-making is of value.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/jinaai/reader-lm-1.5b</span><br><span class="line">Jinaai</span><br><span class="line">9/14/24</span><br><span class="line"></span><br><span class="line">𝗘𝘅𝘁𝗿𝗮𝗰𝘁𝗶𝗻𝗴 𝘆𝗼𝘂𝗿 𝗛𝗧𝗠𝗟 𝘄𝗲𝗯𝗽𝗮𝗴𝗲𝘀 𝘁𝗼 𝗺𝗮𝗿𝗸𝗱𝗼𝘄𝗻 𝗶𝘀 𝗻𝗼𝘄 𝗽𝗼𝘀𝘀𝗶𝗯𝗹𝗲 𝗲𝗻𝗱-𝘁𝗼-𝗲𝗻𝗱 𝘄𝗶𝘁𝗵 𝗮 𝘀𝗶𝗺𝗽𝗹𝗲 𝗟𝗟𝗠! 👏</span><br><span class="line">Jina just released Reader-LM, that handles the whole pipeline of extracting markdown from HTML webpages.</span><br><span class="line">A while ago, Jina had released a completely code-based deterministic program to do this extraction, based on some heuristics : e.g., “if the text is in a &lt;p&gt; tag, keep it, but if it’s hidden behind another, remove it”.</span><br><span class="line">🤔 But they received complaints from readers: some found it too detailed, other not enough, depending on the pages.</span><br><span class="line">➡️ So they decided, 𝗺𝗮𝘆𝗯𝗲 𝗵𝗲𝘂𝗿𝗶𝘀𝘁𝗶𝗰𝘀 𝘄𝗲𝗿𝗲 𝗻𝗼𝘁 𝗲𝗻𝗼𝘂𝗴𝗵: 𝗶𝗻𝘀𝘁𝗲𝗮𝗱, 𝘁𝗵𝗲𝘆 𝘁𝗿𝗶𝗲𝗱 𝘁𝗼 𝘁𝗿𝗮𝗶𝗻 𝗮 𝗟𝗟𝗠 𝘁𝗼 𝗱𝗼 𝘁𝗵𝗲 𝗰𝗼𝗺𝗽𝗹𝗲𝘁𝗲 𝗲𝘅𝘁𝗿𝗮𝗰𝘁𝗶𝗼𝗻. This LLM does not need to be very strong,but it should handle a very long context: it’s a challenging, “shallow-but-wide” architecture.</span><br><span class="line">𝗧𝗲𝗰𝗵𝗻𝗶𝗰𝗮𝗹 𝗶𝗻𝘀𝗶𝗴𝗵𝘁𝘀:</span><br><span class="line">2️⃣ models: Reader-LM-0.5B and 1.5B</span><br><span class="line">⚙️ Two stages of training: first, short and simple HTML to get the basics, then ramp up to longer and harder HTML up to 128k tokens</span><br><span class="line">🔎 Use contrastive search for decoding: this empirically reduces “repeating output” issues</span><br><span class="line">➡️ Their models beat much larger models at HTML extraction 🔥</span><br><span class="line">🤗 Weights available on HF (sadly cc-by-nc license):</span><br><span class="line"></span><br><span class="line">Trained by Jina AI.</span><br><span class="line"></span><br><span class="line">Intro</span><br><span class="line">Jina Reader-LM is a series of models that convert HTML content to Markdown content, which is useful for content conversion tasks. The model is trained on a curated collection of HTML content and its corresponding Markdown content.</span><br><span class="line"></span><br><span class="line">Models</span><br><span class="line">Name	Context Length	Download</span><br><span class="line">reader-lm-0.5b	256K	🤗 Hugging Face</span><br><span class="line">reader-lm-1.5b	256K	🤗 Hugging Face</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/datasets/HuggingFaceFV/finevideo</span><br><span class="line">HuggingFace</span><br><span class="line">9/15/24</span><br><span class="line"></span><br><span class="line">🚨 FineVideo is here: 66M words across 43K videos spanning 3.4K hours - CC-BY licensed video understanding dataset 🔥</span><br><span class="line">&gt; It enables advanced video understanding, focusing on mood analysis, storytelling, and media editing in multimodal settings</span><br><span class="line">&gt; Provides detailed annotations on scenes, characters, and audio-visual interactions</span><br><span class="line">&gt; Dataset&#x27;s unique focus on emotional journey and narrative flow enable context-aware video analysis models</span><br><span class="line">Dataset stats:</span><br><span class="line">&gt; 43,751 videos</span><br><span class="line">&gt; An average video length of 4.7 minutes with approximately 3,425 hours of content</span><br><span class="line">&gt; 122 categories w/ 358.61 videos per category on average</span><br><span class="line">100% commercially permissive - use it as you like for whatever! 🐐</span><br><span class="line"></span><br><span class="line">Description</span><br><span class="line">This dataset opens up new frontiers in video understanding, with special focus on the tricky tasks of mood analysis, storytelling and media edition in multimodal settings.</span><br><span class="line"></span><br><span class="line">It&#x27;s packed with detailed notes on scenes, characters, plot twists, and how audio and visuals play together, making it a versatile tool for everything from beefing up pre-trained models to fine-tuning AI for specific video tasks.</span><br><span class="line"></span><br><span class="line">What sets this dataset apart is its focus on capturing the emotional journey and narrative flow of videos - areas where current multimodal datasets fall short - giving researchers the ingredients to cook up more context-savvy video analysis models.</span><br><span class="line"></span><br><span class="line">Dataset Explorer</span><br><span class="line">You can explore the dataset directly from your browser in the FineVideo Space.</span><br><span class="line"></span><br><span class="line">FineVideo Explorer</span><br><span class="line">Dataset Distribution</span><br><span class="line">This comprehensive dataset includes:</span><br><span class="line"></span><br><span class="line">43,751 videos</span><br><span class="line">An average video length of 4.7 minutes with approximately 3,425 hours of content</span><br><span class="line">Content from 122 categories with 358.61 videos per category on average</span><br><span class="line">Content categories</span><br><span class="line">The videos were originally shared on YouTube under Creative Commons Attribution (CC-BY) licenses. FineVideo obtained these videos along with their speech-to-text transcriptions from YouTube-Commons, a project that aggregates audio transcripts of CC-BY licensed YouTube videos.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2409.06857</span><br><span class="line">[Submitted on 10 Sep 2024 (v1), last revised 12 Sep 2024 (this version, v2)]</span><br><span class="line">What is the Role of Small Models in the LLM Era: A Survey</span><br><span class="line">Lihu Chen, Gaël Varoquaux</span><br><span class="line">Large Language Models (LLMs) have made significant progress in advancing artificial general intelligence (AGI), leading to the development of increasingly large models such as GPT-4 and LLaMA-405B. However, scaling up model sizes results in exponentially higher computational costs and energy consumption, making these models impractical for academic researchers and businesses with limited resources. At the same time, Small Models (SMs) are frequently used in practical settings, although their significance is currently underestimated. This raises important questions about the role of small models in the era of LLMs, a topic that has received limited attention in prior research. In this work, we systematically examine the relationship between LLMs and SMs from two key perspectives: Collaboration and Competition. We hope this survey provides valuable insights for practitioners, fostering a deeper understanding of the contribution of small models and promoting more efficient use of computational resources. The code is available at this https URL https://github.com/tigerchen52/awesome_role_of_small_models</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2409.10482</span><br><span class="line">[Submitted on 16 Sep 2024 (v1), last revised 17 Sep 2024 (this version, v2)]</span><br><span class="line">Schrodinger&#x27;s Memory: Large Language Models</span><br><span class="line">Wei Wang, Qing Li</span><br><span class="line">Memory is the foundation of all human activities; without memory, it would be nearly impossible for people to perform any task in daily life. With the development of Large Language Models (LLMs), their language capabilities are becoming increasingly comparable to those of humans. But do LLMs have memory? Based on current performance, LLMs do appear to exhibit memory. So, what is the underlying mechanism of this memory? Previous research has lacked a deep exploration of LLMs&#x27; memory capabilities and the underlying theory. In this paper, we use Universal Approximation Theorem (UAT) to explain the memory mechanism in LLMs. We also conduct experiments to verify the memory capabilities of various LLMs, proposing a new method to assess their abilities based on these memory ability. We argue that LLM memory operates like Schrödinger&#x27;s memory, meaning that it only becomes observable when a specific memory is queried. We can only determine if the model retains a memory based on its output in response to the query; otherwise, it remains indeterminate. Finally, we expand on this concept by comparing the memory capabilities of the human brain and LLMs, highlighting the similarities and differences in their operational mechanisms.</span><br><span class="line"></span><br><span class="line">This paper provides a closer look at the memory capabilities of LLMs.</span><br><span class="line">It uses the Universal Approximation Theorem to explain the memory mechanism of LLMs. It also proposes a new approach to evaluate LLM performance by comparing the memory capacities of different models.</span><br><span class="line">The Transformer architecture &quot;functions as a dynamic fitting UAT model, with a strong ability to adaptively fit inputs. As a result, LLMs can recall entire content based on minimal input information. Since this memory can only be confirmed when triggered by input, we refer to it as ”Schrodinger’s memory.”</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-09-19</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/09/19/2024-9-19-AI-NEWS copy/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 9월 19일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/10/04/2024-10-4-AI-NEWS/" title="2024년 10월 4일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/09/13/openai-GPT-o1/" title="OpenAI의 o1-preview와 o1-mini 모델 출시">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>