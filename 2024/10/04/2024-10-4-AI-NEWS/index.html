<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 10월 4일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="OpenAI는 2024년 10월 1일, 새로운 Realtime API를 공개하며 실시간 음성 대 음성 상호작용을 보다 자연스럽고 빠르게 구현할 수 있는 API를 제공한다고 발표했습니다. 이와 더불어 Vision Fine-Tuning API, Prompt Caching,"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 10월 4일 AI 소식</a></h3></div><div class="post-content"><p>OpenAI는 2024년 10월 1일, 새로운 Realtime API를 공개하며 실시간 음성 대 음성 상호작용을 보다 자연스럽고 빠르게 구현할 수 있는 API를 제공한다고 발표했습니다. 이와 더불어 Vision Fine-Tuning API, Prompt Caching, Model Distillation 등의 혁신적인 기능들을 발표하며 GPT-4o 모델의 성능 향상을 지원합니다. 특히, 최근 10억 달러 규모의 새로운 신용 한도를 확보함으로써 연구 개발과 인프라 확장에 필요한 재정적 유연성을 크게 확보했습니다. Black Forest Labs는 FLUX1.1 [pro]라는 텍스트-이미지 생성 모델을 발표하며, 6배 향상된 속도와 더 높은 이미지 품질을 제공합니다. Google Cloud는 Vertex AI Prompt Optimizer의 퍼블릭 프리뷰를 통해 프롬프트 최적화를 자동으로 수행하는 기술을 소개했습니다. Beijing Academy of Artificial Intelligence(BAAI)는 Emu3라는 새로운 멀티모달 모델을 발표했고, Salesforce는 RAG(검색 강화 생성)를 위한 SFR-RAG 모델을 공개했습니다.</p>
<h3 id="OpenAI-Realtime-API-발표"><a href="#OpenAI-Realtime-API-발표" class="headerlink" title="OpenAI, Realtime API 발표"></a>OpenAI, Realtime API 발표</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/introducing-the-realtime-api/">링크</a>, 2024년 10월 1일</p>
<ul>
<li><strong>Realtime API 소개</strong>: OpenAI는 실시간 음성 대 음성 상호작용을 가능하게 하는 Realtime API를 발표했습니다. 이 API는 ChatGPT의 고급 음성 모드와 유사한 방식으로 작동하며, 저지연 및 멀티모달 음성 경험을 지원합니다.</li>
<li><strong>기술적 구현</strong>: 이 API는 WebSocket 연결을 통해 지속적인 메시지 교환이 가능하도록 설계되었습니다. 특히 API의 핵심은 자연스러운 대화를 위해 스트리밍 오디오 입력과 출력을 지원하며, 사용자가 대화 중간에 끼어드는 것을 감지하고 이에 자동으로 대응합니다.</li>
<li><strong>적용 사례</strong>: Healthify와 Speak 같은 초기 파트너는 Realtime API를 활용해 자연스러운 AI 기반 음성 상호작용을 구현했습니다. 예를 들어 Healthify는 영양 및 피트니스 앱에서 AI 코치와의 대화를 지원하며, Speak는 언어 학습 애플리케이션에서 음성 역할극 기능을 제공합니다.</li>
</ul>
<h3 id="OpenAI-Vision-Fine-Tuning-API-도입"><a href="#OpenAI-Vision-Fine-Tuning-API-도입" class="headerlink" title="OpenAI, Vision Fine-Tuning API 도입"></a>OpenAI, Vision Fine-Tuning API 도입</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/introducing-vision-to-the-fine-tuning-api/">링크</a>, 2024년 10월 1일</p>
<ul>
<li><strong>Vision Fine-Tuning API</strong>: 이 API는 GPT-4o 모델에서 이미지와 텍스트를 동시에 처리할 수 있도록 Fine-Tuning 기능을 확장한 것입니다. 개발자는 이미지 및 텍스트 데이터를 결합해 모델을 미세 조정할 수 있으며, 이를 통해 시각 인식, 객체 탐지, 시각 검색 등에서 성능을 크게 향상시킬 수 있습니다.</li>
<li><strong>기술적 구현</strong>: Fine-Tuning 과정은 텍스트와 유사한 방식으로 진행됩니다. 이미지 데이터를 일정한 포맷으로 준비해 업로드하면, 100장 이상의 이미지로도 성능을 향상시킬 수 있으며, 더 많은 데이터를 사용하면 더 큰 성능 향상을 기대할 수 있습니다.</li>
<li><strong>적용 사례</strong>: Grab은 도로 이미지를 사용해 지도 데이터를 자동화하는데 Vision Fine-Tuning API를 사용해 차선 수 및 제한 속도 표지판의 정확도를 각각 20%, 13% 향상시켰습니다. Automat은 200장의 스크린샷을 사용해 UI 요소 식별 정확도를 272% 향상시켰습니다.</li>
<li><strong>가격</strong>: Vision Fine-Tuning은 GPT-4o 모델에서 제공되며, 2024년 10월 31일까지 무료로 1M 토큰을 제공합니다.</li>
</ul>
<h3 id="OpenAI-Prompt-Caching-기능-도입"><a href="#OpenAI-Prompt-Caching-기능-도입" class="headerlink" title="OpenAI, Prompt Caching 기능 도입"></a>OpenAI, Prompt Caching 기능 도입</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/api-prompt-caching/">링크</a>, 2024년 10월 1일</p>
<ul>
<li><strong>Prompt Caching 소개</strong>: OpenAI는 API 사용 시 자주 사용되는 프롬프트를 캐시함으로써 처리 비용과 지연 시간을 줄이는 Prompt Caching 기능을 도입했습니다. 캐시된 프롬프트는 기본 프롬프트보다 50% 할인된 비용으로 처리됩니다.</li>
<li><strong>기술적 구현</strong>: Prompt Caching은 1,024 토큰 이상의 프롬프트에 대해 자동으로 작동하며, 최대 128개의 토큰을 추가 캐싱할 수 있습니다. 이 기능은 GPT-4o, GPT-4o mini, o1-preview 등의 모델에서 지원됩니다.</li>
<li><strong>활용 사례</strong>: 긴 대화나 동일한 프롬프트를 반복적으로 사용하는 애플리케이션에서 성능을 크게 향상시킬 수 있으며, 프롬프트 캐싱을 통해 비용을 절감할 수 있습니다.</li>
<li><strong>가격</strong>: 캐시된 입력 토큰은 기본 입력 토큰의 50% 가격으로 제공되며, 출력 토큰은 기존과 동일한 가격으로 처리됩니다.</li>
</ul>
<h3 id="OpenAI-Model-Distillation-기능-발표"><a href="#OpenAI-Model-Distillation-기능-발표" class="headerlink" title="OpenAI, Model Distillation 기능 발표"></a>OpenAI, Model Distillation 기능 발표</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/api-model-distillation/">링크</a>, 2024년 10월 1일</p>
<ul>
<li><strong>Model Distillation 소개</strong>: 대형 모델의 출력을 사용해 더 작은 모델을 미세 조정함으로써 비용 효율적인 모델을 만드는 Model Distillation 기능이 발표되었습니다. 이를 통해 더 작은 모델에서도 대형 모델과 유사한 성능을 얻을 수 있습니다.</li>
<li><strong>기술적 구현</strong>: Stored Completions 기능을 통해 대형 모델의 입력과 출력을 저장하고, 이를 학습 데이터로 활용해 더 작은 모델을 Fine-Tuning할 수 있습니다. 또한, Evals 기능을 사용해 모델 성능을 자동으로 평가하고, Fine-Tuning 과정의 성과를 지속적으로 모니터링할 수 있습니다.</li>
<li><strong>적용 사례</strong>: 대규모 모델의 성능을 소형 모델로 전이시켜 비용 효율적인 운영이 가능하며, 특히 GPT-4o mini와 같은 소형 모델에 적용하면 성능 대비 비용을 최적화할 수 있습니다.</li>
</ul>
<h3 id="OpenAI-10억-달러-규모의-새로운-신용-한도-확보"><a href="#OpenAI-10억-달러-규모의-새로운-신용-한도-확보" class="headerlink" title="OpenAI, 10억 달러 규모의 새로운 신용 한도 확보"></a>OpenAI, 10억 달러 규모의 새로운 신용 한도 확보</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/new-credit-facility-enhances-financial-flexibility/">링크</a>, 2024년 10월 3일</p>
<ul>
<li><strong>신용 한도</strong>: OpenAI는 66억 달러의 새로운 투자와 함께 10억 달러 규모의 신용 한도를 확보했습니다. 이를 통해 AI 연구 및 인프라 확장에 필요한 재정적 유연성을 확보할 수 있습니다.</li>
<li><strong>기술적 의미</strong>: OpenAI는 이번 신용 한도를 통해 AI 연구 개발에 필요한 자금을 더욱 유연하게 조달할 수 있으며, 특히 인프라 확장과 새로운 프로젝트 투자에 더욱 적극적으로 나설 수 있게 되었습니다.</li>
</ul>
<h3 id="Black-Forest-Labs-FLUX1-1-pro-출시"><a href="#Black-Forest-Labs-FLUX1-1-pro-출시" class="headerlink" title="Black Forest Labs, FLUX1.1 [pro] 출시"></a>Black Forest Labs, FLUX1.1 [pro] 출시</h3><p><a target="_blank" rel="noopener" href="https://blackforestlabs.ai/announcing-flux-1-1-pro-and-the-bfl-api/">링크</a>, 2024년 10월 2일</p>
<ul>
<li><strong>FLUX1.1 [pro] 소개</strong>: Black Forest Labs는 이전 모델보다 6배 빠르고 향상된 이미지 품질을 제공하는 FLUX1.1 [pro]를 발표했습니다.</li>
<li><strong>기술적 구현</strong>: FLUX1.1 [pro]는 텍스트-이미지 모델로, 이미지 생성 속도를 크게 향상시킴과 동시에 더 나은 품질의 이미지를 생성할 수 있도록 개선되었습니다. 또한, Artificial Analysis 리더보드에서 최고 점수를 기록하며, FLUX 모델의 성능이 입증되었습니다.</li>
<li><strong>API 지원</strong>: FLUX1.1 [pro]는 Black Forest Labs의 API를 통해 제공되며, 이미지를 생성하는 데 $0.04의 비용이 청구됩니다.</li>
</ul>
<h3 id="Google-Cloud-Vertex-AI-Prompt-Optimizer-공개"><a href="#Google-Cloud-Vertex-AI-Prompt-Optimizer-공개" class="headerlink" title="Google Cloud, Vertex AI Prompt Optimizer 공개"></a>Google Cloud, Vertex AI Prompt Optimizer 공개</h3><p><a target="_blank" rel="noopener" href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-vertex-ai-prompt-optimizer?hl=en">링크</a>, 2024년 9월 27일</p>
<ul>
<li><strong>Vertex AI Prompt Optimizer 소개</strong>: Google Cloud는 Vertex AI Prompt Optimizer의 퍼블릭 프리뷰를 발표하며, 이를 통해 프롬프트를 자동으로 최적화하는 기능을 제공합니다.</li>
<li><strong>기술적 구현</strong>: Prompt Optimizer는 LLM 간의 프롬프트를 이전하거나 최적화하는 작업을 자동화합니다. 이 기능은 Google Research의 NeurIPS 2024 논문에서 발표된 자동 프롬프트 최적화(APO) 방법을 기반으로 하며, 최적의 프롬프트를 생성하기 위해 다양한 평가 지표를 활용합니다.</li>
<li><strong>적용 사례</strong>: AdVon Commerce는 이 기능을 사용해 상품 페이지 생성 속도를 100배 향상시켰으며, Augmedix는 의료 문서 생성에서</li>
</ul>
<p>66%에서 86%로 성능을 크게 향상시켰습니다.</p>
<h3 id="Beijing-Academy-of-Artificial-Intelligence-Emu3-발표"><a href="#Beijing-Academy-of-Artificial-Intelligence-Emu3-발표" class="headerlink" title="Beijing Academy of Artificial Intelligence, Emu3 발표"></a>Beijing Academy of Artificial Intelligence, Emu3 발표</h3><p><a target="_blank" rel="noopener" href="https://github.com/baaivision/Emu3">링크</a>, 2024년 9월 30일</p>
<ul>
<li><strong>Emu3 소개</strong>: Beijing Academy of Artificial Intelligence는 텍스트, 이미지, 비디오를 동시에 처리하는 멀티모달 모델 Emu3를 발표했습니다. 이 모델은 간단한 ‘다음 토큰 예측’ 기법을 사용해 모든 데이터 유형을 하나의 시퀀스로 처리합니다.</li>
<li><strong>기술적 구현</strong>: Emu3는 SBER-MoVQGAN이라는 특수 토크나이저를 사용해 이미지를 4,096개의 토큰으로 변환한 후, 이를 시퀀스로 처리합니다. 이 과정에서 이미지나 텍스트를 별도로 처리할 필요 없이 하나의 시퀀스로 통합하여 처리합니다.</li>
<li><strong>적용 사례</strong>: Emu3는 SDXL과 같은 이미지 생성 모델에 필적하는 성능을 보여주며, 비디오 생성에서도 새로운 혁신을 달성했습니다.</li>
</ul>
<h3 id="Salesforce-SFR-RAG-모델-발표"><a href="#Salesforce-SFR-RAG-모델-발표" class="headerlink" title="Salesforce, SFR-RAG 모델 발표"></a>Salesforce, SFR-RAG 모델 발표</h3><p><a target="_blank" rel="noopener" href="https://github.com/SalesforceAIResearch/SFR-RAG">링크</a>, 2024년 9월 30일</p>
<ul>
<li><strong>SFR-RAG 소개</strong>: Salesforce는 검색 강화 생성(RAG)을 위한 SFR-RAG 모델을 발표했습니다. 이 모델은 9B 파라미터를 가진 모델로, 문맥 이해와 다단계 질문 응답에 특화되었습니다.</li>
<li><strong>기술적 구현</strong>: SFR-RAG는 새로운 확장형 대화 템플릿을 사용해 문맥을 제어하며, Multi-Hop Questions와 Reliable Citations 등 다양한 데이터로 학습되었습니다.</li>
<li><strong>적용 사례</strong>: 여러 RAG 벤치마크에서 OpenAI GPT-4o 및 Cohere Command-R+와 유사한 성능을 보여주며, 소규모 모델이 더 큰 모델을 대체할 수 있음을 입증했습니다.</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://openai.com/devday/</span><br><span class="line">OpenAI DevDay</span><br><span class="line">Oct 1, 2024</span><br><span class="line">OpenAI</span><br><span class="line"></span><br><span class="line">San Francisco, London, and Singapore</span><br><span class="line">We&#x27;re bringing developers together to explore new tools and exchange ideas.</span><br><span class="line">Product announcements</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Product</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Introducing the Realtime API</span><br><span class="line"></span><br><span class="line">Introducing the Realtime API &gt; Card image</span><br><span class="line">Product</span><br><span class="line">Oct 1, 2024</span><br><span class="line">Introducing vision to the fine-tuning API</span><br><span class="line"></span><br><span class="line">Introducing vision to the fine-tuning API &gt; Cover image</span><br><span class="line">Product</span><br><span class="line">Oct 1, 2024</span><br><span class="line">Prompt Caching in the API</span><br><span class="line"></span><br><span class="line">Prompt Caching &gt; Media</span><br><span class="line">Product</span><br><span class="line">Oct 1, 2024</span><br><span class="line">Model Distillation in the API</span><br><span class="line"></span><br><span class="line">Model Distillation in the API &gt; Media</span><br><span class="line">Documentation</span><br><span class="line">Explore guides to start building</span><br><span class="line"></span><br><span class="line">Realtime API</span><br><span class="line">Developers can now build fast speech-to-speech experiences into their applications.</span><br><span class="line">Learn more</span><br><span class="line">Vision Fine-Tuning</span><br><span class="line">Developers can now fine-tune GPT-4o with images and text to improve vision capabilities.</span><br><span class="line">Learn more</span><br><span class="line">Prompt Caching</span><br><span class="line">Get automatic discounts on inputs that the model has recently seen.</span><br><span class="line">Learn more</span><br><span class="line">Distillation</span><br><span class="line">Fine-tune a cost-efficient model with the outputs of a large frontier model.</span><br><span class="line">Learn more</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/introducing-the-realtime-api/</span><br><span class="line">OpenAI</span><br><span class="line">October 1, 2024</span><br><span class="line"></span><br><span class="line">Introducing the Realtime API</span><br><span class="line">Developers can now build fast speech-to-speech experiences into their applications</span><br><span class="line"></span><br><span class="line">DALL·E generated impressionist oil painting of undulating orange audio waves creating a harmonious, layered composition</span><br><span class="line">Today, we&#x27;re introducing a public beta of the Realtime API, enabling all paid developers to build low-latency, multimodal experiences in their apps. Similar to ChatGPT’s Advanced Voice Mode, the Realtime API supports natural speech-to-speech conversations using the six preset voices(opens in a new window) already supported in the API.</span><br><span class="line"></span><br><span class="line">We’re also introducing audio input and output in the Chat Completions API(opens in a new window) to support use cases that don’t require the low-latency benefits of the Realtime API. With this update, developers can pass any text or audio inputs into GPT-4o and have the model respond with their choice of text, audio, or both.</span><br><span class="line"></span><br><span class="line">From language apps and educational software to customer support experiences, developers have already been leveraging voice experiences to connect with their users. Now with the Realtime API and soon with audio in the Chat Completions API, developers no longer have to stitch together multiple models to power these experiences. Instead, you can build natural conversational experiences with a single API call.</span><br><span class="line"></span><br><span class="line">How it works</span><br><span class="line">Previously, to create a similar voice assistant experience, developers had to transcribe audio with an automatic speech recognition model like Whisper, pass the text to a text model for inference or reasoning, and then play the model’s output using a text-to-speech(opens in a new window) model. This approach often resulted in loss of emotion, emphasis and accents, plus noticeable latency. With the Chat Completions API, developers can handle the entire process with a single API call, though it remains slower than human conversation. The Realtime API improves this by streaming audio inputs and outputs directly, enabling more natural conversational experiences. It can also handle interruptions automatically, much like Advanced Voice Mode in ChatGPT.</span><br><span class="line"></span><br><span class="line">Under the hood, the Realtime API lets you create a persistent WebSocket connection to exchange messages with GPT-4o. The API supports function calling(opens in a new window), which makes it possible for voice assistants to respond to user requests by triggering actions or pulling in new context. For example, a voice assistant could place an order on behalf of the user or retrieve relevant customer information to personalize its responses.</span><br><span class="line"></span><br><span class="line">Powering customer support agents, language learning assistants, and more</span><br><span class="line">As part of our iterative deployment strategy, we’ve been testing the Realtime API with a handful of partners to gather feedback while we build. A couple of promising early use cases include:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Healthify, a nutrition and fitness coaching app, uses the Realtime API to enable natural conversations with its AI coach Ria, while involving human dietitians when needed for personalized support.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Speak, a language learning app, uses Realtime API to power its role-play feature, encouraging users to practice conversations in a new language.</span><br><span class="line"></span><br><span class="line">Availability &amp; pricing</span><br><span class="line">The Realtime API will begin rolling out today in public beta to all paid developers. Audio capabilities in the Realtime API are powered by the new GPT-4o model gpt-4o-realtime-preview.</span><br><span class="line"></span><br><span class="line">Audio in the Chat Completions API will be released in the coming weeks, as a new model gpt-4o-audio-preview. With gpt-4o-audio-preview, developers can input text or audio into GPT-4o and receive responses in text, audio, or both.</span><br><span class="line"></span><br><span class="line">The Realtime API uses both text tokens and audio tokens. Text input tokens are priced at $5 per 1M and $20 per 1M output tokens. Audio input is priced at $100 per 1M tokens and output is $200 per 1M tokens. This equates to approximately $0.06 per minute of audio input and $0.24 per minute of audio output. Audio in the Chat Completions API will be the same price.</span><br><span class="line"></span><br><span class="line">Safety &amp; privacy</span><br><span class="line">The Realtime API uses multiple layers of safety protections to mitigate the risk of API abuse, including automated monitoring and human review of flagged model inputs and outputs. The Realtime API is built on the same version of GPT-4o that powers Advanced Voice Mode in ChatGPT, which we carefully assessed using both automated and human evaluations, including evaluations according to our Preparedness Framework, detailed in the GPT-4o System Card. The Realtime API also leverages the same audio safety infrastructure we built for Advanced Voice Mode, which our testing shows has helped to reduce the potential for harm.</span><br><span class="line"></span><br><span class="line">It is against our usage policies to repurpose or distribute output from our services to spam, mislead, or otherwise harm others – and we actively monitor for potential abuse. Our policies also require developers to make it clear to their users that they are interacting with AI, unless it&#x27;s obvious from the context.</span><br><span class="line"></span><br><span class="line">Prior to launch, we tested the Realtime API with our external red teaming network and found that the Realtime API didn’t introduce any high-risk gaps not covered by our existing mitigations. As with all API services, the Realtime API is subject to our Enterprise privacy commitments. We do not train our models on the inputs or outputs used in this service without your explicit permission.</span><br><span class="line"></span><br><span class="line">Getting started</span><br><span class="line">Developers can start building with the Realtime API over the coming days in the Playground(opens in a new window), or by using our docs(opens in a new window) and the reference client(opens in a new window).</span><br><span class="line"></span><br><span class="line">We’ve also worked with LiveKit(opens in a new window) and Agora(opens in a new window) to create client libraries of audio components like echo cancellation, reconnection, and sound isolation, and Twilio(opens in a new window) to integrate the Realtime API with Twilio’s Voice APIs(opens in a new window) which enable developers to seamlessly build, deploy and connect AI virtual agents to customers via voice calls.</span><br><span class="line"></span><br><span class="line">What’s next</span><br><span class="line">As we work towards general availability, we’re actively collecting feedback to improve the Realtime API. Some of the capabilities we plan to introduce include:</span><br><span class="line"></span><br><span class="line">More modalities: To start, the Realtime API will support voice, and we plan to add additional modalities like vision and video over time.</span><br><span class="line"></span><br><span class="line">Increased rate limits: Today the API is rate limited to approximately 100 simultaneous sessions for Tier 5 developers, with lower limits for Tiers 1-4. We will increase these limits over time to support larger deployments.</span><br><span class="line"></span><br><span class="line">Official SDK support: We will integrate support for Realtime API into the OpenAI Python and Node.js SDKs.</span><br><span class="line"></span><br><span class="line">Prompt Caching: We will add support for Prompt Caching(opens in a new window) so previous conversation turns can be reprocessed at a discount.</span><br><span class="line"></span><br><span class="line">Expanded model support: The Realtime API will also support GPT-4o mini in upcoming versions of that model.</span><br><span class="line"></span><br><span class="line">We&#x27;re looking forward to seeing how developers leverage these new capabilities to create compelling new audio experiences for their users across a variety of use cases from education to translation, customer service, accessibility and beyond.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/introducing-vision-to-the-fine-tuning-api/</span><br><span class="line">OpenAI</span><br><span class="line">October 1, 2024</span><br><span class="line"></span><br><span class="line">Introducing vision to the fine-tuning API</span><br><span class="line">Developers can now fine-tune GPT-4o with images and text to improve vision capabilities</span><br><span class="line"></span><br><span class="line">DALL·E generated impressionist oil painting of overlapping translucent rectangles blending in sky blue hues</span><br><span class="line">Today, we’re introducing vision fine-tuning(opens in a new window) on GPT-4o1, making it possible to fine-tune with images, in addition to text. Developers can customize the model to have stronger image understanding capabilities which enables applications like enhanced visual search functionality, improved object detection for autonomous vehicles or smart cities, and more accurate medical image analysis.</span><br><span class="line"></span><br><span class="line">Since we first introduced fine-tuning on GPT-4o, hundreds of thousands of developers have customized our models using text-only datasets to improve performance on specific tasks. However, for many cases, fine-tuning models on text alone doesn’t provide the performance boost expected.</span><br><span class="line"></span><br><span class="line">How it works</span><br><span class="line">Vision fine-tuning follows a similar process to fine-tuning with text—developers can prepare their image datasets to follow the proper format(opens in a new window) and then upload that dataset to our platform. They can improve the performance of GPT-4o for vision tasks with as few as 100 images, and drive even higher performance with larger volumes of text and image data.</span><br><span class="line"></span><br><span class="line">json</span><br><span class="line"></span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">1</span><br><span class="line">&#123;</span><br><span class="line">2</span><br><span class="line">  &quot;messages&quot;: [</span><br><span class="line">3</span><br><span class="line">    &#123; &quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are an assistant that identifies uncommon cheeses.&quot; &#125;,</span><br><span class="line">4</span><br><span class="line">    &#123; &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is this cheese?&quot; &#125;,</span><br><span class="line">5</span><br><span class="line">    &#123; &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [</span><br><span class="line">6</span><br><span class="line">        &#123;</span><br><span class="line">7</span><br><span class="line">          &quot;type&quot;: &quot;image_url&quot;,</span><br><span class="line">8</span><br><span class="line">          &quot;image_url&quot;: &#123;</span><br><span class="line">9</span><br><span class="line">            &quot;url&quot;: &quot;https://upload.wikimedia.org/wikipedia/commons/3/36/Danbo_Cheese.jpg&quot;</span><br><span class="line">10</span><br><span class="line">          &#125;</span><br><span class="line">11</span><br><span class="line">        &#125;</span><br><span class="line">12</span><br><span class="line">      ]</span><br><span class="line">13</span><br><span class="line">    &#125;,</span><br><span class="line">14</span><br><span class="line">    &#123; &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Danbo&quot; &#125;</span><br><span class="line">15</span><br><span class="line">  ]</span><br><span class="line">16</span><br><span class="line">&#125;</span><br><span class="line">17</span><br><span class="line"></span><br><span class="line">We collaborated with a small group of trusted partners to better understand the real-world applications of vision fine-tuning. We’re excited to showcase a few examples of what they built.</span><br><span class="line"></span><br><span class="line">Grab improves image detection and understanding on the road</span><br><span class="line">Grab(opens in a new window), a leading food delivery and rideshare company, turns street-level imagery collected from their drivers into mapping data used to power GrabMaps(opens in a new window), a mapping service enabling all of their Southeast Asia operations. Using vision fine-tuning with only 100 examples, Grab taught GPT-4o to correctly localize traffic signs and count lane dividers to refine their mapping data. As a result, Grab was able to improve lane count accuracy by 20% and speed limit sign localization by 13% over a base GPT-4o model, enabling them to better automate their mapping operations from a previously manual process.</span><br><span class="line"></span><br><span class="line">Example of a speed limit sign tagged successfully by a vision fine-tuned GPT-4o model that was incorrectly tagged by the GPT-4o base model.</span><br><span class="line">Example of a speed limit sign tagged successfully by a vision fine-tuned GPT-4o model that was incorrectly tagged by the GPT-4o base model.</span><br><span class="line"></span><br><span class="line">Automat improves success rate of desktop bots automating business processes</span><br><span class="line">Automat(opens in a new window), an enterprise automation company, builds desktop and web agents that process documents and take UI-based actions to automate business processes. With vision fine-tuning and a dataset of screenshots, Automat trained GPT-4o to locate UI elements on a screen given a natural language description, improving the success rate of their RPA agent from 16.60% to 61.67%—a 272% uplift in performance compared to base GPT-4o. Additionally, Automat trained GPT-4o on just 200 images of unstructured insurance documents to achieve a 7% lift in F1 score on information extraction tasks.</span><br><span class="line"></span><br><span class="line">Example of a desktop bot successfully identifying the center of UI elements via vision fine-tuning with website screenshots.</span><br><span class="line"></span><br><span class="line">Coframe enhances quality of digital content creation</span><br><span class="line">Coframe(opens in a new window) is building an AI growth engineering assistant that helps businesses continuously create and test variations of their websites and UIs to optimize business metrics. A key part of this task is autonomously generating new branded sections of a website, based on the rest of the website. Coframe tasked GPT-4o with generating code for the next section of a website based on images and existing code. By fine-tuning GPT-4o with images and code, they improved the model’s ability to generate websites with consistent visual style and correct layout by 26% compared to base GPT-4o.</span><br><span class="line"></span><br><span class="line">Existing website that the model is meant to match.</span><br><span class="line">Existing website that the model is meant to match.</span><br><span class="line"></span><br><span class="line">Output with the GPT-4o base model</span><br><span class="line">Output with the GPT-4o base model.</span><br><span class="line"></span><br><span class="line">Output with GPT-4o fine-tuned with vision and text, more closely matching the style of the page.</span><br><span class="line">Output with GPT-4o fine-tuned with vision and text, more closely matching the style of the page.</span><br><span class="line"></span><br><span class="line">Safety &amp; privacy</span><br><span class="line">We continuously run automated safety evals on fine-tuned models and monitor usage to ensure applications adhere to our usage policies. As with all API services, vision fine-tuning is subject to our Enterprise privacy commitments. Fine-tuned models remain entirely under your control, with full ownership of your business data. We do not train our models on the inputs or outputs used in this service without your explicit permission.</span><br><span class="line"></span><br><span class="line">Availability &amp; pricing</span><br><span class="line">Vision fine-tuning capabilities are available today for all developers on paid usage tiers(opens in a new window). These capabilities are supported on the latest GPT-4o model snapshot, gpt-4o-2024-08-06. Developers can extend existing fine-tuning training data for images using the same format as our Chat endpoints(opens in a new window).</span><br><span class="line"></span><br><span class="line">We’re offering 1M training tokens per day for free through October 31, 2024 to fine-tune GPT-4o with images. After October 31, 2024, GPT-4o fine-tuning training will cost $25 per 1M tokens and inference will cost $3.75 per 1M input tokens and $15 per 1M output tokens. Image inputs are first tokenized based on image size, and then priced at the same per-token rate as text inputs. More details can be found on the API Pricing page.</span><br><span class="line"></span><br><span class="line">To get started, visit the fine-tuning dashboard(opens in a new window), click ‘create’ and select gpt-4o-2024-08-06 from the base model drop-down. To learn how to fine-tune GPT-4o with images, visit our docs(opens in a new window).</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/api-prompt-caching/</span><br><span class="line">OpenAI</span><br><span class="line"></span><br><span class="line">October 1, 2024</span><br><span class="line"></span><br><span class="line">Prompt Caching in the API</span><br><span class="line">Offering automatic discounts on inputs that the model has recently seen</span><br><span class="line"></span><br><span class="line">DALL·E generated impressionist oil painting of layered light green columns interwoven with parallel emerald streams, forming a harmonious and repetitive tapestry.</span><br><span class="line">Many developers use the same context repeatedly across multiple API calls when building AI applications, like when making edits to a codebase or having long, multi-turn conversations with a chatbot. Today, we’re introducing Prompt Caching, allowing developers to reduce costs and latency. By reusing recently seen input tokens, developers can get a 50% discount and faster prompt processing times.</span><br><span class="line"></span><br><span class="line">Prompt Caching Availability &amp; Pricing</span><br><span class="line">Starting today, Prompt Caching is automatically applied on the latest versions of GPT-4o, GPT-4o mini, o1-preview and o1-mini, as well as fine-tuned versions of those models. Cached prompts are offered at a discount compared to uncached prompts.</span><br><span class="line"></span><br><span class="line">Here&#x27;s an overview of pricing:</span><br><span class="line"></span><br><span class="line">Uncached Input Tokens</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Cached Input Tokens</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Output Tokens</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">GPT-4o</span><br><span class="line"></span><br><span class="line">gpt-4o-2024-08-06</span><br><span class="line"></span><br><span class="line">$2.50</span><br><span class="line"></span><br><span class="line">$1.25</span><br><span class="line"></span><br><span class="line">$10.00</span><br><span class="line"></span><br><span class="line">GPT-4o fine-tuning</span><br><span class="line"></span><br><span class="line">$3.75</span><br><span class="line"></span><br><span class="line">$1.875</span><br><span class="line"></span><br><span class="line">$15.00</span><br><span class="line"></span><br><span class="line">GPT-4o mini</span><br><span class="line"></span><br><span class="line">gpt-4o-mini-2024-07-18</span><br><span class="line"></span><br><span class="line">$0.15</span><br><span class="line"></span><br><span class="line">$0.075</span><br><span class="line"></span><br><span class="line">$0.60</span><br><span class="line"></span><br><span class="line">GPT-4o mini fine-tuning</span><br><span class="line"></span><br><span class="line">$0.30</span><br><span class="line"></span><br><span class="line">$0.15</span><br><span class="line"></span><br><span class="line">$1.20</span><br><span class="line"></span><br><span class="line">o1</span><br><span class="line"></span><br><span class="line">o1-preview</span><br><span class="line"></span><br><span class="line">$15.00</span><br><span class="line"></span><br><span class="line">$7.50</span><br><span class="line"></span><br><span class="line">$60.00</span><br><span class="line"></span><br><span class="line">o1 mini</span><br><span class="line"></span><br><span class="line">$3.00</span><br><span class="line"></span><br><span class="line">$1.50</span><br><span class="line"></span><br><span class="line">$12.00</span><br><span class="line"></span><br><span class="line">Monitoring Cache Usage</span><br><span class="line">API calls to supported models will automatically benefit from Prompt Caching on prompts longer than 1,024 tokens. The API caches the longest prefix of a prompt that has been previously computed, starting at 1,024 tokens and increasing in 128-token increments. If you reuse prompts with common prefixes, we will automatically apply the Prompt Caching discount without requiring you to make any changes to your API integration.</span><br><span class="line"></span><br><span class="line">Requests using Prompt Caching have a cached_tokens value within the usage field in the API response:</span><br><span class="line"></span><br><span class="line">javascript</span><br><span class="line"></span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">1</span><br><span class="line">usage: &#123;</span><br><span class="line">2</span><br><span class="line">  total_tokens: 2306,</span><br><span class="line">3</span><br><span class="line">  prompt_tokens: 2006,</span><br><span class="line">4</span><br><span class="line">  completion_tokens: 300,</span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">6</span><br><span class="line">  prompt_tokens_details: &#123;</span><br><span class="line">7</span><br><span class="line">    cached_tokens: 1920,</span><br><span class="line">8</span><br><span class="line">    audio_tokens: 0,</span><br><span class="line">9</span><br><span class="line">  &#125;,</span><br><span class="line">10</span><br><span class="line">  completion_tokens_details: &#123;</span><br><span class="line">11</span><br><span class="line">    reasoning_tokens: 0,</span><br><span class="line">12</span><br><span class="line">    audio_tokens: 0,</span><br><span class="line">13</span><br><span class="line">  &#125;</span><br><span class="line">14</span><br><span class="line">&#125;</span><br><span class="line">Caches are typically cleared after 5-10 minutes of inactivity and are always removed within one hour of the cache&#x27;s last use. As with all API services, Prompt Caching is subject to our Enterprise privacy commitments. Prompt caches are not shared between organizations.</span><br><span class="line"></span><br><span class="line">Prompt Caching is one of a variety of tools for developers to scale their applications in production while balancing performance, cost and latency. For more information, check out the Prompt Caching docs(opens in a new window).</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/api-model-distillation/</span><br><span class="line">OpenAI</span><br><span class="line">October 1, 2024</span><br><span class="line"></span><br><span class="line">Model Distillation in the API</span><br><span class="line">Fine-tune a cost-efficient model with the outputs of a large frontier model–all on the OpenAI platform</span><br><span class="line"></span><br><span class="line">DALL·E generated impressionist oil painting of stacked light green rectangles serving as columns, with emerald streams weaving repeatedly through each tier</span><br><span class="line">We’re introducing a new Model Distillation offering to provide developers with an integrated workflow to manage the entire distillation pipeline directly within the OpenAI platform. This lets developers easily use the outputs of frontier models like o1-preview and GPT-4o to fine-tune and improve the performance of more cost-efficient models like GPT-4o mini.</span><br><span class="line"></span><br><span class="line">Model distillation involves fine-tuning smaller, cost-efficient models using outputs from more capable models, allowing them to match the performance of advanced models on specific tasks at a much lower cost. Until now, distillation has been a multi-step, error-prone process, which required developers to manually orchestrate multiple operations across disconnected tools, from generating datasets to fine-tuning models and measuring performance improvements. Since distillation is inherently iterative, developers needed to repeatedly run each step, adding significant effort and complexity.</span><br><span class="line"></span><br><span class="line">Our new Model Distillation suite includes:</span><br><span class="line"></span><br><span class="line">Stored Completions(opens in a new window): Developers can now easily generate datasets for distillation by automatically capturing and storing the input-output pairs generated by one of our models, like GPT-4o or o1-preview through our API. With Stored Completions, you can easily build datasets with your production data to evaluate and fine-tune models. Developers can review this integration guide(opens in a new window) to learn how to opt-in to storing completions.</span><br><span class="line"></span><br><span class="line">Evals(opens in a new window) (beta): Developers can now create and run custom evaluations on our platform to measure model performance on specific tasks. Instead of manually creating evaluation scripts and integrating disparate logging tools, Evals provides an integrated way to measure model performance. You can either use data from Stored Completions or upload existing datasets to set up your evaluations. Evals can also be used independently of fine-tuning to quantitatively evaluate model performance for your use cases.</span><br><span class="line"></span><br><span class="line">Fine-tuning(opens in a new window): Stored Completions and Evals are fully integrated with our existing fine-tuning offering. This means that developers can use datasets created with Stored Completions in their fine-tuning jobs and run evaluations on fine-tuned models using Evals, all within our platform.</span><br><span class="line"></span><br><span class="line">How to use Model Distillation</span><br><span class="line">First, create an evaluation(opens in a new window) to measure the performance of the model you want to distill into, which in this example will be GPT-4o mini. This evaluation will be used to continuously test the distilled model’s performance, to help you decide whether to deploy it.</span><br><span class="line"></span><br><span class="line">Example of evaluation used for model distillation</span><br><span class="line">Next, use Stored Completions to create a distillation dataset of real-world examples using GPT-4o’s outputs for the tasks on which you want to fine-tune GPT-4o mini. You can do this by setting the ‘store:true’ flag in the Chat Completions API to automatically store these input-output pairs without any latency impact. These stored completions can be reviewed, filtered, and tagged to create high-quality datasets for fine-tuning or evaluation.</span><br><span class="line"></span><br><span class="line">python</span><br><span class="line"></span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">1</span><br><span class="line">response = client.chat.completions.create(</span><br><span class="line">2</span><br><span class="line">  model=&quot;gpt-4o&quot;,</span><br><span class="line">3</span><br><span class="line">  messages=[</span><br><span class="line">4</span><br><span class="line">    &#123;</span><br><span class="line">5</span><br><span class="line">      &quot;role&quot;: &quot;user&quot;,</span><br><span class="line">6</span><br><span class="line">      &quot;content&quot;: [</span><br><span class="line">7</span><br><span class="line">        &#123;</span><br><span class="line">8</span><br><span class="line">          &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">9</span><br><span class="line">          &quot;text&quot;: &quot;what&#x27;s the capital of the USA?&quot;</span><br><span class="line">10</span><br><span class="line">        &#125;</span><br><span class="line">11</span><br><span class="line">      ]</span><br><span class="line">12</span><br><span class="line">    &#125;</span><br><span class="line">13</span><br><span class="line">  ],</span><br><span class="line">14</span><br><span class="line">  store=True,</span><br><span class="line">15</span><br><span class="line">  metadata=&#123;&quot;username&quot;: &quot;user123&quot;, &quot;user_id&quot;: &quot;123&quot;, &quot;session_id&quot;: &quot;123&quot;&#125;</span><br><span class="line">Finally, use this dataset to fine-tune GPT-4o mini. Stored Completions can be used as a training file when creating a fine-tuned model. Once the model is fine-tuned, you can go back to Evals to test whether the fine-tuned GPT-4o mini model meets your performance criteria when compared to GPT-4o.</span><br><span class="line"></span><br><span class="line">Fine-tuning is an iterative process. If the initial results aren’t satisfactory, you may need to refine the dataset, adjust the training parameters, or capture more specific examples where the model is underperforming. The goal is to incrementally improve the distilled model until it performs well enough for production use.</span><br><span class="line"></span><br><span class="line">Availability &amp; Pricing</span><br><span class="line">Model Distillation is available today to all developers and can be used to distill any of our models, including GPT-4o and o1-preview. As a reminder, we’re also offering 2M free training tokens per day on GPT-4o mini and 1M free training tokens per day on GPT-4o until October 31 to help developers get started with distillation. Beyond that limit, the cost of training and running a distilled model is the same as our standard fine-tuning prices, which you can find on our API pricing page.</span><br><span class="line"></span><br><span class="line">Stored Completions is available for free. Evals, which are available in beta, are charged at standard model prices based on the tokens used. Through the end of the year, developers can run evaluations for free (up to 7 per week) when they opt in(opens in a new window) to share their Evals with OpenAI. Evals shared with us will be used to help us improve and evaluate our future models.</span><br><span class="line"></span><br><span class="line">For more information, check out our Model Distillation docs(opens in a new window).</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/introducing-canvas/</span><br><span class="line">OpenAI</span><br><span class="line">October 3, 2024</span><br><span class="line"></span><br><span class="line">Introducing canvas</span><br><span class="line">A new way of working with ChatGPT to write and code</span><br><span class="line"></span><br><span class="line">The image shows a vertical toolbar featuring five icons arranged in a column on a soft pastel background. The third icon from the top, depicting an open book, is highlighted with a label next to it reading &quot;Reading Level.&quot;</span><br><span class="line">We’re introducing canvas, a new interface for working with ChatGPT on writing and coding projects that go beyond simple chat. Canvas opens in a separate window, allowing you and ChatGPT to collaborate on a project. This early beta introduces a new way of working together—not just through conversation, but by creating and refining ideas side by side.</span><br><span class="line"></span><br><span class="line">Canvas was built with GPT-4o and can be manually selected in the model picker while in beta. Starting today we’re rolling out canvas to ChatGPT Plus and Team users globally. Enterprise and Edu users will get access next week. We also plan to make canvas available to all ChatGPT Free users when it’s out of beta.</span><br><span class="line"></span><br><span class="line">Better collaboration with ChatGPT</span><br><span class="line">People use ChatGPT every day for help with writing and code. Although the chat interface is easy to use and works well for many tasks, it’s limited when you want to work on projects that require editing and revisions. Canvas offers a new interface for this kind of work.</span><br><span class="line"></span><br><span class="line">With canvas, ChatGPT can better understand the context of what you’re trying to accomplish. You can highlight specific sections to indicate exactly what you want ChatGPT to focus on. Like a copy editor or code reviewer, it can give inline feedback and suggestions with the entire project in mind.</span><br><span class="line"></span><br><span class="line">You control the project in canvas. You can directly edit text or code. There’s a menu of shortcuts for you to ask ChatGPT to adjust writing length, debug your code, and quickly perform other useful actions. You can also restore previous versions of your work by using the back button in canvas.</span><br><span class="line"></span><br><span class="line">Canvas opens automatically when ChatGPT detects a scenario in which it could be helpful. You can also include “use canvas” in your prompt to open canvas and use it to work on an existing project.</span><br><span class="line"></span><br><span class="line">Writing shortcuts include:</span><br><span class="line"></span><br><span class="line">Suggest edits: ChatGPT offers inline suggestions and feedback.</span><br><span class="line"></span><br><span class="line">Adjust the length: Edits the document length to be shorter or longer.</span><br><span class="line"></span><br><span class="line">Change reading level: Adjusts the reading level, from Kindergarten to Graduate School.</span><br><span class="line"></span><br><span class="line">Add final polish: Checks for grammar, clarity, and consistency.</span><br><span class="line"></span><br><span class="line">Add emojis: Adds relevant emojis for emphasis and color.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Coding in canvas</span><br><span class="line">Coding is an iterative process, and it can be hard to follow all the revisions to your code in chat. Canvas makes it easier to track and understand ChatGPT’s changes, and we plan to continue improving transparency into these kinds of edits.</span><br><span class="line"></span><br><span class="line">Coding shortcuts include:</span><br><span class="line"></span><br><span class="line">Review code: ChatGPT provides inline suggestions to improve your code.</span><br><span class="line"></span><br><span class="line">Add logs: Inserts print statements to help you debug and understand your code.</span><br><span class="line"></span><br><span class="line">Add comments: Adds comments to the code to make it easier to understand.</span><br><span class="line"></span><br><span class="line">Fix bugs: Detects and rewrites problematic code to resolve errors.</span><br><span class="line"></span><br><span class="line">Port to a language: Translates your code into JavaScript, TypeScript, Python, Java, C++, or PHP.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Training the model to become a collaborator</span><br><span class="line">We trained GPT-4o to collaborate as a creative partner. The model knows when to open a canvas, make targeted edits, and fully rewrite. It also understands broader context to provide precise feedback and suggestions.</span><br><span class="line"></span><br><span class="line">To support this, our research team developed the following core behaviors:</span><br><span class="line"></span><br><span class="line">Triggering the canvas for writing and coding</span><br><span class="line"></span><br><span class="line">Generating diverse content types</span><br><span class="line"></span><br><span class="line">Making targeted edits</span><br><span class="line"></span><br><span class="line">Rewriting documents</span><br><span class="line"></span><br><span class="line">Providing inline critique</span><br><span class="line"></span><br><span class="line">We measured progress with over 20 automated internal evaluations. We used novel synthetic data generation techniques, such as distilling outputs from OpenAI o1-preview, to post-train the model for its core behaviors. This approach allowed us to rapidly address writing quality and new user interactions, all without relying on human-generated data.</span><br><span class="line"></span><br><span class="line">A key challenge was defining when to trigger a canvas. We taught the model to open a canvas for prompts like “Write a blog post about the history of coffee beans” while avoiding over-triggering for general Q&amp;A tasks like “Help me cook a new recipe for dinner.” For writing tasks, we prioritized improving “correct triggers” (at the expense of “correct non-triggers”), reaching 83% compared to a baseline zero-shot GPT-4o with prompted instructions.</span><br><span class="line"></span><br><span class="line">It’s worth noting that the quality of such baselines is highly sensitive to the specific prompt used. With different prompts, the baseline may still perform poorly but in a different manner—for instance, by being evenly inaccurate across coding and writing tasks, resulting in a different distribution of errors and alternative forms of suboptimal performance. For coding, we intentionally biased the model against triggering to avoid disrupting our power users. We&#x27;ll continue refining this based on user feedback.</span><br><span class="line"></span><br><span class="line">Canvas Decision Boundary Trigger - Writing &amp; Coding</span><br><span class="line">Prompted GPT-4o</span><br><span class="line">GPT-4o with canvas</span><br><span class="line">Writing (correct trigger canvas)</span><br><span class="line">Writing (correct don’t trigger canvas)</span><br><span class="line">Coding (correct don’t trigger canvas)</span><br><span class="line">Coding (correct trigger canvas)</span><br><span class="line">0.00</span><br><span class="line">0.10</span><br><span class="line">0.20</span><br><span class="line">0.30</span><br><span class="line">0.40</span><br><span class="line">0.50</span><br><span class="line">0.60</span><br><span class="line">0.70</span><br><span class="line">0.80</span><br><span class="line">0.90</span><br><span class="line">1.00</span><br><span class="line">For writing and coding tasks, we improved correctly triggering the canvas decision boundary, reaching 83% and 94% respectively compared to a baseline zero-shot GPT-4o with prompted instructions.</span><br><span class="line"></span><br><span class="line">A second challenge involved tuning the model&#x27;s editing behavior once the canvas was triggered—specifically deciding when to make a targeted edit versus rewriting the entire content. We trained the model to perform targeted edits when users explicitly select text through the interface, otherwise favoring rewrites. This behavior continues to evolve as we refine the model.</span><br><span class="line"></span><br><span class="line">Canvas Edits Boundary - Writing &amp; Coding</span><br><span class="line">Prompted GPT-4o</span><br><span class="line">GPT-4o with canvas</span><br><span class="line">Canvas full rewrite</span><br><span class="line">Canvas targeted edit</span><br><span class="line">0.0</span><br><span class="line">0.1</span><br><span class="line">0.2</span><br><span class="line">0.3</span><br><span class="line">0.4</span><br><span class="line">0.5</span><br><span class="line">0.6</span><br><span class="line">0.7</span><br><span class="line">0.8</span><br><span class="line">0.9</span><br><span class="line">1.0</span><br><span class="line">For writing and coding tasks, we prioritized improving canvas targeted edits. GPT-4o with canvas performs better than a baseline prompted GPT-4o by 18%.</span><br><span class="line"></span><br><span class="line">Finally, training the model to generate high-quality comments required careful iteration. Unlike the first two cases, which are easily adaptable to automated evaluation with thorough manual reviews, measuring quality in an automated way is particularly challenging. Therefore, we used human evaluations to assess comment quality and accuracy. Our integrated canvas model outperforms the zero-shot GPT-4o with prompted instructions by 30% in accuracy and 16% in quality, showing that synthetic training significantly enhances response quality and behavior compared to zero-shot prompting with detailed instructions.</span><br><span class="line"></span><br><span class="line">Canvas Suggested Comments</span><br><span class="line">Prompted GPT-4o</span><br><span class="line">GPT-4o with canvas</span><br><span class="line">Comment’s Triggering Correctness</span><br><span class="line">Quality of Suggested Comments</span><br><span class="line">0.0</span><br><span class="line">0.1</span><br><span class="line">0.2</span><br><span class="line">0.3</span><br><span class="line">0.4</span><br><span class="line">0.5</span><br><span class="line">0.6</span><br><span class="line">0.7</span><br><span class="line">0.8</span><br><span class="line">0.9</span><br><span class="line">1.0</span><br><span class="line">Human evaluations assessed canvas comment quality and accuracy functionality. Our canvas model outperforms the zero-shot GPT-4o with prompted instructions by 30% in accuracy and 16% in quality.</span><br><span class="line"></span><br><span class="line">What’s next</span><br><span class="line">Making AI more useful and accessible requires rethinking how we interact with it. Canvas is a new approach and the first major update to ChatGPT’s visual interface since we launched two years ago.</span><br><span class="line"></span><br><span class="line">Canvas is in early beta, and we plan to rapidly improve its capabilities.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/datasets/openai/MMMLU</span><br><span class="line">Open Dataset release by OpenAI! 👀 OpenAI just released a Multilingual Massive Multitask Language Understanding (MMMLU) dataset on Hugging Face to more easily evaluate multilingual LLMs!</span><br><span class="line">🌍 MMLU test set available in 14 languages, including Arabic, German, Spanish, French,….</span><br><span class="line">🧠 Covers 57 categories from elementary to advanced professional subjects</span><br><span class="line">🎓 translated by professional human translators</span><br><span class="line">🔬 Evaluates AI models&#x27; general knowledge across diverse cultures, used in openai/simple-evals</span><br><span class="line">🤔 License unclear</span><br><span class="line">Multilingual Massive Multitask Language Understanding (MMMLU)</span><br><span class="line">The MMLU is a widely recognized benchmark of general knowledge attained by AI models. It covers a broad range of topics from 57 different categories, covering elementary-level knowledge up to advanced professional subjects like law, physics, history, and computer science.</span><br><span class="line"></span><br><span class="line">We translated the MMLU’s test set into 14 languages using professional human translators. Relying on human translators for this evaluation increases confidence in the accuracy of the translations, especially for low-resource languages like Yoruba. We are publishing the professional human translations and the code we use to run the evaluations.</span><br><span class="line"></span><br><span class="line">This effort reflects our commitment to improving the multilingual capabilities of AI models, ensuring they perform accurately across languages, particularly for underrepresented communities. By prioritizing high-quality translations, we aim to make AI technology more inclusive and effective for users worldwide.</span><br><span class="line"></span><br><span class="line">Locales</span><br><span class="line">MMMLU contains the MMLU test set translated into the following locales:</span><br><span class="line"></span><br><span class="line">AR_XY (Arabic)</span><br><span class="line">BN_BD (Bengali)</span><br><span class="line">DE_DE (German)</span><br><span class="line">ES_LA (Spanish)</span><br><span class="line">FR_FR (French)</span><br><span class="line">HI_IN (Hindi)</span><br><span class="line">ID_ID (Indonesian)</span><br><span class="line">IT_IT (Italian)</span><br><span class="line">JA_JP (Japanese)</span><br><span class="line">KO_KR (Korean)</span><br><span class="line">PT_BR (Brazilian Portuguese)</span><br><span class="line">SW_KE (Swahili)</span><br><span class="line">YO_NG (Yoruba)</span><br><span class="line">ZH_CH (Simplified Chinese)</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/new-credit-facility-enhances-financial-flexibility/</span><br><span class="line">October 3, 2024</span><br><span class="line"></span><br><span class="line">New Credit Facility Enhances Financial Flexibility</span><br><span class="line">In addition to securing $6.6 billion in new funding from leading investors, we have established a new $4 billion credit facility with JPMorgan Chase, Citi, Goldman Sachs, Morgan Stanley, Santander, Wells Fargo, SMBC, UBS, and HSBC. This is a revolving credit facility that is undrawn at closing.</span><br><span class="line"></span><br><span class="line">This means we now have access to over $10 billion in liquidity, which gives us the flexibility to invest in new initiatives and operate with full agility as we scale. It also reaffirms our partnership with an exceptional group of financial institutions, many of whom are also OpenAI customers.</span><br><span class="line"></span><br><span class="line">“This credit facility further strengthens our balance sheet and provides flexibility to seize future growth opportunities,” said Sarah Friar, CFO of OpenAI. “We are proud to have the strongest banks and investors in the world supporting us.”</span><br><span class="line"></span><br><span class="line">The support of our investors and financial partners enables us to continue investing in groundbreaking research and products that bring AI to the world, expand our infrastructure to meet growing demand, and attract top talent from around the world. As we embark on this next phase, we remain focused on delivering helpful tools that contribute to people’s lives.</span><br><span class="line"></span><br><span class="line">October 2, 2024</span><br><span class="line"></span><br><span class="line">New funding to scale the benefits of AI</span><br><span class="line">DALL·E generated impressionist oil painting of gentle waves reflecting a cascade of glowing words, gentle shimmering ethereal waves</span><br><span class="line">We are making progress on our mission to ensure that artificial general intelligence benefits all of humanity. Every week, over 250 million people around the world use ChatGPT to enhance their work, creativity, and learning. Across industries, businesses are improving productivity and operations, and developers are leveraging our platform to create a new generation of applications. And we’re only getting started.</span><br><span class="line"></span><br><span class="line">We’ve raised $6.6B in new funding at a $157B post-money valuation to accelerate progress on our mission. The new funding will allow us to double down on our leadership in frontier AI research, increase compute capacity, and continue building tools that help people solve hard problems.</span><br><span class="line"></span><br><span class="line">We aim to make advanced intelligence a widely accessible resource. We’re grateful to our investors for their trust in us, and we look forward to working with our partners, developers, and the broader community to shape an AI-powered ecosystem and future that benefits everyone. By collaborating with key partners, including the U.S. and allied governments, we can unlock this technology&#x27;s full potential.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://blackforestlabs.ai/announcing-flux-1-1-pro-and-the-bfl-api/</span><br><span class="line">FLUX just got a big update! Black Forest Labs released FLUX1.1 [pro] an improved text-to-image model! 🤯</span><br><span class="line">FLUX1.1 [pro]:</span><br><span class="line">💡 6x faster than its predecessor and boasts enhanced image quality.</span><br><span class="line">🏆 Achieves highest Elo score on artificialanalysis leaderboard</span><br><span class="line">🛠️ Supported via BFL API with $0.04/img.</span><br><span class="line">🤝 Available also through together.ai, Replicate, fal.ai, and Freepik</span><br><span class="line">🤔 No mentions if it comes to Grok and X (prev. Twitter)</span><br><span class="line">😕 Only [pro] updates, no weights for [dev] or [schnell]</span><br><span class="line"></span><br><span class="line">Announcing FLUX1.1 [pro] and the BFL API</span><br><span class="line">Oct 2, 2024</span><br><span class="line">—</span><br><span class="line"></span><br><span class="line">by</span><br><span class="line"></span><br><span class="line">BlackForestLabs</span><br><span class="line">in Uncategorized</span><br><span class="line">Today, we release FLUX1.1 [pro], our most advanced and efficient model yet, alongside the general availability of the beta BFL API. This release marks a significant step forward in our mission to empower creators, developers, and enterprises with scalable, state-of-the-art generative technology.</span><br><span class="line"></span><br><span class="line">FLUX1.1 [pro]: Faster &amp; Better</span><br><span class="line"></span><br><span class="line">FLUX1.1 [pro] provides six times faster generation than its predecessor FLUX.1 [pro] while also improving image quality, prompt adherence, and diversity. At the same time, we updated FLUX.1 [pro] to generate the same output as before, but two times faster.</span><br><span class="line"></span><br><span class="line">Superior Speed and Efficiency: Faster generation times and reduced latency, enabling more efficient workflows. FLUX1.1 [pro] provides an ideal tradeoff between image quality and inference speed. FLUX1.1 [pro] is three times faster than the currently available FLUX.1 [pro].</span><br><span class="line">Improved Performance: FLUX1.1 [pro] has been introduced and tested under the codename “blueberry” into the Artificial Analysis image arena (https://artificialanalysis.ai/text-to-image), a popular benchmark for text-to-image models. It surpasses all other models on the leaderboard, achieving the highest overall Elo score.</span><br><span class="line"></span><br><span class="line">All metrics from artificialanalysis.ai as of Oct 1, 2024.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">All metrics from artificialanalysis.ai as of Oct 1, 2024, except FLUX.1 inference speeds (benchmarked internally).</span><br><span class="line"></span><br><span class="line">Fast High-res coming soon: FLUX1.1 [pro], natively set up for fast ultra high-resolution generation coming soon to the API. Generate up to 2k images without sacrificing any of the prompt following.</span><br><span class="line">We are excited to announce that FLUX1.1 [pro] will also be available through Together.ai, Replicate, fal.ai, and Freepik.</span><br><span class="line"></span><br><span class="line">Building with the BFL API</span><br><span class="line">Our new beta BFL API brings FLUX’s capabilities directly to developers and businesses looking to integrate state-of-the-art image generation into their own applications. Our API stands out with key advantages over competitors:</span><br><span class="line"></span><br><span class="line">Advanced Customization: Tailor the API outputs to your specific needs with customization options on model choice, image resolution, and content moderation.</span><br><span class="line">Scalability: Seamlessly scale your applications, whether you are building small projects or enterprise-level applications.</span><br><span class="line">Competitive pricing: The API offers superior image quality at a lower cost. The pricing for our FLUX.1 model suite is as follows:</span><br><span class="line">FLUX.1 [dev]: 2.5 cts/img</span><br><span class="line">FLUX.1 [pro]: 5 cts/img</span><br><span class="line">FLUX1.1 [pro]: 4 cts/img</span><br><span class="line">Get started with the BFL API today at:  docs.bfl.ml.</span><br><span class="line"></span><br><span class="line">We are eager to see the creative applications that will emerge from users of the BFL API.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://cloud.google.com/blog/products/ai-machine-learning/announcing-vertex-ai-prompt-optimizer?hl=en</span><br><span class="line">AI &amp; Machine Learning</span><br><span class="line">Announcing Public Preview of Vertex AI Prompt Optimizer</span><br><span class="line">September 27, 2024</span><br><span class="line">George Lee</span><br><span class="line">Product Manager, Cloud AI Research</span><br><span class="line"></span><br><span class="line">Ivan Nardini</span><br><span class="line">Developer Relations Engineer</span><br><span class="line"></span><br><span class="line">Google Cloud Summit Series</span><br><span class="line">Discover the latest in AI, Security, Workspace, App Dev, &amp; more.</span><br><span class="line"></span><br><span class="line">Register</span><br><span class="line">Prompt design and engineering stands out as one of the most approachable methods to drive meaningful output from a Large Language Model (LLM). ​​However, prompting large language models can feel like navigating a complex maze. You must experiment with various combinations of instructions and examples to achieve the desired output. Moreover, even if you find the ideal prompt template, there is no guarantee that it will continue to deliver optimal results for a different LLM.</span><br><span class="line"></span><br><span class="line">Migrating or translating prompts from one LLM to another is challenging because different language models behave differently. Simply reusing prompts is ineffective, so users need an intelligent prompt optimizer to generate useful outputs.</span><br><span class="line"></span><br><span class="line">To help mitigate the &quot;prompt fatigue&quot; experienced by users while they build LLM-based applications, we are announcing Vertex AI Prompt Optimizer in Public Preview.</span><br><span class="line"></span><br><span class="line">What is Vertex AI Prompt Optimizer?</span><br><span class="line">Vertex AI Prompt Optimizer helps you find the best prompt (instruction and demonstrations) for any preferred model on Vertex AI. It is based on Google Research’s publication (accepted by NeurIPS 2024) on automatic prompt optimization (APO) methods, and employs an iterative LLM-based optimization algorithm where the optimizer model [responsible for generating paraphrased instructions] and evaluator model [responsible for evaluating the selected instruction and demonstration] work together to generate and evaluate candidate prompts. Prompt Optimizer subsequently selects the best instructions and demonstrations based on the evaluation metrics the user wants to optimize against. Instructions include the system instruction, context, and task of your prompt template. Demonstrations are the few-shot examples you provide in your prompt to elicit a specific style or tone from the model response.</span><br><span class="line"></span><br><span class="line">With just a few labeled examples and configured optimization settings, Vertex AI Prompt Optimizer finds the best prompt (instruction and demonstrations) for the target model and removes the need for manually optimizing existing prompts every time for a new LLM. You can now easily craft a new prompt for a particular task or translate a prompt from one model to another model on Vertex AI. Here are the key characteristics:</span><br><span class="line"></span><br><span class="line">Easy optimization: Quickly optimize prompts for any target Google model, including migration and translation of prompts from any source model.</span><br><span class="line"></span><br><span class="line">Versatile task handling: Accommodates any text-based task (such as question and answering, summarization, classification, and entity extraction) and expand support for multimodal tasks is coming soon.</span><br><span class="line"></span><br><span class="line">Comprehensive evaluation: Supports a wide array of evaluation metrics, including model-based, computation-based, and custom metrics, to ensure optimal prompt performance against the metrics you care about.</span><br><span class="line"></span><br><span class="line">Flexible and customizable: Tailor the optimization process and latency with advanced settings and utilize various notebook versions to suit your expertise level and needs.</span><br><span class="line"></span><br><span class="line">https://storage.googleapis.com/gweb-cloudblog-publish/original_images/1_dBmMX6I.gif</span><br><span class="line">Why use Vertex AI Prompt Optimizer?</span><br><span class="line">Data-driven optimization: Many existing prompt optimization tools focus on tailoring your prompts to your preferred style and tone and oftentimes still require human verification. However, Vertex AI Prompt Optimizer goes beyond this by optimizing your prompts based on specific evaluation metrics, ensuring the best possible performance for your target model.</span><br><span class="line"></span><br><span class="line">Built for Gemini: If you’re using Gemini, Vertex AI Prompt Optimizer is designed to keep Gemini’s underlying characteristics in mind. It is specifically designed to adapt to the unique attributes of the Gemini and other Google models. This tailored approach allows you to unlock the full potential of Gemini and achieve superior results.</span><br><span class="line"></span><br><span class="line">Getting started with Vertex AI Prompt Optimizer</span><br><span class="line">To start using Vertex AI Prompt Optimizer, you can use the Colab notebook available in the Google Cloud Generative AI repository on Github which contains sample code and notebooks for Generative AI on Google Cloud. Refer to the UI version for basic settings and the SDK version for more advanced settings. More versions of the notebook to support custom metrics and multimodal input will be added in the coming weeks. You can also access it via the Vertex AI Studio console. Look for entry points in the console that indicate “prompt optimizer” or “optimizer your prompt further” (refer to screencasts below).</span><br><span class="line"></span><br><span class="line">To either optimize or translate prompts using Vertex AI Prompt Optimizer, follow these steps:</span><br><span class="line"></span><br><span class="line">Configure your prompt template</span><br><span class="line"></span><br><span class="line">Input your data (labeled examples)</span><br><span class="line"></span><br><span class="line">Configure your optimization settings (target model, evaluation metrics, etc.)</span><br><span class="line"></span><br><span class="line">Run the optimization job</span><br><span class="line"></span><br><span class="line">Inspect the results</span><br><span class="line"></span><br><span class="line">Vertex AI Prompt Optimizer supports any Google models and evaluation metrics supported by the Generative AI Evaluation Service.</span><br><span class="line"></span><br><span class="line">Entry points from Vertex AI Studio to Vertex AI Prompt Optimizer Colab Enterprise Notebook</span><br><span class="line"></span><br><span class="line">A. The Saved prompts page will include a new Prompt optimizer button.</span><br><span class="line"></span><br><span class="line">https://storage.googleapis.com/gweb-cloudblog-publish/original_images/2_oH9kBwq.gif</span><br><span class="line">B. The Prompt assist dialog pop-up will include a new Optimize your prompt further button.</span><br><span class="line"></span><br><span class="line">https://storage.googleapis.com/gweb-cloudblog-publish/original_images/3_PfCSq4n.gif</span><br><span class="line">How AdVon Commerce and Augmedix enhanced Gemini prompts using Vertex AI Prompt Optimizer</span><br><span class="line">AdVon Commerce - a digital commerce platform - partnered with Google Cloud to create quality content at scale for retailers using tailored AI solutions. AdVon Commerce utilizes LLMs to generate accurate and engaging product page content at scale, which incorporates the right keywords and represents the products accurately. When optimizing retail pages, there’s a lot of missing or incorrect data to work through. Creating shopper-first content means accurately completing missing product attributes that are essential for product searchability and customer journey.</span><br><span class="line"></span><br><span class="line">Vertex AI Prompt Optimizer streamlined the creation and refinement of AI prompts, resulting in higher accuracy and relevance. AdVon Commerce observed a 10% increase in attribute accuracy and are able to maintain their commitment to high-quality content while significantly reducing the time they spend on human verification, saving substantial costs. Coupled with Gemini Flash, they have received impressive results with a reduction in incorrect specs and better quality in product page content. For example, AdVon Commerce has recently helped one of the largest global retailers by using Vertex AI Prompt Optimizer and Gemini 1.5 Flash to automate the process of creating populating product attributes for hundreds of millions of items. This resulted in a 100x increase in productivity for the retailer, as it would have taken 100 times longer if they had tried to do that manually.</span><br><span class="line"></span><br><span class="line">Vlad Barshai, Chief Technology Officer at AdVon Commerce, stated “Vertex AI Prompt Optimizer allows us to optimize our prompts for Gemini Flash with 10% incremental improvements for problematic Product Attributes and PDP (Product Detail Page) for retailer listings, significantly surpassing results from all other leading AI models on the market. With Vertex AI Prompt Optimizer, we save time on human verification, allowing us to enrich millions of products in a loop where we optimize prompts and generate AI attributes and PDP content at scale. Coupled with a solid human-in-the-loop process, Vertex AI Prompt Optimizer will help us produce high quality enrichment every time.”</span><br><span class="line"></span><br><span class="line">Augmedix – a leader in ambient AI medical documentation and data solutions that has generated over 10 million medical notes to date – partnered with Google Cloud to enhance medical documentation for healthcare providers. Augmedix utilizes LLMs to improve efficiency and accuracy in capturing patient interactions, reduce clinician administrative burdens, and ultimately, improve patient care. Augmedix adopted a hybrid approach – models are fine-tuned and inputs are prompt-tuned. For many parts of note generation, fine-tuning is best, and basic prompts work well. In other parts of their system, where there may be hundreds of rules that instruct the LLM, prompt-tuning these rules is optimal.</span><br><span class="line"></span><br><span class="line">Augmedix employed Vertex AI Prompt Optimizer to enhance medical note generation from doctor-patient conversations. The feature improved LLM output quality scores from 66% to 86%. In addition, with Vertex AI Prompt Optimizer, Augmedix can test prompt variations quickly, allowing for faster iteration and optimization. The optimized prompts run in 6 seconds, compared to 20 seconds for prompts without Vertex AI Prompt Optimizer.</span><br><span class="line"></span><br><span class="line">Ian Shakil, Founder, Director, and Chief Strategy Officer at Augmedix, stated, “Our partnership with Google Cloud AI enabled us to pioneer the frontier of the LLM wave. With MedLM and Gemini, we have achieved revolutionary advancements, driving cutting-edge innovation in the digital health space. This collaboration empowers us to deliver higher quality outputs, reduced turnaround times, and a richer feature set.”</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/baaivision/Emu3</span><br><span class="line">9/30/24</span><br><span class="line">Emu3: Next-token prediction conquers multimodal tasks 🔥</span><br><span class="line">This is the most important research in months: we’re now very close to having a single architecture to handle all modalities. The folks at Beijing Academy of Artificial Intelligence(BAAI) just released Emu3, a single model that handles text, images, and videos all at once.</span><br><span class="line">𝗪𝗵𝗮𝘁&#x27;𝘀 𝘁𝗵𝗲 𝗯𝗶𝗴 𝗱𝗲𝗮𝗹?</span><br><span class="line">🌟 Emu3 is the first model to truly unify all these different types of data (text, images, video) using just one simple trick: predicting the next token.</span><br><span class="line">And it’s only 8B, but really strong:</span><br><span class="line">🖼️ For image generation, it&#x27;s matching the best specialized models out there, like SDXL.</span><br><span class="line">👁️ In vision tasks, it&#x27;s outperforming top models like LLaVA-1.6-7B, which is a big deal for a model that wasn&#x27;t specifically designed for this.</span><br><span class="line">🎬 It&#x27;s the first to nail video generation without using complicated diffusion techniques.</span><br><span class="line">𝗛𝗼𝘄 𝗱𝗼𝗲𝘀 𝗶𝘁 𝘄𝗼𝗿𝗸?</span><br><span class="line">🧩 Emu3 uses a special tokenizer (SBER-MoVQGAN) to turn images and video clips into sequences of 4,096 tokens.</span><br><span class="line">🔗 Then, it treats everything - text, images, and videos - as one long series of tokens to predict.</span><br><span class="line">🔮 During training, it just tries to guess the next token, whether that&#x27;s a word, part of an image, or a video frame.</span><br><span class="line">To build their multimodal dataset, the team:</span><br><span class="line">🎨 Tossed out low-res and ugly images using an aesthetic model (LAION-AI aesthetic filter) to score pictures and videos</span><br><span class="line">✍️ Got GPT-4V to write captions for 1 million images, same for frames of video, to finally get a truly multimodal dataset.</span><br><span class="line">𝗖𝗮𝘃𝗲𝗮𝘁𝘀 𝗼𝗻 𝘁𝗵𝗲 𝗿𝗲𝘀𝘂𝗹𝘁𝘀:</span><br><span class="line">👉 In image generation, Emu3 beats SDXL, but it’s also much bigger (8B vs 3.5B). It would be more difficult to beat the real diffusion GOAT FLUX-dev.</span><br><span class="line">👉 In vision, authors also don’t show a comparison against all the current SOTA models like Qwen-VL or Pixtral.</span><br><span class="line">On the positive side, this approach is exciting because it&#x27;s simple (next token prediction) and scalable(handles all sorts of data).</span><br><span class="line">On the other hand, it&#x27;s once again the Bitter lesson: no matter your architecture, just throw good data and compute, and you’ll get the best model out there. 😬</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/SalesforceAIResearch/SFR-RAG</span><br><span class="line">Salesforce</span><br><span class="line">9/30/24</span><br><span class="line">SFR-RAG</span><br><span class="line">We introduce SFR-RAG, a 9B LLM trained with an emphasis in contextual comprehension and retrieval augmented generation (RAG) use case.</span><br><span class="line"></span><br><span class="line">ContextualBench</span><br><span class="line">We also introduce ContextualBench - a compilation of 7 popular contextual question answering benchmarks to evaluate LLMs in RAG application.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Reasoning will be the future of AI, but RAG is the present! How can we improve the faithfulness of LLMs? Salesforce released ContextualBench, a leaderboard and evaluation framework combining multiple academic RAG benchmarks such as HotpotQA, and announced SFR-RAG 9B, a fine-tuned LLM for RAG that matches Cohere Command-R+ (104B) and OpenAI GPT-4o. 👀</span><br><span class="line">SFR-RAG is an unreleased 9B LLM fine-tuned and DPOed using a new extended chat template with Thought and Observation roles to control retrieval. It is trained on Multi-Hop Questions, Reliable Citations, and Hallucination Minimization by identifying unanswerable questions. There are no details on whether the model or dataset will be released. ❌</span><br><span class="line">This research emphasizes how important task- and domain-specific models can be for Companies, especially for search or RAG. It should not be the first step, but gathering data, cleaning it, and fine-tuning can achieve superior results with smaller models, which might be easier and cheaper to run.  ✅</span><br><span class="line">P.S. Academic benchmarks are not representative of real-world use cases, but they give us a first understanding.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>기술적으로 최대한 자세하게 적어. 12개의 기사가 있고 하나도 빼먹지 말고 적어.</p>
</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-10-04</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/10/04/2024-10-4-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 10월 4일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/10/15/2024-10-25-AI-NEWS/" title="2024년 10월 15일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/09/19/2024-9-19-AI-NEWS%20copy/" title="2024년 9월 19일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>