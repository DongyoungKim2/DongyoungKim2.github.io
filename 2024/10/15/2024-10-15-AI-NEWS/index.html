<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 10월 15일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="오늘 AI 기술 분야에서는 2024년 노벨 물리학상 및 화학상 수상 소식과 함께, OpenAI, Meta, Rhymes AI, Microsoft 등의 주요 기업들이 최첨단 AI 모델과 기술을 발표하며 중요한 발전을 이루었습니다. OpenAI는 머신러닝 엔지니어링 평가를"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 10월 15일 AI 소식</a></h3></div><div class="post-content"><p>오늘 AI 기술 분야에서는 2024년 노벨 물리학상 및 화학상 수상 소식과 함께, OpenAI, Meta, Rhymes AI, Microsoft 등의 주요 기업들이 최첨단 AI 모델과 기술을 발표하며 중요한 발전을 이루었습니다. OpenAI는 머신러닝 엔지니어링 평가를 위한 새로운 벤치마크와 다중 에이전트 시스템 프레임워크를 공개했으며, Meta는 텍스트에서 고해상도 비디오를 생성하는 Movie Gen 모델을 발표했습니다. Rhymes AI는 새로운 멀티모달 Mixture-of-Experts(MoE) 모델을 공개하여 경쟁력을 강화했으며, Microsoft는 새로운 차별화된 어텐션 메커니즘을 기반으로 하는 Transformer 모델을 선보여 주목받고 있습니다. 이 외에도 다양한 AI 모델들이 텍스트-비디오 생성, 단백질 구조 예측, 텍스트-음성 변환 등에서 혁신적인 진전을 이루며, AI 연구와 실무 적용에서 커다란 영향을 미치고 있습니다.</p>
<h3 id="The-Royal-Swedish-Academy-of-Sciences-2024년-노벨-물리학상-발표"><a href="#The-Royal-Swedish-Academy-of-Sciences-2024년-노벨-물리학상-발표" class="headerlink" title="The Royal Swedish Academy of Sciences, 2024년 노벨 물리학상 발표"></a>The Royal Swedish Academy of Sciences, 2024년 노벨 물리학상 발표</h3><p><a target="_blank" rel="noopener" href="https://www.nobelprize.org/prizes/physics/2024/press-release/">링크</a>, 2024년 10월 8일</p>
<ul>
<li>2024년 물리학상은 인공 신경망을 기반으로 한 기계 학습 기술의 발전에 크게 기여한 John J. Hopfield와 Geoffrey E. Hinton에게 수여</li>
<li><strong>Hopfield</strong>는 연상 기억(Associative Memory) 모델인 <strong>Hopfield Network</strong>를 제안, 물리학에서 스핀 시스템을 설명하는 에너지를 기반으로 이미지와 패턴을 저장하고 복원하는 방식을 제시</li>
<li><strong>Hinton</strong>은 Hopfield 네트워크를 기반으로 한 <strong>Boltzmann Machine</strong>을 통해 패턴 인식 기술을 발전시켰으며, 이는 통계 물리학의 도구를 활용하여 자율적인 데이터 학습과 패턴 인식에 중요한 기여를 함</li>
<li>두 연구자는 1980년대 이후 신경망을 이용한 인공 지능 기술의 토대를 마련하였으며, 이를 통해 현재의 심층 학습(Deep Learning)과 머신러닝 모델의 발전을 가능하게 함</li>
</ul>
<h3 id="The-Royal-Swedish-Academy-of-Sciences-2024년-노벨-화학상-발표"><a href="#The-Royal-Swedish-Academy-of-Sciences-2024년-노벨-화학상-발표" class="headerlink" title="The Royal Swedish Academy of Sciences, 2024년 노벨 화학상 발표"></a>The Royal Swedish Academy of Sciences, 2024년 노벨 화학상 발표</h3><p><a target="_blank" rel="noopener" href="https://www.nobelprize.org/prizes/chemistry/2024/press-release/">링크</a>, 2024년 10월 9일</p>
<ul>
<li><strong>David Baker</strong>는 컴퓨터 기반 단백질 설계(Computational Protein Design)로 수상, 이 설계 기술을 통해 새로운 유형의 단백질을 창조하고 그 활용 가능성을 확장</li>
<li><strong>Demis Hassabis</strong>와 <strong>John Jumper</strong>는 AlphaFold2 모델을 통해 단백질의 3D 구조를 예측하는 문제를 해결, 이는 지난 50년간 단백질 연구에서 가장 큰 난제로 여겨졌던 문제</li>
<li>AlphaFold2는 AI 모델을 통해 20개의 아미노산 배열에서 단백질의 3차원 구조를 예측하며, 2020년 이후 200만 명 이상의 연구자가 이 기술을 활용해 항생제 내성 이해, 플라스틱 분해 효소 설계 등 다방면에서 연구를 진행</li>
<li>AlphaFold2는 전 세계적으로 단백질 기능 연구, 신약 개발, 바이오 엔지니어링 등에서 혁신적인 기여를 하고 있음</li>
</ul>
<h3 id="OpenAI-MLE-bench-발표"><a href="#OpenAI-MLE-bench-발표" class="headerlink" title="OpenAI, MLE-bench 발표"></a>OpenAI, MLE-bench 발표</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/mle-bench/">링크</a>, 2024년 10월 10일</p>
<ul>
<li>OpenAI는 머신러닝 엔지니어링 역량을 측정하기 위한 벤치마크 도구 <strong>MLE-bench</strong>를 발표</li>
<li>MLE-bench는 Kaggle의 75개 대회에서 실제 ML 엔지니어링 기술을 평가하며, 데이터셋 준비, 모델 학습, 실험 실행 등 실무에서 중요한 기술들을 테스트</li>
<li>OpenAI의 <strong>o1-preview</strong> 모델은 <strong>AIDE</strong> scaffolding을 사용하여 벤치마크에서 AI 에이전트가 경쟁자들과 비교하여 얼마나 효율적으로 작업을 수행할 수 있는지 평가, 16.9%의 대회에서 <strong>Kaggle 동메달 수준</strong>의 성과를 달성</li>
<li>추가적으로, 자원 스케일링(Resource Scaling) 및 사전 학습(Pre-training)에서의 오염(Contamination)이 모델 성능에 미치는 영향을 분석하여 AI 에이전트의 성능 최적화에 관한 인사이트를 제공</li>
</ul>
<h3 id="OpenAI-Swarm-라이브러리-출시"><a href="#OpenAI-Swarm-라이브러리-출시" class="headerlink" title="OpenAI, Swarm 라이브러리 출시"></a>OpenAI, Swarm 라이브러리 출시</h3><p><a target="_blank" rel="noopener" href="https://github.com/openai/swarm/tree/main">링크</a>, 2024년 10월 10일</p>
<ul>
<li><strong>Swarm</strong>은 다중 에이전트 시스템을 구축할 수 있는 경량 라이브러리로, <strong>무상태(stateless) 추상화</strong>를 통해 여러 에이전트 간의 상호작용 및 제어 흐름을 관리할 수 있음</li>
<li>각 에이전트는 고유의 **역할(Role)**과 **함수 세트(Available Functions)**를 정의하고, 대화 흐름이나 특정 기준에 따라 다른 에이전트로 제어권을 동적으로 넘길 수 있음</li>
<li><strong>Context Variables</strong>를 사용하여 대화 상태를 유지하고 에이전트 간 정보 공유를 가능하게 함</li>
<li>Swarm은 실시간 상호작용을 위한 스트리밍 응답을 지원하며, 다양한 에이전트의 협업 및 제어에 유연성을 제공함</li>
<li>이 라이브러리는 다양한 실험적 기능을 제공하여 다중 에이전트 시스템을 쉽게 구축하고 테스트할 수 있도록 설계됨</li>
</ul>
<h3 id="Meta-Movie-Gen-모델-발표"><a href="#Meta-Movie-Gen-모델-발표" class="headerlink" title="Meta, Movie Gen 모델 발표"></a>Meta, Movie Gen 모델 발표</h3><p><a target="_blank" rel="noopener" href="https://ai.meta.com/static-resource/movie-gen-research-paper">링크</a>, 2024년 10월 4일</p>
<ul>
<li>Meta는 고품질 1080p HD 비디오를 텍스트에서 생성할 수 있는 <strong>Movie Gen</strong> 모델을 발표</li>
<li>이 모델은 <strong>30B 파라미터</strong>를 사용하며, 최대 16초 길이의 비디오를 생성할 수 있는 <strong>73K 비디오 토큰</strong>을 활용하여 높은 해상도와 긴 문맥을 처리 가능</li>
<li>비디오 생성 외에도 <strong>사용자 이미지를 기반으로 한 개인화된 비디오 생성</strong> 및 <strong>텍스트 기반 비디오 편집</strong> 기능을 제공, 배경 변경 또는 스타일 변경과 같은 전역 편집뿐 아니라 개별 요소 추가, 제거 등 정밀한 편집이 가능</li>
<li>이 모델은 텍스트-비디오 생성, 비디오-오디오 생성, 비디오 편집 등에서 <strong>최첨단 성능</strong>을 보여주며, 다양한 연구와 창의적 작업에 활용 가능</li>
</ul>
<h3 id="Pyramid-Flow-SD3-비디오-생성-모델-발표"><a href="#Pyramid-Flow-SD3-비디오-생성-모델-발표" class="headerlink" title="Pyramid Flow, SD3 비디오 생성 모델 발표"></a>Pyramid Flow, SD3 비디오 생성 모델 발표</h3><p><a target="_blank" rel="noopener" href="https://pyramid-flow.github.io/">링크</a>, 2024년 10월 11일</p>
<ul>
<li><strong>Pyramid Flow SD3</strong>는 2B 파라미터의 <strong>Diffusion Transformer</strong>(DiT) 모델로, 10초 길이의 <strong>768p 해상도, 24fps 비디오</strong>를 생성할 수 있는 텍스트-비디오 생성 모델을 발표</li>
<li>이 모델은 <strong>Flow Matching</strong> 기반의 효율적인 학습을 통해 기존 비디오 생성 모델 대비 빠르고 효율적인 비디오 생성을 지원</li>
<li>두 가지 변형 모델을 제공하며, <strong>MIT 라이선스</strong> 하에 공개되어 오픈 소스 커뮤니티와의 협업이 가능</li>
<li>오픈 데이터셋을 사용하여 훈련되었으며, 20.7K GPU 시간 동안 학습되어 효율성을 극대화함</li>
</ul>
<h3 id="Rhymes-AI-Aria-모델-발표"><a href="#Rhymes-AI-Aria-모델-발표" class="headerlink" title="Rhymes AI, Aria 모델 발표"></a>Rhymes AI, Aria 모델 발표</h3><p><a target="_blank" rel="noopener" href="https://rhymes.ai/blog-details/aria-first-open-multimodal-native-moe-model">링크</a>, 2024년 10월 10일</p>
<ul>
<li>Rhymes AI는 <strong>Aria</strong>라는 첫 오픈 멀티모달 <strong>Mixture-of-Experts(MoE)</strong> 모델을 발표, <strong>3.9B 활성 파라미터</strong>로 텍스트, 이미지, 비디오, 코드 등의 다양한 입력을 처리 가능</li>
<li><strong>64K 토큰</strong>의 긴 문맥을 처리할 수 있으며, <strong>256프레임 비디오</strong>를 10초 만에 캡션할 수 있는 강력한 성능을 제공</li>
<li>Aria는 경쟁사 모델들(GPT-4o 및 Gemini Flash)을 뛰어넘는 성능을 자랑하며, 멀티모달 데이터를 효율적으로 처리</li>
<li>이 모델은 <strong>Apache 2.0 라이선스</strong> 하에 공개되었으며, 오픈 소스 커뮤니티에서 쉽게 확장 가능</li>
</ul>
<h3 id="Microsoft-Diff-Transformer-발표"><a href="#Microsoft-Diff-Transformer-발표" class="headerlink" title="Microsoft, Diff Transformer 발표"></a>Microsoft, Diff Transformer 발표</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2410.05258">링크</a>, 2024년 10월 8일</p>
<ul>
<li><strong>Diff Transformer</strong>는 <strong>차별화된 어텐션 메커니즘</strong>을 도입한 새로운 Transformer 아키텍처로, 기존 self-attention 메커니즘의 한계를 극복</li>
<li>**차별적 어텐션(Differential Attention)**는</li>
</ul>
<p>두 개의 softmax 어텐션 맵 간의 차이를 계산하여 잡음을 제거하고 중요한 정보에 집중하는 **희소 어텐션 패턴(Sparse Attention Patterns)**을 생성</p>
<ul>
<li>이 모델은 <strong>장문 데이터 처리(long-context modeling)</strong> 및 <strong>핵심 정보 검색</strong>에서 기존 Transformer 모델보다 우수한 성능을 발휘</li>
<li><strong>35-40% 적은 파라미터</strong>와 학습 토큰으로 기존 Transformer 대비 유사한 성능을 발휘하며, <strong>환각 현상 감소</strong> 및 <strong>문맥 학습 강화</strong>에 도움을 줌</li>
<li>특히 <strong>플래시 어텐션(FlashAttention) 커널</strong>을 활용하여 기존 하드웨어에서 쉽게 구현 가능</li>
</ul>
<h3 id="F5-TTS-텍스트-음성-변환-모델-발표"><a href="#F5-TTS-텍스트-음성-변환-모델-발표" class="headerlink" title="F5-TTS, 텍스트-음성 변환 모델 발표"></a>F5-TTS, 텍스트-음성 변환 모델 발표</h3><p><a target="_blank" rel="noopener" href="https://github.com/SWivid/F5-TTS">링크</a>, 2024년 10월 9일</p>
<ul>
<li><strong>F5-TTS</strong>는 <strong>Flow Matching 기반</strong>의 비자발적(Non-Autoregressive) 텍스트-음성 변환 시스템으로, 빠르고 효율적인 음성 합성 기능을 제공</li>
<li><strong>ConvNeXt</strong>를 사용하여 텍스트 표현을 개선하고 음성과의 정렬을 쉽게 함</li>
<li>이 모델은 학습과 추론 속도에서 기존 TTS 모델보다 <strong>빠른 성능</strong>을 제공하며, <strong>감정 기반 음성 합성</strong>, <strong>코드 전환</strong>, <strong>속도 제어</strong> 기능을 지원</li>
<li>100K 시간의 데이터를 바탕으로 훈련되어 <strong>자연스럽고 표현력 있는 음성 합성</strong> 성능을 자랑하며, 상업적 이용이 가능한 <strong>CC-BY 라이선스</strong>로 제공됨</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://www.nobelprize.org/prizes/physics/2024/press-release/</span><br><span class="line">8 October 2024</span><br><span class="line"></span><br><span class="line">The Royal Swedish Academy of Sciences has decided to award the Nobel Prize in Physics 2024 to</span><br><span class="line"></span><br><span class="line">John J. Hopfield</span><br><span class="line">Princeton University, NJ, USA</span><br><span class="line"></span><br><span class="line">Geoffrey E. Hinton</span><br><span class="line">University of Toronto, Canada</span><br><span class="line"></span><br><span class="line">“for foundational discoveries and inventions that enable machine learning with artificial neural networks”</span><br><span class="line"></span><br><span class="line">They trained artificial neural networks using physics</span><br><span class="line">This year’s two Nobel Laureates in Physics have used tools from physics to develop methods that are the foundation of today’s powerful machine learning. John Hopfield created an associative memory that can store and reconstruct images and other types of patterns in data. Geoffrey Hinton invented a method that can autonomously find properties in data, and so perform tasks such as identifying specific elements in pictures.</span><br><span class="line"></span><br><span class="line">When we talk about artificial intelligence, we often mean machine learning using artificial neural networks. This technology was originally inspired by the structure of the brain. In an artificial neural network, the brain’s neurons are represented by nodes that have different values. These nodes influence each other through con­nections that can be likened to synapses and which can be made stronger or weaker. The network is trained, for example by developing stronger connections between nodes with simultaneously high values. This year’s laureates have conducted important work with artificial neural networks from the 1980s onward.</span><br><span class="line"></span><br><span class="line">John Hopfield invented a network that uses a method for saving and recreating patterns. We can imagine the nodes as pixels. The Hopfield network utilises physics that describes a material’s characteristics due to its atomic spin – a property that makes each atom a tiny magnet. The network as a whole is described in a manner equivalent to the energy in the spin system found in physics, and is trained by finding values for the connections between the nodes so that the saved images have low energy. When the Hopfield network is fed a distorted or incomplete image, it methodically works through the nodes and updates their values so the network’s energy falls. The network thus works stepwise to find the saved image that is most like the imperfect one it was fed with.</span><br><span class="line"></span><br><span class="line">Geoffrey Hinton used the Hopfield network as the foundation for a new network that uses a different method: the Boltzmann machine. This can learn to recognise characteristic elements in a given type of data. Hinton used tools from statistical physics, the science of systems built from many similar components. The machine is trained by feeding it examples that are very likely to arise when the machine is run. The Boltzmann machine can be used to classify images or create new examples of the type of pattern on which it was trained. Hinton has built upon this work, helping initiate the current explosive development of machine learning.</span><br><span class="line"></span><br><span class="line">“The laureates’ work has already been of the greatest benefit. In physics we use artificial neural networks in a vast range of areas, such as developing new materials with specific properties,” says Ellen Moons, Chair of the Nobel Committee for Physics.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.nobelprize.org/prizes/chemistry/2024/press-release/</span><br><span class="line">Press release</span><br><span class="line">English</span><br><span class="line">English (pdf)</span><br><span class="line">Swedish</span><br><span class="line">Swedish (pdf)</span><br><span class="line">Logo</span><br><span class="line">9 October 2024</span><br><span class="line"></span><br><span class="line">The Royal Swedish Academy of Sciences has decided to award the Nobel Prize in Chemistry 2024</span><br><span class="line"></span><br><span class="line">with one half to</span><br><span class="line"></span><br><span class="line">David Baker</span><br><span class="line">University of Washington, Seattle, WA, USA</span><br><span class="line">Howard Hughes Medical Institute, USA</span><br><span class="line"></span><br><span class="line">“for computational protein design”</span><br><span class="line"></span><br><span class="line">and the other half jointly to</span><br><span class="line"></span><br><span class="line">Demis Hassabis</span><br><span class="line">Google DeepMind, London, UK</span><br><span class="line"></span><br><span class="line">John M. Jumper</span><br><span class="line">Google DeepMind, London, UK</span><br><span class="line"></span><br><span class="line">“for protein structure prediction”</span><br><span class="line"></span><br><span class="line">They cracked the code for proteins’ amazing structures</span><br><span class="line">The Nobel Prize in Chemistry 2024 is about pro­teins, life’s ingenious chemical tools. David Baker has succeeded with the almost impossible feat of building entirely new kinds of proteins. Demis Hassabis and John Jumper have developed an AI model to solve a 50-year-old problem: predicting proteins’ complex structures. These discoveries hold enormous potential.</span><br><span class="line"></span><br><span class="line">The diversity of life testifies to proteins’ amazing capacity as chemical tools. They control and drive all the chemi­cal reactions that together are the basis of life. Proteins also function as hormones, signal substances, antibodies and the building blocks of different tissues.</span><br><span class="line"></span><br><span class="line">“One of the discoveries being recognised this year concerns the construction of spectacular proteins. The other is about fulfilling a 50-year-old dream: predicting protein structures from their amino acid sequences. Both of these discoveries open up vast possibilities,” says Heiner Linke, Chair of the Nobel Committee for Chemistry.</span><br><span class="line"></span><br><span class="line">Proteins generally consist of 20 different amino acids, which can be described as life’s building blocks. In 2003, David Baker succeeded in using these blocks to design a new protein that was unlike any other protein. Since then, his research group has produced one imaginative protein creation after another, including proteins that can be used as pharmaceuticals, vaccines, nanomaterials and tiny sensors.</span><br><span class="line"></span><br><span class="line">The second discovery concerns the prediction of protein structures. In proteins, amino acids are linked together in long strings that fold up to make a three-dimensional structure, which is decisive for the protein’s function. Since the 1970s, researchers had tried to predict protein structures from amino acid sequences, but this was notoriously difficult. However, four years ago, there was a stunning breakthrough.</span><br><span class="line"></span><br><span class="line">In 2020, Demis Hassabis and John Jumper presented an AI model called AlphaFold2. With its help, they have been able to predict the structure of virtually all the 200 million proteins that researchers have identified. Since their breakthrough, AlphaFold2 has been used by more than two million people from 190 countries. Among a myriad of scientific applications, researchers can now better understand antibiotic resistance and create images of enzymes that can decompose plastic.</span><br><span class="line"></span><br><span class="line">Life could not exist without proteins. That we can now predict protein structures and design our own proteins confers the greatest benefit to humankind.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/mle-bench/</span><br><span class="line">OpenAI</span><br><span class="line">October 10, 2024</span><br><span class="line"></span><br><span class="line">MLE-bench</span><br><span class="line">Evaluating Machine Learning Agents on Machine Learning Engineering</span><br><span class="line"></span><br><span class="line">Read paper(opens in a new window)</span><br><span class="line">DALL·E generated impressionistic oil painting of concentric pastel rectangular bars representing layers of assessment</span><br><span class="line">We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle&#x27;s publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup — OpenAI&#x27;s o1-preview with AIDE scaffolding — achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource-scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code(opens in a new window) to facilitate future research in understanding the ML engineering capabilities of AI agents.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/openai/swarm/tree/main</span><br><span class="line">OpenAI</span><br><span class="line">OpenAI released Swarm, a lightweight library for building multi-agent systems. Swarm provides a stateless abstraction to manage interactions and handoffs between multiple agents and does not use the Assistants API. 👀</span><br><span class="line">How it works:</span><br><span class="line">1️⃣ Define Agents, each with its own instructions, role (e.g., &quot;Sales Agent&quot;), and available functions (will be converted to JSON structures).</span><br><span class="line">2️⃣ Define logic for transferring control to another agent based on conversation flow or specific criteria within agent functions. This handoff is achieved by simply returning the next agent to call within the function.</span><br><span class="line">3️⃣ Context Variables provide initial context and update them throughout the conversation to maintain state and share information between agents.</span><br><span class="line">4️⃣ Client.run() initiate and manage the multi-agent conversation. Needs initial agent, user messages, and context and returns a response containing updated messages, context variables, and the last active agent.</span><br><span class="line">Insights</span><br><span class="line">🔄 Swarm manages a loop of agent interactions, function calls, and potential handoffs.</span><br><span class="line">🧩 Agents encapsulate instructions, available functions (tools), and handoff logic.</span><br><span class="line">🔌 The framework is stateless between calls, offering transparency and fine-grained control.</span><br><span class="line">🛠️ Swarm supports direct Python function calling within agents.</span><br><span class="line">📊 Context variables enable state management across agent interactions.</span><br><span class="line">🔄 Agent handoffs allow for dynamic switching between specialized agents.</span><br><span class="line">📡 Streaming responses are supported for real-time interaction.</span><br><span class="line">🧪 The framework is experimental. Maybe to collect feedback?</span><br><span class="line">🔧 Flexible and works with any OpenAI client, e.g. Hugging Face TGI or vLLM hosted models.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Overview</span><br><span class="line">Swarm focuses on making agent coordination and execution lightweight, highly controllable, and easily testable.</span><br><span class="line"></span><br><span class="line">It accomplishes this through two primitive abstractions: Agents and handoffs. An Agent encompasses instructions and tools, and can at any point choose to hand off a conversation to another Agent.</span><br><span class="line"></span><br><span class="line">These primitives are powerful enough to express rich dynamics between tools and networks of agents, allowing you to build scalable, real-world solutions while avoiding a steep learning curve.</span><br><span class="line"></span><br><span class="line">Note</span><br><span class="line"></span><br><span class="line">Swarm Agents are not related to Assistants in the Assistants API. They are named similarly for convenience, but are otherwise completely unrelated. Swarm is entirely powered by the Chat Completions API and is hence stateless between calls.</span><br><span class="line"></span><br><span class="line">Why Swarm</span><br><span class="line">Swarm explores patterns that are lightweight, scalable, and highly customizable by design. Approaches similar to Swarm are best suited for situations dealing with a large number of independent capabilities and instructions that are difficult to encode into a single prompt.</span><br><span class="line"></span><br><span class="line">The Assistants API is a great option for developers looking for fully-hosted threads and built in memory management and retrieval. However, Swarm is an educational resource for developers curious to learn about multi-agent orchestration. Swarm runs (almost) entirely on the client and, much like the Chat Completions API, does not store state between calls.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://ai.meta.com/static-resource/movie-gen-research-paper</span><br><span class="line">META</span><br><span class="line">Date: October 4, 2024</span><br><span class="line"></span><br><span class="line">Movie Gen: A Cast of Media Foundation Models</span><br><span class="line">The Movie Gen team @ Meta1</span><br><span class="line">1A detailed contributor list can be found in the appendix of this paper.</span><br><span class="line">We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos</span><br><span class="line">with different aspect ratios and synchronized audio. We also show additional capabilities such as</span><br><span class="line">precise instruction-based video editing and generation of personalized videos based on a user’s image.</span><br><span class="line">Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization,</span><br><span class="line">video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation</span><br><span class="line">model is a 30B parameter transformer trained with a maximum context length of 73K video tokens,</span><br><span class="line">corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical</span><br><span class="line">innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data</span><br><span class="line">curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to</span><br><span class="line">reap the benefits of scaling pre-training data, model size, and training compute for training large scale</span><br><span class="line">media generation models. We hope this paper helps the research community to accelerate progress</span><br><span class="line">and innovation in media generation models.</span><br><span class="line">All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.</span><br><span class="line"></span><br><span class="line">Meta Movie Gen: the most advanced media foundation models to-date.</span><br><span class="line">Developed by AI research teams at Meta, Movie Gen delivers state-of-the-art results across a range of capabilities. We’re excited for the potential of this line of research to usher in entirely new possibilities for casual creators and creative professionals alike.</span><br><span class="line">More details and examples of what Movie Gen can do ➡️</span><br><span class="line">https://go.fb.me/00mlgt</span><br><span class="line">Movie Gen Research Paper ➡️</span><br><span class="line">https://go.fb.me/zfa8wf</span><br><span class="line"></span><br><span class="line">🛠️ Movie Gen models and capabilities</span><br><span class="line">• Movie Gen Video: A 30B parameter transformer model that can generate high-quality and high-definition images and videos from a single text prompt.</span><br><span class="line">• Movie Gen Audio: A 13B parameter transformer model can take a video input along with optional text prompts for controllability to generate high-fidelity audio synced to the video. It can generate ambient sound, instrumental background music and foley sound — delivering state-of-the-art results in audio quality, video-to-audio alignment and text-to-audio alignment.</span><br><span class="line">• Precise video editing: Using a generated or existing video and accompanying text instructions as an input it can perform localized edits such as adding, removing or replacing elements — or global changes like background or style changes.</span><br><span class="line">• Personalized videos: Using an image of a person and a text prompt, the model can generate a video with state-of-the-art results on character preservation and natural movement in video.</span><br><span class="line">We’re continuing to work closely with creative professionals from across the field to integrate their feedback as we work towards a potential release. We look forward to sharing more on this work and the creative possibilities it will enable in the future.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://pyramid-flow.github.io/</span><br><span class="line">2024.10.11</span><br><span class="line">First real good open-source text-to-video model with MIT license! Pyramid Flow SD3 is a 2B Diffusion Transformer (DiT) that can generate 10-second videos at 768p with 24fps! 🤯 🎥✨</span><br><span class="line">TL;DR;</span><br><span class="line">🎬 Can Generate 10-second videos at 768p/24FPS</span><br><span class="line">🍹 2B parameter single unified Diffusion Transformer (DiT)</span><br><span class="line">🖼️ Supports both text-to-video AND image-to-video</span><br><span class="line">🧠 Uses Flow Matching for efficient training</span><br><span class="line">💻 Two model variants: 384p (5s) and 768p (10s)</span><br><span class="line">📼 example videos on project page</span><br><span class="line">🛠️ Simple two-step implementation process</span><br><span class="line">📚 MIT License and available on</span><br><span class="line">Hugging Face</span><br><span class="line">✅ Trained only on open-source datasets</span><br><span class="line">🔜 Training code coming soon!</span><br><span class="line"></span><br><span class="line">Pyramidal Flow Matching for Efficient Video Generative Modeling</span><br><span class="line">Yang Jin1, Zhicheng Sun1, Ningyuan Li3, Kun Xu2, Kun Xu2, Hao Jiang1, Nan Zhuang2, Quzhe Huang2, Yang Song, Yadong Mu1†, Zhouchen Lin1</span><br><span class="line">1Peking University, 2Kuaishou Technology, 3Beijing University of Posts and Telecommunications</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The following videos are from a training-efficient Autoregressive Video Generation model based on Flow Matching. It is trained only on open-source datasets within 20.7k A100 GPU hours.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://rhymes.ai/blog-details/aria-first-open-multimodal-native-moe-model</span><br><span class="line">🚨 Rhymes.AI released Aria - Multimodal MoE (3.9B active), 64K tokens, caption 256 frames in 10 sec, Apache 2.0 licensed! Beats GPT4o &amp; Gemini Flash ⚡</span><br><span class="line">&gt; 3.9B Active, 25.3B Total parameters</span><br><span class="line">&gt; Significantly better than Pixtral 12B, Llama Vision 11B &amp; Qwen VL</span><br><span class="line">&gt; Trained on 7.5T tokens</span><br><span class="line">&gt; Four stage training:</span><br><span class="line">- 6.4T language pre-training</span><br><span class="line">- 1.4T multimodal pre-training</span><br><span class="line">- 35B long context training</span><br><span class="line">- 20B high quality post-training</span><br><span class="line">Architecture:</span><br><span class="line">&gt; Aria consists of a vision encoder and a mixture-of-experts (MoE) decoder</span><br><span class="line">&gt; Vision encoder:</span><br><span class="line">- Produces visual tokens for images/videos in native aspect ratio</span><br><span class="line">- Operates in three resolution modes: medium, high, and ultra-high</span><br><span class="line">- Medium-resolution: 128 visual tokens</span><br><span class="line">- High-resolution: 256 visual tokens</span><br><span class="line">- Ultra-high resolution: Dynamically decomposed into multiple high-resolution sub-images</span><br><span class="line">&gt; MoE decoder:</span><br><span class="line">- Multimodal native, conditioned on both language and visual input tokens</span><br><span class="line">- 66 experts per MoE layer</span><br><span class="line">- 2 experts shared among all inputs to capture common knowledge</span><br><span class="line">- 6 additional experts activated per token by a router module</span><br><span class="line">&gt; Models on the Hub &amp; Integrated with Transformers!</span><br><span class="line">October 10, 2024</span><br><span class="line">10 min read</span><br><span class="line">Rhymes AI Team</span><br><span class="line">Share:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Aria: First Open Multimodal Native MoE Model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Aria Multimodal Native MoE - An Open Model for ALL Modalities</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Rhymes AI is proud to introduce Aria, the world’s first open-source, multimodal native Mixture-of-Experts (MoE) model.</span><br><span class="line"></span><br><span class="line">In short, Aria features:</span><br><span class="line"></span><br><span class="line">Multimodal native understanding:</span><br><span class="line"></span><br><span class="line">State-of-the-art performance on a wide range of multimodal and language tasks</span><br><span class="line">Pre-trained from scratch on a mixture of multimodal and language data</span><br><span class="line">Lightweight and fast:</span><br><span class="line"></span><br><span class="line">Fine-grained mixture-of-expert model with 3.9B activated parameters per token</span><br><span class="line">Efficient and informative visual encoding of variable image sizes and aspect ratios</span><br><span class="line">Long context window:</span><br><span class="line"></span><br><span class="line">Long multimodal context window of 64K tokens, captioning a 256-frame video in 10 seconds</span><br><span class="line">Open:</span><br><span class="line"></span><br><span class="line">Open model weights 🤗, code repository 💻, technical report 📝 for collaborative development.</span><br><span class="line">License: Apache 2.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Multimodal Native Performance</span><br><span class="line"></span><br><span class="line">Aria Multimodal Native MoE - An Open Model for ALL ModalitiesFigure 1. Aria is a multimodal native model that excels at understanding text, vision, code.</span><br><span class="line"></span><br><span class="line">Aria processes text, images, video, and code all at once, without needing separate setups for each type, demonstrating the advantages of a multimodal native model.</span><br><span class="line"></span><br><span class="line">We provide a quantifiable definition for the term multimodal native:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">A multimodal native model refers to a single model with strong understanding capabilities across multiple input modalities (e.g., text, code, image, video) that matches or exceeds the modality-specialized models of similar capacities.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">We compared Aria against the best open and closed multimodal native models across established benchmarks, highlighting the following key observations:</span><br><span class="line"></span><br><span class="line">Best-in-Class Performance: Aria is the leading multimodal native model, demonstrating clear advantages over Pixtral-12B and Llama3.2-11B across a range of multimodal, language, and coding tasks.</span><br><span class="line">Competitive Against Proprietary Models: Aria performs competitively against proprietary models like GPT-4o and Gemini-1.5 on multimodal tasks, including document understanding, chart reading, scene text recognition, and video understanding.</span><br><span class="line">Parameter Efficiency: Aria is the most parameter-efficient open model. Thanks to the MoE framework, Aria activates only 3.9 billion parameters, compared to the full activation in models like Pixtral-12B and Llama3.2-11B.</span><br><span class="line"></span><br><span class="line">Aria Multimodal Native MoE - An Open Model for ALL ModalitiesFigure 2. Aria shows best-in-class benchmark performance on multimodal, language, coding tasks.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Long Multimodal Input Understanding</span><br><span class="line"></span><br><span class="line">Multimodal data is often complex, involving long sequences that combine visuals and text, like videos with subtitles or long documents. For a model to be effective in real-world applications, it must be capable of understanding and processing such data efficiently.</span><br><span class="line"></span><br><span class="line">Aria excels in this area, demonstrating superior long multimodal input understanding. It outperforms larger open models, proving its efficiency and effectiveness despite its size. When compared to proprietary models, Aria surpasses GPT-4o mini in long video understanding and outperforms Gemini-1.5-Flash in long document understanding. This makes Aria a preferred choice for processing extensive multimodal data in a compute-and-time-efficient manner, delivering faster and more accurate results in real-world scenarios.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Aria Multimodal Native MoE - An Open Model for ALL Modalities</span><br><span class="line">Figure 3. Aria excels in long multimodal input understanding, such as long video understanding.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Instruction Following</span><br><span class="line"></span><br><span class="line">Aria is highly effective at understanding and following instructions on both multimodal and language inputs, performing better than top open-source models on both MIA-Bench and MT-Bench.</span><br><span class="line"></span><br><span class="line">Aria Multimodal Native MoE - An Open Model for ALL ModalitiesFigure 4. Aria is highly effective at instruction following on multimodal and language inputs.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2410.05258</span><br><span class="line">Microsoft</span><br><span class="line"></span><br><span class="line">Differential Transformer</span><br><span class="line">Published on Oct 8</span><br><span class="line">·</span><br><span class="line">Submitted by</span><br><span class="line">unilm</span><br><span class="line">on Oct 8</span><br><span class="line">#1 Paper of the day</span><br><span class="line">Authors:</span><br><span class="line"></span><br><span class="line">Tianzhu Ye</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Li Dong</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Yuqing Xia</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Yutao Sun</span><br><span class="line">,</span><br><span class="line">Yi Zhu</span><br><span class="line">,</span><br><span class="line">Gao Huang</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Furu Wei</span><br><span class="line">Abstract</span><br><span class="line">Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.</span><br><span class="line"></span><br><span class="line">⚡️ Most important breakthrough this month: Differential Transformer vastly improves attention ⇒ better retrieval and fewer hallucinations!</span><br><span class="line">Thought that self-attention could not be improved anymore?</span><br><span class="line">Researchers from Microsoft Research and Tsinghua University have dropped a novel &quot;differential attention&quot; mechanism that amplifies focus on relevant context while canceling out noise. It sounds like a free lunch, but it does really seem to vastly improve LLM performance!</span><br><span class="line">Key insights:</span><br><span class="line">🧠 Differential attention computes the difference between two separate softmax attention maps, canceling out noise and promoting sparse attention patterns</span><br><span class="line">🔥 DIFF Transformer outperforms standard Transformers while using 35-40% fewer parameters or training tokens</span><br><span class="line">📏 Scales well to long contexts up to 64K tokens, leveraging increasing context length more effectively</span><br><span class="line">🔎 Dramatically improves key information retrieval, enhancing in-context learning, and possibly reducing risk of hallucinations 🤯</span><br><span class="line">🔢 Reduces activation outliers, potentially enabling lower-bit quantization without performance drop!</span><br><span class="line">⚙️ Can be directly implemented using existing FlashAttention kernels</span><br><span class="line">This new architecture could lead much more capable LLMs, with vastly improved strengths in long-context understanding and factual accuracy.</span><br><span class="line">But they didn’t release weights on the Hub: let’s wait for the community to train the first open-weights DiffTransformer! 🚀</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/SWivid/F5-TTS</span><br><span class="line">[Submitted on 9 Oct 2024]</span><br><span class="line">F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching</span><br><span class="line">Yushen Chen, Zhikang Niu, Ziyang Ma, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, Xie Chen</span><br><span class="line">This paper introduces F5-TTS, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length as input speech, and then the denoising is performed for speech generation, which was originally proved feasible by E2 TTS. However, the original design of E2 TTS makes it hard to follow due to its slow convergence and low robustness. To address these issues, we first model the input with ConvNeXt to refine the text representation, making it easy to align with the speech. We further propose an inference-time Sway Sampling strategy, which significantly improves our model&#x27;s performance and efficiency. This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining. Our design allows faster training and achieves an inference RTF of 0.15, which is greatly improved compared to state-of-the-art diffusion-based TTS models. Trained on a public 100K hours multilingual dataset, our Fairytaler Fakes Fluent and Faithful speech with Flow matching (F5-TTS) exhibits highly natural and expressive zero-shot ability, seamless code-switching capability, and speed control efficiency. Demo samples can be found at this https URL. We release all code and checkpoints to promote community development.</span><br><span class="line"></span><br><span class="line">Let&#x27;s goo! F5-TTS 🔊</span><br><span class="line">&gt; Trained on 100K hours of data</span><br><span class="line">&gt; Zero-shot voice cloning</span><br><span class="line">&gt; Speed control (based on total duration)</span><br><span class="line">&gt; Emotion based synthesis</span><br><span class="line">&gt; Long-form synthesis</span><br><span class="line">&gt; Supports code-switching</span><br><span class="line">&gt; Best part: CC-BY license (commercially permissive)🔥</span><br><span class="line">Diffusion based architecture:</span><br><span class="line">&gt; Non-Autoregressive + Flow Matching with DiT</span><br><span class="line">&gt; Uses ConvNeXt to refine text representation, alignment</span><br><span class="line">Synthesised: I was, like, talking to my friend, and she’s all, um, excited about her, uh, trip to Europe, and I’m just, like, so jealous, right? (Happy emotion)</span><br><span class="line">The TTS scene is on fire! 🐐</span><br></pre></td></tr></table></figure>

<p>기술적으로 최대한 자세하게 적어. 9개의 기사가 있고 하나도 빼먹지 말고 적어.</p>
</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-10-15</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/10/15/2024-10-15-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 10월 15일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/10/04/2024-10-4-AI-NEWS/" title="2024년 10월 4일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>