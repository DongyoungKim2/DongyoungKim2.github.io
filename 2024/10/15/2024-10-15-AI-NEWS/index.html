<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 10ì›” 15ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="ì˜¤ëŠ˜ AI ê¸°ìˆ  ë¶„ì•¼ì—ì„œëŠ” 2024ë…„ ë…¸ë²¨ ë¬¼ë¦¬í•™ìƒ ë° í™”í•™ìƒ ìˆ˜ìƒ ì†Œì‹ê³¼ í•¨ê»˜, OpenAI, Meta, Rhymes AI, Microsoft ë“±ì˜ ì£¼ìš” ê¸°ì—…ë“¤ì´ ìµœì²¨ë‹¨ AI ëª¨ë¸ê³¼ ê¸°ìˆ ì„ ë°œí‘œí•˜ë©° ì¤‘ìš”í•œ ë°œì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. OpenAIëŠ” ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´ë§ í‰ê°€ë¥¼"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 10ì›” 15ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><p>ì˜¤ëŠ˜ AI ê¸°ìˆ  ë¶„ì•¼ì—ì„œëŠ” 2024ë…„ ë…¸ë²¨ ë¬¼ë¦¬í•™ìƒ ë° í™”í•™ìƒ ìˆ˜ìƒ ì†Œì‹ê³¼ í•¨ê»˜, OpenAI, Meta, Rhymes AI, Microsoft ë“±ì˜ ì£¼ìš” ê¸°ì—…ë“¤ì´ ìµœì²¨ë‹¨ AI ëª¨ë¸ê³¼ ê¸°ìˆ ì„ ë°œí‘œí•˜ë©° ì¤‘ìš”í•œ ë°œì „ì„ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤. OpenAIëŠ” ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´ë§ í‰ê°€ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ì™€ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í”„ë ˆì„ì›Œí¬ë¥¼ ê³µê°œí–ˆìœ¼ë©°, MetaëŠ” í…ìŠ¤íŠ¸ì—ì„œ ê³ í•´ìƒë„ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” Movie Gen ëª¨ë¸ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. Rhymes AIëŠ” ìƒˆë¡œìš´ ë©€í‹°ëª¨ë‹¬ Mixture-of-Experts(MoE) ëª¨ë¸ì„ ê³µê°œí•˜ì—¬ ê²½ìŸë ¥ì„ ê°•í™”í–ˆìœ¼ë©°, MicrosoftëŠ” ìƒˆë¡œìš´ ì°¨ë³„í™”ëœ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” Transformer ëª¨ë¸ì„ ì„ ë³´ì—¬ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. ì´ ì™¸ì—ë„ ë‹¤ì–‘í•œ AI ëª¨ë¸ë“¤ì´ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìƒì„±, ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡, í…ìŠ¤íŠ¸-ìŒì„± ë³€í™˜ ë“±ì—ì„œ í˜ì‹ ì ì¸ ì§„ì „ì„ ì´ë£¨ë©°, AI ì—°êµ¬ì™€ ì‹¤ë¬´ ì ìš©ì—ì„œ ì»¤ë‹¤ë€ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤.</p>
<h3 id="The-Royal-Swedish-Academy-of-Sciences-2024ë…„-ë…¸ë²¨-ë¬¼ë¦¬í•™ìƒ-ë°œí‘œ"><a href="#The-Royal-Swedish-Academy-of-Sciences-2024ë…„-ë…¸ë²¨-ë¬¼ë¦¬í•™ìƒ-ë°œí‘œ" class="headerlink" title="The Royal Swedish Academy of Sciences, 2024ë…„ ë…¸ë²¨ ë¬¼ë¦¬í•™ìƒ ë°œí‘œ"></a>The Royal Swedish Academy of Sciences, 2024ë…„ ë…¸ë²¨ ë¬¼ë¦¬í•™ìƒ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://www.nobelprize.org/prizes/physics/2024/press-release/">ë§í¬</a>, 2024ë…„ 10ì›” 8ì¼</p>
<ul>
<li>2024ë…„ ë¬¼ë¦¬í•™ìƒì€ ì¸ê³µ ì‹ ê²½ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê¸°ê³„ í•™ìŠµ ê¸°ìˆ ì˜ ë°œì „ì— í¬ê²Œ ê¸°ì—¬í•œ John J. Hopfieldì™€ Geoffrey E. Hintonì—ê²Œ ìˆ˜ì—¬</li>
<li><strong>Hopfield</strong>ëŠ” ì—°ìƒ ê¸°ì–µ(Associative Memory) ëª¨ë¸ì¸ <strong>Hopfield Network</strong>ë¥¼ ì œì•ˆ, ë¬¼ë¦¬í•™ì—ì„œ ìŠ¤í•€ ì‹œìŠ¤í…œì„ ì„¤ëª…í•˜ëŠ” ì—ë„ˆì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ì™€ íŒ¨í„´ì„ ì €ì¥í•˜ê³  ë³µì›í•˜ëŠ” ë°©ì‹ì„ ì œì‹œ</li>
<li><strong>Hinton</strong>ì€ Hopfield ë„¤íŠ¸ì›Œí¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ <strong>Boltzmann Machine</strong>ì„ í†µí•´ íŒ¨í„´ ì¸ì‹ ê¸°ìˆ ì„ ë°œì „ì‹œì¼°ìœ¼ë©°, ì´ëŠ” í†µê³„ ë¬¼ë¦¬í•™ì˜ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ììœ¨ì ì¸ ë°ì´í„° í•™ìŠµê³¼ íŒ¨í„´ ì¸ì‹ì— ì¤‘ìš”í•œ ê¸°ì—¬ë¥¼ í•¨</li>
<li>ë‘ ì—°êµ¬ìëŠ” 1980ë…„ëŒ€ ì´í›„ ì‹ ê²½ë§ì„ ì´ìš©í•œ ì¸ê³µ ì§€ëŠ¥ ê¸°ìˆ ì˜ í† ëŒ€ë¥¼ ë§ˆë ¨í•˜ì˜€ìœ¼ë©°, ì´ë¥¼ í†µí•´ í˜„ì¬ì˜ ì‹¬ì¸µ í•™ìŠµ(Deep Learning)ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ë°œì „ì„ ê°€ëŠ¥í•˜ê²Œ í•¨</li>
</ul>
<h3 id="The-Royal-Swedish-Academy-of-Sciences-2024ë…„-ë…¸ë²¨-í™”í•™ìƒ-ë°œí‘œ"><a href="#The-Royal-Swedish-Academy-of-Sciences-2024ë…„-ë…¸ë²¨-í™”í•™ìƒ-ë°œí‘œ" class="headerlink" title="The Royal Swedish Academy of Sciences, 2024ë…„ ë…¸ë²¨ í™”í•™ìƒ ë°œí‘œ"></a>The Royal Swedish Academy of Sciences, 2024ë…„ ë…¸ë²¨ í™”í•™ìƒ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://www.nobelprize.org/prizes/chemistry/2024/press-release/">ë§í¬</a>, 2024ë…„ 10ì›” 9ì¼</p>
<ul>
<li><strong>David Baker</strong>ëŠ” ì»´í“¨í„° ê¸°ë°˜ ë‹¨ë°±ì§ˆ ì„¤ê³„(Computational Protein Design)ë¡œ ìˆ˜ìƒ, ì´ ì„¤ê³„ ê¸°ìˆ ì„ í†µí•´ ìƒˆë¡œìš´ ìœ í˜•ì˜ ë‹¨ë°±ì§ˆì„ ì°½ì¡°í•˜ê³  ê·¸ í™œìš© ê°€ëŠ¥ì„±ì„ í™•ì¥</li>
<li><strong>Demis Hassabis</strong>ì™€ <strong>John Jumper</strong>ëŠ” AlphaFold2 ëª¨ë¸ì„ í†µí•´ ë‹¨ë°±ì§ˆì˜ 3D êµ¬ì¡°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°, ì´ëŠ” ì§€ë‚œ 50ë…„ê°„ ë‹¨ë°±ì§ˆ ì—°êµ¬ì—ì„œ ê°€ì¥ í° ë‚œì œë¡œ ì—¬ê²¨ì¡Œë˜ ë¬¸ì œ</li>
<li>AlphaFold2ëŠ” AI ëª¨ë¸ì„ í†µí•´ 20ê°œì˜ ì•„ë¯¸ë…¸ì‚° ë°°ì—´ì—ì„œ ë‹¨ë°±ì§ˆì˜ 3ì°¨ì› êµ¬ì¡°ë¥¼ ì˜ˆì¸¡í•˜ë©°, 2020ë…„ ì´í›„ 200ë§Œ ëª… ì´ìƒì˜ ì—°êµ¬ìê°€ ì´ ê¸°ìˆ ì„ í™œìš©í•´ í•­ìƒì œ ë‚´ì„± ì´í•´, í”Œë¼ìŠ¤í‹± ë¶„í•´ íš¨ì†Œ ì„¤ê³„ ë“± ë‹¤ë°©ë©´ì—ì„œ ì—°êµ¬ë¥¼ ì§„í–‰</li>
<li>AlphaFold2ëŠ” ì „ ì„¸ê³„ì ìœ¼ë¡œ ë‹¨ë°±ì§ˆ ê¸°ëŠ¥ ì—°êµ¬, ì‹ ì•½ ê°œë°œ, ë°”ì´ì˜¤ ì—”ì§€ë‹ˆì–´ë§ ë“±ì—ì„œ í˜ì‹ ì ì¸ ê¸°ì—¬ë¥¼ í•˜ê³  ìˆìŒ</li>
</ul>
<h3 id="OpenAI-MLE-bench-ë°œí‘œ"><a href="#OpenAI-MLE-bench-ë°œí‘œ" class="headerlink" title="OpenAI, MLE-bench ë°œí‘œ"></a>OpenAI, MLE-bench ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/mle-bench/">ë§í¬</a>, 2024ë…„ 10ì›” 10ì¼</p>
<ul>
<li>OpenAIëŠ” ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´ë§ ì—­ëŸ‰ì„ ì¸¡ì •í•˜ê¸° ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ ë„êµ¬ <strong>MLE-bench</strong>ë¥¼ ë°œí‘œ</li>
<li>MLE-benchëŠ” Kaggleì˜ 75ê°œ ëŒ€íšŒì—ì„œ ì‹¤ì œ ML ì—”ì§€ë‹ˆì–´ë§ ê¸°ìˆ ì„ í‰ê°€í•˜ë©°, ë°ì´í„°ì…‹ ì¤€ë¹„, ëª¨ë¸ í•™ìŠµ, ì‹¤í—˜ ì‹¤í–‰ ë“± ì‹¤ë¬´ì—ì„œ ì¤‘ìš”í•œ ê¸°ìˆ ë“¤ì„ í…ŒìŠ¤íŠ¸</li>
<li>OpenAIì˜ <strong>o1-preview</strong> ëª¨ë¸ì€ <strong>AIDE</strong> scaffoldingì„ ì‚¬ìš©í•˜ì—¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ AI ì—ì´ì „íŠ¸ê°€ ê²½ìŸìë“¤ê³¼ ë¹„êµí•˜ì—¬ ì–¼ë§ˆë‚˜ íš¨ìœ¨ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ”ì§€ í‰ê°€, 16.9%ì˜ ëŒ€íšŒì—ì„œ <strong>Kaggle ë™ë©”ë‹¬ ìˆ˜ì¤€</strong>ì˜ ì„±ê³¼ë¥¼ ë‹¬ì„±</li>
<li>ì¶”ê°€ì ìœ¼ë¡œ, ìì› ìŠ¤ì¼€ì¼ë§(Resource Scaling) ë° ì‚¬ì „ í•™ìŠµ(Pre-training)ì—ì„œì˜ ì˜¤ì—¼(Contamination)ì´ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ì—¬ AI ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ ìµœì í™”ì— ê´€í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µ</li>
</ul>
<h3 id="OpenAI-Swarm-ë¼ì´ë¸ŒëŸ¬ë¦¬-ì¶œì‹œ"><a href="#OpenAI-Swarm-ë¼ì´ë¸ŒëŸ¬ë¦¬-ì¶œì‹œ" class="headerlink" title="OpenAI, Swarm ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶œì‹œ"></a>OpenAI, Swarm ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://github.com/openai/swarm/tree/main">ë§í¬</a>, 2024ë…„ 10ì›” 10ì¼</p>
<ul>
<li><strong>Swarm</strong>ì€ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆëŠ” ê²½ëŸ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, <strong>ë¬´ìƒíƒœ(stateless) ì¶”ìƒí™”</strong>ë¥¼ í†µí•´ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ ê°„ì˜ ìƒí˜¸ì‘ìš© ë° ì œì–´ íë¦„ì„ ê´€ë¦¬í•  ìˆ˜ ìˆìŒ</li>
<li>ê° ì—ì´ì „íŠ¸ëŠ” ê³ ìœ ì˜ **ì—­í• (Role)**ê³¼ **í•¨ìˆ˜ ì„¸íŠ¸(Available Functions)**ë¥¼ ì •ì˜í•˜ê³ , ëŒ€í™” íë¦„ì´ë‚˜ íŠ¹ì • ê¸°ì¤€ì— ë”°ë¼ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë¡œ ì œì–´ê¶Œì„ ë™ì ìœ¼ë¡œ ë„˜ê¸¸ ìˆ˜ ìˆìŒ</li>
<li><strong>Context Variables</strong>ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ìƒíƒœë¥¼ ìœ ì§€í•˜ê³  ì—ì´ì „íŠ¸ ê°„ ì •ë³´ ê³µìœ ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•¨</li>
<li>Swarmì€ ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©ì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì§€ì›í•˜ë©°, ë‹¤ì–‘í•œ ì—ì´ì „íŠ¸ì˜ í˜‘ì—… ë° ì œì–´ì— ìœ ì—°ì„±ì„ ì œê³µí•¨</li>
<li>ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë‹¤ì–‘í•œ ì‹¤í—˜ì  ê¸°ëŠ¥ì„ ì œê³µí•˜ì—¬ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ ì‰½ê²Œ êµ¬ì¶•í•˜ê³  í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë¨</li>
</ul>
<h3 id="Meta-Movie-Gen-ëª¨ë¸-ë°œí‘œ"><a href="#Meta-Movie-Gen-ëª¨ë¸-ë°œí‘œ" class="headerlink" title="Meta, Movie Gen ëª¨ë¸ ë°œí‘œ"></a>Meta, Movie Gen ëª¨ë¸ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://ai.meta.com/static-resource/movie-gen-research-paper">ë§í¬</a>, 2024ë…„ 10ì›” 4ì¼</p>
<ul>
<li>MetaëŠ” ê³ í’ˆì§ˆ 1080p HD ë¹„ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ì—ì„œ ìƒì„±í•  ìˆ˜ ìˆëŠ” <strong>Movie Gen</strong> ëª¨ë¸ì„ ë°œí‘œ</li>
<li>ì´ ëª¨ë¸ì€ <strong>30B íŒŒë¼ë¯¸í„°</strong>ë¥¼ ì‚¬ìš©í•˜ë©°, ìµœëŒ€ 16ì´ˆ ê¸¸ì´ì˜ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” <strong>73K ë¹„ë””ì˜¤ í† í°</strong>ì„ í™œìš©í•˜ì—¬ ë†’ì€ í•´ìƒë„ì™€ ê¸´ ë¬¸ë§¥ì„ ì²˜ë¦¬ ê°€ëŠ¥</li>
<li>ë¹„ë””ì˜¤ ìƒì„± ì™¸ì—ë„ <strong>ì‚¬ìš©ì ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ê°œì¸í™”ëœ ë¹„ë””ì˜¤ ìƒì„±</strong> ë° <strong>í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¹„ë””ì˜¤ í¸ì§‘</strong> ê¸°ëŠ¥ì„ ì œê³µ, ë°°ê²½ ë³€ê²½ ë˜ëŠ” ìŠ¤íƒ€ì¼ ë³€ê²½ê³¼ ê°™ì€ ì „ì—­ í¸ì§‘ë¿ ì•„ë‹ˆë¼ ê°œë³„ ìš”ì†Œ ì¶”ê°€, ì œê±° ë“± ì •ë°€í•œ í¸ì§‘ì´ ê°€ëŠ¥</li>
<li>ì´ ëª¨ë¸ì€ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìƒì„±, ë¹„ë””ì˜¤-ì˜¤ë””ì˜¤ ìƒì„±, ë¹„ë””ì˜¤ í¸ì§‘ ë“±ì—ì„œ <strong>ìµœì²¨ë‹¨ ì„±ëŠ¥</strong>ì„ ë³´ì—¬ì£¼ë©°, ë‹¤ì–‘í•œ ì—°êµ¬ì™€ ì°½ì˜ì  ì‘ì—…ì— í™œìš© ê°€ëŠ¥</li>
</ul>
<h3 id="Pyramid-Flow-SD3-ë¹„ë””ì˜¤-ìƒì„±-ëª¨ë¸-ë°œí‘œ"><a href="#Pyramid-Flow-SD3-ë¹„ë””ì˜¤-ìƒì„±-ëª¨ë¸-ë°œí‘œ" class="headerlink" title="Pyramid Flow, SD3 ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ ë°œí‘œ"></a>Pyramid Flow, SD3 ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://pyramid-flow.github.io/">ë§í¬</a>, 2024ë…„ 10ì›” 11ì¼</p>
<ul>
<li><strong>Pyramid Flow SD3</strong>ëŠ” 2B íŒŒë¼ë¯¸í„°ì˜ <strong>Diffusion Transformer</strong>(DiT) ëª¨ë¸ë¡œ, 10ì´ˆ ê¸¸ì´ì˜ <strong>768p í•´ìƒë„, 24fps ë¹„ë””ì˜¤</strong>ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ì„ ë°œí‘œ</li>
<li>ì´ ëª¨ë¸ì€ <strong>Flow Matching</strong> ê¸°ë°˜ì˜ íš¨ìœ¨ì ì¸ í•™ìŠµì„ í†µí•´ ê¸°ì¡´ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ ëŒ€ë¹„ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ë¹„ë””ì˜¤ ìƒì„±ì„ ì§€ì›</li>
<li>ë‘ ê°€ì§€ ë³€í˜• ëª¨ë¸ì„ ì œê³µí•˜ë©°, <strong>MIT ë¼ì´ì„ ìŠ¤</strong> í•˜ì— ê³µê°œë˜ì–´ ì˜¤í”ˆ ì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹°ì™€ì˜ í˜‘ì—…ì´ ê°€ëŠ¥</li>
<li>ì˜¤í”ˆ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë˜ì—ˆìœ¼ë©°, 20.7K GPU ì‹œê°„ ë™ì•ˆ í•™ìŠµë˜ì–´ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•¨</li>
</ul>
<h3 id="Rhymes-AI-Aria-ëª¨ë¸-ë°œí‘œ"><a href="#Rhymes-AI-Aria-ëª¨ë¸-ë°œí‘œ" class="headerlink" title="Rhymes AI, Aria ëª¨ë¸ ë°œí‘œ"></a>Rhymes AI, Aria ëª¨ë¸ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://rhymes.ai/blog-details/aria-first-open-multimodal-native-moe-model">ë§í¬</a>, 2024ë…„ 10ì›” 10ì¼</p>
<ul>
<li>Rhymes AIëŠ” <strong>Aria</strong>ë¼ëŠ” ì²« ì˜¤í”ˆ ë©€í‹°ëª¨ë‹¬ <strong>Mixture-of-Experts(MoE)</strong> ëª¨ë¸ì„ ë°œí‘œ, <strong>3.9B í™œì„± íŒŒë¼ë¯¸í„°</strong>ë¡œ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ì½”ë“œ ë“±ì˜ ë‹¤ì–‘í•œ ì…ë ¥ì„ ì²˜ë¦¬ ê°€ëŠ¥</li>
<li><strong>64K í† í°</strong>ì˜ ê¸´ ë¬¸ë§¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, <strong>256í”„ë ˆì„ ë¹„ë””ì˜¤</strong>ë¥¼ 10ì´ˆ ë§Œì— ìº¡ì…˜í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ì„±ëŠ¥ì„ ì œê³µ</li>
<li>AriaëŠ” ê²½ìŸì‚¬ ëª¨ë¸ë“¤(GPT-4o ë° Gemini Flash)ì„ ë›°ì–´ë„˜ëŠ” ì„±ëŠ¥ì„ ìë‘í•˜ë©°, ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬</li>
<li>ì´ ëª¨ë¸ì€ <strong>Apache 2.0 ë¼ì´ì„ ìŠ¤</strong> í•˜ì— ê³µê°œë˜ì—ˆìœ¼ë©°, ì˜¤í”ˆ ì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì‰½ê²Œ í™•ì¥ ê°€ëŠ¥</li>
</ul>
<h3 id="Microsoft-Diff-Transformer-ë°œí‘œ"><a href="#Microsoft-Diff-Transformer-ë°œí‘œ" class="headerlink" title="Microsoft, Diff Transformer ë°œí‘œ"></a>Microsoft, Diff Transformer ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2410.05258">ë§í¬</a>, 2024ë…„ 10ì›” 8ì¼</p>
<ul>
<li><strong>Diff Transformer</strong>ëŠ” <strong>ì°¨ë³„í™”ëœ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜</strong>ì„ ë„ì…í•œ ìƒˆë¡œìš´ Transformer ì•„í‚¤í…ì²˜ë¡œ, ê¸°ì¡´ self-attention ë©”ì»¤ë‹ˆì¦˜ì˜ í•œê³„ë¥¼ ê·¹ë³µ</li>
<li>**ì°¨ë³„ì  ì–´í…ì…˜(Differential Attention)**ëŠ”</li>
</ul>
<p>ë‘ ê°œì˜ softmax ì–´í…ì…˜ ë§µ ê°„ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ ì¡ìŒì„ ì œê±°í•˜ê³  ì¤‘ìš”í•œ ì •ë³´ì— ì§‘ì¤‘í•˜ëŠ” **í¬ì†Œ ì–´í…ì…˜ íŒ¨í„´(Sparse Attention Patterns)**ì„ ìƒì„±</p>
<ul>
<li>ì´ ëª¨ë¸ì€ <strong>ì¥ë¬¸ ë°ì´í„° ì²˜ë¦¬(long-context modeling)</strong> ë° <strong>í•µì‹¬ ì •ë³´ ê²€ìƒ‰</strong>ì—ì„œ ê¸°ì¡´ Transformer ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë°œíœ˜</li>
<li><strong>35-40% ì ì€ íŒŒë¼ë¯¸í„°</strong>ì™€ í•™ìŠµ í† í°ìœ¼ë¡œ ê¸°ì¡´ Transformer ëŒ€ë¹„ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ë©°, <strong>í™˜ê° í˜„ìƒ ê°ì†Œ</strong> ë° <strong>ë¬¸ë§¥ í•™ìŠµ ê°•í™”</strong>ì— ë„ì›€ì„ ì¤Œ</li>
<li>íŠ¹íˆ <strong>í”Œë˜ì‹œ ì–´í…ì…˜(FlashAttention) ì»¤ë„</strong>ì„ í™œìš©í•˜ì—¬ ê¸°ì¡´ í•˜ë“œì›¨ì–´ì—ì„œ ì‰½ê²Œ êµ¬í˜„ ê°€ëŠ¥</li>
</ul>
<h3 id="F5-TTS-í…ìŠ¤íŠ¸-ìŒì„±-ë³€í™˜-ëª¨ë¸-ë°œí‘œ"><a href="#F5-TTS-í…ìŠ¤íŠ¸-ìŒì„±-ë³€í™˜-ëª¨ë¸-ë°œí‘œ" class="headerlink" title="F5-TTS, í…ìŠ¤íŠ¸-ìŒì„± ë³€í™˜ ëª¨ë¸ ë°œí‘œ"></a>F5-TTS, í…ìŠ¤íŠ¸-ìŒì„± ë³€í™˜ ëª¨ë¸ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://github.com/SWivid/F5-TTS">ë§í¬</a>, 2024ë…„ 10ì›” 9ì¼</p>
<ul>
<li><strong>F5-TTS</strong>ëŠ” <strong>Flow Matching ê¸°ë°˜</strong>ì˜ ë¹„ìë°œì (Non-Autoregressive) í…ìŠ¤íŠ¸-ìŒì„± ë³€í™˜ ì‹œìŠ¤í…œìœ¼ë¡œ, ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ìŒì„± í•©ì„± ê¸°ëŠ¥ì„ ì œê³µ</li>
<li><strong>ConvNeXt</strong>ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í‘œí˜„ì„ ê°œì„ í•˜ê³  ìŒì„±ê³¼ì˜ ì •ë ¬ì„ ì‰½ê²Œ í•¨</li>
<li>ì´ ëª¨ë¸ì€ í•™ìŠµê³¼ ì¶”ë¡  ì†ë„ì—ì„œ ê¸°ì¡´ TTS ëª¨ë¸ë³´ë‹¤ <strong>ë¹ ë¥¸ ì„±ëŠ¥</strong>ì„ ì œê³µí•˜ë©°, <strong>ê°ì • ê¸°ë°˜ ìŒì„± í•©ì„±</strong>, <strong>ì½”ë“œ ì „í™˜</strong>, <strong>ì†ë„ ì œì–´</strong> ê¸°ëŠ¥ì„ ì§€ì›</li>
<li>100K ì‹œê°„ì˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í›ˆë ¨ë˜ì–´ <strong>ìì—°ìŠ¤ëŸ½ê³  í‘œí˜„ë ¥ ìˆëŠ” ìŒì„± í•©ì„±</strong> ì„±ëŠ¥ì„ ìë‘í•˜ë©°, ìƒì—…ì  ì´ìš©ì´ ê°€ëŠ¥í•œ <strong>CC-BY ë¼ì´ì„ ìŠ¤</strong>ë¡œ ì œê³µë¨</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹"><a href="#todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹" class="headerlink" title="(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,"></a>(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAIì—ì„œëŠ” ~~~ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<p><a href="link">ë§í¬</a>, date,</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://www.nobelprize.org/prizes/physics/2024/press-release/</span><br><span class="line">8 October 2024</span><br><span class="line"></span><br><span class="line">The Royal Swedish Academy of Sciences has decided to award the Nobel Prize in Physics 2024 to</span><br><span class="line"></span><br><span class="line">John J. Hopfield</span><br><span class="line">Princeton University, NJ, USA</span><br><span class="line"></span><br><span class="line">Geoffrey E. Hinton</span><br><span class="line">University of Toronto, Canada</span><br><span class="line"></span><br><span class="line">â€œfor foundational discoveries and inventions that enable machine learning with artificial neural networksâ€</span><br><span class="line"></span><br><span class="line">They trained artificial neural networks using physics</span><br><span class="line">This yearâ€™s two Nobel Laureates in Physics have used tools from physics to develop methods that are the foundation of todayâ€™s powerful machine learning. John Hopfield created an associative memory that can store and reconstruct images and other types of patterns in data. Geoffrey Hinton invented a method that can autonomously find properties in data, and so perform tasks such as identifying specific elements in pictures.</span><br><span class="line"></span><br><span class="line">When we talk about artificial intelligence, we often mean machine learning using artificial neural networks. This technology was originally inspired by the structure of the brain. In an artificial neural network, the brainâ€™s neurons are represented by nodes that have different values. These nodes influence each other through conÂ­nections that can be likened to synapses and which can be made stronger or weaker. The network is trained, for example by developing stronger connections between nodes with simultaneously high values. This yearâ€™s laureates have conducted important work with artificial neural networks from the 1980s onward.</span><br><span class="line"></span><br><span class="line">John Hopfield invented a network that uses a method for saving and recreating patterns. We can imagine the nodes as pixels. The Hopfield network utilises physics that describes a materialâ€™s characteristics due to its atomic spin â€“ a property that makes each atom a tiny magnet. The network as a whole is described in a manner equivalent to the energy in the spin system found in physics, and is trained by finding values for the connections between the nodes so that the saved images have low energy. When the Hopfield network is fed a distorted or incomplete image, it methodically works through the nodes and updates their values so the networkâ€™s energy falls. The network thus works stepwise to find the saved image that is most like the imperfect one it was fed with.</span><br><span class="line"></span><br><span class="line">Geoffrey Hinton used the Hopfield network as the foundation for a new network that uses a different method: the Boltzmann machine. This can learn to recognise characteristic elements in a given type of data. Hinton used tools from statistical physics, the science of systems built from many similar components. The machine is trained by feeding it examples that are very likely to arise when the machine is run. The Boltzmann machine can be used to classify images or create new examples of the type of pattern on which it was trained. Hinton has built upon this work, helping initiate the current explosive development of machine learning.</span><br><span class="line"></span><br><span class="line">â€œThe laureatesâ€™ work has already been of the greatest benefit. In physics we use artificial neural networks in a vast range of areas, such as developing new materials with specific properties,â€ says Ellen Moons, Chair of the Nobel Committee for Physics.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.nobelprize.org/prizes/chemistry/2024/press-release/</span><br><span class="line">Press release</span><br><span class="line">English</span><br><span class="line">English (pdf)</span><br><span class="line">Swedish</span><br><span class="line">Swedish (pdf)</span><br><span class="line">Logo</span><br><span class="line">9 October 2024</span><br><span class="line"></span><br><span class="line">The Royal Swedish Academy of Sciences has decided to award the Nobel Prize in Chemistry 2024</span><br><span class="line"></span><br><span class="line">with one half to</span><br><span class="line"></span><br><span class="line">David Baker</span><br><span class="line">University of Washington, Seattle, WA, USA</span><br><span class="line">Howard Hughes Medical Institute, USA</span><br><span class="line"></span><br><span class="line">â€œfor computational protein designâ€</span><br><span class="line"></span><br><span class="line">and the other half jointly to</span><br><span class="line"></span><br><span class="line">Demis Hassabis</span><br><span class="line">Google DeepMind, London, UK</span><br><span class="line"></span><br><span class="line">John M. Jumper</span><br><span class="line">Google DeepMind, London, UK</span><br><span class="line"></span><br><span class="line">â€œfor protein structure predictionâ€</span><br><span class="line"></span><br><span class="line">They cracked the code for proteinsâ€™ amazing structures</span><br><span class="line">The Nobel Prize in Chemistry 2024 is about proÂ­teins, lifeâ€™s ingenious chemical tools. David Baker has succeeded with the almost impossible feat of building entirely new kinds of proteins. Demis Hassabis and John Jumper have developed an AI model to solve a 50-year-old problem: predicting proteinsâ€™ complex structures. These discoveries hold enormous potential.</span><br><span class="line"></span><br><span class="line">The diversity of life testifies to proteinsâ€™ amazing capacity as chemical tools. They control and drive all the chemiÂ­cal reactions that together are the basis of life. Proteins also function as hormones, signal substances, antibodies and the building blocks of different tissues.</span><br><span class="line"></span><br><span class="line">â€œOne of the discoveries being recognised this year concerns the construction of spectacular proteins. The other is about fulfilling a 50-year-old dream: predicting protein structures from their amino acid sequences. Both of these discoveries open up vast possibilities,â€ says Heiner Linke, Chair of the Nobel Committee for Chemistry.</span><br><span class="line"></span><br><span class="line">Proteins generally consist of 20 different amino acids, which can be described as lifeâ€™s building blocks. In 2003, David Baker succeeded in using these blocks to design a new protein that was unlike any other protein. Since then, his research group has produced one imaginative protein creation after another, including proteins that can be used as pharmaceuticals, vaccines, nanomaterials and tiny sensors.</span><br><span class="line"></span><br><span class="line">The second discovery concerns the prediction of protein structures. In proteins, amino acids are linked together in long strings that fold up to make a three-dimensional structure, which is decisive for the proteinâ€™s function. Since the 1970s, researchers had tried to predict protein structures from amino acid sequences, but this was notoriously difficult. However, four years ago, there was a stunning breakthrough.</span><br><span class="line"></span><br><span class="line">In 2020, Demis Hassabis and John Jumper presented an AI model called AlphaFold2. With its help, they have been able to predict the structure of virtually all the 200 million proteins that researchers have identified. Since their breakthrough, AlphaFold2 has been used by more than two million people from 190 countries. Among a myriad of scientific applications, researchers can now better understand antibiotic resistance and create images of enzymes that can decompose plastic.</span><br><span class="line"></span><br><span class="line">Life could not exist without proteins. That we can now predict protein structures and design our own proteins confers the greatest benefit to humankind.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/mle-bench/</span><br><span class="line">OpenAI</span><br><span class="line">October 10, 2024</span><br><span class="line"></span><br><span class="line">MLE-bench</span><br><span class="line">Evaluating Machine Learning Agents on Machine Learning Engineering</span><br><span class="line"></span><br><span class="line">Read paper(opens in a new window)</span><br><span class="line">DALLÂ·E generated impressionistic oil painting of concentric pastel rectangular bars representing layers of assessment</span><br><span class="line">We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle&#x27;s publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup â€” OpenAI&#x27;s o1-preview with AIDE scaffolding â€” achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource-scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code(opens in a new window) to facilitate future research in understanding the ML engineering capabilities of AI agents.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/openai/swarm/tree/main</span><br><span class="line">OpenAI</span><br><span class="line">OpenAI released Swarm, a lightweight library for building multi-agent systems. Swarm provides a stateless abstraction to manage interactions and handoffs between multiple agents and does not use the Assistants API. ğŸ‘€</span><br><span class="line">How it works:</span><br><span class="line">1ï¸âƒ£ Define Agents, each with its own instructions, role (e.g., &quot;Sales Agent&quot;), and available functions (will be converted to JSON structures).</span><br><span class="line">2ï¸âƒ£ Define logic for transferring control to another agent based on conversation flow or specific criteria within agent functions. This handoff is achieved by simply returning the next agent to call within the function.</span><br><span class="line">3ï¸âƒ£ Context Variables provide initial context and update them throughout the conversation to maintain state and share information between agents.</span><br><span class="line">4ï¸âƒ£ Client.run() initiate and manage the multi-agent conversation. Needs initial agent, user messages, and context and returns a response containing updated messages, context variables, and the last active agent.</span><br><span class="line">Insights</span><br><span class="line">ğŸ”„ Swarm manages a loop of agent interactions, function calls, and potential handoffs.</span><br><span class="line">ğŸ§© Agents encapsulate instructions, available functions (tools), and handoff logic.</span><br><span class="line">ğŸ”Œ The framework is stateless between calls, offering transparency and fine-grained control.</span><br><span class="line">ğŸ› ï¸ Swarm supports direct Python function calling within agents.</span><br><span class="line">ğŸ“Š Context variables enable state management across agent interactions.</span><br><span class="line">ğŸ”„ Agent handoffs allow for dynamic switching between specialized agents.</span><br><span class="line">ğŸ“¡ Streaming responses are supported for real-time interaction.</span><br><span class="line">ğŸ§ª The framework is experimental. Maybe to collect feedback?</span><br><span class="line">ğŸ”§ Flexible and works with any OpenAI client, e.g. Hugging Face TGI or vLLM hosted models.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Overview</span><br><span class="line">Swarm focuses on making agent coordination and execution lightweight, highly controllable, and easily testable.</span><br><span class="line"></span><br><span class="line">It accomplishes this through two primitive abstractions: Agents and handoffs. An Agent encompasses instructions and tools, and can at any point choose to hand off a conversation to another Agent.</span><br><span class="line"></span><br><span class="line">These primitives are powerful enough to express rich dynamics between tools and networks of agents, allowing you to build scalable, real-world solutions while avoiding a steep learning curve.</span><br><span class="line"></span><br><span class="line">Note</span><br><span class="line"></span><br><span class="line">Swarm Agents are not related to Assistants in the Assistants API. They are named similarly for convenience, but are otherwise completely unrelated. Swarm is entirely powered by the Chat Completions API and is hence stateless between calls.</span><br><span class="line"></span><br><span class="line">Why Swarm</span><br><span class="line">Swarm explores patterns that are lightweight, scalable, and highly customizable by design. Approaches similar to Swarm are best suited for situations dealing with a large number of independent capabilities and instructions that are difficult to encode into a single prompt.</span><br><span class="line"></span><br><span class="line">The Assistants API is a great option for developers looking for fully-hosted threads and built in memory management and retrieval. However, Swarm is an educational resource for developers curious to learn about multi-agent orchestration. Swarm runs (almost) entirely on the client and, much like the Chat Completions API, does not store state between calls.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://ai.meta.com/static-resource/movie-gen-research-paper</span><br><span class="line">META</span><br><span class="line">Date: October 4, 2024</span><br><span class="line"></span><br><span class="line">Movie Gen: A Cast of Media Foundation Models</span><br><span class="line">The Movie Gen team @ Meta1</span><br><span class="line">1A detailed contributor list can be found in the appendix of this paper.</span><br><span class="line">We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos</span><br><span class="line">with different aspect ratios and synchronized audio. We also show additional capabilities such as</span><br><span class="line">precise instruction-based video editing and generation of personalized videos based on a userâ€™s image.</span><br><span class="line">Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization,</span><br><span class="line">video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation</span><br><span class="line">model is a 30B parameter transformer trained with a maximum context length of 73K video tokens,</span><br><span class="line">corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical</span><br><span class="line">innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data</span><br><span class="line">curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to</span><br><span class="line">reap the benefits of scaling pre-training data, model size, and training compute for training large scale</span><br><span class="line">media generation models. We hope this paper helps the research community to accelerate progress</span><br><span class="line">and innovation in media generation models.</span><br><span class="line">All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.</span><br><span class="line"></span><br><span class="line">Meta Movie Gen: the most advanced media foundation models to-date.</span><br><span class="line">Developed by AI research teams at Meta, Movie Gen delivers state-of-the-art results across a range of capabilities. Weâ€™re excited for the potential of this line of research to usher in entirely new possibilities for casual creators and creative professionals alike.</span><br><span class="line">More details and examples of what Movie Gen can do â¡ï¸</span><br><span class="line">https://go.fb.me/00mlgt</span><br><span class="line">Movie Gen Research Paper â¡ï¸</span><br><span class="line">https://go.fb.me/zfa8wf</span><br><span class="line"></span><br><span class="line">ğŸ› ï¸ Movie Gen models and capabilities</span><br><span class="line">â€¢ Movie Gen Video: A 30B parameter transformer model that can generate high-quality and high-definition images and videos from a single text prompt.</span><br><span class="line">â€¢ Movie Gen Audio: A 13B parameter transformer model can take a video input along with optional text prompts for controllability to generate high-fidelity audio synced to the video. It can generate ambient sound, instrumental background music and foley sound â€” delivering state-of-the-art results in audio quality, video-to-audio alignment and text-to-audio alignment.</span><br><span class="line">â€¢ Precise video editing: Using a generated or existing video and accompanying text instructions as an input it can perform localized edits such as adding, removing or replacing elements â€” or global changes like background or style changes.</span><br><span class="line">â€¢ Personalized videos: Using an image of a person and a text prompt, the model can generate a video with state-of-the-art results on character preservation and natural movement in video.</span><br><span class="line">Weâ€™re continuing to work closely with creative professionals from across the field to integrate their feedback as we work towards a potential release. We look forward to sharing more on this work and the creative possibilities it will enable in the future.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://pyramid-flow.github.io/</span><br><span class="line">2024.10.11</span><br><span class="line">First real good open-source text-to-video model with MIT license! Pyramid Flow SD3 is a 2B Diffusion Transformer (DiT) that can generate 10-second videos at 768p with 24fps! ğŸ¤¯ ğŸ¥âœ¨</span><br><span class="line">TL;DR;</span><br><span class="line">ğŸ¬ Can Generate 10-second videos at 768p/24FPS</span><br><span class="line">ğŸ¹ 2B parameter single unified Diffusion Transformer (DiT)</span><br><span class="line">ğŸ–¼ï¸ Supports both text-to-video AND image-to-video</span><br><span class="line">ğŸ§  Uses Flow Matching for efficient training</span><br><span class="line">ğŸ’» Two model variants: 384p (5s) and 768p (10s)</span><br><span class="line">ğŸ“¼ example videos on project page</span><br><span class="line">ğŸ› ï¸ Simple two-step implementation process</span><br><span class="line">ğŸ“š MIT License and available on</span><br><span class="line">Hugging Face</span><br><span class="line">âœ… Trained only on open-source datasets</span><br><span class="line">ğŸ”œ Training code coming soon!</span><br><span class="line"></span><br><span class="line">Pyramidal Flow Matching for Efficient Video Generative Modeling</span><br><span class="line">Yang Jin1, Zhicheng Sun1, Ningyuan Li3, Kun Xu2, Kun Xu2, Hao Jiang1, Nan Zhuang2, Quzhe Huang2, Yang Song, Yadong Mu1â€ , Zhouchen Lin1</span><br><span class="line">1Peking University, 2Kuaishou Technology, 3Beijing University of Posts and Telecommunications</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The following videos are from a training-efficient Autoregressive Video Generation model based on Flow Matching. It is trained only on open-source datasets within 20.7k A100 GPU hours.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://rhymes.ai/blog-details/aria-first-open-multimodal-native-moe-model</span><br><span class="line">ğŸš¨ Rhymes.AI released Aria - Multimodal MoE (3.9B active), 64K tokens, caption 256 frames in 10 sec, Apache 2.0 licensed! Beats GPT4o &amp; Gemini Flash âš¡</span><br><span class="line">&gt; 3.9B Active, 25.3B Total parameters</span><br><span class="line">&gt; Significantly better than Pixtral 12B, Llama Vision 11B &amp; Qwen VL</span><br><span class="line">&gt; Trained on 7.5T tokens</span><br><span class="line">&gt; Four stage training:</span><br><span class="line">- 6.4T language pre-training</span><br><span class="line">- 1.4T multimodal pre-training</span><br><span class="line">- 35B long context training</span><br><span class="line">- 20B high quality post-training</span><br><span class="line">Architecture:</span><br><span class="line">&gt; Aria consists of a vision encoder and a mixture-of-experts (MoE) decoder</span><br><span class="line">&gt; Vision encoder:</span><br><span class="line">- Produces visual tokens for images/videos in native aspect ratio</span><br><span class="line">- Operates in three resolution modes: medium, high, and ultra-high</span><br><span class="line">- Medium-resolution: 128 visual tokens</span><br><span class="line">- High-resolution: 256 visual tokens</span><br><span class="line">- Ultra-high resolution: Dynamically decomposed into multiple high-resolution sub-images</span><br><span class="line">&gt; MoE decoder:</span><br><span class="line">- Multimodal native, conditioned on both language and visual input tokens</span><br><span class="line">- 66 experts per MoE layer</span><br><span class="line">- 2 experts shared among all inputs to capture common knowledge</span><br><span class="line">- 6 additional experts activated per token by a router module</span><br><span class="line">&gt; Models on the Hub &amp; Integrated with Transformers!</span><br><span class="line">October 10, 2024</span><br><span class="line">10 min read</span><br><span class="line">Rhymes AI Team</span><br><span class="line">Share:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Aria: First Open Multimodal Native MoE Model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Aria Multimodal Native MoE - An Open Model for ALL Modalities</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Rhymes AI is proud to introduce Aria, the worldâ€™s first open-source, multimodal native Mixture-of-Experts (MoE) model.</span><br><span class="line"></span><br><span class="line">In short, Aria features:</span><br><span class="line"></span><br><span class="line">Multimodal native understanding:</span><br><span class="line"></span><br><span class="line">State-of-the-art performance on a wide range of multimodal and language tasks</span><br><span class="line">Pre-trained from scratch on a mixture of multimodal and language data</span><br><span class="line">Lightweight and fast:</span><br><span class="line"></span><br><span class="line">Fine-grained mixture-of-expert model with 3.9B activated parameters per token</span><br><span class="line">Efficient and informative visual encoding of variable image sizes and aspect ratios</span><br><span class="line">Long context window:</span><br><span class="line"></span><br><span class="line">Long multimodal context window of 64K tokens, captioning a 256-frame video in 10 seconds</span><br><span class="line">Open:</span><br><span class="line"></span><br><span class="line">Open model weights ğŸ¤—, code repository ğŸ’», technical report ğŸ“ for collaborative development.</span><br><span class="line">License: Apache 2.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Multimodal Native Performance</span><br><span class="line"></span><br><span class="line">Aria Multimodal Native MoE - An Open Model for ALL ModalitiesFigure 1. Aria is a multimodal native model that excels at understanding text, vision, code.</span><br><span class="line"></span><br><span class="line">Aria processes text, images, video, and code all at once, without needing separate setups for each type, demonstrating the advantages of a multimodal native model.</span><br><span class="line"></span><br><span class="line">We provide a quantifiable definition for the term multimodal native:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">A multimodal native model refers to a single model with strong understanding capabilities across multiple input modalities (e.g., text, code, image, video) that matches or exceeds the modality-specialized models of similar capacities.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">We compared Aria against the best open and closed multimodal native models across established benchmarks, highlighting the following key observations:</span><br><span class="line"></span><br><span class="line">Best-in-Class Performance: Aria is the leading multimodal native model, demonstrating clear advantages over Pixtral-12B and Llama3.2-11B across a range of multimodal, language, and coding tasks.</span><br><span class="line">Competitive Against Proprietary Models: Aria performs competitively against proprietary models like GPT-4o and Gemini-1.5 on multimodal tasks, including document understanding, chart reading, scene text recognition, and video understanding.</span><br><span class="line">Parameter Efficiency: Aria is the most parameter-efficient open model. Thanks to the MoE framework, Aria activates only 3.9 billion parameters, compared to the full activation in models like Pixtral-12B and Llama3.2-11B.</span><br><span class="line"></span><br><span class="line">Aria Multimodal Native MoE - An Open Model for ALL ModalitiesFigure 2. Aria shows best-in-class benchmark performance on multimodal, language, coding tasks.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Long Multimodal Input Understanding</span><br><span class="line"></span><br><span class="line">Multimodal data is often complex, involving long sequences that combine visuals and text, like videos with subtitles or long documents. For a model to be effective in real-world applications, it must be capable of understanding and processing such data efficiently.</span><br><span class="line"></span><br><span class="line">Aria excels in this area, demonstrating superior long multimodal input understanding. It outperforms larger open models, proving its efficiency and effectiveness despite its size. When compared to proprietary models, Aria surpasses GPT-4o mini in long video understanding and outperforms Gemini-1.5-Flash in long document understanding. This makes Aria a preferred choice for processing extensive multimodal data in a compute-and-time-efficient manner, delivering faster and more accurate results in real-world scenarios.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Aria Multimodal Native MoE - An Open Model for ALL Modalities</span><br><span class="line">Figure 3. Aria excels in long multimodal input understanding, such as long video understanding.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Instruction Following</span><br><span class="line"></span><br><span class="line">Aria is highly effective at understanding and following instructions on both multimodal and language inputs, performing better than top open-source models on both MIA-Bench and MT-Bench.</span><br><span class="line"></span><br><span class="line">Aria Multimodal Native MoE - An Open Model for ALL ModalitiesFigure 4. Aria is highly effective at instruction following on multimodal and language inputs.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2410.05258</span><br><span class="line">Microsoft</span><br><span class="line"></span><br><span class="line">Differential Transformer</span><br><span class="line">Published on Oct 8</span><br><span class="line">Â·</span><br><span class="line">Submitted by</span><br><span class="line">unilm</span><br><span class="line">on Oct 8</span><br><span class="line">#1 Paper of the day</span><br><span class="line">Authors:</span><br><span class="line"></span><br><span class="line">Tianzhu Ye</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Li Dong</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Yuqing Xia</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Yutao Sun</span><br><span class="line">,</span><br><span class="line">Yi Zhu</span><br><span class="line">,</span><br><span class="line">Gao Huang</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Furu Wei</span><br><span class="line">Abstract</span><br><span class="line">Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.</span><br><span class="line"></span><br><span class="line">âš¡ï¸ Most important breakthrough this month: Differential Transformer vastly improves attention â‡’ better retrieval and fewer hallucinations!</span><br><span class="line">Thought that self-attention could not be improved anymore?</span><br><span class="line">Researchers from Microsoft Research and Tsinghua University have dropped a novel &quot;differential attention&quot; mechanism that amplifies focus on relevant context while canceling out noise. It sounds like a free lunch, but it does really seem to vastly improve LLM performance!</span><br><span class="line">Key insights:</span><br><span class="line">ğŸ§  Differential attention computes the difference between two separate softmax attention maps, canceling out noise and promoting sparse attention patterns</span><br><span class="line">ğŸ”¥ DIFF Transformer outperforms standard Transformers while using 35-40% fewer parameters or training tokens</span><br><span class="line">ğŸ“ Scales well to long contexts up to 64K tokens, leveraging increasing context length more effectively</span><br><span class="line">ğŸ” Dramatically improves key information retrieval, enhancing in-context learning, and possibly reducing risk of hallucinations ğŸ¤¯</span><br><span class="line">ğŸ”¢ Reduces activation outliers, potentially enabling lower-bit quantization without performance drop!</span><br><span class="line">âš™ï¸ Can be directly implemented using existing FlashAttention kernels</span><br><span class="line">This new architecture could lead much more capable LLMs, with vastly improved strengths in long-context understanding and factual accuracy.</span><br><span class="line">But they didnâ€™t release weights on the Hub: letâ€™s wait for the community to train the first open-weights DiffTransformer! ğŸš€</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/SWivid/F5-TTS</span><br><span class="line">[Submitted on 9 Oct 2024]</span><br><span class="line">F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching</span><br><span class="line">Yushen Chen, Zhikang Niu, Ziyang Ma, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, Xie Chen</span><br><span class="line">This paper introduces F5-TTS, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length as input speech, and then the denoising is performed for speech generation, which was originally proved feasible by E2 TTS. However, the original design of E2 TTS makes it hard to follow due to its slow convergence and low robustness. To address these issues, we first model the input with ConvNeXt to refine the text representation, making it easy to align with the speech. We further propose an inference-time Sway Sampling strategy, which significantly improves our model&#x27;s performance and efficiency. This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining. Our design allows faster training and achieves an inference RTF of 0.15, which is greatly improved compared to state-of-the-art diffusion-based TTS models. Trained on a public 100K hours multilingual dataset, our Fairytaler Fakes Fluent and Faithful speech with Flow matching (F5-TTS) exhibits highly natural and expressive zero-shot ability, seamless code-switching capability, and speed control efficiency. Demo samples can be found at this https URL. We release all code and checkpoints to promote community development.</span><br><span class="line"></span><br><span class="line">Let&#x27;s goo! F5-TTS ğŸ”Š</span><br><span class="line">&gt; Trained on 100K hours of data</span><br><span class="line">&gt; Zero-shot voice cloning</span><br><span class="line">&gt; Speed control (based on total duration)</span><br><span class="line">&gt; Emotion based synthesis</span><br><span class="line">&gt; Long-form synthesis</span><br><span class="line">&gt; Supports code-switching</span><br><span class="line">&gt; Best part: CC-BY license (commercially permissive)ğŸ”¥</span><br><span class="line">Diffusion based architecture:</span><br><span class="line">&gt; Non-Autoregressive + Flow Matching with DiT</span><br><span class="line">&gt; Uses ConvNeXt to refine text representation, alignment</span><br><span class="line">Synthesised: I was, like, talking to my friend, and sheâ€™s all, um, excited about her, uh, trip to Europe, and Iâ€™m just, like, so jealous, right? (Happy emotion)</span><br><span class="line">The TTS scene is on fire! ğŸ</span><br></pre></td></tr></table></figure>

<p>ê¸°ìˆ ì ìœ¼ë¡œ ìµœëŒ€í•œ ìì„¸í•˜ê²Œ ì ì–´. 9ê°œì˜ ê¸°ì‚¬ê°€ ìˆê³  í•˜ë‚˜ë„ ë¹¼ë¨¹ì§€ ë§ê³  ì ì–´.</p>
</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-10-15</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/10/15/2024-10-15-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 10ì›” 15ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/10/04/2024-10-4-AI-NEWS/" title="2024ë…„ 10ì›” 4ì¼ AI ì†Œì‹">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>