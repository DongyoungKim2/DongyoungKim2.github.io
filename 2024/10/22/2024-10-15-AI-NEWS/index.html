<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 10월 22일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="Meta에서는 텍스트와 음성을 자유롭게 혼합할 수 있는 다중모달 언어 모델인 Spirit LM을 공개하였습니다. NVIDIA는 인간의 선호도에 더 잘 맞는 LLM 정렬을 위해 새로운 보상 모델을 발표하였으며, IBM은 고정밀도와 효율성을 갖춘 새로운 Granite 3."><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 10월 22일 AI 소식</a></h3></div><div class="post-content"><p>Meta에서는 텍스트와 음성을 자유롭게 혼합할 수 있는 다중모달 언어 모델인 Spirit LM을 공개하였습니다. NVIDIA는 인간의 선호도에 더 잘 맞는 LLM 정렬을 위해 새로운 보상 모델을 발표하였으며, IBM은 고정밀도와 효율성을 갖춘 새로운 Granite 3.0 생성 AI 모델을 출시하였습니다. Mistral AI는 엣지 컴퓨팅을 위한 고성능 모델인 Ministral 3B와 8B를 소개하였고, Microsoft는 1비트 LLM의 공식 추론 프레임워크인 bitnet.cpp를 공개하였습니다. Neural Magic은 양자화된 LLM에 대한 광범위한 평가를 실시하여 성능 저하가 미미하다는 것을 발견하였습니다. NVIDIA는 기존의 대형 언어 모델을 Mixture of Experts(MoE)로 업사이클링하는 연구를 발표하였으며, Google DeepMind는 LLM의 추론 능력을 향상시키기 위한 새로운 프로세스 보상 모델을 제안하였습니다. 또한, simular-ai는 컴퓨터와 인간처럼 상호 작용할 수 있는 오픈 에이전트 프레임워크인 Agent S를 소개하였고, Apple은 LLM의 수학적 추론 한계를 이해하기 위한 GSM-Symbolic을 발표하였습니다.</p>
<h3 id="Meta-SPIRIT-LM-Interleaved-Spoken-and-Written-Language-Model"><a href="#Meta-SPIRIT-LM-Interleaved-Spoken-and-Written-Language-Model" class="headerlink" title="Meta, SPIRIT-LM: Interleaved Spoken and Written Language Model"></a>Meta, SPIRIT-LM: Interleaved Spoken and Written Language Model</h3><p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/spiritlm">링크</a>, 2024년 10월 22일</p>
<ul>
<li>텍스트와 음성을 자유롭게 혼합할 수 있는 다중모달 언어 모델 Spirit LM을 소개</li>
<li>7B 사전학습된 텍스트 언어 모델을 기반으로 음성 모달리티를 추가</li>
<li>텍스트와 음성 시퀀스를 단일 토큰 스트림으로 연결하여 학습</li>
<li>음성-텍스트 병렬 코퍼스를 활용한 단어 수준의 인터리빙 방법 사용</li>
<li>베이스 버전과 익스프레시브 버전 두 가지 모델 제공<ul>
<li>베이스 버전은 음성 음운 단위(HuBERT) 사용</li>
<li>익스프레시브 버전은 음운 단위에 피치와 스타일 단위를 추가하여 표현력 모델링</li>
</ul>
</li>
<li>다양한 작업을 소수의 예제로 학습 가능 (ASR, TTS, 음성 분류 등)</li>
<li>연구 커뮤니티가 텍스트와 음성 통합을 위한 새로운 접근법을 발전시키길 희망</li>
</ul>
<h3 id="NVIDIA-New-Reward-Model-Helps-Improve-LLM-Alignment-with-Human-Preferences"><a href="#NVIDIA-New-Reward-Model-Helps-Improve-LLM-Alignment-with-Human-Preferences" class="headerlink" title="NVIDIA, New Reward Model Helps Improve LLM Alignment with Human Preferences"></a>NVIDIA, New Reward Model Helps Improve LLM Alignment with Human Preferences</h3><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/new-reward-model-helps-improve-llm-alignment-with-human-preferences/">링크</a>, 2024년 10월 3일</p>
<ul>
<li>인간의 선호도에 맞게 AI 시스템을 정렬하기 위한 강화 학습(RLHF)의 중요성 강조</li>
<li>NVIDIA에서 Llama 3.1-Nemotron-70B-Reward라는 최첨단 보상 모델 출시<ul>
<li>Hugging Face RewardBench 리더보드에서 전체 1위 달성 (94.1% 정확도)</li>
<li>다양한 카테고리에서 높은 성능 (안전성 95.1%, 추론 98.1%)</li>
</ul>
</li>
<li>보상 모델을 활용하여 Llama 3.1-Nemotron-70B-Instruct 모델을 훈련<ul>
<li>Arena Hard 리더보드에서 상위권 달성</li>
</ul>
</li>
<li>보상 모델은 CC-BY-4.0 라이선스의 HelpSteer2 데이터를 사용하여 훈련</li>
<li>NVIDIA NIM inference microservice로 쉽게 배포 가능</li>
<li>기업용 사례에 적용할 수 있도록 모델과 데이터 공개</li>
</ul>
<h3 id="Mistral-AI-Un-Ministral-des-Ministraux"><a href="#Mistral-AI-Un-Ministral-des-Ministraux" class="headerlink" title="Mistral AI, Un Ministral, des Ministraux"></a>Mistral AI, Un Ministral, des Ministraux</h3><p><a target="_blank" rel="noopener" href="https://mistral.ai/news/ministraux/">링크</a>, 2024년 10월 16일</p>
<ul>
<li>엣지 컴퓨팅을 위한 고성능 모델 Ministral 3B와 Ministral 8B 출시</li>
<li>작은 모델 크기로도 높은 지식, 상식, 추론 능력 제공</li>
<li>128k의 컨텍스트 길이 지원 (현재는 vLLM에서 32k 지원)</li>
<li>Ministral 8B는 빠르고 메모리 효율적인 추론을 위한 인터리브드 슬라이딩 윈도우 어텐션 패턴 사용</li>
<li>다양한 사용 사례에 적용 가능 (에이전틱 워크플로우, 전문 작업자 등)</li>
<li>Mistral 7B 대비 향상된 성능과 효율성 제공</li>
<li>API 및 라이선스 정보 제공, 모델 가중치 연구용으로 공개</li>
</ul>
<h3 id="IBM-IBM’s-New-Granite-3-0-Generative-AI-Models-Are-Small-Yet-Highly-Accurate-and-Efficient"><a href="#IBM-IBM’s-New-Granite-3-0-Generative-AI-Models-Are-Small-Yet-Highly-Accurate-and-Efficient" class="headerlink" title="IBM, IBM’s New Granite 3.0 Generative AI Models Are Small, Yet Highly Accurate and Efficient"></a>IBM, IBM’s New Granite 3.0 Generative AI Models Are Small, Yet Highly Accurate and Efficient</h3><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/ibms-new-granite-3-0-generative-ai-models-are-small-yet-highly-accurate-and-efficient/?ncid=so-link-654107">링크</a>, 2024년 10월 21일</p>
<ul>
<li>IBM에서 Granite 3.0 시리즈 모델 출시<ul>
<li>Granite 3.0 8B, Granite 3.0 2B 등</li>
<li>Mixture of Experts(MoE) LLMs: Granite 3.0 3B-A800M, Granite 3.0 1B-A400M</li>
</ul>
</li>
<li>고정밀도와 효율성을 겸비한 모델로, 기업용 워크플로우의 기본 구성 요소로 설계</li>
<li>함수 호출을 지원하여 도구 기반의 다양한 사용 사례에 적용 가능</li>
<li>그룹 쿼리 어텐션(GQA) 및 RoPE 등 최적화된 아키텍처 사용</li>
<li>추론 속도를 높이기 위한 speculative decoding 기법 적용<ul>
<li>추론 속도를 높이고 자원 사용을 최적화</li>
</ul>
</li>
<li>NVIDIA NIM inference microservice로 패키징되어 배포 용이</li>
</ul>
<h3 id="Microsoft-bitnet-cpp"><a href="#Microsoft-bitnet-cpp" class="headerlink" title="Microsoft, bitnet.cpp"></a>Microsoft, bitnet.cpp</h3><p><a target="_blank" rel="noopener" href="https://github.com/microsoft/BitNet">링크</a>, 2024년 10월 18일</p>
<ul>
<li>1비트 LLM의 공식 추론 프레임워크인 bitnet.cpp 공개</li>
<li>CPU에서 빠르고 손실 없는 추론을 지원하는 최적화된 커널 제공</li>
<li>NPU 및 GPU 지원 예정</li>
<li>ARM 및 x86 CPU에서 최대 6배 이상의 속도 향상 및 에너지 소비 감소 달성</li>
<li>100B BitNet b1.58 모델을 단일 CPU에서 실행 가능</li>
<li>MIT 라이선스 버전으로 공개</li>
</ul>
<h3 id="Neural-Magic-We-Ran-Over-Half-a-Million-Evaluations-on-Quantized-LLMs-Here’s-What-We-Found"><a href="#Neural-Magic-We-Ran-Over-Half-a-Million-Evaluations-on-Quantized-LLMs-Here’s-What-We-Found" class="headerlink" title="Neural Magic, We Ran Over Half a Million Evaluations on Quantized LLMs: Here’s What We Found"></a>Neural Magic, We Ran Over Half a Million Evaluations on Quantized LLMs: Here’s What We Found</h3><p><a target="_blank" rel="noopener" href="https://neuralmagic.com/blog/we-ran-over-half-a-million-evaluations-on-quantized-llms-heres-what-we-found/">링크</a>, 2024년 10월 17일</p>
<ul>
<li>양자화된 LLM이 성능 저하 없이 효율성을 높일 수 있는지에 대한 연구 수행</li>
<li>Llama 3.1 모델에 대해 다양한 양자화 스킴으로 평가 진행</li>
<li>양자화로 인해 성능 저하는 미미하며, 최대 2.4배의 추론 속도 향상 및 3.5배의 모델 크기 감소 달성</li>
<li>Academic 및 Real-World 벤치마크에서 평균 99% 이상의 정확도 유지</li>
<li>W8A8-FP8 동적 양자화가 가장 우수한 결과를 보임</li>
<li>모델은 Hugging Face에서 이용 가능</li>
</ul>
<h3 id="NVIDIA-Upcycling-Large-Language-Models-into-Mixture-of-Experts"><a href="#NVIDIA-Upcycling-Large-Language-Models-into-Mixture-of-Experts" class="headerlink" title="NVIDIA, Upcycling Large Language Models into Mixture of Experts"></a>NVIDIA, Upcycling Large Language Models into Mixture of Experts</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.07524">링크</a>, 2024년 10월 11일</p>
<ul>
<li>기존의 대형 언어 모델을 Mixture of Experts(MoE)로 업사이클링하는 연구 발표</li>
<li>새로운 “가상 그룹” 초기화 스킴 및 가중치 스케일링 방법 제안</li>
<li>업사이클링을 통해 동일한 데이터로 학습한 모델보다 성능 향상</li>
<li>Nemotron-4 15B 모델을 1조 토큰으로 업사이클링하여 MMLU 성능 향상 (65.3% → 67.6%)</li>
<li>MoE 언어 모델 구축을 위한 최적의 방법과 인사이트 제공</li>
</ul>
<h3 id="Google-DeepMind-Rewarding-Progress-Scaling-Automated-Process-Verifiers-for-LLM-Reasoning"><a href="#Google-DeepMind-Rewarding-Progress-Scaling-Automated-Process-Verifiers-for-LLM-Reasoning" class="headerlink" title="Google DeepMind, Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning"></a>Google DeepMind, Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/papers/2410.08146">링크</a>, 2024년 10월 11일</p>
<ul>
<li>LLM의 추론 능력을 향상시키기 위한 새로운 프로세스 보상 모델(PRMs) 제안</li>
<li>인간의 레이블 없이 자동화된 데이터로 PRMs를 훈련하는 방법 제시</li>
<li>단계별로 진척도를 측정하여 보상함으로써 강화 학습의 효율성 향상</li>
<li>베이스 모델과 별도의 “Prover” LLM을 사용하여 성능 향상</li>
<li>기존의 결과 기반 보상 모델보다 정확도가 8% 이상 향상되고, 데이터 효율성은 최대 6배 개선</li>
</ul>
<h3 id="simular-ai-Agent-S-An-Open-Agentic-Framework-that-Uses-Computers-Like-a-Human"><a href="#simular-ai-Agent-S-An-Open-Agentic-Framework-that-Uses-Computers-Like-a-Human" class="headerlink" title="simular-ai, Agent S: An Open Agentic Framework that Uses Computers Like a Human"></a>simular-ai, Agent S: An Open Agentic Framework that Uses Computers Like a Human</h3><p><a target="_blank" rel="noopener" href="https://github.com/simular-ai/Agent-S">링크</a>, 2024년 10월 10일</p>
<ul>
<li>인간처럼 컴퓨터와 상호 작용할 수 있는 오픈 에이전트 프레임워크 Agent S 공개</li>
<li>그래픽 사용자 인터페이스(GUI)를 통해 자동화된 복잡한 다단계 작업 수행 가능</li>
<li>도메인별 지식 획득, 긴 작업 계획, 동적 인터페이스 처리 등의 문제 해결</li>
<li>경험 기반의 계층적 플래닝 도입으로 효율적인 작업 계획 및 하위 작업 실행</li>
<li>다중모달 대형 언어 모델(MLLMs)을 기반으로 한 에이전트-컴퓨터 인터페이스(ACI) 사용</li>
<li>OSWorld 벤치마크에서 기존 대비 83.6%의 상대적 개선 달성</li>
<li>WindowsAgentArena 벤치마크에서의 일반화 능력 입증</li>
</ul>
<h3 id="Apple-GSM-Symbolic-Understanding-the-Limitations-of-Mathematical-Reasoning-in-Large-Language-Models"><a href="#Apple-GSM-Symbolic-Understanding-the-Limitations-of-Mathematical-Reasoning-in-Large-Language-Models" class="headerlink" title="Apple, GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models"></a>Apple, GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models</h3><p><a target="_blank" rel="noopener" href="https://machinelearning.apple.com/research/gsm-symbolic">링크</a>, 2024년 10월</p>
<ul>
<li>LLM의 수학적 추론 능력의 한계를 이해하기 위한 연구 발표</li>
<li>GSM8K 벤치마크의 한계를 극복하기 위해 GSM-Symbolic 벤치마크 도입</li>
<li>심볼릭 템플릿을 사용하여 다양한 수학 문제 생성 및 평가</li>
<li>LLM이 동일한 문제의 수치만 변경해도 성능이 저하되는 현상 발견</li>
<li>논리적 추론보다 학습 데이터의 추론 단계를 복제하는 경향 분석</li>
<li>추가적인 조건이 포함되면 성능이 크게 저하되는 취약성 확인</li>
<li>LLM의 수학적 추론 능력에 대한 보다 깊은 이해 제공</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://github.com/facebookresearch/spiritlm</span><br><span class="line">META</span><br><span class="line">SPIRIT-LM: Interleaved Spoken and Written Language Model</span><br><span class="line"></span><br><span class="line">We introduce Spirit LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single stream of tokens, and trained with a word-level interleaving method using a small automatically-curated speech-text parallel corpus. Spirit LM comes in two versions: a Base version that uses speech phonetic units (HuBERT) and an Expressive version that models expressivity using pitch and style units in addition to the phonetic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that Spirit LM can learn new tasks in a few-shot fashion across modalities (i.e. ASR, TTS, Speech Classification).</span><br><span class="line">.</span><br><span class="line"></span><br><span class="line">Meta Spirit LM is our first open source multimodal language model that freely mixes text and speech.</span><br><span class="line">Details, code and model weights ➡️</span><br><span class="line">https://go.fb.me/2jooyy</span><br><span class="line"></span><br><span class="line">Many existing AI voice experiences today use ASR to techniques to process speech before synthesizing with an LLM to generate text — but these approaches compromise the expressive aspects of speech in inputs and outputs. Using phonetic, pitch and tone tokens, Spirit LM models can overcome these limitations to better understand and generate more natural sounding speech while also learning new tasks across ASR, TTS and speech classification.</span><br><span class="line">We hope that sharing this work will enable the research community to further new approaches for text and speech integration.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://developer.nvidia.com/blog/new-reward-model-helps-improve-llm-alignment-with-human-preferences/</span><br><span class="line">NVIDIA</span><br><span class="line">New Reward Model Helps Improve LLM Alignment with Human Preferences</span><br><span class="line">Oct 03, 2024</span><br><span class="line">By Zhilin Wang and Chintan Patel</span><br><span class="line"></span><br><span class="line">+8</span><br><span class="line">Like</span><br><span class="line"> Discuss (0)</span><br><span class="line">Nemotron icon in front of multiple tiles with icons and three sliders each, in colors of green, purple, and grey.</span><br><span class="line">Reinforcement learning from human feedback (RLHF) is essential for developing AI systems that are aligned with human values and preferences. RLHF enables the most capable LLMs, including ChatGPT, Claude, and Nemotron families, to generate exceptional responses.</span><br><span class="line"></span><br><span class="line">By integrating human feedback into the training process, RLHF enables models to learn more nuanced behaviors and make decisions that better reflect user expectations. This approach enhances the quality of AI-generated responses and fosters trust and reliability in AI applications.</span><br><span class="line"></span><br><span class="line">To help the AI community easily adopt RLHF to build and customize models, NVIDIA has released Llama 3.1-Nemotron-70B-Reward, a state-of-the-art reward model that scores the responses generated by LLMs. Such scores can be used to improve LLM response quality, making a more positive and impactful interaction between humans and AI.</span><br><span class="line"></span><br><span class="line">NVIDIA researchers leveraged the reward model to train Llama 3.1-Nemotron-70B-Instruct model, which is among the top models on Arena Hard leaderboard.</span><br><span class="line"></span><br><span class="line">#1 reward model</span><br><span class="line">The Llama 3.1-Nemotron-70B-Reward model currently is in first place on the Hugging Face RewardBench leaderboard for evaluating the capabilities, safety, and pitfalls of reward models.</span><br><span class="line"></span><br><span class="line">The model scored 94.1% on Overall RewardBench, meaning that it can identify responses that align with human preferences 94% of the time.</span><br><span class="line"></span><br><span class="line">Screenshot of the leaderboard shows the ranking of various reward models and their accuracy across different categories. The model on the top of the RewardBench leaderboard is NVIDIA’s Llama-3.1-Nemotron-70B Reward model.</span><br><span class="line">Figure 1. Llama-3.1-Nemtron-70B-Reward tops RewardBench leaderboard across various categories</span><br><span class="line">The model scores well across all four categories: Chat, Chat-Hard, Safety, and Reasoning. It has an impressive performance for Safety and Reasoning, achieving 95.1% and 98.1% accuracy, respectively. This means that the model can safely reject potential unsafe responses and support RLHF in domains like math and code.</span><br><span class="line"></span><br><span class="line">With just a fifth the size of Nemotron-4 340B Reward, this model delivers high compute efficiency coupled with superior accuracy. This model is also trained only on CC-BY-4.0-licensed HelpSteer2 data, which makes it feasible for enterprise use cases.</span><br><span class="line"></span><br><span class="line">Implementation</span><br><span class="line">To train this model, we combined two popular approaches to make the best of both worlds:</span><br><span class="line"></span><br><span class="line">Regression-style reward models</span><br><span class="line">Bradley-Terry reward model</span><br><span class="line">We trained with both approaches using data that we released in HelpSteer2. An important contributor to the model performance is high data quality, which we meticulously curated and then released to advance AI for all.</span><br><span class="line"></span><br><span class="line">Leading large language model</span><br><span class="line">Using the trained Reward Model and HelpSteer2-Preference Prompts for RLHF training (specifically with the REINFORCE algorithm) produces a model that scores 85 on Arena Hard, a popular automatic evaluation tool for instruction-tuned LLMs. This makes this the best leading models on the Arena Hard Leaderboard, among models that do not require additional test-time compute.</span><br><span class="line"></span><br><span class="line">The Llama-3.1-Nemotron-70B-Instruct model comes with Llama-3.1 License, making it easy for research and enterprises to customize and integrate this model in their applications.</span><br><span class="line"></span><br><span class="line">Easy deployment with NVIDIA NIM</span><br><span class="line">The Nemotron Reward model is packaged as an NVIDIA NIM inference microservice to streamline and accelerate the deployment of generative AI models across NVIDIA-accelerated infrastructure anywhere, including cloud, data center, and workstations.</span><br><span class="line"></span><br><span class="line">NIM uses inference optimization engines, industry-standard APIs, and prebuilt containers to provide high-throughput AI inference that scales with demand.</span><br><span class="line"></span><br><span class="line">Getting started</span><br><span class="line">Experience the Llama 3.1-Nemotron-70B-Reward model from a browser today or test it at scale and build a proof of concept (PoC) with the NVIDIA-hosted API endpoint running on a fully accelerated stack.</span><br><span class="line"></span><br><span class="line">Get started at ai.nvidia.com with free NVIDIA cloud credits or download the model from Hugging Face.</span><br><span class="line"></span><br><span class="line">For more information about how the model was trained and can be used for RLHF, see HelpSteer2-Preference: Complementing Ratings with Preferences.</span><br><span class="line"></span><br><span class="line">e&#x27;re soo back!: Nvidia Nemotron 70B - beats Llama 3.1 405B, GPT4o &amp; Claude 3.5 Sonnet! 🔥</span><br><span class="line">Evals (Nemotron 70B vs Claude 3.5 vs GPT4o)</span><br><span class="line">&gt; Arena Hard - 85.0 vs 79.2 vs 79.3</span><br><span class="line">&gt; AlpacaEval 2 LC - 57.6 vs 52.4 vs 57.5</span><br><span class="line">&gt; MT Bench - 8.98 vs 8.81 vs 8.74</span><br><span class="line">Secret Sauce?</span><br><span class="line">RLHF (REINFORCE) with Llama-3.1-Nemotron-70B-Reward and HelpSteer2-Preference prompts</span><br><span class="line">They release the Instruct model, reward model and the dataset all on Hugging Face! 🤗</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://mistral.ai/news/ministraux/</span><br><span class="line">Mistral AI</span><br><span class="line">Un Ministral, des Ministraux</span><br><span class="line">Introducing the world’s best edge models.</span><br><span class="line"></span><br><span class="line">October 16, 2024 Mistral AI team</span><br><span class="line">Introducing the world’s best edge models</span><br><span class="line">On the first anniversary of the release of Mistral 7B, the model that revolutionized independent frontier AI innovation for millions, we are proud to introduce two new state-of-the-art models for on-device computing and at-the-edge use cases. We call them les Ministraux: Ministral 3B and Ministral 8B.</span><br><span class="line"></span><br><span class="line">These models set a new frontier in knowledge, commonsense, reasoning, function-calling, and efficiency in the sub-10B category, and can be used or tuned to a variety of uses, from orchestrating agentic workflows to creating specialist task workers. Both models support up to 128k context length (currently 32k on vLLM) and Ministral 8B has a special interleaved sliding-window attention pattern for faster and memory-efficient inference.</span><br><span class="line"></span><br><span class="line">Use cases</span><br><span class="line">Our most innovative customers and partners have increasingly been asking for local, privacy-first inference for critical applications such as on-device translation, internet-less smart assistants, local analytics, and autonomous robotics. Les Ministraux were built to provide a compute-efficient and low-latency solution for these scenarios. From independent hobbyists to global manufacturing teams, les Ministraux deliver for a wide variety of use cases.</span><br><span class="line"></span><br><span class="line">Used in conjunction with larger language models such as Mistral Large, les Ministraux are also efficient intermediaries for function-calling in multi-step agentic workflows. They can be tuned to handle input parsing, task routing, and calling APIs based on user intent across multiple contexts at extremely low latency and cost.</span><br><span class="line"></span><br><span class="line">Benchmarks</span><br><span class="line">We demonstrate the performance of les Ministraux across multiple tasks where they consistently outperform their peers. We re-evaluated all models with our internal framework for fair comparison.</span><br><span class="line"></span><br><span class="line">Pretrained Models</span><br><span class="line">Pretrained model comparison table</span><br><span class="line">Table 1: Ministral 3B and 8B models compared to Gemma 2 2B, Llama 3.2 3B, Llama 3.1 8B and Mistral 7B on multiple categories</span><br><span class="line"></span><br><span class="line">Pretrained model comparison graph</span><br><span class="line">Figure 1: Ministral 3B and 8B base models compared to Gemma 2 2B, Llama 3.2 3B, Llama 3.1 8B and Mistral 7B</span><br><span class="line"></span><br><span class="line">Instruct Models</span><br><span class="line">Instruct model comparison table</span><br><span class="line">Table 2: Ministral 3B and 8B Instruct models compared to Gemma 2 2B, Llama 3.2 3B, Llama 3.1 8B, Gemma 2 9B and Mistral 7B on different evaluation categories.</span><br><span class="line"></span><br><span class="line">3B Instruct model comparison graph</span><br><span class="line">Figure 2: A comparison of the 3B family of Instruct models - Gemma 2 2B, Llama 3.2 3B and Ministral 3B. The figure showcases the improvements of Ministral 3B over the much larger Mistral 7B.</span><br><span class="line"></span><br><span class="line">8B Instruct model comparison graph</span><br><span class="line">Figure 3: A comparison of the 8B family of Instruct models - Gemma 2 9B, Llama 3.1 8B, Mistral 7B and Ministral 8B.</span><br><span class="line"></span><br><span class="line">Availability and pricing</span><br><span class="line">Both models are available starting today.</span><br><span class="line"></span><br><span class="line">Model	API	Pricing on la Plateforme	License</span><br><span class="line">Ministral 8B	ministral-8b-latest	$0.1 / M tokens (input and output)	Mistral Commercial License</span><br><span class="line">Mistral Research License</span><br><span class="line">Ministral 3B	ministral-3b-latest	$0.04 / M tokens (input and output)	Mistral Commercial License</span><br><span class="line">For self-deployed use, please reach out to us for commercial licenses. We will also assist you in lossless quantization of the models for your specific use-cases to derive maximum performance.</span><br><span class="line"></span><br><span class="line">The model weights for Ministral 8B Instruct are available for research use. Both models will be available from our cloud partners shortly.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://developer.nvidia.com/blog/ibms-new-granite-3-0-generative-ai-models-are-small-yet-highly-accurate-and-efficient/?ncid=so-link-654107</span><br><span class="line">IBM</span><br><span class="line">IBM’s New Granite 3.0 Generative AI Models Are Small, Yet Highly Accurate and Efficient</span><br><span class="line">Oct 21, 2024</span><br><span class="line">By Maryam Ashoori and Chintan Patel</span><br><span class="line"></span><br><span class="line">0</span><br><span class="line">Like</span><br><span class="line"> Discuss (0)</span><br><span class="line"></span><br><span class="line">Today, IBM released the third generation of IBM Granite, a collection of open language models and complementary tools. Prior generations of Granite focused on domain-specific use cases; the latest IBM Granite models meet or exceed the performance of leading similarly sized open models across both academic and enterprise benchmarks.</span><br><span class="line"></span><br><span class="line">The developer-friendly Granite 3.0 generative AI models are designed for function calling, supporting tool-based use cases. They were developed as workhorse enterprise models capable of serving as the primary building block of sophisticated workflows across use cases including text generation, agentic AI, classification, tool calling, summarization, entity extraction, customer service chatbots, and more.</span><br><span class="line"></span><br><span class="line">Introducing IBM’s Granite Generation 3 family</span><br><span class="line"></span><br><span class="line">IBM developed the Granite series, available as an NVIDIA NIM microservice, for enterprise use, prioritizing industry-leading trust, safety and cost efficiency without compromising performance.</span><br><span class="line"></span><br><span class="line">In its entirety, the Granite 3.0 release comprises of</span><br><span class="line"></span><br><span class="line">Dense, text-only LLMs: Granite 3.0 8B, Granite 3.0 2B</span><br><span class="line">Mixture of Experts (MoE) LLMs: Granite 3.0 3B-A800M, Granite 3.0 1B-A400M</span><br><span class="line">LLM-based input-output guardrail models: Granite Guardian 8B, Granite Guardian 2B</span><br><span class="line">Core components of Granite’s architecture are: Group-query attention (GQA) and Rotary Position Encodings (RoPE) for positional information, multilayer perceptron (MLP) with SwiGLU activation, RMSNorm, and shared input/output embeddings.</span><br><span class="line"></span><br><span class="line">Optimized performance with speculative decoding</span><br><span class="line"></span><br><span class="line">Trained on over 12 trillion tokens of carefully curated enterprise data, the new 8B and 2B models demonstrate significant improvements over their predecessors in both performance and speed.</span><br><span class="line"></span><br><span class="line">Speculative decoding is an optimization technique for accelerating model inference speed, helping LLMs generate text faster while using the same (or less) compute resources, and allowing more users to utilize a model at the same time. For example, in a recent IBM Research breakthrough, speculative decoding was used to cut the latency of Granite Code 20B in half while quadrupling its throughput.</span><br><span class="line"></span><br><span class="line">In standard inferencing, LLMs process each previous token they’ve generated thus far, then generate one token at a time. In speculative decoding, LLMs also evaluate several prospective tokens that might come after the token they’re about to generate—if these “speculated” tokens are verified as sufficiently accurate, one pass can produce two or more tokens for the computational “price” of one.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/microsoft/BitNet</span><br><span class="line">Microsoft</span><br><span class="line">10/18/2024</span><br><span class="line"></span><br><span class="line">bitnet.cpp</span><br><span class="line">License: MIT version</span><br><span class="line"></span><br><span class="line">bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support fast and lossless inference of 1.58-bit models on CPU (with NPU and GPU support coming next).</span><br><span class="line"></span><br><span class="line">The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of 1.37x to 5.07x on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by 55.4% to 70.0%, further boosting overall efficiency. On x86 CPUs, speedups range from 2.37x to 6.17x with energy reductions between 71.9% to 82.2%. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. More details will be provided soon.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://neuralmagic.com/blog/we-ran-over-half-a-million-evaluations-on-quantized-llms-heres-what-we-found/</span><br><span class="line">Neural Magic</span><br><span class="line">We Ran Over Half a Million Evaluations on Quantized LLMs: Here&#x27;s What We Found</span><br><span class="line"></span><br><span class="line">Oct 17, 2024</span><br><span class="line"></span><br><span class="line">Quantizing models to lower precision formats, such as 8-bit or 4-bit, significantly reduces computational costs and accelerates inference. However, there has been a persistent question of whether these quantized models retain the same level of accuracy and quality. Recently, the machine learning (ML) community has raised significant concerns about whether quantized large language models (LLMs) can truly compete with their uncompressed counterparts in accuracy and the general quality of generated responses.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">An example tweet from the community to highlight the concerns around quantized models.</span><br><span class="line">In this blog, we address these concerns directly to answer a key question: How much accuracy do we sacrifice when quantizing LLMs? To find out, we conducted over half a million evaluations across various benchmarks, including academic datasets, real-world tasks, and manual inspections, rigorously testing our latest quantized models. Our results revealed several likely sources for the community’s concerns, such as overly sensitive evaluations, models susceptible to the formatting of the chat template, and insufficient hyperparameter tuning in widely used quantization algorithms. By addressing these issues, we have produced highly accurate quantized models that, on average, show no discernible differences from their full-precision counterparts.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Figure 1: Accuracy recovery of the Llama 3.1 models at FP8 precision compared to their full-precision baselines across various academic and real-world evaluation benchmarks.</span><br><span class="line">As seen in Figure 1, we achieved full accuracy recovery across various academic and real-world tasks, including the ArenaHard benchmark the community found issues with. Let’s take a closer look!</span><br><span class="line"></span><br><span class="line">Exploring Our Approach and Rationale</span><br><span class="line">Our evaluation focused on extensive testing of the Llama 3.1 series of models, which have gained significant traction in research and deployment contexts. With its streamlined and efficient base architecture, Llama 3.1 is an ideal candidate for assessing various quantization schemes.</span><br><span class="line"></span><br><span class="line">For each Llama 3.1 size (8B, 70B, and 405B), we tested three distinct quantization schemes alongside the baseline 16-bit model. These schemes were selected to accommodate different hardware and deployment requirements, and all performance claims were validated using vLLM (0.6.2):</span><br><span class="line"></span><br><span class="line">W8A8-INT:  This quantization scheme reduces weights and activations to 8-bit integer values, making it ideal for server or throughput-based deployments on Nvidia Ampere (A100 GPUs) and older hardware. It provides approximately 2x model size compression and delivers an average 1.8x performance speedup across various server (multi-request) scenarios.</span><br><span class="line">W8A8-FP: This quantization scheme uses an 8-bit floating point format for weights and activations rather than integer values. This simplifies the compression process but is supported only on the latest Nvidia Hopper (H100 GPUs) and Ada Lovelace hardware. It provides approximately 2x model size compression and delivers an average 1.8x performance speedup across various server (multi-request) scenarios.</span><br><span class="line">W4A16-INT: In this scheme, weights are quantized to 4-bit integers while the activations remain at 16-bit precision. This approach is optimal for latency-critical applications and edge use cases where model size and single request response time are key factors. This means that the model inference is dominated by memory access for loading the weights instead of compute-intensive operations. In this regime, W4A16 provides approximately 3.5x model size compression and delivers an average speedup of 2.4x for single-stream scenarios.</span><br><span class="line">Each quantized model was created by optimizing hyperparameter and algorithmic choices on the OpenLLM Leaderboard v1 benchmarks and then evaluated across many other benchmarks to ensure it generalizes across diverse scenarios. The best choices varied by model and scheme but comprised some combination of SmoothQuant, GPTQ, and/or standard round-to-nearest quantization algorithms. Detailed documentation for each model, including the specific approaches used, can be found in the model cards available in our HuggingFace collection.</span><br><span class="line"></span><br><span class="line">We designed our evaluation suite to cover a broad spectrum of inference scenarios and use cases, providing a comprehensive analysis across multiple model sizes and quantization schemes:</span><br><span class="line"></span><br><span class="line">Academic Benchmarks: These benchmarks, such as OpenLLM Leaderboard v1 and v2, are key for evaluating research developments and model improvements. They focus on structured tasks like question-answering and reasoning, providing consistent and easily validated accuracy scores. However, they often fail to reflect real-world scenarios where semantics, variability, and context are critical.</span><br><span class="line">Real-World Benchmarks: Unlike academic benchmarks, real-world benchmarks test models in scenarios that mimic human usage, such as instruction following, chat, and code generation. These benchmarks include ArenaHard and HumanEval, which offer a broader range of tasks with higher variation but better reflect real-world model performance. These benchmarks provide a more comprehensive view of models&#x27; performance in live environments.</span><br><span class="line">Text Similarity: Text similarity measures how closely quantized models’ outputs match their unquantized counterparts. Metrics such as ROUGE, BERTScore, and Semantic Textual Similarity (STS) evaluate the semantic and structural consistency, ensuring that the generated text&#x27;s intended meaning and quality are preserved.</span><br><span class="line">With this extensive evaluation framework, we ensured that deployment scenarios ranging from structured, research-driven tasks to open-ended, real-world applications were covered, providing a holistic view of the performance and capabilities of quantized LLMs.</span><br><span class="line"></span><br><span class="line">Academic Benchmark Performance</span><br><span class="line">Academic benchmarks are an excellent starting point for evaluating language models’ accuracy and reasoning capabilities. They provide structured tasks, making them essential for comparing models on well-defined criteria. Our evaluations focused on OpenLLM Leaderboard v1 and v2, ensuring consistent results across both older and newer, more challenging benchmarks. Additionally, testing on both allowed us to prevent overfitting to v1, where we optimized our quantization hyperparameters.</span><br><span class="line"></span><br><span class="line">We evaluated OpenLLM Leaderboard v1 by utilizing Meta’s prompts for the Llama-3.1 models. We base our comparisons and recovery percentages on the average score and report a full per-task breakdown of results in our HuggingFace model collection. The Leaderboard v1 benchmark consists of a diverse range of topics, including:</span><br><span class="line"></span><br><span class="line">Grade school math: GSM8k</span><br><span class="line">World knowledge and reasoning: MMLU, ARC-Challenge</span><br><span class="line">Language understanding: Winogrande, HellaSwag</span><br><span class="line">Truthfulness: TruthfulQA.</span><br><span class="line">As illustrated in Figure 2 (left) below, all quantization schemes—regardless of model size—recover over 99% of the average score achieved by the unquantized baseline.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Figure 2: OpenLLM Leaderboard v1 (left)  and v2 (right) average scores for baseline (BF16) and various quantized versions of Llama 3.1 (405B, 70B, and 8B).</span><br><span class="line">The community has evolved, and with it, so have the benchmarks. As scores began to plateau on v1, OpenLLM Leaderboard v2 was introduced to push models further, offering more challenging tasks that test deeper reasoning and knowledge capabilities. Like v1, we measured the recovery percentages based on the average scores across the v2 benchmarks (full results in our HuggingFace model collection). The benchmarks in v2 include more complex topics, such as:</span><br><span class="line"></span><br><span class="line">Expert knowledge and reasoning: MMLU-Pro, GPQA, Big Bench Hard</span><br><span class="line">Multistep reasoning: MuSR</span><br><span class="line">Advanced math problems: MATH Level 5</span><br><span class="line">Instruction following: IFEval.</span><br><span class="line">As illustrated in Figure 2 (right) above, the quantized models recover close to 99% of the baseline’s average score on average, with all models maintaining at least 96% recovery. However, the increased difficulty of these tasks, especially for smaller models, resulted in higher variance for benchmarks like GPQA and MuSR, where scores approached the random guessing threshold even for the full-precision baseline. This led to more variability in the quantized versions&#x27; scores and a lack of a clear signal for accuracy recovery.</span><br><span class="line"></span><br><span class="line">Real-World Benchmark Performance</span><br><span class="line">While academic benchmarks provide structured evaluations, real-world open-ended benchmarks better represent how models perform in dynamic environments like human-chat interactions or coding tasks. These benchmarks test models on varied prompts with longer generations and multiple potential solutions, focusing on responses&#x27; correctness and semantic quality. Our evaluations targeted three key real-world benchmarks: Arena-Hard, HumanEval, and HumanEval+, which measure performance in chat, instruction-following, and code generation.</span><br><span class="line"></span><br><span class="line">The LMSYS Chatbot Arena has established itself as a leading benchmark for LLMs, assessing how models align with human preferences. Arena-Hard Auto is an automated extension of this benchmark, where an LLM judges responses to 500 complex prompts on various topics. It has demonstrated a strong correlation with human evaluations, achieving a state-of-the-art 89% agreement with human preference rankings.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Figure 4: Arena-Hard-Auto average scores for baseline (BF16) and various quantized versions of Llama 3.1 (405B, 70B, and 8B).</span><br><span class="line">Figure 4 shows how well-quantized models compare to their full-precision counterparts on the Arena-Hard-Auto benchmark, averaging results from two evaluation runs per model. The results illustrate that the response quality of quantized models remains highly competitive with their unquantized counterparts. As shown in the detailed results on our HuggingFace Hub, the 95% confidence intervals overlap for all model sizes and quantization schemes, highlighting the minimal impact on accuracy.</span><br><span class="line"></span><br><span class="line">In addition to chat-based interactions, LLMs are widely deployed as coding assistants. To evaluate the performance of quantized models in code generation, we tested them on HumanEval and its more challenging variant, HumanEval+. These benchmarks measure a model’s ability to generate correct and functional code based on programming problems, with HumanEval+ introducing more complex, multi-step tasks requiring deeper reasoning and problem-solving. Figure 5 below presents the pass@1 scores obtained using the EvalPlus library.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Figure 5: HumanEval and HumanEval+ pass@1 score for baseline (BF16) and various quantized versions of Llama 3.1 (405B, 70B, and 8B).</span><br><span class="line">As illustrated in Figure 5, quantized models demonstrate exceptional performance on both HumanEval and HumanEval+, with 8-bit models achieving 99.9% accuracy recovery and 4-bit models recovering 98.9%. These results highlight that quantized models not only maintain high performance in simpler coding tasks but also excel in more complex scenarios, proving their reliability for real-world coding applications with minimal loss in accuracy.</span><br><span class="line"></span><br><span class="line">Text Similarity and Manual Inspection</span><br><span class="line">After evaluating quantized models across various academic and real-world benchmarks, we put them through the final test: How similar is the text generated by quantized models compared to their unquantized counterparts?</span><br><span class="line"></span><br><span class="line">We used four key metrics to answer this:</span><br><span class="line"></span><br><span class="line">ROUGE-1 measures word-level overlap between outputs of quantized and unquantized models.</span><br><span class="line">ROUGE-L captures structural similarity by focusing on the longest common subsequence.</span><br><span class="line">BERTScore evaluates the contextual similarity at the token-level.</span><br><span class="line">STS assesses overall semantic similarity at the sentence level.</span><br><span class="line">These metrics were computed across responses generated from the ArenaHard prompts, allowing us to analyze how well-quantized models preserve the meaning and structure of outputs compared to full-precision models. The results are summarized in Figure 6 below.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Figure 6: Various text similarity metrics comparing the outputs of quantized Llama 3.1 models (405B, 70B, and 8B) to their full-precision baselines.</span><br><span class="line">The results show that larger quantized models (70B and 405B) maintain a high degree of text similarity to their full-precision counterparts, with ROUGE-1 and ROUGE-L scores indicating strong preservation of word choice and structure. BERTScore and STS further confirm that the overall meaning remains consistent, even with slight token variations introduced by quantization. While 8B models exhibit more variability in word selection, they still preserve the core semantic meaning as shown in the BERTScore and STS results. This demonstrates that quantized models maintain high-quality output across all model sizes and quantization schemes.</span><br><span class="line"></span><br><span class="line">So far, we’ve evaluated the performance of quantized models using a variety of benchmarks and comparison metrics distilled into raw numbers. Now, it’s time to see the results for yourself. Our interactive demo app (built on top of the fantastic HuggingFace Spaces) lets you select different models and quantization schemes to compare generated responses side-by-side with their full-precision counterparts. This tool offers an intuitive way to visually assess how quantization affects model outputs and the quality of the generated text.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">If the interactive demo isn&#x27;t rendering nicely in your browser, visit our HuggingFace Space for a smoother UI experience: https://huggingface.co/spaces/neuralmagic/quant-llms-text-generation-comparison.</span><br><span class="line"></span><br><span class="line">Why Quantization is Here to Stay</span><br><span class="line">In conclusion, our comprehensive evaluation demonstrates that quantized models maintain impressive accuracy and quality compared to their full-precision counterparts, making them an essential tool for optimizing LLMs in real-world deployments.</span><br><span class="line"></span><br><span class="line">Consistent Performance: 8-bit and 4-bit quantized LLMs show very competitive accuracy recovery across diverse benchmarks, including Arena-Hard, OpenLLM Leaderboards v1 and v2, and coding benchmarks like HumanEval and HumanEval+.</span><br><span class="line">Minimal Trade-offs: Larger models (70B, 405B) show negligible performance degradation. In comparison, smaller models (8B) may experience slight variability but still preserve their outputs&#x27; core semantic meaning and structural coherence.</span><br><span class="line">Efficiency and Scalability: Quantization provides significant computational savings and faster inference speeds while maintaining the semantic quality and reliability of responses.</span><br><span class="line">These findings confirm that quantization offers large benefits in terms of cost, energy, and performance without sacrificing the integrity of the models. As LLMs grow in size and complexity, quantization will play a pivotal role in enabling organizations to deploy state-of-the-art models efficiently.</span><br><span class="line"></span><br><span class="line">Ready to explore how quantized LLMs can enhance your business&#x27;s efficiency and performance? Connect with our experts to discuss enterprise solutions tailored to your needs.</span><br><span class="line"></span><br><span class="line">How does quantization impact the performance of LLMs? Only minimal! 🤯 A new study ran 500,000 different evaluations on Meta Llama using different quantization strategies. The impact is &lt;1%, but the benefits are up to 2.4 faster inference and 3.5 model size reduction! 🔥</span><br><span class="line">TL;DR;</span><br><span class="line">💯 Quantized models achieve 99% accuracy recovery compared to full-precision</span><br><span class="line">🚀 Up to 2.4x speedup and 3.5x model size reduction with quantization.</span><br><span class="line">📊 Tested Llama 3.1 8B, 70B, and 405B models on OpenLLM Leaderboard, ArenaHard, HumanEval, and text similarity metrics.</span><br><span class="line">🥇W8A8-FP8 dynamic yields the best results</span><br><span class="line">🤗 Quantized models available on Hugging Face.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2410.07524</span><br><span class="line">NVIDIA</span><br><span class="line">Upcycling Large Language Models into Mixture of Experts</span><br><span class="line">Ethan He, Abhinav Khattar, Ryan Prenger, Vijay Korthikanti, Zijie Yan, Tong Liu, Shiqing Fan, Ashwath Aithal, Mohammad Shoeybi, Bryan Catanzaro</span><br><span class="line">Upcycling pre-trained dense language models into sparse mixture-of-experts (MoE) models is an efficient approach to increase the model capacity of already trained models. However, optimal techniques for upcycling at scale remain unclear. In this work, we conduct an extensive study of upcycling methods and hyperparameters for billion-parameter scale language models. We propose a novel &quot;virtual group&quot; initialization scheme and weight scaling approach to enable upcycling into fine-grained MoE architectures. Through ablations, we find that upcycling outperforms continued dense model training. In addition, we show that softmax-then-topK expert routing improves over topK-then-softmax approach and higher granularity MoEs can help improve accuracy. Finally, we upcycled Nemotron-4 15B on 1T tokens and compared it to a continuously trained version of the same model on the same 1T tokens: the continuous trained model achieved 65.3% MMLU, whereas the upcycled model achieved 67.6%. Our results offer insights and best practices to effectively leverage upcycling for building MoE language models.</span><br><span class="line"></span><br><span class="line">Our new #NVIDIAResearch paper presents training recipes and mechanisms to consistently upcycle billion-parameter scale #LLMs, resulting in models that outperform the original dense models. 👀 See &quot;Upcycling Large Language Models into Mixture of Experts&quot; with Megatron-LM</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/papers/2410.08146</span><br><span class="line">Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning</span><br><span class="line">Published on Oct 11</span><br><span class="line">Authors:</span><br><span class="line">Amrith Setlur</span><br><span class="line">,</span><br><span class="line">Chirag Nagpal</span><br><span class="line">,</span><br><span class="line">Adam Fisch</span><br><span class="line">,</span><br><span class="line">Xinyang Geng</span><br><span class="line">,</span><br><span class="line">Jacob Eisenstein</span><br><span class="line">,</span><br><span class="line">Rishabh Agarwal</span><br><span class="line">,</span><br><span class="line">Alekh Agarwal</span><br><span class="line">,</span><br><span class="line">Jonathan Berant</span><br><span class="line">,</span><br><span class="line">Aviral Kumar</span><br><span class="line">Abstract</span><br><span class="line">A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, potentially improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. To improve a base policy by running search against a PRM or using it as dense rewards for reinforcement learning (RL), we ask: &quot;How should we design process rewards?&quot;. Our key insight is that, to be effective, the process reward for a step should measure progress: a change in the likelihood of producing a correct response in the future, before and after taking the step, corresponding to the notion of step-level advantages in RL. Crucially, this progress should be measured under a prover policy distinct from the base policy. We theoretically characterize the set of good provers and our results show that optimizing process rewards from such provers improves exploration during test-time search and online RL. In fact, our characterization shows that weak prover policies can substantially improve a stronger base policy, which we also observe empirically. We validate our claims by training process advantage verifiers (PAVs) to predict progress under such provers, and show that compared to ORMs, test-time search against PAVs is &gt;8% more accurate, and 1.5-5times more compute-efficient. Online RL with dense rewards from PAVs enables one of the first results with 5-6times gain in sample efficiency, and &gt;6% gain in accuracy, over ORMs.</span><br><span class="line"></span><br><span class="line">Process Reward Models (PRM) can provide feedback on each step of LLM reasoning but normally require a huge amount of human label data. Google DeepMind is trying to solve this by using progress (likelihood) improvements after each reasoning step and a “prover” LLM to correctly predict the answer, leading to 8% higher accuracy and up to 6x better data efficiency compared to standard outcome-based Reward Models.</span><br><span class="line">Implementation</span><br><span class="line">1️⃣ Select a base LLM and a distinct prover LLM (can be weaker).</span><br><span class="line">2️⃣ Generate reasoning traces using the base LLM on a reasoning dataset with correct answers.</span><br><span class="line">3️⃣ Use the prover to solve the problem multiple times before and after the step.</span><br><span class="line">4️⃣ Calculate Advantage/progress for each step by subtracting the &quot;before&quot; success rate from the &quot;after&quot; success rate.</span><br><span class="line">5️⃣ Create Training Data for each step with the problem, steps taken so far (the prefix), the current step, and calculated advantage (target label for RM).</span><br><span class="line">6️⃣ Train a Reward Model to predict these advantage values and use it during RLHF</span><br><span class="line">Insights</span><br><span class="line">🎯 Achieve &gt;8% higher accuracy and 1.5-5x better compute efficiency than traditional outcome reward models</span><br><span class="line">🔄 Weaker prover can help improve stronger base models through better exploration</span><br><span class="line">⚡ More efficient exploration by rewarding intermediate progress steps</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/simular-ai/Agent-S</span><br><span class="line">[Submitted on 10 Oct 2024]</span><br><span class="line">Agent S: An Open Agentic Framework that Uses Computers Like a Human</span><br><span class="line">Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, Xin Eric Wang</span><br><span class="line">We present Agent S, an open agentic framework that enables autonomous interaction with computers through a Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S aims to address three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. In addition, it employs an Agent-Computer Interface (ACI) to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models (MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37% on success rate (an 83.6% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark. Code available at this https URL.</span><br><span class="line"></span><br><span class="line">Logo Agent S:</span><br><span class="line">An Open Agentic Framework that Uses Computers Like a Human</span><br><span class="line">🌐[Website] 📄[Paper] 🎥[Video] 🗨️[Discord]</span><br><span class="line"></span><br><span class="line">💡 Introduction</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Welcome to Agent S, an open-source framework designed to enable autonomous interaction with computers through Agent-Computer Interface. Our mission is to build intelligent GUI agents that can learn from past experiences and perform complex tasks autonomously on your computer.</span><br><span class="line"></span><br><span class="line">Whether you&#x27;re interested in AI, automation, or contributing to cutting-edge agent-based systems, we&#x27;re excited to have you here!</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://machinelearning.apple.com/research/gsm-symbolic</span><br><span class="line">Apple</span><br><span class="line">paper | publishedOctober 2024</span><br><span class="line">GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models</span><br><span class="line">AuthorsIman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar</span><br><span class="line"></span><br><span class="line">View publication</span><br><span class="line"></span><br><span class="line">Copy Bibtex</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models. Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn&#x27;t contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs&#x27; capabilities and limitations in mathematical reasoning.</span><br></pre></td></tr></table></figure>

<p>기술적으로 최대한 자세하게 적어. 10개의 기사가 있고 하나도 빼먹지 말고 적어.</p>
</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-10-22</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/10/22/2024-10-15-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 10월 22일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/10/25/2024-10-25-AI-NEWS/" title="2024년 10월 25일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/10/15/2024-10-22-AI-NEWS/" title="2024년 10월 15일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>