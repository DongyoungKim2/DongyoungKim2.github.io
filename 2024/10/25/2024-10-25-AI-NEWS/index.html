<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024ë…„ 10ì›” 25ì¼ AI ì†Œì‹ Â· TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="OpenAIì—ì„œëŠ” ChatGPTì˜ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ëŒ€í­ ê°œì„ í•œ â€˜ChatGPT searchâ€™ë¥¼ ë°œí‘œí•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ê´€ë ¨ ì›¹ ì†ŒìŠ¤ë¥¼ ì œê³µí•˜ê³  ì‚¬ìš©ì ê²½í—˜ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ë˜í•œ macOSì™€ Windows ë°ìŠ¤í¬í†± ì•±ì—ì„œ Advanced Voice ê¸°ëŠ¥ì„ ì¶œì‹œí•˜ì—¬ ìŒì„± ëŒ€"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">Â© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024ë…„ 10ì›” 25ì¼ AI ì†Œì‹</a></h3></div><div class="post-content"><p>OpenAIì—ì„œëŠ” ChatGPTì˜ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ëŒ€í­ ê°œì„ í•œ â€˜ChatGPT searchâ€™ë¥¼ ë°œí‘œí•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ê´€ë ¨ ì›¹ ì†ŒìŠ¤ë¥¼ ì œê³µí•˜ê³  ì‚¬ìš©ì ê²½í—˜ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ë˜í•œ macOSì™€ Windows ë°ìŠ¤í¬í†± ì•±ì—ì„œ Advanced Voice ê¸°ëŠ¥ì„ ì¶œì‹œí•˜ì—¬ ìŒì„± ëŒ€í™” ê¸°ëŠ¥ì„ í™•ì¥í•˜ì˜€ìŠµë‹ˆë‹¤. Stability AIëŠ” â€˜Stable Diffusion 3.5â€™ë¥¼ ê³µê°œí•˜ì—¬ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±ì„ ì§€ì›í•˜ê³ , ë‹¤ì–‘í•œ ëª¨ë¸ ë³€í˜•ì„ ì œê³µí•˜ì—¬ ì‚¬ìš©ì ì»¤ìŠ¤í„°ë§ˆì´ì§•ì„ ìš©ì´í•˜ê²Œ í•˜ì˜€ìŠµë‹ˆë‹¤. Googleì€ Gemini APIì™€ Google AI Studioì— â€˜Grounding with Google Searchâ€™ ê¸°ëŠ¥ì„ ë„ì…í•˜ì—¬ ëª¨ë¸ì˜ ì‘ë‹µ ì •í™•ì„±ê³¼ ìµœì‹ ì„±ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. MetaëŠ” ëª¨ë°”ì¼ í™˜ê²½ì—ì„œ íš¨ìœ¨ì ì¸ LLM í™œìš©ì„ ìœ„í•œ â€˜MobileLLMâ€™ê³¼ ì—°êµ¬ìë“¤ì„ ìœ„í•œ LLM í›ˆë ¨ ë° ì¶”ë¡  ë¼ì´ë¸ŒëŸ¬ë¦¬ â€˜Meta Linguaâ€™ë¥¼ ì¶œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. Hugging FaceëŠ” ì†Œí˜•ì´ì§€ë§Œ ê°•ë ¥í•œ LLM ì„¸íŠ¸ì¸ â€˜SmolLM 2â€™ë¥¼ ê³µê°œí•˜ì—¬ ì˜¨ë””ë°”ì´ìŠ¤ ìµœì í™”ë¥¼ ë‹¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. Inspiraiì™€ ì¹­í™”ëŒ€í•™êµëŠ” ì´ë¯¸ì§€, ìŒì„±, í…ìŠ¤íŠ¸ ì…ë ¥ì„ ëª¨ë‘ ì´í•´í•˜ëŠ” ë¹„ì£¼ì–¼-ì˜¤ë””ì˜¤ ì–´ì‹œìŠ¤í„´íŠ¸ â€˜Mini-Omni 2â€™ë¥¼ ë°œí‘œí•˜ì˜€ê³ , MicrosoftëŠ” ë§ˆì»¤ ì—†ì´ë„ ê³ í’ˆì§ˆ ì „ì‹  í¼í¬ë¨¼ìŠ¤ ìº¡ì²˜ê°€ ê°€ëŠ¥í•œ â€˜SynthMoCapâ€™ ê¸°ìˆ ì„ ì†Œê°œí•˜ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ MaskGCTì—ì„œëŠ” ìƒˆë¡œìš´ ìµœì²¨ë‹¨ í…ìŠ¤íŠ¸-íˆ¬-ìŠ¤í”¼ì¹˜(TTS) ëª¨ë¸ì„ ë°œí‘œí•˜ì˜€ê³ , ì—°êµ¬ìë“¤ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì´ ê°œë°œìì˜ ì´ë°ì˜¬ë¡œê¸°ë¥¼ ë°˜ì˜í•œë‹¤ëŠ” ì—°êµ¬ ê²°ê³¼ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.</p>
<h3 id="OpenAI-ChatGPT-search-ì†Œê°œ"><a href="#OpenAI-ChatGPT-search-ì†Œê°œ" class="headerlink" title="OpenAI, ChatGPT search ì†Œê°œ"></a>OpenAI, ChatGPT search ì†Œê°œ</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/introducing-chatgpt-search/">ë§í¬</a>, 2024ë…„ 10ì›” 31ì¼</p>
<ul>
<li><strong>ChatGPTì˜ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ ëŒ€í­ ê°œì„ </strong>: ì´ì œ ChatGPTëŠ” ì´ì „ë³´ë‹¤ í›¨ì”¬ í–¥ìƒëœ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì„ í†µí•´ ì‹ ì†í•˜ê³  ì‹œê¸°ì ì ˆí•œ ë‹µë³€ê³¼ ê´€ë ¨ ì›¹ ì†ŒìŠ¤ ë§í¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.</li>
<li><strong>Plus, Team, SearchGPT ëŒ€ê¸°ì ëª…ë‹¨ ì‚¬ìš©ì ìš°ì„  ì œê³µ</strong>: í˜„ì¬ Plus, Team, ê·¸ë¦¬ê³  SearchGPT ëŒ€ê¸°ì ëª…ë‹¨ ì‚¬ìš©ìë“¤ì—ê²Œ ë¡¤ì•„ì›ƒ ì¤‘ì´ë©°, ëª¨ë°”ì¼ ë° ë°ìŠ¤í¬í†± ì•±ê³¼ chatgpt.comì—ì„œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.</li>
<li><strong>Enterpriseì™€ Edu ì‚¬ìš©ì ì œê³µ ì˜ˆì •</strong>: Enterpriseì™€ Edu ì‚¬ìš©ìë“¤ì€ ëª‡ ì£¼ ë‚´ì— ì ‘ê·¼ ê°€ëŠ¥í•˜ë©°, ë¬´ë£Œ ì‚¬ìš©ìë“¤ì—ê²ŒëŠ” í–¥í›„ ëª‡ ë‹¬ì— ê±¸ì³ ì œê³µë  ì˜ˆì •ì…ë‹ˆë‹¤.</li>
<li><strong>ìë™ ë° ìˆ˜ë™ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥</strong>: ChatGPTëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì›¹ì„ ê²€ìƒ‰í•˜ê±°ë‚˜, ì‚¬ìš©ìê°€ ì›¹ ê²€ìƒ‰ ì•„ì´ì½˜ì„ í´ë¦­í•˜ì—¬ ìˆ˜ë™ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
<li><strong>ìƒˆë¡œìš´ ì‹œê°ì  ë””ìì¸ ì¶”ê°€</strong>: ë‚ ì”¨, ì£¼ì‹, ìŠ¤í¬ì¸ , ë‰´ìŠ¤, ì§€ë„ ë“±ì˜ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ìµœì‹  ì •ë³´ì™€ ìƒˆë¡œìš´ ì‹œê°ì  ë””ìì¸ì„ ì œê³µí•©ë‹ˆë‹¤.</li>
<li><strong>ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‰´ìŠ¤ ë° ë°ì´í„° ì œê³µìì™€ì˜ íŒŒíŠ¸ë„ˆì‹­</strong>: AP News, Financial Times, Reuters ë“±ê³¼ì˜ í˜‘ë ¥ì„ í†µí•´ ìµœì‹  ì •ë³´ì™€ ì‹œê° ìë£Œë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>ì¶œì²˜ ë§í¬ ì œê³µìœ¼ë¡œ íˆ¬ëª…ì„± ê°•í™”</strong>: ë‹µë³€ì— ì¶œì²˜ ë§í¬ë¥¼ í¬í•¨í•˜ì—¬ ì‚¬ìš©ìê°€ ì¶”ê°€ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤.</li>
<li><strong>ê²€ìƒ‰ ëª¨ë¸ì˜ ê¸°ìˆ ì  ê°œì„ </strong>: GPT-4oì˜ ë¯¸ì„¸ ì¡°ì • ë²„ì „ì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, ìƒˆë¡œìš´ í•©ì„± ë°ì´í„° ìƒì„± ê¸°ìˆ ì„ ì ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>í–¥í›„ ê³„íš</strong>: Advanced Voice ë° ìº”ë²„ìŠ¤ì— ìƒˆë¡œìš´ ê²€ìƒ‰ ê²½í—˜ì„ ë„ì…í•  ì˜ˆì •ì´ë©°, ë¬´ë£Œ ë° ë¡œê·¸ì•„ì›ƒ ì‚¬ìš©ìë“¤ì—ê²Œë„ ê¸°ëŠ¥ì„ í™•ëŒ€í•  ê³„íšì…ë‹ˆë‹¤.</li>
</ul>
<h3 id="OpenAI-ë°ìŠ¤í¬í†±-ì•±ì—-Advanced-Voice-ê¸°ëŠ¥-ì¶”ê°€"><a href="#OpenAI-ë°ìŠ¤í¬í†±-ì•±ì—-Advanced-Voice-ê¸°ëŠ¥-ì¶”ê°€" class="headerlink" title="OpenAI, ë°ìŠ¤í¬í†± ì•±ì— Advanced Voice ê¸°ëŠ¥ ì¶”ê°€"></a>OpenAI, ë°ìŠ¤í¬í†± ì•±ì— Advanced Voice ê¸°ëŠ¥ ì¶”ê°€</h3><p><a target="_blank" rel="noopener" href="https://openai.com/chatgpt/download/">ë§í¬</a>, 2024ë…„ 10ì›” 31ì¼</p>
<ul>
<li><strong>macOSì™€ Windows ë°ìŠ¤í¬í†± ì•±ì—ì„œ Advanced Voice ê¸°ëŠ¥ ì œê³µ</strong>: ë°ìŠ¤í¬í†± í™˜ê²½ì—ì„œ ìŒì„± ëŒ€í™” ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.</li>
<li><strong>ìµœì‹  ë²„ì „ì˜ ì•± í•„ìš”</strong>: Advanced Voice ê¸°ëŠ¥ì„ ì´ìš©í•˜ë ¤ë©´ ìµœì‹  ë²„ì „ì˜ ë°ìŠ¤í¬í†± ì•±ì„ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.</li>
<li><strong>ëª¨ë°”ì¼ ë° ë°ìŠ¤í¬í†± ì§€ì›</strong>: ChatGPTëŠ” ì´ì œ ëª¨ë°”ì¼ê³¼ ë°ìŠ¤í¬í†±ì—ì„œ ëª¨ë‘ Advanced Voice ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.</li>
</ul>
<h3 id="OpenAI-Playgroundì—ì„œ-í”„ë¡¬í”„íŠ¸-ìƒì„±-ê¸°ëŠ¥-ë„ì…"><a href="#OpenAI-Playgroundì—ì„œ-í”„ë¡¬í”„íŠ¸-ìƒì„±-ê¸°ëŠ¥-ë„ì…" class="headerlink" title="OpenAI, Playgroundì—ì„œ í”„ë¡¬í”„íŠ¸ ìƒì„± ê¸°ëŠ¥ ë„ì…"></a>OpenAI, Playgroundì—ì„œ í”„ë¡¬í”„íŠ¸ ìƒì„± ê¸°ëŠ¥ ë„ì…</h3><p><a target="_blank" rel="noopener" href="https://platform.openai.com/docs/guides/prompt-generation">ë§í¬</a>, 2024ë…„ 10ì›” 31ì¼</p>
<ul>
<li><strong>Playgroundì˜ Generate ë²„íŠ¼ ì†Œê°œ</strong>: ê°„ë‹¨í•œ ì‘ì—… ì„¤ëª…ë§Œìœ¼ë¡œ í”„ë¡¬í”„íŠ¸, í•¨ìˆ˜, ìŠ¤í‚¤ë§ˆë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.</li>
<li><strong>ë©”íƒ€ í”„ë¡¬í”„íŠ¸ì™€ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©</strong>: ìµœìƒì˜ í”„ë¡¬í”„íŠ¸ì™€ ìŠ¤í‚¤ë§ˆë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ë©”íƒ€ í”„ë¡¬í”„íŠ¸ì™€ ë©”íƒ€ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.</li>
<li><strong>ìƒì„± ê³¼ì • ê°„ì†Œí™”</strong>: í”„ë¡¬í”„íŠ¸ì™€ ìŠ¤í‚¤ë§ˆë¥¼ ì²˜ìŒë¶€í„° ì‘ì„±í•˜ëŠ” ë° ì†Œìš”ë˜ëŠ” ì‹œê°„ì„ ì ˆì•½í•˜ê³  ë¹ ë¥´ê²Œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
<li><strong>í–¥í›„ ë°œì „ëœ ê¸°ìˆ  í†µí•© ì˜ˆì •</strong>: ì•ìœ¼ë¡œ DSPyì™€ â€œGradient Descentâ€ì™€ ê°™ì€ ë” ë°œì „ëœ ê¸°ìˆ ì„ í†µí•©í•  ê³„íšì…ë‹ˆë‹¤.</li>
</ul>
<h3 id="OpenAI-SimpleQA-ë²¤ì¹˜ë§ˆí¬-ê³µê°œ"><a href="#OpenAI-SimpleQA-ë²¤ì¹˜ë§ˆí¬-ê³µê°œ" class="headerlink" title="OpenAI, SimpleQA ë²¤ì¹˜ë§ˆí¬ ê³µê°œ"></a>OpenAI, SimpleQA ë²¤ì¹˜ë§ˆí¬ ê³µê°œ</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/introducing-simpleqa/">ë§í¬</a>, 2024ë…„ 10ì›” 30ì¼</p>
<ul>
<li><strong>ì‚¬ì‹¤ì„± ì¸¡ì •ì„ ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ â€˜SimpleQAâ€™ ì˜¤í”ˆ ì†ŒìŠ¤í™”</strong>: ì–¸ì–´ ëª¨ë¸ì˜ ì‚¬ì‹¤ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ê³µê°œí•˜ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>ë‹¨ë‹µí˜• ì‚¬ì‹¤ ì§ˆë¬¸ì— ì´ˆì </strong>: ë³µì¡í•œ ì‚¬ì‹¤ì„± ì¸¡ì • ë¬¸ì œë¥¼ ë‹¨ìˆœí™”í•˜ì—¬ ë‹¨ë‹µí˜• ì§ˆë¬¸ìœ¼ë¡œ ëª¨ë¸ì˜ ì •í™•ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.</li>
<li><strong>ë†’ì€ ì •í™•ë„ì™€ ë‹¤ì–‘ì„±</strong>: ë‹¤ì–‘í•œ ì£¼ì œì™€ ë†’ì€ ì •í™•ë„ë¥¼ ê°€ì§„ ì§ˆë¬¸ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.</li>
<li><strong>ìµœì‹  ëª¨ë¸ì— ëŒ€í•œ ë„ì „ì„±</strong>: GPT-4o ë“± ìµœì‹  ëª¨ë¸ë“¤ë„ ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ê¸° ì–´ë ¤ìš´ ë„ì „ì ì¸ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.</li>
<li><strong>ì—°êµ¬ì ì¹œí™”ì  ì‚¬ìš©ì ê²½í—˜</strong>: ë¹ ë¥´ê³  ê°„ë‹¨í•˜ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆìœ¼ë©°, í‰ê°€ ë³€ë™ì„±ì´ ë‚®ìŠµë‹ˆë‹¤.</li>
<li><strong>ëª¨ë¸ ê°„ ë¹„êµ ë° êµì • ì¸¡ì •</strong>: ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ ë¹„êµì™€ ëª¨ë¸ì˜ êµì • ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ë° í™œìš©ë©ë‹ˆë‹¤.</li>
<li><strong>ì˜¤í”ˆ ì†ŒìŠ¤ ë°ì´í„°ì…‹ ì œê³µ</strong>: ì—°êµ¬ìë“¤ì´ ììœ ë¡­ê²Œ ì‚¬ìš©í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•  ìˆ˜ ìˆë„ë¡ ë°ì´í„°ì…‹ì„ ê³µê°œí•˜ì˜€ìŠµë‹ˆë‹¤.</li>
</ul>
<h3 id="Stability-AI-Stable-Diffusion-3-5-ì¶œì‹œ"><a href="#Stability-AI-Stable-Diffusion-3-5-ì¶œì‹œ" class="headerlink" title="Stability AI, Stable Diffusion 3.5 ì¶œì‹œ"></a>Stability AI, Stable Diffusion 3.5 ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://stability.ai/news/introducing-stable-diffusion-3-5">ë§í¬</a>, 2024ë…„ 10ì›” 29ì¼ ì—…ë°ì´íŠ¸</p>
<ul>
<li><strong>Stable Diffusion 3.5 Medium ëª¨ë¸ ê³µê°œ</strong>: 2.5ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¡œ êµ¬ì„±ëœ ì´ ëª¨ë¸ì€ ì†Œë¹„ì í•˜ë“œì›¨ì–´ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.</li>
<li><strong>ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±</strong>: í¬ê¸°ì— ë¹„í•´ ìµœê³ ì˜ ì´ë¯¸ì§€ ìƒì„± í’ˆì§ˆì„ ì œê³µí•˜ë©°, ê³ ê¸‰ ë‹¤ì¤‘ í•´ìƒë„ ê¸°ëŠ¥ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤.</li>
<li><strong>ì—¬ëŸ¬ ëª¨ë¸ ë³€í˜• ì œê³µ</strong>: Large, Large Turbo, Medium ë“± ë‹¤ì–‘í•œ ëª¨ë¸ ë³€í˜•ì„ í†µí•´ ì‚¬ìš©ì ìš”êµ¬ì— ë§ê²Œ ì„ íƒ ê°€ëŠ¥í•˜ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>ìƒì—…ì  ë° ë¹„ìƒì—…ì  ì‚¬ìš© í—ˆê°€</strong>: Stability AI ì»¤ë®¤ë‹ˆí‹° ë¼ì´ì„ ìŠ¤ í•˜ì— ìƒì—…ì  ë° ë¹„ìƒì—…ì  ìš©ë„ë¡œ ë¬´ë£Œë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.</li>
<li><strong>ì‚¬ìš©ì ì •ì˜ ë° íš¨ìœ¨ì„± ê°•ì¡°</strong>: ëª¨ë¸ ì•„í‚¤í…ì²˜ì™€ í›ˆë ¨ ë°©ë²•ì„ ê°œì„ í•˜ì—¬ í’ˆì§ˆ, ì¼ê´€ì„±, ë‹¤ì¤‘ í•´ìƒë„ ìƒì„± ëŠ¥ë ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.</li>
<li><strong>ì•ˆì „í•œ AI ê°œë°œ ì¤€ìˆ˜</strong>: ì•ˆì „í•˜ê³  ì±…ì„ê° ìˆëŠ” AI ê´€í–‰ì„ ì¤€ìˆ˜í•˜ë©°, ì ì¬ì ì¸ ì˜¤ìš©ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì¡°ì¹˜ë¥¼ ì·¨í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
</ul>
<h3 id="Google-Gemini-APIì™€-Google-AI-Studioì—-Grounding-with-Google-Search-ë„ì…"><a href="#Google-Gemini-APIì™€-Google-AI-Studioì—-Grounding-with-Google-Search-ë„ì…" class="headerlink" title="Google, Gemini APIì™€ Google AI Studioì— Grounding with Google Search ë„ì…"></a>Google, Gemini APIì™€ Google AI Studioì— Grounding with Google Search ë„ì…</h3><p><a target="_blank" rel="noopener" href="https://developers.googleblog.com/en/gemini-api-and-ai-studio-now-offer-grounding-with-google-search/">ë§í¬</a>, 2024ë…„ 10ì›” 31ì¼</p>
<ul>
<li><strong>Grounding with Google Search ê¸°ëŠ¥ ì œê³µ</strong>: Gemini APIì™€ Google AI Studioì—ì„œ ëª¨ë¸ ì‘ë‹µì˜ ì •í™•ì„±ê³¼ ìµœì‹ ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì´ ê¸°ëŠ¥ì„ ë„ì…í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>ì§€ì› ë§í¬ ë° ê²€ìƒ‰ ì œì•ˆ ì œê³µ</strong>: ëª¨ë¸ ì‘ë‹µì— ê·¼ê±° ì¶œì²˜ ë§í¬ì™€ ê´€ë ¨ ê²€ìƒ‰ ì œì•ˆì„ í¬í•¨í•˜ì—¬ íˆ¬ëª…ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>ëª¨ë“  Gemini 1.5 ëª¨ë¸ì—ì„œ ì§€ì›</strong>: ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  Gemini 1.5 ëª¨ë¸ ë²„ì „ì—ì„œ ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
<li><strong>ê°œë°œì ì„¤ì • ê°€ëŠ¥</strong>: Google AI Studioì—ì„œ ê°œë°œìê°€ ê¸°ëŠ¥ì„ í™œì„±í™”í•˜ê±°ë‚˜ APIì—ì„œ â€˜google_search_retrievalâ€™ ë„êµ¬ë¥¼ í†µí•´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
<li><strong>ë™ì  ê²€ìƒ‰ ì„¤ì • ì œê³µ</strong>: ì¶”ê°€ ë¹„ìš©ê³¼ ì§€ì—°ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ë™ì  ê²€ìƒ‰ êµ¬ì„±ìœ¼ë¡œ ê²€ìƒ‰ì´ í•„ìš”í•œ ì¿¼ë¦¬ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.</li>
</ul>
<h3 id="Inspirai-ì¹­í™”ëŒ€í•™êµ-Mini-Omni-2-ë°œí‘œ"><a href="#Inspirai-ì¹­í™”ëŒ€í•™êµ-Mini-Omni-2-ë°œí‘œ" class="headerlink" title="Inspirai, ì¹­í™”ëŒ€í•™êµ, Mini-Omni 2 ë°œí‘œ"></a>Inspirai, ì¹­í™”ëŒ€í•™êµ, Mini-Omni 2 ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/gpt-omni/mini-omni2">ë§í¬</a>, 2024ë…„ 10ì›” 25ì¼</p>
<ul>
<li><strong>ì´ë¯¸ì§€, ìŒì„±, í…ìŠ¤íŠ¸ ì…ë ¥ì„ ì´í•´í•˜ëŠ” ë¹„ì£¼ì–¼-ì˜¤ë””ì˜¤ ì–´ì‹œìŠ¤í„´íŠ¸ ê³µê°œ</strong>: GPT-4oì˜ ê¸°ëŠ¥ê³¼ ìœ ì‚¬í•œ ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>ì‹¤ì‹œê°„ ìŒì„± ì‘ë‹µ ë° ëŒ€í™” ì¤‘ë‹¨ ê¸°ëŠ¥ ì§€ì›</strong>: ì‚¬ìš©ìì™€ì˜ ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™” ì¤‘ì—ë„ ì¤‘ë‹¨ì´ ê°€ëŠ¥í•˜ì—¬ ìœ ì—°í•œ ìƒí˜¸ì‘ìš©ì„ ì œê³µí•©ë‹ˆë‹¤.</li>
<li><strong>ê¸°ìˆ ì  í˜ì‹ </strong>: ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, í…ìŠ¤íŠ¸ í”¼ì²˜ë¥¼ ì…ë ¥ìœ¼ë¡œ ê²°í•©í•˜ê³ , í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ ì§€ì—°ëœ ë³‘ë ¬ ì¶œë ¥ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ ìŒì„± ìƒì„±ì„ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>ì„¸ ê°€ì§€ ë‹¨ê³„ì˜ í•™ìŠµ ê³¼ì •</strong>: ì¸ì½”ë” ì ì‘, ëª¨ë‹¬ ì •ë ¬, ë©€í‹°ëª¨ë‹¬ íŒŒì¸íŠœë‹ì„ í†µí•´ ëª¨ë¸ì„ í•™ìŠµì‹œì¼°ìŠµë‹ˆë‹¤.</li>
<li><strong>MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ê³µê°œ</strong>: ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ê³µê°œë˜ì–´ ì—°êµ¬ìë“¤ê³¼ ê°œë°œìë“¤ì´ ììœ ë¡­ê²Œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
</ul>
<h3 id="Microsoft-SynthMoCap-ì†Œê°œ"><a href="#Microsoft-SynthMoCap-ì†Œê°œ" class="headerlink" title="Microsoft, SynthMoCap ì†Œê°œ"></a>Microsoft, SynthMoCap ì†Œê°œ</h3><p><a target="_blank" rel="noopener" href="https://microsoft.github.io/SynthMoCap/">ë§í¬</a>, 2024ë…„ 10ì›” 25ì¼</p>
<ul>
<li><strong>ë§ˆì»¤ ì—†ì´ ê³ í’ˆì§ˆ ì „ì‹  í¼í¬ë¨¼ìŠ¤ ìº¡ì²˜ ê¸°ìˆ  ë°œí‘œ</strong>: ë³µì¡í•œ í•˜ë“œì›¨ì–´ë‚˜ ìˆ˜ë™ ê°œì… ì—†ì´ ì–¼êµ´, ì‹ ì²´, ì†ì˜ ë™ì‘ì„ ë™ì‹œì— ìº¡ì²˜í•  ìˆ˜ ìˆëŠ” ê¸°ìˆ ì„ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>ê¸°ìˆ ì  í•˜ì´ë¼ì´íŠ¸</strong>:<ul>
<li>ì‹ ì²´ í˜•íƒœì™€ ìì„¸, ì–¼êµ´ í˜•íƒœì™€ í‘œì •ì„ ë™ì‹œì— ìº¡ì²˜,</li>
<li>ì†ê³¼ í˜€ì˜ ì›€ì§ì„, ëˆˆì˜ ì‹œì„ ê¹Œì§€ ì¶”ì  ê°€ëŠ¥,</li>
<li>ë‹¨ì¼ ë° ë‹¤ì¤‘ ë·° ì‹œë‚˜ë¦¬ì˜¤ì— ì í•©,</li>
<li>í•©ì„± ë°ì´í„°ë¥¼ í™œìš©í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨,</li>
</ul>
</li>
<li><strong>í•©ì„± ë°ì´í„°ì…‹ ê³µê°œ</strong>: SynthBody, SynthFace, SynthHand ë°ì´í„°ì…‹ì„ ê³µê°œí•˜ì—¬ ì—°êµ¬ìë“¤ì´ ë‹¤ì–‘í•œ ì‘ì—…ì— í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
</ul>
<h3 id="Meta-MobileLLM-ì¶œì‹œ"><a href="#Meta-MobileLLM-ì¶œì‹œ" class="headerlink" title="Meta, MobileLLM ì¶œì‹œ"></a>Meta, MobileLLM ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/MobileLLM">ë§í¬</a>, 2024ë…„ 10ì›” 30ì¼</p>
<ul>
<li><strong>125M, 350M, 600M, 1B ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê³µê°œ</strong>: ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” LLMì„ ì œê³µí•©ë‹ˆë‹¤.</li>
<li><strong>ì†Œí˜• LLMì˜ ì„±ëŠ¥ ìµœì í™”</strong>: ê¹Šì´ì™€ í­ì˜ ì¡°ì •ì„ í†µí•´ ì‘ì€ ëª¨ë¸ì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>ì„ë² ë”© ê³µìœ ì™€ ê·¸ë£¹ ì¿¼ë¦¬ ì–´í…ì…˜ ì‚¬ìš©</strong>: ëª¨ë¸ì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì„ë² ë”© ê³µìœ  ë° ìµœì í™”ëœ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>ì¦‰ê°ì ì¸ ë¸”ë¡ë³„ ê°€ì¤‘ì¹˜ ê³µìœ </strong>: ì§€ì—° ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜ ì´ë™ì„ í”¼í•˜ë©´ì„œë„ ìµœì†Œí•œì˜ ì˜¤ë²„í—¤ë“œë¡œ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>ì„±ëŠ¥ í–¥ìƒ</strong>:<ul>
<li>ì œë¡œìƒ· ì‘ì—…ì—ì„œ ì´ì „ SOTA 125M&#x2F;350M ëª¨ë¸ë³´ë‹¤ 2.7%&#x2F;4.3% í–¥ìƒ,</li>
<li>API í˜¸ì¶œì—ì„œ ë” í° LLaMA-v2 7B ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ì •í™•ë„ ë‹¬ì„±,</li>
</ul>
</li>
</ul>
<h3 id="Meta-Meta-Lingua-ê³µê°œ"><a href="#Meta-Meta-Lingua-ê³µê°œ" class="headerlink" title="Meta, Meta Lingua ê³µê°œ"></a>Meta, Meta Lingua ê³µê°œ</h3><p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/lingua">ë§í¬</a>, 2024ë…„ 10ì›” 25ì¼</p>
<ul>
<li><strong>ì—°êµ¬ë¥¼ ìœ„í•œ ìµœì†Œí•œì˜ ë¹ ë¥¸ LLM í›ˆë ¨ ë° ì¶”ë¡  ë¼ì´ë¸ŒëŸ¬ë¦¬</strong>: ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜, ì†ì‹¤ í•¨ìˆ˜, ë°ì´í„° ë“±ì„ ì‹¤í—˜í•˜ê¸° ìœ„í•œ PyTorch ê¸°ë°˜ì˜ ì»´í¬ë„ŒíŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.</li>
<li><strong>ì—”ë“œ íˆ¬ ì—”ë“œ í›ˆë ¨, ì¶”ë¡  ë° í‰ê°€ ì§€ì›</strong>: ëª¨ë¸ì˜ ì†ë„ì™€ ì•ˆì •ì„±ì„ ì´í•´í•˜ê³  ê°œì„ í•˜ê¸° ìœ„í•œ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.</li>
<li><strong>ì˜¤í”ˆ ì†ŒìŠ¤ ì½”ë“œë² ì´ìŠ¤</strong>: í˜„ì¬ ê°œë°œ ì¤‘ì´ë©°, ë‹¤ì–‘í•œ ì•±ì„ í†µí•´ ì‚¬ìš©ë²•ì„ ì‹œì—°í•˜ê³  ìˆìŠµë‹ˆë‹¤.</li>
<li><strong>ì—°êµ¬ì ì¹œí™”ì  ì„¤ê³„</strong>: ì‹¤í—˜ê³¼ ì—°êµ¬ë¥¼ ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•´ ìµœì†Œí•œì˜ ë³µì¡ì„±ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.</li>
</ul>
<h3 id="Hugging-Face-SmolLM-2-ì¶œì‹œ"><a href="#Hugging-Face-SmolLM-2-ì¶œì‹œ" class="headerlink" title="Hugging Face, SmolLM 2 ì¶œì‹œ"></a>Hugging Face, SmolLM 2 ì¶œì‹œ</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9">ë§í¬</a>, 2024ë…„ 11ì›” 1ì¼</p>
<ul>
<li><strong>ìƒˆë¡œìš´ ì†Œí˜• LLM ì„¸íŠ¸ ê³µê°œ</strong>: ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ìµœì í™”ëœ ì‘ì€ í¬ê¸°ì˜ LLMì„ ì¶œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>ì„¸ ê°€ì§€ í¬ê¸°ë¡œ ì œê³µ</strong>: 0.1B, 0.3B, 1.7B íŒŒë¼ë¯¸í„°ë¡œ êµ¬ì„±ëœ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤.</li>
<li><strong>Apache 2.0 ë¼ì´ì„ ìŠ¤ í•˜ì— ê³µê°œ</strong>: ììœ ë¡­ê²Œ ì‚¬ìš©í•˜ê³  ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
<li><strong>ì„±ëŠ¥ í–¥ìƒ</strong>:<ul>
<li>Meta Llama 3.2 1B ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.</li>
<li>ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ í‰ê°€ì—ì„œ ë†’ì€ ì ìˆ˜ ë‹¬ì„±,</li>
</ul>
</li>
<li><strong>ì˜¨ë””ë°”ì´ìŠ¤ ì‹¤í–‰ ì§€ì›</strong>: llama.cpp ë˜ëŠ” Transformers.jsë¥¼ í†µí•´ ë””ë°”ì´ìŠ¤ë‚˜ ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.</li>
</ul>
<h3 id="MaskGCT-ìƒˆë¡œìš´-ìµœì²¨ë‹¨-TTS-ëª¨ë¸-ë°œí‘œ"><a href="#MaskGCT-ìƒˆë¡œìš´-ìµœì²¨ë‹¨-TTS-ëª¨ë¸-ë°œí‘œ" class="headerlink" title="MaskGCT, ìƒˆë¡œìš´ ìµœì²¨ë‹¨ TTS ëª¨ë¸ ë°œí‘œ"></a>MaskGCT, ìƒˆë¡œìš´ ìµœì²¨ë‹¨ TTS ëª¨ë¸ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://maskgct.github.io/">ë§í¬</a>, 2024ë…„ 10ì›” 24ì¼</p>
<ul>
<li><strong>Zero-shot ìŒì„± í´ë¡œë‹ ë° ê°ì • TTS ì§€ì›</strong>: ìƒˆë¡œìš´ í…ìŠ¤íŠ¸-íˆ¬-ìŠ¤í”¼ì¹˜ ëª¨ë¸ë¡œ ìŒì„± í´ë¡œë‹ê³¼ ê°ì • í‘œí˜„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.</li>
<li><strong>10ë§Œ ì‹œê°„ì˜ ë°ì´í„°ë¡œ í›ˆë ¨</strong>: ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ í›ˆë ¨ë˜ì–´ ì¥ë¬¸ í•©ì„±ê³¼ ê°€ë³€ ì†ë„ í•©ì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.</li>
<li><strong>ì´ì¤‘ì–¸ì–´ ì§€ì›</strong>: ì¤‘êµ­ì–´ì™€ ì˜ì–´ë¥¼ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.</li>
<li><strong>ì™„ì „í•œ ë¹„ìë™íšŒê·€ ì•„í‚¤í…ì²˜</strong>:<ul>
<li>ë‹¨ê³„ 1: í…ìŠ¤íŠ¸ë¡œë¶€í„° ìŒì„± SSL ëª¨ë¸ì—ì„œ ì¶”ì¶œí•œ ì‹œë§¨í‹± í† í° ì˜ˆì¸¡,</li>
<li>ë‹¨ê³„ 2: ì‹œë§¨í‹± í† í°ì„ ê¸°ë°˜ìœ¼ë¡œ ìŒí–¥ í† í° ì˜ˆì¸¡,</li>
</ul>
</li>
<li><strong>í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥</strong>: ëª¨ë¸ê³¼ ì½”ë“œë¥¼ ê³µê°œí•˜ì—¬ ì—°êµ¬ìë“¤ì´ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
</ul>
<h3 id="ì—°êµ¬-ëŒ€í˜•-ì–¸ì–´-ëª¨ë¸ì´-ê°œë°œìì˜-ì´ë°ì˜¬ë¡œê¸°ë¥¼-ë°˜ì˜í•œë‹¤ëŠ”-ê²°ê³¼-ë°œí‘œ"><a href="#ì—°êµ¬-ëŒ€í˜•-ì–¸ì–´-ëª¨ë¸ì´-ê°œë°œìì˜-ì´ë°ì˜¬ë¡œê¸°ë¥¼-ë°˜ì˜í•œë‹¤ëŠ”-ê²°ê³¼-ë°œí‘œ" class="headerlink" title="ì—°êµ¬, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì´ ê°œë°œìì˜ ì´ë°ì˜¬ë¡œê¸°ë¥¼ ë°˜ì˜í•œë‹¤ëŠ” ê²°ê³¼ ë°œí‘œ"></a>ì—°êµ¬, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì´ ê°œë°œìì˜ ì´ë°ì˜¬ë¡œê¸°ë¥¼ ë°˜ì˜í•œë‹¤ëŠ” ê²°ê³¼ ë°œí‘œ</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.18417">ë§í¬</a>, 2024ë…„ 10ì›” 24ì¼ ì œì¶œ</p>
<ul>
<li><strong>LLMì˜ ì´ë°ì˜¬ë¡œê¸°ì  í¸í–¥ ì—°êµ¬</strong>: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì´ ê°œë°œìì˜ ì„¸ê³„ê´€ì„ ë°˜ì˜í•˜ì—¬ ë‹¤ì–‘í•œ ì´ë°ì˜¬ë¡œê¸°ì  ìŠ¤íƒ ìŠ¤ë¥¼ ë³´ì„ì„ ë°œê²¬í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ì–¸ì–´ì—ì„œ ì‹¤í—˜ ìˆ˜í–‰</strong>: ì¸ê¸° ìˆëŠ” ì—¬ëŸ¬ LLMì„ ëŒ€ìƒìœ¼ë¡œ ì˜ì–´ì™€ ì¤‘êµ­ì–´ì—ì„œ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>ì‘ë‹µì˜ ì´ë…ì  ì°¨ì´ ê°•ì¡°</strong>: ë™ì¼í•œ ëª¨ë¸ì´ë¼ë„ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ì™€ ì„¤ê³„ì— ë”°ë¼ ì‘ë‹µì˜ ì´ë…ì  ì°¨ì´ê°€ ë°œìƒí•¨ì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>í¸í–¥ ì œê±° ë…¸ë ¥ì— ëŒ€í•œ ìš°ë ¤ ì œê¸°</strong>: LLMì˜ ì´ë…ì  â€˜í¸í–¥â€™ì„ ì œê±°í•˜ë ¤ëŠ” ê¸°ìˆ  ë° ê·œì œ ë…¸ë ¥ì— ëŒ€í•œ ì¤‘ìš”í•œ ë¬¸ì œë¥¼ ì œê¸°í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
<li><strong>ì •ì¹˜ì  ë„êµ¬í™”ì˜ ìœ„í—˜ì„± ë…¼ì˜</strong>: LLMì´ ì •ì¹˜ì  ëª©ì ì— ì´ìš©ë  ìˆ˜ ìˆëŠ” ìœ„í—˜ì„±ì— ëŒ€í•´ ê°•ì¡°í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹"><a href="#todayâ€™s-date-in-ë…„-ì›”-ì¼-AI-ì†Œì‹" class="headerlink" title="(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,"></a>(todayâ€™s date in ë…„ ì›” ì¼) AI ì†Œì‹,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAIì—ì„œëŠ” ~~~ë¥¼ ë°œí‘œí•˜ì˜€ìŠµë‹ˆë‹¤.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">ë§í¬</a>, date</p>
<p><a href="link">ë§í¬</a>, date,</p>
<ul>
<li>detailed summary1, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)</li>
<li>detailed summary2, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
<li>detailed summary N, (ê°œì¡°ì‹ ë¬¸ì²´ ì‚¬ìš©)<br>â€¦</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://openai.com/index/introducing-chatgpt-search/</span><br><span class="line">OpenAI</span><br><span class="line"></span><br><span class="line">ğŸŒ Introducing ChatGPT search ğŸŒ</span><br><span class="line">ChatGPT can now search the web in a much better way than before, so you get fast, timely answers with links to relevant web sources.</span><br><span class="line">Weâ€™re rolling out now to Plus, Team, and SearchGPT waitlist users on our mobile and desktop apps and chatgpt.com.</span><br><span class="line">Enterprise and Edu users will have access in the next few weeks. Weâ€™ll roll out to Free users over the coming months.</span><br><span class="line"></span><br><span class="line">October 31, 2024</span><br><span class="line"></span><br><span class="line">Introducing ChatGPT search</span><br><span class="line">Get fast, timely answers with links to relevant web sources.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ChatGPT can now search the web in a much better way than before. You can get fast, timely answers with links to relevant web sources, which you would have previously needed to go to a search engine for. This blends the benefits of a natural language interface with the value of up-to-date sports scores, news, stock quotes, and more.</span><br><span class="line"></span><br><span class="line">ChatGPT will choose to search the web based on what you ask, or you can manually choose to search by clicking the web search icon.</span><br><span class="line"></span><br><span class="line">Close-up of a search bar with the text â€˜Message ChatGPTâ€™ and a blinking text entry cursor. Below, a globe icon with â€˜Searchâ€™ is shown, and a cursor hovers over the search button. A paperclip icon is also visible.</span><br><span class="line">Search will be available at chatgpt.comâ (opens in a new window), as well as on our desktop and mobile apps. All ChatGPT Plus and Team users, as well as SearchGPT waitlist users, will have access today. Enterprise and Edu users will get access in the next few weeks. Weâ€™ll roll out to all Free users over the coming months.</span><br><span class="line"></span><br><span class="line">Designed to get you to a better answer</span><br><span class="line">Getting useful answers on the web can take a lot of effort. It often requires multiple searches and digging through links to find quality sources and the right information for you.</span><br><span class="line"></span><br><span class="line">Now, chat can get you to a better answer: Ask a question in a more natural, conversational way, and ChatGPT can choose to respond with information from the web. Go deeper with follow-up questions, and ChatGPT will consider the full context of your chat to get a better answer for you.</span><br><span class="line"></span><br><span class="line">A conversation about the weather forecast for Positano, Italy, on November 2-3, 2024, showing mild temperatures and rain. The user then asks for dinner recommendations in Positano on Friday night, with responses listing local restaurants.</span><br><span class="line">We also partnered with news and data providers to add up-to-date information and new visual designs for categories like weather, stocks, sports, news, and maps.</span><br><span class="line"></span><br><span class="line">Weather</span><br><span class="line">Stocks</span><br><span class="line">Sports</span><br><span class="line">News</span><br><span class="line">Maps</span><br><span class="line">Weather</span><br><span class="line">Stocks</span><br><span class="line">Sports</span><br><span class="line">News</span><br><span class="line">Maps</span><br><span class="line">AccuWeather forecast for New York, NY, currently 59Â°F and sunny. This week: Monday, 61Â°F, partly cloudy; Tuesday, 65Â°F, pleasant; Wednesday, 76Â°F, warm; Thursday, 77Â°F, mostly sunny; Friday, 70Â°F with showers. A â€˜Sourcesâ€™ button is below.</span><br><span class="line">NVIDIA Corp (NVDA) stock chart for the past month, showing an upward trend with peaks in mid and late October. Text above notes a 0.58% increase from previous close. Additional text mentions NVIDIAâ€™s market cap surge, recently surpassing expectations.</span><br><span class="line">Screenshot showing the Golden State Warriors 2024-25 schedule with a highlighted game against the Boston Celtics on November 6 at 4:30 PM. The text notes advice for ticket purchases, mentioning a game in San Francisco on January 20, 2025.</span><br><span class="line">News headlines on Cubaâ€™s energy crisis, with articles from AP News, Financial Times, and Reuters covering grid failures and blackouts affecting residents. A â€˜Sourcesâ€™ button with icons of AP, FT, Reuters, and others is shown below.</span><br><span class="line">Map view of Morningside Heights and Harlem, New York, showing the location of Absolute Bagels, a bagel shop open until 7 pm. Image includes a photo of an everything bagel with lox and cream cheese, plus options for directions, website, and call.</span><br><span class="line">â€œChatGPT search promises to better highlight and attribute information from trustworthy news sources, benefiting audiences while expanding the reach of publishers like ourselves who produce premium journalism.â€</span><br><span class="line">Pam Wasserstein, President, Vox Media</span><br><span class="line">Go straight to the source</span><br><span class="line">Chats now include links to sources, such as news articles and blog posts, giving you a way to learn more. Click the Sources button below the response to open a sidebar with the references.</span><br><span class="line"></span><br><span class="line">Screenshot of backyard improvement suggestions, including cozy seating, outdoor lighting, and fire pits, with images of stylish backyard setups. A sidebar lists citations from sources like The Spruce, Family Handyman, and Better Homes &amp; Gardens.</span><br><span class="line">â€œWe are convinced that AI search will be, in a near future and for the next generations, a primary way to access information, and partnering with OpenAI positions Le Monde at the forefront of this shift. It allows us to test innovations at an early stage while safeguarding journalismâ€™s core values and integrity.â€</span><br><span class="line">Louis Dreyfus, CEO &amp; Publisher of Le Monde</span><br><span class="line">ChatGPT search connects people with original, high-quality content from the web and makes it part of their conversation. By integrating search with a chat interface, users can engage with information in a new way, while content owners gain new opportunities to reach a broader audience. We hope to help users discover publishers and websites, while bringing more choice to search.</span><br><span class="line"></span><br><span class="line">â€œAs AI reshapes the media landscape, Axel Springerâ€™s partnership with OpenAI opens up tremendous opportunities for innovative advancements. Together, we&#x27;re driving new business models that ensure journalism remains both trustworthy and profitable.â€</span><br><span class="line">Mathias Sanchez, SVP Global Strategic Partnerships Axel Springer SE</span><br><span class="line">We collaborated extensively with the news industry and carefully listened to feedback from our global publisher partners, including Associated Press, Axel Springer, CondÃ© Nast, Dotdash Meredith, Financial Times, GEDI, Hearst, Le Monde, News Corp, Prisa (El PaÃ­s), Reuters, The Atlantic, Time, and Vox Media. Any website or publisher can choose to appearâ (opens in a new window) in ChatGPT search. If youâ€™d like to share feedback, please email us at publishers-feedback@openai.comâ .</span><br><span class="line"></span><br><span class="line">How it works and what comes next</span><br><span class="line">The search model is a fine-tuned version of GPT-4o, post-trained using novel synthetic data generation techniques, including distilling outputs from OpenAI o1-preview. ChatGPT search leverages third-party search providers, as well as content provided directly by our partners, to provide the information users are looking for. Learn more hereâ (opens in a new window).</span><br><span class="line"></span><br><span class="line">Thanks to feedback from the SearchGPT prototype, we brought the best of the SearchGPT experience into ChatGPT. We plan to keep improving search, particularly in areas like shopping and travel, and leverage the reasoning capabilities of the OpenAI o1 series to do deeper research. We also plan to bring our new search experience to Advanced Voice and canvas, as well as to Free and logged out users in the future.</span><br><span class="line"></span><br><span class="line">ChatGPT Plus and Team users can try it out today at chatgpt.comâ (opens in a new window).</span><br><span class="line"></span><br><span class="line">OpenAI brings a new web search tool to ChatGPT</span><br><span class="line">The new tool puts OpenAI squarely in competition with the search giants, and will help fuel its next generation of AI agents</span><br><span class="line"></span><br><span class="line">By Melissa HeikkilÃ¤ &amp; Mat Honan</span><br><span class="line">October 31, 2024</span><br><span class="line"></span><br><span class="line">OpenAI</span><br><span class="line"></span><br><span class="line">ChatGPT can now search the web for up-to-date answers to a userâ€™s queries, OpenAI announced today.</span><br><span class="line"></span><br><span class="line">Until now, ChatGPT was mostly restricted to generating answers from its training data, which is current up to October 2023 for GPT-4o, and had limited web search capabilities. Searches about generalized topics will still draw on this information from the model itself, but now ChatGPT will automatically search the web in response to queries about recent information such as sports, stocks, or news of the day, and can deliver rich multi-media results. Users can also manually trigger a web search, but for the most part, the chatbot will make its own decision about when an answer would benefit from information taken from the web, says Adam Fry, OpenAIâ€™s product lead for search.</span><br><span class="line"></span><br><span class="line">â€œOur goal is to make ChatGPT the smartest assistant, and now weâ€™re really enhancing its capabilities in terms of what it has access to from the web,â€ Fry tells MIT Technology Review. The feature is available today for the chatbotâ€™s paying users.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ChatGPT triggers a web search when the user asks about local restaurants in this example</span><br><span class="line">While ChatGPT search, as it is known, is initially available to paying customers, OpenAI intends to make it available for free later, even when people are logged out. The company also plans to combine search with its voice features and Canvas, its interactive platform for coding and writing, although these capabilities will not be available in todayâ€™s initial launch.</span><br><span class="line"></span><br><span class="line">The company unveiled a standalone prototype of web search in July. Those capabilities are now built directly into the chatbot. OpenAI says it has â€œbrought the best of the SearchGPT experience into ChatGPT.â€</span><br><span class="line"></span><br><span class="line">OpenAI is the latest tech company to debut an AI-powered search assistant, challenging similar tools from competitors such as Google, Microsoft, and startup Perplexity. Meta, too, is reportedly developing its own AI search engine. As with Perplexityâ€™s interface, users of ChatGPT search can interact with the chatbot in natural language, and it will offer an AI-generated answer with sources and links to further reading. In contrast, Googleâ€™s AI Overviews offer a short AI-generated summary at the top of the website, as well as a traditional list of indexed links.</span><br><span class="line"></span><br><span class="line">These new tools could eventually challenge Googleâ€™s 90% market share in online search. AI search is a very important way to draw more users, says Chirag Shah, a professor at the University of Washington, who specializes in online search. But he says it is unlikely to chip away at Googleâ€™s search dominance. Microsoftâ€™s high-profile attempt with Bing barely made a dent in the market, Shah says.</span><br><span class="line"></span><br><span class="line">Instead, OpenAI is trying to create a new market for more powerful and interactive AI agents, which can take complex actions in the real world, Shah says.</span><br><span class="line"></span><br><span class="line">The new search function in ChatGPT is a step toward these agents.</span><br><span class="line"></span><br><span class="line">It can also deliver highly contextualized responses that take advantage of chat histories, allowing users to go deeper in a search. Currently, ChatGPT search is able to recall conversation histories and continue the conversation with questions on the same topic.</span><br><span class="line"></span><br><span class="line">ChatGPT itself can also remember things about users that it can use later â€”sometimes it does this automatically, or you can ask it to remember something. Those â€œlong-termâ€ memories affect how it responds to chats. Search doesnâ€™t have this yetâ€”a new web search starts from scratchâ€” but it should get this capability in the â€œnext couple of quarters,â€ says Fry. When it does, OpenAI says it will allow it to deliver far more personalized results based on what it knows.</span><br><span class="line"></span><br><span class="line">â€œThose might be persistent memories, like â€˜Iâ€™m a vegetarian,â€™ or it might be contextual, like â€˜Iâ€™m going to New York in the next few days,â€™â€ says Fry. â€œIf you say â€˜Iâ€™m going to New York in four days,â€™ it can remember that fact and the nuance of that point,â€ he adds.</span><br><span class="line"></span><br><span class="line">To help develop ChatGPTâ€™s web search, OpenAI says it leveraged its partnerships with news organizations such as Reuters, the Atlantic, Le Monde, the Financial Times, Axel Springer, CondÃ© Nast, and Time. However, its results include information not only from these publishers, but any other source online that does not actively block its search crawler.</span><br><span class="line"></span><br><span class="line">Itâ€™s a positive development that ChatGPT will now be able to retrieve information from these reputable online sources and generate answers based on them, says Suzan Verberne, a professor of natural-language processing at Leiden University, who has studied information retrieval. It also allows users to ask follow-up questions.</span><br><span class="line"></span><br><span class="line">But despite the enhanced ability to search the web and cross-check sources, the tool is not immune from the persistent tendency of AI language models to make things up or get it wrong. When MIT Technology Review tested the new search function and asked it for vacation destination ideas, ChatGPT suggested â€œluxury European destinationsâ€ such as Japan, Dubai, the Caribbean islands, Bali, the Seychelles, and Thailand. It offered as a source an article from the Times, a British newspaper, which listed these locations as well as those in Europe as luxury holiday options.</span><br><span class="line"></span><br><span class="line">â€œEspecially when you ask about untrue facts or events that never happened, the engine might still try to formulate a plausible response that is not necessarily correct,â€ says Verberne. There is also a risk that misinformation might seep into ChatGPTâ€™s answers from the internet if the company has not filtered its sources well enough, she adds.</span><br><span class="line"></span><br><span class="line">Another risk is that the current push to access the web through AI search will disrupt the internetâ€™s digital economy, argues Benjamin Brooks, a fellow at Harvard Universityâ€™s Berkman Klein Center, who previously led public policy for Stability AI, in an op-ed published by MIT Technology Review today.</span><br><span class="line"></span><br><span class="line">â€œBy shielding the web behind an all-knowing chatbot, AI search could deprive creators of the visits and â€˜eyeballsâ€™ they need to survive,â€ Brooks writes.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/chatgpt/download/</span><br><span class="line">OpenAI</span><br><span class="line">10/31/24</span><br><span class="line">Big day for desktops.</span><br><span class="line">Advanced Voice is now available in the macOS and Windows desktop apps.</span><br><span class="line">To access Advanced Voice on desktop, remember to download the latest version of the app.</span><br><span class="line"></span><br><span class="line">Download ChatGPT</span><br><span class="line">Get ChatGPT on mobile or desktop.</span><br><span class="line"></span><br><span class="line">A screenshot of the ChatGPT app on a mobile device with a pink gradient background, showing chat suggestions and a 9:41 time display.</span><br><span class="line">For Mobile</span><br><span class="line">Chat on the go, have voice conversations, and ask about photos.</span><br><span class="line"></span><br><span class="line">Download on App Store(opens in a new window)</span><br><span class="line">Download on Google Play(opens in a new window)</span><br><span class="line">A screenshot of a desktop showing two Word documents, &quot;Meeting Notes&quot; and &quot;Product Ideas,&quot; with the prompt &quot;Summarize key themes&quot; below. The app dock with icons is visible at the bottom.</span><br><span class="line">For Desktop</span><br><span class="line">Chat about email, screenshots, files, and anything on your screen.</span><br><span class="line"></span><br><span class="line">Download for macOS*(opens in a new window)</span><br><span class="line">Learn more about the macOS app</span><br><span class="line">Test an early version of the Windows app**(opens in a new window)</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://platform.openai.com/docs/guides/prompt-generation</span><br><span class="line">OpenAI</span><br><span class="line">10/31/24</span><br><span class="line"></span><br><span class="line">Prompt generation</span><br><span class="line">Generate prompts and schemas in Playground.</span><br><span class="line">The Generate button in the Playground lets you generate prompts, functions, and schemas from just a description of your task. This guide will walk through exactly how it works.</span><br><span class="line"></span><br><span class="line">Overview</span><br><span class="line">Creating prompts and schemas from scratch can be time-consuming, so generating them can help you get started quickly. The Generate button uses two main approaches:</span><br><span class="line"></span><br><span class="line">Prompts: We use meta-prompts that incorporate best practices to generate or improve prompts.</span><br><span class="line">Schemas: We use meta-schemas that produce valid JSON and function syntax.</span><br><span class="line">While we currently use meta prompts and schemas, we may integrate more advanced techniques in the future like DSPy and &quot;Gradient Descent&quot;.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/introducing-simpleqa/</span><br><span class="line">OpenAI</span><br><span class="line">October 30, 2024</span><br><span class="line"></span><br><span class="line">Introducing SimpleQA</span><br><span class="line">A factuality benchmark called SimpleQA that measures the ability for language models to answer short, fact-seeking questions.</span><br><span class="line"></span><br><span class="line">Read paper(opens in a new window)</span><br><span class="line">An open problem in artificial intelligence is how to train models that produce responses that are factually correct. Current language models sometimes produce false outputs or answers unsubstantiated by evidence, a problem known as â€œhallucinationsâ€. Language models that generate more accurate responses with fewer hallucinations are more trustworthy and can be used in a broader range of applications. To measure the factuality of language models, we are open-sourcingâ (opens in a new window) a new benchmark called SimpleQA.</span><br><span class="line"></span><br><span class="line">About the SimpleQA benchmark</span><br><span class="line">Factuality is a complicated topic because it is hard to measureâ€”evaluating the factuality of any given arbitrary claim is challenging, and language models can generate long completions that contain dozens of factual claims. In SimpleQA, we will focus on short, fact-seeking queries, which reduces the scope of the benchmark but makes measuring factuality much more tractable.</span><br><span class="line"></span><br><span class="line">With SimpleQA, our goal was to create a dataset with the following properties:</span><br><span class="line"></span><br><span class="line">High correctness. Reference answers to questions are supported by sources from two independent AI trainers, and questions were written in such a way that the predicted answers are easy to grade.</span><br><span class="line"></span><br><span class="line">Diversity. SimpleQA covers a wide range of topics, from science and technology to TV shows and video games.</span><br><span class="line"></span><br><span class="line">Challenging for frontier models. Compared to older benchmarks such as TriviaQAâ (opens in a new window) (2017) or NQâ (opens in a new window) (2019), which have become saturated, SimpleQA was created to be a greater challenge for frontier models (e.g., GPT-4o scores less than 40%).</span><br><span class="line"></span><br><span class="line">Good researcher UX. SimpleQA is intended to be fast and simple to run due to its concise questions and answers. Grading is also efficient whether through the OpenAI API or another frontier model API. Additionally, with 4,326 questions, SimpleQA should have relatively low variance as an evaluation benchmark.</span><br><span class="line"></span><br><span class="line">We hired AI trainers to browse the web and create short, fact-seeking questions and corresponding answers. To be included in the dataset, each question had to meet a strict set of criteria: it must have a single, indisputable answer for easy grading; the answer to the question should not change over time; and most questions had to induce hallucinations from either GPT-4o or GPT-3.5. To further improve the quality of the dataset, a second, independent AI trainer answered each question without seeing the original response. Only questions where both AI trainersâ€™ answers agreed were included.</span><br><span class="line"></span><br><span class="line">As a final verification of quality, we had a third AI trainer answer a random sample of 1,000 questions from the dataset. We found that the third AI trainerâ€™s answer matched the original agreed answers 94.4% of the time, with a 5.6% disagreement rate. We then manually inspected these examples, and found that 2.8% of the 5.6% of disagreements were due to grader false negatives or human errors from the third trainer (e.g., incomplete answers or misinterpreting sources), and the remaining 2.8% were due to real issues with the question (e.g., ambiguous questions, or different websites giving conflicting answers). Hence, we estimate the inherent error rate of this dataset to be approximately 3%.</span><br><span class="line"></span><br><span class="line">Question diversity in SimpleQA</span><br><span class="line">The pie chart below shows the diversity of topics in the SimpleQA benchmark, with examples of each question shown if you hover over the pie on the pie chart.</span><br><span class="line"></span><br><span class="line">Distribution of Tasks per Category (Number of Tasks)</span><br><span class="line">Music: 341</span><br><span class="line">Sports: 368</span><br><span class="line">Geography: 424</span><br><span class="line">Other: 475</span><br><span class="line">Art: 550</span><br><span class="line">Politics: 709</span><br><span class="line">Science andtechnology: 858</span><br><span class="line">Video games: 135</span><br><span class="line">History: 173</span><br><span class="line">TV Shows: 293</span><br><span class="line">Using SimpleQA to compare language models</span><br><span class="line">To grade questions, we use a prompted ChatGPT classifier that sees both the predicted answer from the model and the ground-truth answer, and then grades the predicted answer as either â€œcorrectâ€, â€œincorrectâ€, or â€œnot attemptedâ€.</span><br><span class="line"></span><br><span class="line">A definition and corresponding examples for each grade are shown in the table below.</span><br><span class="line"></span><br><span class="line">Grade	Definition	Examples for the question â€œWhich Dutch player scored an open-play goal in the 2022 Netherlands vs Argentina game in the menâ€™s FIFA World Cup?â€ (Answer: Wout Weghorst)</span><br><span class="line">â€œCorrectâ€	The predicted answer fully contains the ground-truth answer without contradicting the reference answer.</span><br><span class="line">â€œWout Weghorstâ€</span><br><span class="line">â€œWout Weghorst scored at 83â€™ and 90+11â€™ in that gameâ€</span><br><span class="line">â€œIncorrectâ€	The predicted answer contradicts the ground-truth answer in any way, even if the contradiction is hedged.</span><br><span class="line">â€œVirgil van Dijkâ€</span><br><span class="line">â€œVirgil van Dijk and Wout Weghorstâ€</span><br><span class="line">â€œWout Weghorst and I think van Dijk scored, but I am not totally sureâ€</span><br><span class="line">â€œNot attemptedâ€	The ground truth target is not fully given in the answer, and there are no contradictions with the reference answer.</span><br><span class="line">â€œI donâ€™t know the answer to that questionâ€</span><br><span class="line">â€œTo find which Dutch player scored in that game, please browse the internet yourselfâ€</span><br><span class="line">A model will ideally answer as many questions as possible (highest number of correct), while minimizing the number of incorrect answers.</span><br><span class="line"></span><br><span class="line">Using this classification, we can then measure the performance of several OpenAI models without browsing, including gpt-4o-mini, o1-mini, gpt-4o, and o1-preview. As expected, gpt-4o-mini and o1-mini answer fewer questions correctly compared to gpt-4o and o1-preview, likely because smaller models typically have less world knowledge. We also see that o1-mini and o1-preview, which are designed to spend more time thinking, choose to &quot;not attempt&quot; questions more often than gpt-4o-mini and gpt-4o. This may be because they can use their reasoning capacity to recognize when they donâ€™t know the answer to a question, instead of hallucinating.</span><br><span class="line"></span><br><span class="line">correct</span><br><span class="line">not attempted</span><br><span class="line">incorrect</span><br><span class="line">0.0%</span><br><span class="line">25.0%</span><br><span class="line">50.0%</span><br><span class="line">75.0%</span><br><span class="line">100.0%</span><br><span class="line">GPT-4o mini</span><br><span class="line">o1-mini</span><br><span class="line">GPT-4o</span><br><span class="line">o1-preview</span><br><span class="line">Using SimpleQA to measure the calibration of large language models</span><br><span class="line">A factuality benchmark like SimpleQA also allows us to measure the scientific phenomenon known as calibration, or whether language models â€œknow what they know.â€ One way to measure calibration is to directly ask the language model to state its confidence in its answer using the prompt: â€œPlease give your best guess, along with your confidence as a percentage that that is the correct answer.â€ Then we can plot the correlation between the stated confidence of the model, and how accurate the model actually was. A perfectly calibrated model would have the same actual accuracy as stated confidence. For instance, on all prompts where the model stated a confidence of 75%, the accuracy would be 75% for a perfectly calibrated model.</span><br><span class="line"></span><br><span class="line">This result is shown in the figure below. The positive correlation between stated confidence and accuracy is a reassuring sign that models have some notion of confidence. We see that o1-preview is more calibrated than o1-mini, and gpt4o is more calibrated than gpt4o-mini, which is consistent with prior workâ (opens in a new window) showing that larger models are more calibrated. However, the fact that performance is well below the line y=x means that models consistently overstate their confidence. Hence, there is a lot of room to improve the calibration of large language models in terms of stated confidence.</span><br><span class="line"></span><br><span class="line">Calibration (Uniform)</span><br><span class="line">GPT-4o</span><br><span class="line">GPT-4o-mini</span><br><span class="line">o1-preview</span><br><span class="line">o1-mini</span><br><span class="line">Perfect Calibration</span><br><span class="line">0.0</span><br><span class="line">0.2</span><br><span class="line">0.4</span><br><span class="line">0.6</span><br><span class="line">0.8</span><br><span class="line">1.0</span><br><span class="line">Accuracy</span><br><span class="line">0</span><br><span class="line">0.2</span><br><span class="line">0.4</span><br><span class="line">0.6</span><br><span class="line">0.8</span><br><span class="line">1</span><br><span class="line">Average Stated Confidence</span><br><span class="line">Another way to measure calibration is to ask the language model the same question 100 times. Since language models may produce different answers upon repeated attempts, we can assess whether the frequency of a particular answer corresponds to its correctness. Higher frequency typically indicates that the model is more confident in its answers, as the model is giving the same answer repeatedly. A well-calibrated model would have the same actual accuracy as frequency.</span><br><span class="line"></span><br><span class="line">In the plot below, we show the calibration of language models as measured by the frequency of their responses. Here, we simply use string match to group together different answers from the language model. We see across all models that accuracy increases with frequency, and that o1-preview has the highest level of calibration, where the frequency of the response is roughly equivalent to the accuracy of the response. Similar to calibration via stated confidence plot above, we again see o1-preview is more calibrated than o1-mini, and gpt4o is more calibrated than o1-mini.</span><br><span class="line"></span><br><span class="line">Accuracy vs Consistency - String Match (Quantile, n=30)</span><br><span class="line">GPT-4o</span><br><span class="line">GPT-4o-mini</span><br><span class="line">o1-preview</span><br><span class="line">o1-mini</span><br><span class="line">Perfect Calibration</span><br><span class="line">0.0</span><br><span class="line">0.2</span><br><span class="line">0.4</span><br><span class="line">0.6</span><br><span class="line">0.8</span><br><span class="line">1.0</span><br><span class="line">Accuracy</span><br><span class="line">0.2</span><br><span class="line">0.4</span><br><span class="line">0.6</span><br><span class="line">0.8</span><br><span class="line">1</span><br><span class="line">Frequency of answer</span><br><span class="line">Conclusions</span><br><span class="line">SimpleQA is a simple but challenging benchmark for evaluating the factuality of frontier models. A main limitation in SimpleQA is its scopeâ€”while SimpleQA is accurate it only measures factuality under the constrained setting of short, fact-seeking queries with a single, verifiable answer. Whether the ability to provide factual short answers correlates with the ability to write lengthy responses filled with numerous facts remains an open research question. We hope that open-sourcing SimpleQA drives the research on more trustworthy and reliable AI forward, and we invite researchers to evaluate the factuality of language models with it, and to provide us feedback.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://stability.ai/news/introducing-stable-diffusion-3-5</span><br><span class="line">Stability AI</span><br><span class="line">Introducing Stable Diffusion 3.5</span><br><span class="line">Updated October 29th with release of Stable Diffusion 3.5 Medium</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 Medium is here! Whether a startup or creator, access to this technology shouldnâ€™t be restricted by hardware limitations. With 2.5 billion parameters, this model is designed to run â€œout of the boxâ€ on consumer hardware.</span><br><span class="line">This model delivers best-in-class image generation for its size, with advanced multi-resolution capabilities. It surpasses other medium-sized models with its prompt adherence and image quality, making it a top choice for efficient, high-quality performance.</span><br><span class="line">This model is available for both commercial and non-commercial use under the Stability AI Community License.</span><br><span class="line"></span><br><span class="line">Key Takeaways:</span><br><span class="line"></span><br><span class="line">Today we are introducing Stable Diffusion 3.5. This open release includes multiple model variants, including Stable Diffusion 3.5 Large and Stable Diffusion 3.5 Large Turbo, and as of October 29th, Stable Diffusion 3.5 Medium.</span><br><span class="line"></span><br><span class="line">These models are highly customizable for their size, run on consumer hardware, and are free for both commercial and non-commercial use under the permissive Stability AI Community License.</span><br><span class="line"></span><br><span class="line">You can download all Stable Diffusion 3.5 models from Hugging Face and the inference code on GitHub now.</span><br><span class="line"></span><br><span class="line">View fullsize</span><br><span class="line"></span><br><span class="line">Today we are releasing Stable Diffusion 3.5, our most powerful models yet. This open release includes multiple variants that are customizable, run on consumer hardware, and are available for use under the permissive Stability AI Community License. You can download Stable Diffusion 3.5 Large and Stable Diffusion 3.5 Large Turbo models from Hugging Face and the inference code on GitHub now.</span><br><span class="line"></span><br><span class="line">In June, we released Stable Diffusion 3 Medium, the first open release from the Stable Diffusion 3 series. This release didn&#x27;t fully meet our standards or our communitiesâ€™ expectations. After listening to the valuable community feedback, instead of a quick fix, we took the time to further develop a version that advances our mission to transform visual media.</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 reflects our commitment to empower builders and creators with tools that are widely accessible, cutting-edge, and free for most use cases. We encourage the distribution and monetization of work across the entire pipeline - whether it&#x27;s fine-tuning, LoRA, optimizations, applications, or artwork.</span><br><span class="line"></span><br><span class="line">Whatâ€™s being released</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 offers a variety of models developed to meet the needs of scientific researchers, hobbyists, startups, and enterprises alike:</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 Large: At 8.1 billion parameters, with superior quality and prompt adherence, this base model is the most powerful in the Stable Diffusion family. This model is ideal for professional use cases at 1 megapixel resolution.</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 Large Turbo: A distilled version of Stable Diffusion 3.5 Large generates high-quality images with exceptional prompt adherence in just 4 steps, making it considerably faster than Stable Diffusion 3.5 Large.</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 Medium: At 2.5 billion parameters, with improved MMDiT-X architecture and training methods, this model is designed to run â€œout of the boxâ€ on consumer hardware, striking a balance between quality and ease of customization. It is capable of generating images ranging between 0.25 and 2 megapixel resolution.</span><br><span class="line"></span><br><span class="line">Developing the models</span><br><span class="line"></span><br><span class="line">In developing the models, we prioritized customizability to offer a flexible base to build upon. To achieve this, we integrated Query-Key Normalization into the transformer blocks, stabilizing the model training process and simplifying further fine-tuning and development.</span><br><span class="line"></span><br><span class="line">To support this level of downstream flexibility, we had to make some trade-offs. Greater variation in outputs from the same prompt with different seeds may occur, which is intentional as it helps preserve a broader knowledge-base and diverse styles in the base models. However, as a result, prompts lacking specificity might lead to increased uncertainty in the output, and the aesthetic level may vary.</span><br><span class="line"></span><br><span class="line">For the Medium model specifically, we made several adjustments to the architecture and training protocols to enhance quality, coherence, and multi-resolution generation abilities.</span><br><span class="line"></span><br><span class="line">Where the models excel</span><br><span class="line"></span><br><span class="line">The Stable Diffusion 3.5 version excels in the following areas, making it one of the most customizable and accessible image models on the market, while maintaining top-tier performance in prompt adherence and image quality:</span><br><span class="line"></span><br><span class="line">Customizability: Easily fine-tune the model to meet your specific creative needs, or build applications based on customized workflows.</span><br><span class="line"></span><br><span class="line">Efficient Performance: Optimized to run on standard consumer hardware without heavy demands, especially the Stable Diffusion 3.5 Medium and Stable Diffusion 3.5 Large Turbo models.</span><br><span class="line"></span><br><span class="line">We took a look at the hardware compatibility for running Stable Diffusion 3.5 Medium alongside other open-image base models. This model only requires 9.9 GB of VRAM (excluding text encoders) to unlock its full performance, making it highly accessible and compatible with most consumer GPUs.</span><br><span class="line"></span><br><span class="line">View fullsize</span><br><span class="line"></span><br><span class="line">Diverse Outputs: Creates images representative of the world, not just one type of person, with different skin tones and features, without the need for extensive prompting.</span><br><span class="line"></span><br><span class="line">View fullsize</span><br><span class="line"></span><br><span class="line">Versatile Styles: Capable of generating a wide range of styles and aesthetics like 3D, photography, painting, line art, and virtually any visual style imaginable.</span><br><span class="line"></span><br><span class="line">View fullsize</span><br><span class="line"></span><br><span class="line">Additionally, our analysis shows that Stable Diffusion 3.5 Large leads the market in prompt adherence and rivals much larger models in image quality.</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 Large Turbo offers some of the fastest inference times for its size, while remaining highly competitive in both image quality and prompt adherence, even when compared to non-distilled models of similar size</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 Medium outperforms other medium-sized models, offering a balance of prompt adherence and image quality, making it a top choice for efficient, high-quality performance.</span><br><span class="line"></span><br><span class="line">View fullsize</span><br><span class="line"></span><br><span class="line">View fullsize</span><br><span class="line"></span><br><span class="line">The Stability AI Community license at a glance</span><br><span class="line"></span><br><span class="line">We are pleased to release this model under our permissive community license. Here are the key components of the license:</span><br><span class="line"></span><br><span class="line">Free for non-commercial use: Individuals and organizations can use the model free of charge for non-commercial use, including scientific research.</span><br><span class="line"></span><br><span class="line">Free for commercial use (up to $1M in annual revenue): Startups, small to medium-sized businesses, and creators can use the model for commercial purposes at no cost, as long as their total annual revenue is less than $1M.</span><br><span class="line"></span><br><span class="line">Ownership of outputs: Retain ownership of the media generated without restrictive licensing implications.</span><br><span class="line"></span><br><span class="line">For organizations with annual revenue more than $1M, please contact us here to inquire about an Enterprise License.</span><br><span class="line"></span><br><span class="line">More ways to access the models</span><br><span class="line"></span><br><span class="line">While the model weights are available on Hugging Face now for self-hosting, you can also access the model through the following platforms:</span><br><span class="line"></span><br><span class="line">Stability AI API</span><br><span class="line"></span><br><span class="line">Replicate</span><br><span class="line"></span><br><span class="line">DeepInfra</span><br><span class="line"></span><br><span class="line">ComfyUI</span><br><span class="line"></span><br><span class="line">Our commitment to safety</span><br><span class="line"></span><br><span class="line">We believe in safe, responsible AI practices and take deliberate measures to ensure Integrity starts at the early stages of development. This means we have taken and continue to take reasonable steps to prevent the misuse of Stable Diffusion 3.5 by bad actors. For more information about our approach to Safety please visit our Stable Safety page.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://developers.googleblog.com/en/gemini-api-and-ai-studio-now-offer-grounding-with-google-search/</span><br><span class="line">Google</span><br><span class="line">Gemini API and Google AI Studio now offer Grounding with Google Search</span><br><span class="line">OCT 31, 2024</span><br><span class="line">Shrestha Basu Mallick</span><br><span class="line">Group Product Manager</span><br><span class="line">Gemini API</span><br><span class="line">Logan Kilpatrick</span><br><span class="line">Senior Product Manager</span><br><span class="line">Gemini API and Google AI Studio</span><br><span class="line"></span><br><span class="line">Share</span><br><span class="line">GeminiGrounding_Banner</span><br><span class="line">Today, we are rolling out Grounding with Google Search in Google AI Studio and the Gemini API, enabling developers to get more accurate and fresh responses from the Gemini models aided by Google Search. In addition to more accurate responses, the model returns grounding sources (in-line supporting links) and Search Suggestions that point users to the search results corresponding to the grounded response.</span><br><span class="line"></span><br><span class="line">Model response with Grounding sources and Search Suggestions when Grounding with Google Search is turned on</span><br><span class="line">Grounding with Google Search is supported with all generally available versions of Gemini 1.5 models. Developers can turn it on in Google AI Studio under the â€œToolsâ€ section or in the API by enabling the &#x27;google_search_retrieval&#x27; tool. Grounding is available to test for free in Google AI Studio. In the API, developers can access the tool with the paid tier for $35 per 1,000 grounded queries.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">When should developers use grounding?</span><br><span class="line">Developers should enable Grounding with Google Search for queries and applications which could benefit from any of the following:</span><br><span class="line"></span><br><span class="line">Reduced hallucinations: Grounding helps ensure that AI applications provide users with more factual information.</span><br><span class="line">More up-to-date information: With grounding, models can access real-time information, making AI applications relevant and applicable to a wider range of scenarios.</span><br><span class="line">Enhanced trustworthiness and traffic to publishers: By providing supporting links, grounding brings transparency to AI applications, making them more trustworthy and encouraging users to click on the underlying sources to find out more.</span><br><span class="line">Richer information: By drawing information from Google Search to enhance the model response, grounding is able to provide richer color on many queries.</span><br><span class="line"></span><br><span class="line">Grounding with Google Search in action</span><br><span class="line">We show a couple of examples below, using AI Studioâ€™s new Compare Mode, where the model response benefits from Grounding with Google Search. In the first example, the model provides an out of date answer based on its knowledge cut-off (on the left) but answers more accurately based on the latest available sources (on the right) when grounding is turned on.</span><br><span class="line"></span><br><span class="line">Model response in Google AI Studio compare mode</span><br><span class="line">Model response in Google AI Studio compare mode, without grounding (left) and with grounding (right)</span><br><span class="line">In this example, without grounding enabled (on the left), the model intentionally presents a minimal response by default. With grounding (on the right), the model comes back with a richer response including supporting links.</span><br><span class="line"></span><br><span class="line">Richer response by the latest Gemini 1.5 Flash model</span><br><span class="line">Richer response by the latest Gemini 1.5 Flash model using Grounding with Google Search (right)</span><br><span class="line">How does Grounding with Google Search work?</span><br><span class="line">When a user makes a query with grounding turned on, the service uses Googleâ€™s search engine to find up-to-date and comprehensive information that is relevant to the query, and sends it to the model. The model then responds with higher accuracy and freshness, providing in-line grounding sources (supporting links) and Search Suggestions.</span><br><span class="line"></span><br><span class="line">import google.generativeai as genai</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">genai.configure(api_key=os.environ[&quot;API_KEY&quot;])</span><br><span class="line">model = genai.GenerativeModel(&#x27;models/gemini-1.5-flash-002&#x27;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">response = model.generate_content(contents=&quot;Who won Wimbledon this year?&quot;,</span><br><span class="line">                                  tools=&#x27;google_search_retrieval&#x27;)</span><br><span class="line"></span><br><span class="line">print(response)</span><br><span class="line"># Response contains `groundingMetadata` with grounding sources, confidence scores, and search suggestions</span><br><span class="line">Refer to the documentation for complete code.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Even when Grounding with Google Search is on, not every query in a session necessarily requires grounding, which results in additional cost and latency. This is where developers have a second layer of control with dynamic retrieval.</span><br><span class="line"></span><br><span class="line">When developers request a grounded answer, the dynamic retrieval configuration assigns the prompt a prediction score, which is a floating point value between 0 and 1. The value is higher when a prompt is more likely to benefit from grounding. In their requests, developers can set a threshold for what scores should result in grounding (the default threshold value is 0.3). Developers should test various options for the threshold value to see what best works for their applications.</span><br><span class="line"></span><br><span class="line">Dynamic retrieval for Grounding Search in Google AI Studio</span><br><span class="line">Dynamic retrieval for Grounding with Google Search in Google AI Studio</span><br><span class="line">By using Googleâ€™s search results to ground Gemini-based applications, developers can provide their users with more accurate, relevant, and trustworthy information. Refer to our documentation for detailed code examples and step-by-step instructions.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">We look forward to your feedback and are excited to see what you build with this new capability!</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/gpt-omni/mini-omni2</span><br><span class="line">10/25/24</span><br><span class="line">Inspirai, Tsinghua University</span><br><span class="line"></span><br><span class="line">Mini-Omni 2 understands image, audio and text inputs all via end-to-end voice conversations with users ğŸ”¥</span><br><span class="line">&gt; Understands and processes images, speech, and text</span><br><span class="line">&gt; Generates real-time speech responses</span><br><span class="line">&gt; Supports interruptions during speech</span><br><span class="line">Technical Overview:</span><br><span class="line">&gt; Concatenates image, audio, and text features for input.</span><br><span class="line">&gt; Uses text-guided delayed parallel output for real-time speech</span><br><span class="line">&gt; Involves encoder adaptation, modal alignment, and multimodal fine-tuning</span><br><span class="line">Best part: MIT licensed âš¡</span><br><span class="line"></span><br><span class="line">Zhifei Xie, Changqiao Wu</span><br><span class="line">GPT-4o, an all-encompassing model, represents a milestone in the development of large multi-modal language models. It can understand visual, auditory, and textual modalities, directly output audio, and support flexible duplex interaction. Models from the open-source community often achieve some functionalities of GPT-4o, such as visual understanding and voice chat. Nevertheless, training a unified model that incorporates all modalities is challenging due to the complexities of multi-modal data, intricate model architectures, and training processes. In this paper, we introduce Mini-Omni2, a visual-audio assistant capable of providing real-time, end-to-end voice responses to visoin and audio queries. By integrating pretrained visual and auditory encoders, Mini-Omni2 maintains performance in individual modalities. We propose a three-stage training process to align modalities, allowing the language model to handle multi-modal inputs and outputs after training on a limited dataset. For interaction, we introduce a command-based interruption mechanism, enabling more flexible interaction with users. To the best of our knowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have similar form of functionality, and we hope it can offer valuable insights for subsequent research.</span><br><span class="line">Mini-Omni2 is an omni-interactive model. It can understand image, audio and text inputs and has end-to-end voice conversations with users. Featuring real-time voice output, omni-capable multimodal understanding and flexible interaction ability with interruption mechanism while speaking.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Updates</span><br><span class="line">2024.10: Release the model, technical report, inference and chat demo code.</span><br><span class="line">Features</span><br><span class="line">âœ… Multimodal interaction: with the ability to understand images, speech and text, just like GPT-4o.</span><br><span class="line"></span><br><span class="line">âœ… Real-time speech-to-speech conversational capabilities. No extra ASR or TTS models required, just like Mini-Omni.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://microsoft.github.io/SynthMoCap/</span><br><span class="line">Microsoft</span><br><span class="line">10/25/24</span><br><span class="line"></span><br><span class="line">Look Ma, no markers</span><br><span class="line">Holistic performance capture without the hassle</span><br><span class="line"></span><br><span class="line">ACM Transactions on Graphics</span><br><span class="line"></span><br><span class="line">SIGGRAPH Asia 2024</span><br><span class="line"></span><br><span class="line">Charlie Hewitt Fatemeh Saleh Sadegh Aliakbarian Lohit Petikam Shideh Rezaeifar Louis Florentin Zafiirah Hosenie Thomas J Cashman Julien Valentin Darren Cosker Tadas BaltruÅ¡aitis</span><br><span class="line"></span><br><span class="line"> Microsoft</span><br><span class="line">unveils the first technique for marker-free, HQ reconstruction of COMPLETE human body, including eyes &amp; tongue, without requiring any calibration, manual intervention or custom hardware. Impressive results! Repo for training &amp; DatasetğŸ’™</span><br><span class="line"></span><br><span class="line">ğ‡ğ¢ğ ğ¡ğ¥ğ¢ğ ğ¡ğ­ğ¬:</span><br><span class="line"></span><br><span class="line">âœ…Novel SOTA holistic 3D human reconstruction</span><br><span class="line"></span><br><span class="line">âœ…Body shape/pose + face shape/expression</span><br><span class="line"></span><br><span class="line">âœ…Hand &amp; tongue articulation + eye gaze</span><br><span class="line"></span><br><span class="line">âœ…Suitable for mono &amp; multi-view scenario</span><br><span class="line"></span><br><span class="line">âœ…Data pipeline for generating synthetic data</span><br><span class="line"></span><br><span class="line">âœ…Body, face and hands dataset released!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Abstract</span><br><span class="line">We tackle the problem of highly-accurate, holistic performance capture for the face, body and hands simultaneously. Motion-capture technologies used in film and game production typically focus only on face, body or hand capture independently, involve complex and expensive hardware and a high degree of manual intervention from skilled operators. While machine-learning-based approaches exist to overcome these problems, they usually only support a single camera, often operate on a single part of the body, do not produce precise world-space results, and rarely generalize outside specific contexts. In this work, we introduce the first technique for marker-free, high-quality reconstruction of the complete human body, including eyes and tongue, without requiring any calibration, manual intervention or custom hardware. Our approach produces stable world-space results from arbitrary camera rigs as well as supporting varied capture environments and clothing. We achieve this through a hybrid approach that leverages machine learning models trained exclusively on synthetic data and powerful parametric models of human shape and motion. We evaluate our method on a number of body, face and hand reconstruction benchmarks and demonstrate state-of-the-art results that generalize on diverse datasets.</span><br><span class="line">Holistic Performance Capture</span><br><span class="line">Our approach combines machine-learning models for dense-landmark and parameter prediction with model-fitting to provide a robust, accurate and adaptable system. Our method supports registration of the face, body and hands; in isolation, and together in a single take.</span><br><span class="line"></span><br><span class="line">Our parametric model captures body and hand pose, body and face shape, and facial expression.</span><br><span class="line"></span><br><span class="line">We can also track tongue articulation and eye gaze.</span><br><span class="line"></span><br><span class="line">Our method achieves state-of-the-art results on a number of 3D reconstruction benchmarks.</span><br><span class="line"></span><br><span class="line">No Hassle</span><br><span class="line">Motion capture shoots typically require specialist hardware, skilled experts and a lot of time to get right. This can make them expensive and challenging to manage in a tight production schedule. Our method aims to eliminate this inconvenience by providing a marker-less, calibration-free solution that can be used with off-the-shelf hardware. This allows for quick and easy capture of high-quality motion data in a variety of environments.</span><br><span class="line"></span><br><span class="line">Using just two uncalibrated mobile-phone cameras we can achieve high quality results in world-space.</span><br><span class="line"></span><br><span class="line">Our method even works with a single, moving camera in an unconstrained environment with arbitrary clothing.</span><br><span class="line"></span><br><span class="line">Synthetic Datasets</span><br><span class="line">Our method is trained exclusively on synthetic data, generated using a conventional computer graphics pipeline. The three datasets used in the paper are available to download here.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SynthBody can be used for tasks such as skeletal tracking and body pose prediction.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SynthFace can be used for tasks such as facial landmark and head pose prediction or face parsing.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SynthHand can be used for tasks such as hand pose prediction or landmark regression.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/facebookresearch/MobileLLM</span><br><span class="line">META</span><br><span class="line">10/30/24</span><br><span class="line"></span><br><span class="line">ğŸš¨ Meta released MobileLLM - 125M, 350M, 600M, and 1B model checkpoints! ğŸ”¥</span><br><span class="line">Notes on the release:</span><br><span class="line">Depth vs. Width: Contrary to the scaling law (Kaplan et al., 2020), depth is more critical than width for small LLMs, enhancing abstract concept capture and final performance</span><br><span class="line">Embedding Sharing: Revisited and implemented embedding sharing methods ( to maximize weight utilization</span><br><span class="line">Grouped Query Attention: Adopted from Ainslie et al. (2023) to optimize attention mechanisms</span><br><span class="line">Immediate Block-wise Weight Sharing: Reduces latency by avoiding weight movement with minimal overhead</span><br><span class="line">Performance:</span><br><span class="line">&gt; Zero-Shot Tasks: MobileLLM outperforms previous SOTA 125M/350M models by 2.7%/4.3%.</span><br><span class="line">&gt; API Calling: Comparable exact-match score to the larger LLaMA-v2 7B model 7B</span><br><span class="line">Models are available on the Hub &amp; integrated with Transformers! ğŸ”¥</span><br><span class="line"></span><br><span class="line">MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases</span><br><span class="line">Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas Chandra</span><br><span class="line">This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight-sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a further accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover, MobileLLM model family shows significant improvements compared to previous sub-billion models on chat benchmarks, and demonstrates close correctness to LLaMA-v2 7B in API calling tasks, highlighting the capability of small models for common on-device use cases.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/facebookresearch/lingua</span><br><span class="line">META</span><br><span class="line">10/25/24</span><br><span class="line">Meta Lingua</span><br><span class="line">Mathurin Videau*, Badr Youbi Idrissi*, Daniel Haziza, Luca Wehrstedt, Jade Copet, Olivier Teytaud, David Lopez-Paz. *Equal and main contribution</span><br><span class="line"></span><br><span class="line">Meta Lingua is a minimal and fast LLM training and inference library designed for research. Meta Lingua uses easy-to-modify PyTorch components in order to try new architectures, losses, data, etc. We aim for this code to enable end to end training, inference and evaluation as well as provide tools to better understand speed and stability. While Meta Lingua is currently under development, we provide you with multiple apps to showcase how to use this codebase.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9</span><br><span class="line">Huggingface</span><br><span class="line">11/1/24</span><br><span class="line">Pushing the boundaries of Small LLMs! We just released SmolLM 2, a new set of small, powerful LLMs optimized for on-device, outperforming Meta Llama 3.2 1B! SmolLM comes in three sizes, 0.1B, 0.3B, and 1.7B and under Apache 2.0. ğŸš€</span><br><span class="line"></span><br><span class="line">TL;DR;</span><br><span class="line">ğŸ”¢ Comes in 3 sizes with 135M, 360M, and 1.7B parameters.</span><br><span class="line">ğŸ“š Trained on 11 trillion mostly English tokens from FineWeb-Edu, DCLM, the Stackâ€¦</span><br><span class="line">ğŸš€ Trained for text rewriting, summarization, and function calling (27% on BFCL Leaderboard)</span><br><span class="line">ğŸ¥‡ Matches or outperforms Llama 3.2 1B and Qwen2.5 1B</span><br><span class="line">ğŸ† 56.7 IFEval; 6.13 MT Bench; 19.3 MMLU-Pro; 48.2 GMS8k</span><br><span class="line">ğŸ”§ Post-Training with SFT â†’ DPO</span><br><span class="line">ğŸ“± Runs on-device with llama.cpp or in browser with Transformers.js</span><br><span class="line">ğŸ¤— Available on Hugging Face and under Apache 2.0</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://maskgct.github.io/</span><br><span class="line">10/24/24</span><br><span class="line">MaskGCT - New open SoTA Text to Speech model! ğŸ”¥</span><br><span class="line">&gt; Zero-shot voice cloning</span><br><span class="line">&gt; Emotional TTS</span><br><span class="line">&gt; Trained on 100K hours of data</span><br><span class="line">&gt; Long form synthesis</span><br><span class="line">&gt; Variable speed synthesis</span><br><span class="line">&gt; Bilingual - Chinese &amp; English</span><br><span class="line">&gt; Available on Hugging Face</span><br><span class="line"></span><br><span class="line">Fully non-autoregressive architecture:</span><br><span class="line">&gt; Stage 1: Predicts semantic tokens from text, using tokens extracted from a speech self-supervised learning (SSL) model</span><br><span class="line">&gt; Stage 2: Predicts acoustic tokens conditioned on the semantic tokens.</span><br><span class="line"></span><br><span class="line">Synthesised: &quot;Would you guys personally like to have a fake fireplace, an electric one, in your house? Or would you rather have a real fireplace? Let me know down below. Okay everybody, that&#x27;s all for today&#x27;s video and I hope you guys learned a bunch of furniture vocabulary!&quot;</span><br><span class="line"></span><br><span class="line">TTS scene keeps getting lit! ğŸhttps://maskgct.github.io/</span><br><span class="line">Zero-Shot Text-to-Speech with Masked Generative Codec Transformer</span><br><span class="line">Abstract The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2410.18417</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Submitted on 24 Oct 2024]</span><br><span class="line">Large Language Models Reflect the Ideology of their Creators</span><br><span class="line">Maarten Buyl, Alexander Rogiers, Sander Noels, Iris Dominguez-Catena, Edith Heiter, Raphael Romero, Iman Johary, Alexandru-Cristian Mara, Jefrey Lijffijt, Tijl De Bie</span><br><span class="line">Large language models (LLMs) are trained on vast amounts of data to generate natural language, enabling them to perform tasks like text summarization and question answering. These models have become popular in artificial intelligence (AI) assistants like ChatGPT and already play an influential role in how humans access information. However, the behavior of LLMs varies depending on their design, training, and use.</span><br><span class="line">In this paper, we uncover notable diversity in the ideological stance exhibited across different LLMs and languages in which they are accessed. We do this by prompting a diverse panel of popular LLMs to describe a large number of prominent and controversial personalities from recent world history, both in English and in Chinese. By identifying and analyzing moral assessments reflected in the generated descriptions, we find consistent normative differences between how the same LLM responds in Chinese compared to English. Similarly, we identify normative disagreements between Western and non-Western LLMs about prominent actors in geopolitical conflicts. Furthermore, popularly hypothesized disparities in political goals among Western models are reflected in significant normative differences related to inclusion, social inequality, and political scandals.</span><br><span class="line">Our results show that the ideological stance of an LLM often reflects the worldview of its creators. This raises important concerns around technological and regulatory efforts with the stated aim of making LLMs ideologically `unbiased&#x27;, and it poses risks for political instrumentalization.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>ê¸°ìˆ ì ìœ¼ë¡œ ìµœëŒ€í•œ ìì„¸í•˜ê²Œ ì ì–´. 13ê°œì˜ ê¸°ì‚¬ê°€ ìˆê³  í•˜ë‚˜ë„ ë¹¼ë¨¹ì§€ ë§ê³  ì ì–´.</p>
</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-10-25</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/10/25/2024-10-25-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024ë…„ 10ì›” 25ì¼ AI ì†Œì‹,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/11/01/2024-11-1-AI-NEWS/" title="2024ë…„ 11ì›” 1ì¼ AI ì†Œì‹">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/10/22/2024-10-15-AI-NEWS/" title="2024ë…„ 10ì›” 22ì¼ AI ì†Œì‹">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>