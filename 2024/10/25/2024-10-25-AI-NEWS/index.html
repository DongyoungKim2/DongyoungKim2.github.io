<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 10월 25일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="OpenAI에서는 ChatGPT의 웹 검색 기능을 대폭 개선한 ‘ChatGPT search’를 발표하여 실시간으로 관련 웹 소스를 제공하고 사용자 경험을 향상시켰습니다. 또한 macOS와 Windows 데스크톱 앱에서 Advanced Voice 기능을 출시하여 음성 대"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 10월 25일 AI 소식</a></h3></div><div class="post-content"><p>OpenAI에서는 ChatGPT의 웹 검색 기능을 대폭 개선한 ‘ChatGPT search’를 발표하여 실시간으로 관련 웹 소스를 제공하고 사용자 경험을 향상시켰습니다. 또한 macOS와 Windows 데스크톱 앱에서 Advanced Voice 기능을 출시하여 음성 대화 기능을 확장하였습니다. Stability AI는 ‘Stable Diffusion 3.5’를 공개하여 고품질 이미지 생성을 지원하고, 다양한 모델 변형을 제공하여 사용자 커스터마이징을 용이하게 하였습니다. Google은 Gemini API와 Google AI Studio에 ‘Grounding with Google Search’ 기능을 도입하여 모델의 응답 정확성과 최신성을 향상시켰습니다. Meta는 모바일 환경에서 효율적인 LLM 활용을 위한 ‘MobileLLM’과 연구자들을 위한 LLM 훈련 및 추론 라이브러리 ‘Meta Lingua’를 출시하였습니다. Hugging Face는 소형이지만 강력한 LLM 세트인 ‘SmolLM 2’를 공개하여 온디바이스 최적화를 달성하였습니다. Inspirai와 칭화대학교는 이미지, 음성, 텍스트 입력을 모두 이해하는 비주얼-오디오 어시스턴트 ‘Mini-Omni 2’를 발표하였고, Microsoft는 마커 없이도 고품질 전신 퍼포먼스 캡처가 가능한 ‘SynthMoCap’ 기술을 소개하였습니다. 또한 MaskGCT에서는 새로운 최첨단 텍스트-투-스피치(TTS) 모델을 발표하였고, 연구자들은 대형 언어 모델이 개발자의 이데올로기를 반영한다는 연구 결과를 발표하였습니다.</p>
<h3 id="OpenAI-ChatGPT-search-소개"><a href="#OpenAI-ChatGPT-search-소개" class="headerlink" title="OpenAI, ChatGPT search 소개"></a>OpenAI, ChatGPT search 소개</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/introducing-chatgpt-search/">링크</a>, 2024년 10월 31일</p>
<ul>
<li><strong>ChatGPT의 웹 검색 기능 대폭 개선</strong>: 이제 ChatGPT는 이전보다 훨씬 향상된 웹 검색 기능을 통해 신속하고 시기적절한 답변과 관련 웹 소스 링크를 제공합니다.</li>
<li><strong>Plus, Team, SearchGPT 대기자 명단 사용자 우선 제공</strong>: 현재 Plus, Team, 그리고 SearchGPT 대기자 명단 사용자들에게 롤아웃 중이며, 모바일 및 데스크톱 앱과 chatgpt.com에서 이용 가능합니다.</li>
<li><strong>Enterprise와 Edu 사용자 제공 예정</strong>: Enterprise와 Edu 사용자들은 몇 주 내에 접근 가능하며, 무료 사용자들에게는 향후 몇 달에 걸쳐 제공될 예정입니다.</li>
<li><strong>자동 및 수동 웹 검색 기능</strong>: ChatGPT는 사용자의 질문에 따라 자동으로 웹을 검색하거나, 사용자가 웹 검색 아이콘을 클릭하여 수동으로 검색할 수 있습니다.</li>
<li><strong>새로운 시각적 디자인 추가</strong>: 날씨, 주식, 스포츠, 뉴스, 지도 등의 카테고리에 대해 최신 정보와 새로운 시각적 디자인을 제공합니다.</li>
<li><strong>신뢰할 수 있는 뉴스 및 데이터 제공자와의 파트너십</strong>: AP News, Financial Times, Reuters 등과의 협력을 통해 최신 정보와 시각 자료를 추가하였습니다.</li>
<li><strong>출처 링크 제공으로 투명성 강화</strong>: 답변에 출처 링크를 포함하여 사용자가 추가 정보를 얻을 수 있도록 지원합니다.</li>
<li><strong>검색 모델의 기술적 개선</strong>: GPT-4o의 미세 조정 버전을 사용하였으며, 새로운 합성 데이터 생성 기술을 적용하였습니다.</li>
<li><strong>향후 계획</strong>: Advanced Voice 및 캔버스에 새로운 검색 경험을 도입할 예정이며, 무료 및 로그아웃 사용자들에게도 기능을 확대할 계획입니다.</li>
</ul>
<h3 id="OpenAI-데스크톱-앱에-Advanced-Voice-기능-추가"><a href="#OpenAI-데스크톱-앱에-Advanced-Voice-기능-추가" class="headerlink" title="OpenAI, 데스크톱 앱에 Advanced Voice 기능 추가"></a>OpenAI, 데스크톱 앱에 Advanced Voice 기능 추가</h3><p><a target="_blank" rel="noopener" href="https://openai.com/chatgpt/download/">링크</a>, 2024년 10월 31일</p>
<ul>
<li><strong>macOS와 Windows 데스크톱 앱에서 Advanced Voice 기능 제공</strong>: 데스크톱 환경에서 음성 대화 기능을 사용할 수 있게 되었습니다.</li>
<li><strong>최신 버전의 앱 필요</strong>: Advanced Voice 기능을 이용하려면 최신 버전의 데스크톱 앱을 다운로드해야 합니다.</li>
<li><strong>모바일 및 데스크톱 지원</strong>: ChatGPT는 이제 모바일과 데스크톱에서 모두 Advanced Voice 기능을 제공합니다.</li>
</ul>
<h3 id="OpenAI-Playground에서-프롬프트-생성-기능-도입"><a href="#OpenAI-Playground에서-프롬프트-생성-기능-도입" class="headerlink" title="OpenAI, Playground에서 프롬프트 생성 기능 도입"></a>OpenAI, Playground에서 프롬프트 생성 기능 도입</h3><p><a target="_blank" rel="noopener" href="https://platform.openai.com/docs/guides/prompt-generation">링크</a>, 2024년 10월 31일</p>
<ul>
<li><strong>Playground의 Generate 버튼 소개</strong>: 간단한 작업 설명만으로 프롬프트, 함수, 스키마를 생성할 수 있는 기능을 제공합니다.</li>
<li><strong>메타 프롬프트와 스키마 사용</strong>: 최상의 프롬프트와 스키마를 생성하기 위해 메타 프롬프트와 메타 스키마를 사용합니다.</li>
<li><strong>생성 과정 간소화</strong>: 프롬프트와 스키마를 처음부터 작성하는 데 소요되는 시간을 절약하고 빠르게 시작할 수 있습니다.</li>
<li><strong>향후 발전된 기술 통합 예정</strong>: 앞으로 DSPy와 “Gradient Descent”와 같은 더 발전된 기술을 통합할 계획입니다.</li>
</ul>
<h3 id="OpenAI-SimpleQA-벤치마크-공개"><a href="#OpenAI-SimpleQA-벤치마크-공개" class="headerlink" title="OpenAI, SimpleQA 벤치마크 공개"></a>OpenAI, SimpleQA 벤치마크 공개</h3><p><a target="_blank" rel="noopener" href="https://openai.com/index/introducing-simpleqa/">링크</a>, 2024년 10월 30일</p>
<ul>
<li><strong>사실성 측정을 위한 벤치마크 ‘SimpleQA’ 오픈 소스화</strong>: 언어 모델의 사실성을 평가하기 위한 새로운 벤치마크를 공개하였습니다.</li>
<li><strong>단답형 사실 질문에 초점</strong>: 복잡한 사실성 측정 문제를 단순화하여 단답형 질문으로 모델의 정확성을 평가합니다.</li>
<li><strong>높은 정확도와 다양성</strong>: 다양한 주제와 높은 정확도를 가진 질문으로 구성되어 있습니다.</li>
<li><strong>최신 모델에 대한 도전성</strong>: GPT-4o 등 최신 모델들도 높은 정확도를 달성하기 어려운 도전적인 데이터셋입니다.</li>
<li><strong>연구자 친화적 사용자 경험</strong>: 빠르고 간단하게 실행할 수 있으며, 평가 변동성이 낮습니다.</li>
<li><strong>모델 간 비교 및 교정 측정</strong>: 다양한 언어 모델의 성능 비교와 모델의 교정 능력을 평가하는 데 활용됩니다.</li>
<li><strong>오픈 소스 데이터셋 제공</strong>: 연구자들이 자유롭게 사용하고 피드백을 제공할 수 있도록 데이터셋을 공개하였습니다.</li>
</ul>
<h3 id="Stability-AI-Stable-Diffusion-3-5-출시"><a href="#Stability-AI-Stable-Diffusion-3-5-출시" class="headerlink" title="Stability AI, Stable Diffusion 3.5 출시"></a>Stability AI, Stable Diffusion 3.5 출시</h3><p><a target="_blank" rel="noopener" href="https://stability.ai/news/introducing-stable-diffusion-3-5">링크</a>, 2024년 10월 29일 업데이트</p>
<ul>
<li><strong>Stable Diffusion 3.5 Medium 모델 공개</strong>: 2.5억 개의 파라미터로 구성된 이 모델은 소비자 하드웨어에서 실행 가능하도록 설계되었습니다.</li>
<li><strong>고품질 이미지 생성</strong>: 크기에 비해 최고의 이미지 생성 품질을 제공하며, 고급 다중 해상도 기능을 갖추고 있습니다.</li>
<li><strong>여러 모델 변형 제공</strong>: Large, Large Turbo, Medium 등 다양한 모델 변형을 통해 사용자 요구에 맞게 선택 가능하도록 하였습니다.</li>
<li><strong>상업적 및 비상업적 사용 허가</strong>: Stability AI 커뮤니티 라이선스 하에 상업적 및 비상업적 용도로 무료로 사용 가능합니다.</li>
<li><strong>사용자 정의 및 효율성 강조</strong>: 모델 아키텍처와 훈련 방법을 개선하여 품질, 일관성, 다중 해상도 생성 능력을 향상시켰습니다.</li>
<li><strong>안전한 AI 개발 준수</strong>: 안전하고 책임감 있는 AI 관행을 준수하며, 잠재적인 오용을 방지하기 위한 조치를 취하였습니다.</li>
</ul>
<h3 id="Google-Gemini-API와-Google-AI-Studio에-Grounding-with-Google-Search-도입"><a href="#Google-Gemini-API와-Google-AI-Studio에-Grounding-with-Google-Search-도입" class="headerlink" title="Google, Gemini API와 Google AI Studio에 Grounding with Google Search 도입"></a>Google, Gemini API와 Google AI Studio에 Grounding with Google Search 도입</h3><p><a target="_blank" rel="noopener" href="https://developers.googleblog.com/en/gemini-api-and-ai-studio-now-offer-grounding-with-google-search/">링크</a>, 2024년 10월 31일</p>
<ul>
<li><strong>Grounding with Google Search 기능 제공</strong>: Gemini API와 Google AI Studio에서 모델 응답의 정확성과 최신성을 높이기 위해 이 기능을 도입하였습니다.</li>
<li><strong>지원 링크 및 검색 제안 제공</strong>: 모델 응답에 근거 출처 링크와 관련 검색 제안을 포함하여 투명성을 높였습니다.</li>
<li><strong>모든 Gemini 1.5 모델에서 지원</strong>: 일반적으로 사용 가능한 모든 Gemini 1.5 모델 버전에서 이 기능을 사용할 수 있습니다.</li>
<li><strong>개발자 설정 가능</strong>: Google AI Studio에서 개발자가 기능을 활성화하거나 API에서 ‘google_search_retrieval’ 도구를 통해 사용할 수 있습니다.</li>
<li><strong>동적 검색 설정 제공</strong>: 추가 비용과 지연을 최소화하기 위해 동적 검색 구성으로 검색이 필요한 쿼리를 판단합니다.</li>
</ul>
<h3 id="Inspirai-칭화대학교-Mini-Omni-2-발표"><a href="#Inspirai-칭화대학교-Mini-Omni-2-발표" class="headerlink" title="Inspirai, 칭화대학교, Mini-Omni 2 발표"></a>Inspirai, 칭화대학교, Mini-Omni 2 발표</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/gpt-omni/mini-omni2">링크</a>, 2024년 10월 25일</p>
<ul>
<li><strong>이미지, 음성, 텍스트 입력을 이해하는 비주얼-오디오 어시스턴트 공개</strong>: GPT-4o의 기능과 유사한 멀티모달 입력 처리가 가능한 모델을 선보였습니다.</li>
<li><strong>실시간 음성 응답 및 대화 중단 기능 지원</strong>: 사용자와의 실시간 음성 대화 중에도 중단이 가능하여 유연한 상호작용을 제공합니다.</li>
<li><strong>기술적 혁신</strong>: 이미지, 오디오, 텍스트 피처를 입력으로 결합하고, 텍스트 기반의 지연된 병렬 출력을 사용하여 실시간 음성 생성을 구현하였습니다.</li>
<li><strong>세 가지 단계의 학습 과정</strong>: 인코더 적응, 모달 정렬, 멀티모달 파인튜닝을 통해 모델을 학습시켰습니다.</li>
<li><strong>MIT 라이선스 하에 공개</strong>: 오픈 소스로 공개되어 연구자들과 개발자들이 자유롭게 활용할 수 있습니다.</li>
</ul>
<h3 id="Microsoft-SynthMoCap-소개"><a href="#Microsoft-SynthMoCap-소개" class="headerlink" title="Microsoft, SynthMoCap 소개"></a>Microsoft, SynthMoCap 소개</h3><p><a target="_blank" rel="noopener" href="https://microsoft.github.io/SynthMoCap/">링크</a>, 2024년 10월 25일</p>
<ul>
<li><strong>마커 없이 고품질 전신 퍼포먼스 캡처 기술 발표</strong>: 복잡한 하드웨어나 수동 개입 없이 얼굴, 신체, 손의 동작을 동시에 캡처할 수 있는 기술을 선보였습니다.</li>
<li><strong>기술적 하이라이트</strong>:<ul>
<li>신체 형태와 자세, 얼굴 형태와 표정을 동시에 캡처,</li>
<li>손과 혀의 움직임, 눈의 시선까지 추적 가능,</li>
<li>단일 및 다중 뷰 시나리오에 적합,</li>
<li>합성 데이터를 활용한 머신러닝 모델 훈련,</li>
</ul>
</li>
<li><strong>합성 데이터셋 공개</strong>: SynthBody, SynthFace, SynthHand 데이터셋을 공개하여 연구자들이 다양한 작업에 활용할 수 있도록 하였습니다.</li>
</ul>
<h3 id="Meta-MobileLLM-출시"><a href="#Meta-MobileLLM-출시" class="headerlink" title="Meta, MobileLLM 출시"></a>Meta, MobileLLM 출시</h3><p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/MobileLLM">링크</a>, 2024년 10월 30일</p>
<ul>
<li><strong>125M, 350M, 600M, 1B 모델 체크포인트 공개</strong>: 모바일 디바이스에서 효율적으로 동작하는 LLM을 제공합니다.</li>
<li><strong>소형 LLM의 성능 최적화</strong>: 깊이와 폭의 조정을 통해 작은 모델에서도 높은 성능을 달성하였습니다.</li>
<li><strong>임베딩 공유와 그룹 쿼리 어텐션 사용</strong>: 모델의 효율성을 높이기 위해 임베딩 공유 및 최적화된 어텐션 메커니즘을 도입하였습니다.</li>
<li><strong>즉각적인 블록별 가중치 공유</strong>: 지연 시간을 줄이기 위해 가중치 이동을 피하면서도 최소한의 오버헤드로 구현하였습니다.</li>
<li><strong>성능 향상</strong>:<ul>
<li>제로샷 작업에서 이전 SOTA 125M&#x2F;350M 모델보다 2.7%&#x2F;4.3% 향상,</li>
<li>API 호출에서 더 큰 LLaMA-v2 7B 모델과 유사한 정확도 달성,</li>
</ul>
</li>
</ul>
<h3 id="Meta-Meta-Lingua-공개"><a href="#Meta-Meta-Lingua-공개" class="headerlink" title="Meta, Meta Lingua 공개"></a>Meta, Meta Lingua 공개</h3><p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/lingua">링크</a>, 2024년 10월 25일</p>
<ul>
<li><strong>연구를 위한 최소한의 빠른 LLM 훈련 및 추론 라이브러리</strong>: 새로운 아키텍처, 손실 함수, 데이터 등을 실험하기 위한 PyTorch 기반의 컴포넌트를 제공합니다.</li>
<li><strong>엔드 투 엔드 훈련, 추론 및 평가 지원</strong>: 모델의 속도와 안정성을 이해하고 개선하기 위한 도구를 제공합니다.</li>
<li><strong>오픈 소스 코드베이스</strong>: 현재 개발 중이며, 다양한 앱을 통해 사용법을 시연하고 있습니다.</li>
<li><strong>연구자 친화적 설계</strong>: 실험과 연구를 용이하게 하기 위해 최소한의 복잡성으로 설계되었습니다.</li>
</ul>
<h3 id="Hugging-Face-SmolLM-2-출시"><a href="#Hugging-Face-SmolLM-2-출시" class="headerlink" title="Hugging Face, SmolLM 2 출시"></a>Hugging Face, SmolLM 2 출시</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9">링크</a>, 2024년 11월 1일</p>
<ul>
<li><strong>새로운 소형 LLM 세트 공개</strong>: 온디바이스에서 최적화된 작은 크기의 LLM을 출시하였습니다.</li>
<li><strong>세 가지 크기로 제공</strong>: 0.1B, 0.3B, 1.7B 파라미터로 구성된 모델을 제공합니다.</li>
<li><strong>Apache 2.0 라이선스 하에 공개</strong>: 자유롭게 사용하고 수정할 수 있습니다.</li>
<li><strong>성능 향상</strong>:<ul>
<li>Meta Llama 3.2 1B 모델을 능가하는 성능을 보여줍니다.</li>
<li>다양한 언어 모델 평가에서 높은 점수 달성,</li>
</ul>
</li>
<li><strong>온디바이스 실행 지원</strong>: llama.cpp 또는 Transformers.js를 통해 디바이스나 브라우저에서 실행 가능합니다.</li>
</ul>
<h3 id="MaskGCT-새로운-최첨단-TTS-모델-발표"><a href="#MaskGCT-새로운-최첨단-TTS-모델-발표" class="headerlink" title="MaskGCT, 새로운 최첨단 TTS 모델 발표"></a>MaskGCT, 새로운 최첨단 TTS 모델 발표</h3><p><a target="_blank" rel="noopener" href="https://maskgct.github.io/">링크</a>, 2024년 10월 24일</p>
<ul>
<li><strong>Zero-shot 음성 클로닝 및 감정 TTS 지원</strong>: 새로운 텍스트-투-스피치 모델로 음성 클로닝과 감정 표현이 가능합니다.</li>
<li><strong>10만 시간의 데이터로 훈련</strong>: 대규모 데이터로 훈련되어 장문 합성과 가변 속도 합성이 가능합니다.</li>
<li><strong>이중언어 지원</strong>: 중국어와 영어를 모두 지원합니다.</li>
<li><strong>완전한 비자동회귀 아키텍처</strong>:<ul>
<li>단계 1: 텍스트로부터 음성 SSL 모델에서 추출한 시맨틱 토큰 예측,</li>
<li>단계 2: 시맨틱 토큰을 기반으로 음향 토큰 예측,</li>
</ul>
</li>
<li><strong>허깅페이스에서 사용 가능</strong>: 모델과 코드를 공개하여 연구자들이 활용할 수 있도록 하였습니다.</li>
</ul>
<h3 id="연구-대형-언어-모델이-개발자의-이데올로기를-반영한다는-결과-발표"><a href="#연구-대형-언어-모델이-개발자의-이데올로기를-반영한다는-결과-발표" class="headerlink" title="연구, 대형 언어 모델이 개발자의 이데올로기를 반영한다는 결과 발표"></a>연구, 대형 언어 모델이 개발자의 이데올로기를 반영한다는 결과 발표</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.18417">링크</a>, 2024년 10월 24일 제출</p>
<ul>
<li><strong>LLM의 이데올로기적 편향 연구</strong>: 대형 언어 모델이 개발자의 세계관을 반영하여 다양한 이데올로기적 스탠스를 보임을 발견하였습니다.</li>
<li><strong>다양한 모델과 언어에서 실험 수행</strong>: 인기 있는 여러 LLM을 대상으로 영어와 중국어에서 실험을 진행하였습니다.</li>
<li><strong>응답의 이념적 차이 강조</strong>: 동일한 모델이라도 사용하는 언어와 설계에 따라 응답의 이념적 차이가 발생함을 확인하였습니다.</li>
<li><strong>편향 제거 노력에 대한 우려 제기</strong>: LLM의 이념적 ‘편향’을 제거하려는 기술 및 규제 노력에 대한 중요한 문제를 제기하였습니다.</li>
<li><strong>정치적 도구화의 위험성 논의</strong>: LLM이 정치적 목적에 이용될 수 있는 위험성에 대해 강조하였습니다.</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://openai.com/index/introducing-chatgpt-search/</span><br><span class="line">OpenAI</span><br><span class="line"></span><br><span class="line">🌐 Introducing ChatGPT search 🌐</span><br><span class="line">ChatGPT can now search the web in a much better way than before, so you get fast, timely answers with links to relevant web sources.</span><br><span class="line">We’re rolling out now to Plus, Team, and SearchGPT waitlist users on our mobile and desktop apps and chatgpt.com.</span><br><span class="line">Enterprise and Edu users will have access in the next few weeks. We’ll roll out to Free users over the coming months.</span><br><span class="line"></span><br><span class="line">October 31, 2024</span><br><span class="line"></span><br><span class="line">Introducing ChatGPT search</span><br><span class="line">Get fast, timely answers with links to relevant web sources.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ChatGPT can now search the web in a much better way than before. You can get fast, timely answers with links to relevant web sources, which you would have previously needed to go to a search engine for. This blends the benefits of a natural language interface with the value of up-to-date sports scores, news, stock quotes, and more.</span><br><span class="line"></span><br><span class="line">ChatGPT will choose to search the web based on what you ask, or you can manually choose to search by clicking the web search icon.</span><br><span class="line"></span><br><span class="line">Close-up of a search bar with the text ‘Message ChatGPT’ and a blinking text entry cursor. Below, a globe icon with ‘Search’ is shown, and a cursor hovers over the search button. A paperclip icon is also visible.</span><br><span class="line">Search will be available at chatgpt.com⁠(opens in a new window), as well as on our desktop and mobile apps. All ChatGPT Plus and Team users, as well as SearchGPT waitlist users, will have access today. Enterprise and Edu users will get access in the next few weeks. We’ll roll out to all Free users over the coming months.</span><br><span class="line"></span><br><span class="line">Designed to get you to a better answer</span><br><span class="line">Getting useful answers on the web can take a lot of effort. It often requires multiple searches and digging through links to find quality sources and the right information for you.</span><br><span class="line"></span><br><span class="line">Now, chat can get you to a better answer: Ask a question in a more natural, conversational way, and ChatGPT can choose to respond with information from the web. Go deeper with follow-up questions, and ChatGPT will consider the full context of your chat to get a better answer for you.</span><br><span class="line"></span><br><span class="line">A conversation about the weather forecast for Positano, Italy, on November 2-3, 2024, showing mild temperatures and rain. The user then asks for dinner recommendations in Positano on Friday night, with responses listing local restaurants.</span><br><span class="line">We also partnered with news and data providers to add up-to-date information and new visual designs for categories like weather, stocks, sports, news, and maps.</span><br><span class="line"></span><br><span class="line">Weather</span><br><span class="line">Stocks</span><br><span class="line">Sports</span><br><span class="line">News</span><br><span class="line">Maps</span><br><span class="line">Weather</span><br><span class="line">Stocks</span><br><span class="line">Sports</span><br><span class="line">News</span><br><span class="line">Maps</span><br><span class="line">AccuWeather forecast for New York, NY, currently 59°F and sunny. This week: Monday, 61°F, partly cloudy; Tuesday, 65°F, pleasant; Wednesday, 76°F, warm; Thursday, 77°F, mostly sunny; Friday, 70°F with showers. A ‘Sources’ button is below.</span><br><span class="line">NVIDIA Corp (NVDA) stock chart for the past month, showing an upward trend with peaks in mid and late October. Text above notes a 0.58% increase from previous close. Additional text mentions NVIDIA’s market cap surge, recently surpassing expectations.</span><br><span class="line">Screenshot showing the Golden State Warriors 2024-25 schedule with a highlighted game against the Boston Celtics on November 6 at 4:30 PM. The text notes advice for ticket purchases, mentioning a game in San Francisco on January 20, 2025.</span><br><span class="line">News headlines on Cuba’s energy crisis, with articles from AP News, Financial Times, and Reuters covering grid failures and blackouts affecting residents. A ‘Sources’ button with icons of AP, FT, Reuters, and others is shown below.</span><br><span class="line">Map view of Morningside Heights and Harlem, New York, showing the location of Absolute Bagels, a bagel shop open until 7 pm. Image includes a photo of an everything bagel with lox and cream cheese, plus options for directions, website, and call.</span><br><span class="line">“ChatGPT search promises to better highlight and attribute information from trustworthy news sources, benefiting audiences while expanding the reach of publishers like ourselves who produce premium journalism.”</span><br><span class="line">Pam Wasserstein, President, Vox Media</span><br><span class="line">Go straight to the source</span><br><span class="line">Chats now include links to sources, such as news articles and blog posts, giving you a way to learn more. Click the Sources button below the response to open a sidebar with the references.</span><br><span class="line"></span><br><span class="line">Screenshot of backyard improvement suggestions, including cozy seating, outdoor lighting, and fire pits, with images of stylish backyard setups. A sidebar lists citations from sources like The Spruce, Family Handyman, and Better Homes &amp; Gardens.</span><br><span class="line">“We are convinced that AI search will be, in a near future and for the next generations, a primary way to access information, and partnering with OpenAI positions Le Monde at the forefront of this shift. It allows us to test innovations at an early stage while safeguarding journalism’s core values and integrity.”</span><br><span class="line">Louis Dreyfus, CEO &amp; Publisher of Le Monde</span><br><span class="line">ChatGPT search connects people with original, high-quality content from the web and makes it part of their conversation. By integrating search with a chat interface, users can engage with information in a new way, while content owners gain new opportunities to reach a broader audience. We hope to help users discover publishers and websites, while bringing more choice to search.</span><br><span class="line"></span><br><span class="line">“As AI reshapes the media landscape, Axel Springer’s partnership with OpenAI opens up tremendous opportunities for innovative advancements. Together, we&#x27;re driving new business models that ensure journalism remains both trustworthy and profitable.”</span><br><span class="line">Mathias Sanchez, SVP Global Strategic Partnerships Axel Springer SE</span><br><span class="line">We collaborated extensively with the news industry and carefully listened to feedback from our global publisher partners, including Associated Press, Axel Springer, Condé Nast, Dotdash Meredith, Financial Times, GEDI, Hearst, Le Monde, News Corp, Prisa (El País), Reuters, The Atlantic, Time, and Vox Media. Any website or publisher can choose to appear⁠(opens in a new window) in ChatGPT search. If you’d like to share feedback, please email us at publishers-feedback@openai.com⁠.</span><br><span class="line"></span><br><span class="line">How it works and what comes next</span><br><span class="line">The search model is a fine-tuned version of GPT-4o, post-trained using novel synthetic data generation techniques, including distilling outputs from OpenAI o1-preview. ChatGPT search leverages third-party search providers, as well as content provided directly by our partners, to provide the information users are looking for. Learn more here⁠(opens in a new window).</span><br><span class="line"></span><br><span class="line">Thanks to feedback from the SearchGPT prototype, we brought the best of the SearchGPT experience into ChatGPT. We plan to keep improving search, particularly in areas like shopping and travel, and leverage the reasoning capabilities of the OpenAI o1 series to do deeper research. We also plan to bring our new search experience to Advanced Voice and canvas, as well as to Free and logged out users in the future.</span><br><span class="line"></span><br><span class="line">ChatGPT Plus and Team users can try it out today at chatgpt.com⁠(opens in a new window).</span><br><span class="line"></span><br><span class="line">OpenAI brings a new web search tool to ChatGPT</span><br><span class="line">The new tool puts OpenAI squarely in competition with the search giants, and will help fuel its next generation of AI agents</span><br><span class="line"></span><br><span class="line">By Melissa Heikkilä &amp; Mat Honan</span><br><span class="line">October 31, 2024</span><br><span class="line"></span><br><span class="line">OpenAI</span><br><span class="line"></span><br><span class="line">ChatGPT can now search the web for up-to-date answers to a user’s queries, OpenAI announced today.</span><br><span class="line"></span><br><span class="line">Until now, ChatGPT was mostly restricted to generating answers from its training data, which is current up to October 2023 for GPT-4o, and had limited web search capabilities. Searches about generalized topics will still draw on this information from the model itself, but now ChatGPT will automatically search the web in response to queries about recent information such as sports, stocks, or news of the day, and can deliver rich multi-media results. Users can also manually trigger a web search, but for the most part, the chatbot will make its own decision about when an answer would benefit from information taken from the web, says Adam Fry, OpenAI’s product lead for search.</span><br><span class="line"></span><br><span class="line">“Our goal is to make ChatGPT the smartest assistant, and now we’re really enhancing its capabilities in terms of what it has access to from the web,” Fry tells MIT Technology Review. The feature is available today for the chatbot’s paying users.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ChatGPT triggers a web search when the user asks about local restaurants in this example</span><br><span class="line">While ChatGPT search, as it is known, is initially available to paying customers, OpenAI intends to make it available for free later, even when people are logged out. The company also plans to combine search with its voice features and Canvas, its interactive platform for coding and writing, although these capabilities will not be available in today’s initial launch.</span><br><span class="line"></span><br><span class="line">The company unveiled a standalone prototype of web search in July. Those capabilities are now built directly into the chatbot. OpenAI says it has “brought the best of the SearchGPT experience into ChatGPT.”</span><br><span class="line"></span><br><span class="line">OpenAI is the latest tech company to debut an AI-powered search assistant, challenging similar tools from competitors such as Google, Microsoft, and startup Perplexity. Meta, too, is reportedly developing its own AI search engine. As with Perplexity’s interface, users of ChatGPT search can interact with the chatbot in natural language, and it will offer an AI-generated answer with sources and links to further reading. In contrast, Google’s AI Overviews offer a short AI-generated summary at the top of the website, as well as a traditional list of indexed links.</span><br><span class="line"></span><br><span class="line">These new tools could eventually challenge Google’s 90% market share in online search. AI search is a very important way to draw more users, says Chirag Shah, a professor at the University of Washington, who specializes in online search. But he says it is unlikely to chip away at Google’s search dominance. Microsoft’s high-profile attempt with Bing barely made a dent in the market, Shah says.</span><br><span class="line"></span><br><span class="line">Instead, OpenAI is trying to create a new market for more powerful and interactive AI agents, which can take complex actions in the real world, Shah says.</span><br><span class="line"></span><br><span class="line">The new search function in ChatGPT is a step toward these agents.</span><br><span class="line"></span><br><span class="line">It can also deliver highly contextualized responses that take advantage of chat histories, allowing users to go deeper in a search. Currently, ChatGPT search is able to recall conversation histories and continue the conversation with questions on the same topic.</span><br><span class="line"></span><br><span class="line">ChatGPT itself can also remember things about users that it can use later —sometimes it does this automatically, or you can ask it to remember something. Those “long-term” memories affect how it responds to chats. Search doesn’t have this yet—a new web search starts from scratch— but it should get this capability in the “next couple of quarters,” says Fry. When it does, OpenAI says it will allow it to deliver far more personalized results based on what it knows.</span><br><span class="line"></span><br><span class="line">“Those might be persistent memories, like ‘I’m a vegetarian,’ or it might be contextual, like ‘I’m going to New York in the next few days,’” says Fry. “If you say ‘I’m going to New York in four days,’ it can remember that fact and the nuance of that point,” he adds.</span><br><span class="line"></span><br><span class="line">To help develop ChatGPT’s web search, OpenAI says it leveraged its partnerships with news organizations such as Reuters, the Atlantic, Le Monde, the Financial Times, Axel Springer, Condé Nast, and Time. However, its results include information not only from these publishers, but any other source online that does not actively block its search crawler.</span><br><span class="line"></span><br><span class="line">It’s a positive development that ChatGPT will now be able to retrieve information from these reputable online sources and generate answers based on them, says Suzan Verberne, a professor of natural-language processing at Leiden University, who has studied information retrieval. It also allows users to ask follow-up questions.</span><br><span class="line"></span><br><span class="line">But despite the enhanced ability to search the web and cross-check sources, the tool is not immune from the persistent tendency of AI language models to make things up or get it wrong. When MIT Technology Review tested the new search function and asked it for vacation destination ideas, ChatGPT suggested “luxury European destinations” such as Japan, Dubai, the Caribbean islands, Bali, the Seychelles, and Thailand. It offered as a source an article from the Times, a British newspaper, which listed these locations as well as those in Europe as luxury holiday options.</span><br><span class="line"></span><br><span class="line">“Especially when you ask about untrue facts or events that never happened, the engine might still try to formulate a plausible response that is not necessarily correct,” says Verberne. There is also a risk that misinformation might seep into ChatGPT’s answers from the internet if the company has not filtered its sources well enough, she adds.</span><br><span class="line"></span><br><span class="line">Another risk is that the current push to access the web through AI search will disrupt the internet’s digital economy, argues Benjamin Brooks, a fellow at Harvard University’s Berkman Klein Center, who previously led public policy for Stability AI, in an op-ed published by MIT Technology Review today.</span><br><span class="line"></span><br><span class="line">“By shielding the web behind an all-knowing chatbot, AI search could deprive creators of the visits and ‘eyeballs’ they need to survive,” Brooks writes.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/chatgpt/download/</span><br><span class="line">OpenAI</span><br><span class="line">10/31/24</span><br><span class="line">Big day for desktops.</span><br><span class="line">Advanced Voice is now available in the macOS and Windows desktop apps.</span><br><span class="line">To access Advanced Voice on desktop, remember to download the latest version of the app.</span><br><span class="line"></span><br><span class="line">Download ChatGPT</span><br><span class="line">Get ChatGPT on mobile or desktop.</span><br><span class="line"></span><br><span class="line">A screenshot of the ChatGPT app on a mobile device with a pink gradient background, showing chat suggestions and a 9:41 time display.</span><br><span class="line">For Mobile</span><br><span class="line">Chat on the go, have voice conversations, and ask about photos.</span><br><span class="line"></span><br><span class="line">Download on App Store(opens in a new window)</span><br><span class="line">Download on Google Play(opens in a new window)</span><br><span class="line">A screenshot of a desktop showing two Word documents, &quot;Meeting Notes&quot; and &quot;Product Ideas,&quot; with the prompt &quot;Summarize key themes&quot; below. The app dock with icons is visible at the bottom.</span><br><span class="line">For Desktop</span><br><span class="line">Chat about email, screenshots, files, and anything on your screen.</span><br><span class="line"></span><br><span class="line">Download for macOS*(opens in a new window)</span><br><span class="line">Learn more about the macOS app</span><br><span class="line">Test an early version of the Windows app**(opens in a new window)</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://platform.openai.com/docs/guides/prompt-generation</span><br><span class="line">OpenAI</span><br><span class="line">10/31/24</span><br><span class="line"></span><br><span class="line">Prompt generation</span><br><span class="line">Generate prompts and schemas in Playground.</span><br><span class="line">The Generate button in the Playground lets you generate prompts, functions, and schemas from just a description of your task. This guide will walk through exactly how it works.</span><br><span class="line"></span><br><span class="line">Overview</span><br><span class="line">Creating prompts and schemas from scratch can be time-consuming, so generating them can help you get started quickly. The Generate button uses two main approaches:</span><br><span class="line"></span><br><span class="line">Prompts: We use meta-prompts that incorporate best practices to generate or improve prompts.</span><br><span class="line">Schemas: We use meta-schemas that produce valid JSON and function syntax.</span><br><span class="line">While we currently use meta prompts and schemas, we may integrate more advanced techniques in the future like DSPy and &quot;Gradient Descent&quot;.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://openai.com/index/introducing-simpleqa/</span><br><span class="line">OpenAI</span><br><span class="line">October 30, 2024</span><br><span class="line"></span><br><span class="line">Introducing SimpleQA</span><br><span class="line">A factuality benchmark called SimpleQA that measures the ability for language models to answer short, fact-seeking questions.</span><br><span class="line"></span><br><span class="line">Read paper(opens in a new window)</span><br><span class="line">An open problem in artificial intelligence is how to train models that produce responses that are factually correct. Current language models sometimes produce false outputs or answers unsubstantiated by evidence, a problem known as “hallucinations”. Language models that generate more accurate responses with fewer hallucinations are more trustworthy and can be used in a broader range of applications. To measure the factuality of language models, we are open-sourcing⁠(opens in a new window) a new benchmark called SimpleQA.</span><br><span class="line"></span><br><span class="line">About the SimpleQA benchmark</span><br><span class="line">Factuality is a complicated topic because it is hard to measure—evaluating the factuality of any given arbitrary claim is challenging, and language models can generate long completions that contain dozens of factual claims. In SimpleQA, we will focus on short, fact-seeking queries, which reduces the scope of the benchmark but makes measuring factuality much more tractable.</span><br><span class="line"></span><br><span class="line">With SimpleQA, our goal was to create a dataset with the following properties:</span><br><span class="line"></span><br><span class="line">High correctness. Reference answers to questions are supported by sources from two independent AI trainers, and questions were written in such a way that the predicted answers are easy to grade.</span><br><span class="line"></span><br><span class="line">Diversity. SimpleQA covers a wide range of topics, from science and technology to TV shows and video games.</span><br><span class="line"></span><br><span class="line">Challenging for frontier models. Compared to older benchmarks such as TriviaQA⁠(opens in a new window) (2017) or NQ⁠(opens in a new window) (2019), which have become saturated, SimpleQA was created to be a greater challenge for frontier models (e.g., GPT-4o scores less than 40%).</span><br><span class="line"></span><br><span class="line">Good researcher UX. SimpleQA is intended to be fast and simple to run due to its concise questions and answers. Grading is also efficient whether through the OpenAI API or another frontier model API. Additionally, with 4,326 questions, SimpleQA should have relatively low variance as an evaluation benchmark.</span><br><span class="line"></span><br><span class="line">We hired AI trainers to browse the web and create short, fact-seeking questions and corresponding answers. To be included in the dataset, each question had to meet a strict set of criteria: it must have a single, indisputable answer for easy grading; the answer to the question should not change over time; and most questions had to induce hallucinations from either GPT-4o or GPT-3.5. To further improve the quality of the dataset, a second, independent AI trainer answered each question without seeing the original response. Only questions where both AI trainers’ answers agreed were included.</span><br><span class="line"></span><br><span class="line">As a final verification of quality, we had a third AI trainer answer a random sample of 1,000 questions from the dataset. We found that the third AI trainer’s answer matched the original agreed answers 94.4% of the time, with a 5.6% disagreement rate. We then manually inspected these examples, and found that 2.8% of the 5.6% of disagreements were due to grader false negatives or human errors from the third trainer (e.g., incomplete answers or misinterpreting sources), and the remaining 2.8% were due to real issues with the question (e.g., ambiguous questions, or different websites giving conflicting answers). Hence, we estimate the inherent error rate of this dataset to be approximately 3%.</span><br><span class="line"></span><br><span class="line">Question diversity in SimpleQA</span><br><span class="line">The pie chart below shows the diversity of topics in the SimpleQA benchmark, with examples of each question shown if you hover over the pie on the pie chart.</span><br><span class="line"></span><br><span class="line">Distribution of Tasks per Category (Number of Tasks)</span><br><span class="line">Music: 341</span><br><span class="line">Sports: 368</span><br><span class="line">Geography: 424</span><br><span class="line">Other: 475</span><br><span class="line">Art: 550</span><br><span class="line">Politics: 709</span><br><span class="line">Science andtechnology: 858</span><br><span class="line">Video games: 135</span><br><span class="line">History: 173</span><br><span class="line">TV Shows: 293</span><br><span class="line">Using SimpleQA to compare language models</span><br><span class="line">To grade questions, we use a prompted ChatGPT classifier that sees both the predicted answer from the model and the ground-truth answer, and then grades the predicted answer as either “correct”, “incorrect”, or “not attempted”.</span><br><span class="line"></span><br><span class="line">A definition and corresponding examples for each grade are shown in the table below.</span><br><span class="line"></span><br><span class="line">Grade	Definition	Examples for the question “Which Dutch player scored an open-play goal in the 2022 Netherlands vs Argentina game in the men’s FIFA World Cup?” (Answer: Wout Weghorst)</span><br><span class="line">“Correct”	The predicted answer fully contains the ground-truth answer without contradicting the reference answer.</span><br><span class="line">“Wout Weghorst”</span><br><span class="line">“Wout Weghorst scored at 83’ and 90+11’ in that game”</span><br><span class="line">“Incorrect”	The predicted answer contradicts the ground-truth answer in any way, even if the contradiction is hedged.</span><br><span class="line">“Virgil van Dijk”</span><br><span class="line">“Virgil van Dijk and Wout Weghorst”</span><br><span class="line">“Wout Weghorst and I think van Dijk scored, but I am not totally sure”</span><br><span class="line">“Not attempted”	The ground truth target is not fully given in the answer, and there are no contradictions with the reference answer.</span><br><span class="line">“I don’t know the answer to that question”</span><br><span class="line">“To find which Dutch player scored in that game, please browse the internet yourself”</span><br><span class="line">A model will ideally answer as many questions as possible (highest number of correct), while minimizing the number of incorrect answers.</span><br><span class="line"></span><br><span class="line">Using this classification, we can then measure the performance of several OpenAI models without browsing, including gpt-4o-mini, o1-mini, gpt-4o, and o1-preview. As expected, gpt-4o-mini and o1-mini answer fewer questions correctly compared to gpt-4o and o1-preview, likely because smaller models typically have less world knowledge. We also see that o1-mini and o1-preview, which are designed to spend more time thinking, choose to &quot;not attempt&quot; questions more often than gpt-4o-mini and gpt-4o. This may be because they can use their reasoning capacity to recognize when they don’t know the answer to a question, instead of hallucinating.</span><br><span class="line"></span><br><span class="line">correct</span><br><span class="line">not attempted</span><br><span class="line">incorrect</span><br><span class="line">0.0%</span><br><span class="line">25.0%</span><br><span class="line">50.0%</span><br><span class="line">75.0%</span><br><span class="line">100.0%</span><br><span class="line">GPT-4o mini</span><br><span class="line">o1-mini</span><br><span class="line">GPT-4o</span><br><span class="line">o1-preview</span><br><span class="line">Using SimpleQA to measure the calibration of large language models</span><br><span class="line">A factuality benchmark like SimpleQA also allows us to measure the scientific phenomenon known as calibration, or whether language models “know what they know.” One way to measure calibration is to directly ask the language model to state its confidence in its answer using the prompt: “Please give your best guess, along with your confidence as a percentage that that is the correct answer.” Then we can plot the correlation between the stated confidence of the model, and how accurate the model actually was. A perfectly calibrated model would have the same actual accuracy as stated confidence. For instance, on all prompts where the model stated a confidence of 75%, the accuracy would be 75% for a perfectly calibrated model.</span><br><span class="line"></span><br><span class="line">This result is shown in the figure below. The positive correlation between stated confidence and accuracy is a reassuring sign that models have some notion of confidence. We see that o1-preview is more calibrated than o1-mini, and gpt4o is more calibrated than gpt4o-mini, which is consistent with prior work⁠(opens in a new window) showing that larger models are more calibrated. However, the fact that performance is well below the line y=x means that models consistently overstate their confidence. Hence, there is a lot of room to improve the calibration of large language models in terms of stated confidence.</span><br><span class="line"></span><br><span class="line">Calibration (Uniform)</span><br><span class="line">GPT-4o</span><br><span class="line">GPT-4o-mini</span><br><span class="line">o1-preview</span><br><span class="line">o1-mini</span><br><span class="line">Perfect Calibration</span><br><span class="line">0.0</span><br><span class="line">0.2</span><br><span class="line">0.4</span><br><span class="line">0.6</span><br><span class="line">0.8</span><br><span class="line">1.0</span><br><span class="line">Accuracy</span><br><span class="line">0</span><br><span class="line">0.2</span><br><span class="line">0.4</span><br><span class="line">0.6</span><br><span class="line">0.8</span><br><span class="line">1</span><br><span class="line">Average Stated Confidence</span><br><span class="line">Another way to measure calibration is to ask the language model the same question 100 times. Since language models may produce different answers upon repeated attempts, we can assess whether the frequency of a particular answer corresponds to its correctness. Higher frequency typically indicates that the model is more confident in its answers, as the model is giving the same answer repeatedly. A well-calibrated model would have the same actual accuracy as frequency.</span><br><span class="line"></span><br><span class="line">In the plot below, we show the calibration of language models as measured by the frequency of their responses. Here, we simply use string match to group together different answers from the language model. We see across all models that accuracy increases with frequency, and that o1-preview has the highest level of calibration, where the frequency of the response is roughly equivalent to the accuracy of the response. Similar to calibration via stated confidence plot above, we again see o1-preview is more calibrated than o1-mini, and gpt4o is more calibrated than o1-mini.</span><br><span class="line"></span><br><span class="line">Accuracy vs Consistency - String Match (Quantile, n=30)</span><br><span class="line">GPT-4o</span><br><span class="line">GPT-4o-mini</span><br><span class="line">o1-preview</span><br><span class="line">o1-mini</span><br><span class="line">Perfect Calibration</span><br><span class="line">0.0</span><br><span class="line">0.2</span><br><span class="line">0.4</span><br><span class="line">0.6</span><br><span class="line">0.8</span><br><span class="line">1.0</span><br><span class="line">Accuracy</span><br><span class="line">0.2</span><br><span class="line">0.4</span><br><span class="line">0.6</span><br><span class="line">0.8</span><br><span class="line">1</span><br><span class="line">Frequency of answer</span><br><span class="line">Conclusions</span><br><span class="line">SimpleQA is a simple but challenging benchmark for evaluating the factuality of frontier models. A main limitation in SimpleQA is its scope—while SimpleQA is accurate it only measures factuality under the constrained setting of short, fact-seeking queries with a single, verifiable answer. Whether the ability to provide factual short answers correlates with the ability to write lengthy responses filled with numerous facts remains an open research question. We hope that open-sourcing SimpleQA drives the research on more trustworthy and reliable AI forward, and we invite researchers to evaluate the factuality of language models with it, and to provide us feedback.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://stability.ai/news/introducing-stable-diffusion-3-5</span><br><span class="line">Stability AI</span><br><span class="line">Introducing Stable Diffusion 3.5</span><br><span class="line">Updated October 29th with release of Stable Diffusion 3.5 Medium</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 Medium is here! Whether a startup or creator, access to this technology shouldn’t be restricted by hardware limitations. With 2.5 billion parameters, this model is designed to run “out of the box” on consumer hardware.</span><br><span class="line">This model delivers best-in-class image generation for its size, with advanced multi-resolution capabilities. It surpasses other medium-sized models with its prompt adherence and image quality, making it a top choice for efficient, high-quality performance.</span><br><span class="line">This model is available for both commercial and non-commercial use under the Stability AI Community License.</span><br><span class="line"></span><br><span class="line">Key Takeaways:</span><br><span class="line"></span><br><span class="line">Today we are introducing Stable Diffusion 3.5. This open release includes multiple model variants, including Stable Diffusion 3.5 Large and Stable Diffusion 3.5 Large Turbo, and as of October 29th, Stable Diffusion 3.5 Medium.</span><br><span class="line"></span><br><span class="line">These models are highly customizable for their size, run on consumer hardware, and are free for both commercial and non-commercial use under the permissive Stability AI Community License.</span><br><span class="line"></span><br><span class="line">You can download all Stable Diffusion 3.5 models from Hugging Face and the inference code on GitHub now.</span><br><span class="line"></span><br><span class="line">View fullsize</span><br><span class="line"></span><br><span class="line">Today we are releasing Stable Diffusion 3.5, our most powerful models yet. This open release includes multiple variants that are customizable, run on consumer hardware, and are available for use under the permissive Stability AI Community License. You can download Stable Diffusion 3.5 Large and Stable Diffusion 3.5 Large Turbo models from Hugging Face and the inference code on GitHub now.</span><br><span class="line"></span><br><span class="line">In June, we released Stable Diffusion 3 Medium, the first open release from the Stable Diffusion 3 series. This release didn&#x27;t fully meet our standards or our communities’ expectations. After listening to the valuable community feedback, instead of a quick fix, we took the time to further develop a version that advances our mission to transform visual media.</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 reflects our commitment to empower builders and creators with tools that are widely accessible, cutting-edge, and free for most use cases. We encourage the distribution and monetization of work across the entire pipeline - whether it&#x27;s fine-tuning, LoRA, optimizations, applications, or artwork.</span><br><span class="line"></span><br><span class="line">What’s being released</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 offers a variety of models developed to meet the needs of scientific researchers, hobbyists, startups, and enterprises alike:</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 Large: At 8.1 billion parameters, with superior quality and prompt adherence, this base model is the most powerful in the Stable Diffusion family. This model is ideal for professional use cases at 1 megapixel resolution.</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 Large Turbo: A distilled version of Stable Diffusion 3.5 Large generates high-quality images with exceptional prompt adherence in just 4 steps, making it considerably faster than Stable Diffusion 3.5 Large.</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 Medium: At 2.5 billion parameters, with improved MMDiT-X architecture and training methods, this model is designed to run “out of the box” on consumer hardware, striking a balance between quality and ease of customization. It is capable of generating images ranging between 0.25 and 2 megapixel resolution.</span><br><span class="line"></span><br><span class="line">Developing the models</span><br><span class="line"></span><br><span class="line">In developing the models, we prioritized customizability to offer a flexible base to build upon. To achieve this, we integrated Query-Key Normalization into the transformer blocks, stabilizing the model training process and simplifying further fine-tuning and development.</span><br><span class="line"></span><br><span class="line">To support this level of downstream flexibility, we had to make some trade-offs. Greater variation in outputs from the same prompt with different seeds may occur, which is intentional as it helps preserve a broader knowledge-base and diverse styles in the base models. However, as a result, prompts lacking specificity might lead to increased uncertainty in the output, and the aesthetic level may vary.</span><br><span class="line"></span><br><span class="line">For the Medium model specifically, we made several adjustments to the architecture and training protocols to enhance quality, coherence, and multi-resolution generation abilities.</span><br><span class="line"></span><br><span class="line">Where the models excel</span><br><span class="line"></span><br><span class="line">The Stable Diffusion 3.5 version excels in the following areas, making it one of the most customizable and accessible image models on the market, while maintaining top-tier performance in prompt adherence and image quality:</span><br><span class="line"></span><br><span class="line">Customizability: Easily fine-tune the model to meet your specific creative needs, or build applications based on customized workflows.</span><br><span class="line"></span><br><span class="line">Efficient Performance: Optimized to run on standard consumer hardware without heavy demands, especially the Stable Diffusion 3.5 Medium and Stable Diffusion 3.5 Large Turbo models.</span><br><span class="line"></span><br><span class="line">We took a look at the hardware compatibility for running Stable Diffusion 3.5 Medium alongside other open-image base models. This model only requires 9.9 GB of VRAM (excluding text encoders) to unlock its full performance, making it highly accessible and compatible with most consumer GPUs.</span><br><span class="line"></span><br><span class="line">View fullsize</span><br><span class="line"></span><br><span class="line">Diverse Outputs: Creates images representative of the world, not just one type of person, with different skin tones and features, without the need for extensive prompting.</span><br><span class="line"></span><br><span class="line">View fullsize</span><br><span class="line"></span><br><span class="line">Versatile Styles: Capable of generating a wide range of styles and aesthetics like 3D, photography, painting, line art, and virtually any visual style imaginable.</span><br><span class="line"></span><br><span class="line">View fullsize</span><br><span class="line"></span><br><span class="line">Additionally, our analysis shows that Stable Diffusion 3.5 Large leads the market in prompt adherence and rivals much larger models in image quality.</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 Large Turbo offers some of the fastest inference times for its size, while remaining highly competitive in both image quality and prompt adherence, even when compared to non-distilled models of similar size</span><br><span class="line"></span><br><span class="line">Stable Diffusion 3.5 Medium outperforms other medium-sized models, offering a balance of prompt adherence and image quality, making it a top choice for efficient, high-quality performance.</span><br><span class="line"></span><br><span class="line">View fullsize</span><br><span class="line"></span><br><span class="line">View fullsize</span><br><span class="line"></span><br><span class="line">The Stability AI Community license at a glance</span><br><span class="line"></span><br><span class="line">We are pleased to release this model under our permissive community license. Here are the key components of the license:</span><br><span class="line"></span><br><span class="line">Free for non-commercial use: Individuals and organizations can use the model free of charge for non-commercial use, including scientific research.</span><br><span class="line"></span><br><span class="line">Free for commercial use (up to $1M in annual revenue): Startups, small to medium-sized businesses, and creators can use the model for commercial purposes at no cost, as long as their total annual revenue is less than $1M.</span><br><span class="line"></span><br><span class="line">Ownership of outputs: Retain ownership of the media generated without restrictive licensing implications.</span><br><span class="line"></span><br><span class="line">For organizations with annual revenue more than $1M, please contact us here to inquire about an Enterprise License.</span><br><span class="line"></span><br><span class="line">More ways to access the models</span><br><span class="line"></span><br><span class="line">While the model weights are available on Hugging Face now for self-hosting, you can also access the model through the following platforms:</span><br><span class="line"></span><br><span class="line">Stability AI API</span><br><span class="line"></span><br><span class="line">Replicate</span><br><span class="line"></span><br><span class="line">DeepInfra</span><br><span class="line"></span><br><span class="line">ComfyUI</span><br><span class="line"></span><br><span class="line">Our commitment to safety</span><br><span class="line"></span><br><span class="line">We believe in safe, responsible AI practices and take deliberate measures to ensure Integrity starts at the early stages of development. This means we have taken and continue to take reasonable steps to prevent the misuse of Stable Diffusion 3.5 by bad actors. For more information about our approach to Safety please visit our Stable Safety page.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://developers.googleblog.com/en/gemini-api-and-ai-studio-now-offer-grounding-with-google-search/</span><br><span class="line">Google</span><br><span class="line">Gemini API and Google AI Studio now offer Grounding with Google Search</span><br><span class="line">OCT 31, 2024</span><br><span class="line">Shrestha Basu Mallick</span><br><span class="line">Group Product Manager</span><br><span class="line">Gemini API</span><br><span class="line">Logan Kilpatrick</span><br><span class="line">Senior Product Manager</span><br><span class="line">Gemini API and Google AI Studio</span><br><span class="line"></span><br><span class="line">Share</span><br><span class="line">GeminiGrounding_Banner</span><br><span class="line">Today, we are rolling out Grounding with Google Search in Google AI Studio and the Gemini API, enabling developers to get more accurate and fresh responses from the Gemini models aided by Google Search. In addition to more accurate responses, the model returns grounding sources (in-line supporting links) and Search Suggestions that point users to the search results corresponding to the grounded response.</span><br><span class="line"></span><br><span class="line">Model response with Grounding sources and Search Suggestions when Grounding with Google Search is turned on</span><br><span class="line">Grounding with Google Search is supported with all generally available versions of Gemini 1.5 models. Developers can turn it on in Google AI Studio under the “Tools” section or in the API by enabling the &#x27;google_search_retrieval&#x27; tool. Grounding is available to test for free in Google AI Studio. In the API, developers can access the tool with the paid tier for $35 per 1,000 grounded queries.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">When should developers use grounding?</span><br><span class="line">Developers should enable Grounding with Google Search for queries and applications which could benefit from any of the following:</span><br><span class="line"></span><br><span class="line">Reduced hallucinations: Grounding helps ensure that AI applications provide users with more factual information.</span><br><span class="line">More up-to-date information: With grounding, models can access real-time information, making AI applications relevant and applicable to a wider range of scenarios.</span><br><span class="line">Enhanced trustworthiness and traffic to publishers: By providing supporting links, grounding brings transparency to AI applications, making them more trustworthy and encouraging users to click on the underlying sources to find out more.</span><br><span class="line">Richer information: By drawing information from Google Search to enhance the model response, grounding is able to provide richer color on many queries.</span><br><span class="line"></span><br><span class="line">Grounding with Google Search in action</span><br><span class="line">We show a couple of examples below, using AI Studio’s new Compare Mode, where the model response benefits from Grounding with Google Search. In the first example, the model provides an out of date answer based on its knowledge cut-off (on the left) but answers more accurately based on the latest available sources (on the right) when grounding is turned on.</span><br><span class="line"></span><br><span class="line">Model response in Google AI Studio compare mode</span><br><span class="line">Model response in Google AI Studio compare mode, without grounding (left) and with grounding (right)</span><br><span class="line">In this example, without grounding enabled (on the left), the model intentionally presents a minimal response by default. With grounding (on the right), the model comes back with a richer response including supporting links.</span><br><span class="line"></span><br><span class="line">Richer response by the latest Gemini 1.5 Flash model</span><br><span class="line">Richer response by the latest Gemini 1.5 Flash model using Grounding with Google Search (right)</span><br><span class="line">How does Grounding with Google Search work?</span><br><span class="line">When a user makes a query with grounding turned on, the service uses Google’s search engine to find up-to-date and comprehensive information that is relevant to the query, and sends it to the model. The model then responds with higher accuracy and freshness, providing in-line grounding sources (supporting links) and Search Suggestions.</span><br><span class="line"></span><br><span class="line">import google.generativeai as genai</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">genai.configure(api_key=os.environ[&quot;API_KEY&quot;])</span><br><span class="line">model = genai.GenerativeModel(&#x27;models/gemini-1.5-flash-002&#x27;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">response = model.generate_content(contents=&quot;Who won Wimbledon this year?&quot;,</span><br><span class="line">                                  tools=&#x27;google_search_retrieval&#x27;)</span><br><span class="line"></span><br><span class="line">print(response)</span><br><span class="line"># Response contains `groundingMetadata` with grounding sources, confidence scores, and search suggestions</span><br><span class="line">Refer to the documentation for complete code.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Even when Grounding with Google Search is on, not every query in a session necessarily requires grounding, which results in additional cost and latency. This is where developers have a second layer of control with dynamic retrieval.</span><br><span class="line"></span><br><span class="line">When developers request a grounded answer, the dynamic retrieval configuration assigns the prompt a prediction score, which is a floating point value between 0 and 1. The value is higher when a prompt is more likely to benefit from grounding. In their requests, developers can set a threshold for what scores should result in grounding (the default threshold value is 0.3). Developers should test various options for the threshold value to see what best works for their applications.</span><br><span class="line"></span><br><span class="line">Dynamic retrieval for Grounding Search in Google AI Studio</span><br><span class="line">Dynamic retrieval for Grounding with Google Search in Google AI Studio</span><br><span class="line">By using Google’s search results to ground Gemini-based applications, developers can provide their users with more accurate, relevant, and trustworthy information. Refer to our documentation for detailed code examples and step-by-step instructions.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">We look forward to your feedback and are excited to see what you build with this new capability!</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/gpt-omni/mini-omni2</span><br><span class="line">10/25/24</span><br><span class="line">Inspirai, Tsinghua University</span><br><span class="line"></span><br><span class="line">Mini-Omni 2 understands image, audio and text inputs all via end-to-end voice conversations with users 🔥</span><br><span class="line">&gt; Understands and processes images, speech, and text</span><br><span class="line">&gt; Generates real-time speech responses</span><br><span class="line">&gt; Supports interruptions during speech</span><br><span class="line">Technical Overview:</span><br><span class="line">&gt; Concatenates image, audio, and text features for input.</span><br><span class="line">&gt; Uses text-guided delayed parallel output for real-time speech</span><br><span class="line">&gt; Involves encoder adaptation, modal alignment, and multimodal fine-tuning</span><br><span class="line">Best part: MIT licensed ⚡</span><br><span class="line"></span><br><span class="line">Zhifei Xie, Changqiao Wu</span><br><span class="line">GPT-4o, an all-encompassing model, represents a milestone in the development of large multi-modal language models. It can understand visual, auditory, and textual modalities, directly output audio, and support flexible duplex interaction. Models from the open-source community often achieve some functionalities of GPT-4o, such as visual understanding and voice chat. Nevertheless, training a unified model that incorporates all modalities is challenging due to the complexities of multi-modal data, intricate model architectures, and training processes. In this paper, we introduce Mini-Omni2, a visual-audio assistant capable of providing real-time, end-to-end voice responses to visoin and audio queries. By integrating pretrained visual and auditory encoders, Mini-Omni2 maintains performance in individual modalities. We propose a three-stage training process to align modalities, allowing the language model to handle multi-modal inputs and outputs after training on a limited dataset. For interaction, we introduce a command-based interruption mechanism, enabling more flexible interaction with users. To the best of our knowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have similar form of functionality, and we hope it can offer valuable insights for subsequent research.</span><br><span class="line">Mini-Omni2 is an omni-interactive model. It can understand image, audio and text inputs and has end-to-end voice conversations with users. Featuring real-time voice output, omni-capable multimodal understanding and flexible interaction ability with interruption mechanism while speaking.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Updates</span><br><span class="line">2024.10: Release the model, technical report, inference and chat demo code.</span><br><span class="line">Features</span><br><span class="line">✅ Multimodal interaction: with the ability to understand images, speech and text, just like GPT-4o.</span><br><span class="line"></span><br><span class="line">✅ Real-time speech-to-speech conversational capabilities. No extra ASR or TTS models required, just like Mini-Omni.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://microsoft.github.io/SynthMoCap/</span><br><span class="line">Microsoft</span><br><span class="line">10/25/24</span><br><span class="line"></span><br><span class="line">Look Ma, no markers</span><br><span class="line">Holistic performance capture without the hassle</span><br><span class="line"></span><br><span class="line">ACM Transactions on Graphics</span><br><span class="line"></span><br><span class="line">SIGGRAPH Asia 2024</span><br><span class="line"></span><br><span class="line">Charlie Hewitt Fatemeh Saleh Sadegh Aliakbarian Lohit Petikam Shideh Rezaeifar Louis Florentin Zafiirah Hosenie Thomas J Cashman Julien Valentin Darren Cosker Tadas Baltrušaitis</span><br><span class="line"></span><br><span class="line"> Microsoft</span><br><span class="line">unveils the first technique for marker-free, HQ reconstruction of COMPLETE human body, including eyes &amp; tongue, without requiring any calibration, manual intervention or custom hardware. Impressive results! Repo for training &amp; Dataset💙</span><br><span class="line"></span><br><span class="line">𝐇𝐢𝐠𝐡𝐥𝐢𝐠𝐡𝐭𝐬:</span><br><span class="line"></span><br><span class="line">✅Novel SOTA holistic 3D human reconstruction</span><br><span class="line"></span><br><span class="line">✅Body shape/pose + face shape/expression</span><br><span class="line"></span><br><span class="line">✅Hand &amp; tongue articulation + eye gaze</span><br><span class="line"></span><br><span class="line">✅Suitable for mono &amp; multi-view scenario</span><br><span class="line"></span><br><span class="line">✅Data pipeline for generating synthetic data</span><br><span class="line"></span><br><span class="line">✅Body, face and hands dataset released!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Abstract</span><br><span class="line">We tackle the problem of highly-accurate, holistic performance capture for the face, body and hands simultaneously. Motion-capture technologies used in film and game production typically focus only on face, body or hand capture independently, involve complex and expensive hardware and a high degree of manual intervention from skilled operators. While machine-learning-based approaches exist to overcome these problems, they usually only support a single camera, often operate on a single part of the body, do not produce precise world-space results, and rarely generalize outside specific contexts. In this work, we introduce the first technique for marker-free, high-quality reconstruction of the complete human body, including eyes and tongue, without requiring any calibration, manual intervention or custom hardware. Our approach produces stable world-space results from arbitrary camera rigs as well as supporting varied capture environments and clothing. We achieve this through a hybrid approach that leverages machine learning models trained exclusively on synthetic data and powerful parametric models of human shape and motion. We evaluate our method on a number of body, face and hand reconstruction benchmarks and demonstrate state-of-the-art results that generalize on diverse datasets.</span><br><span class="line">Holistic Performance Capture</span><br><span class="line">Our approach combines machine-learning models for dense-landmark and parameter prediction with model-fitting to provide a robust, accurate and adaptable system. Our method supports registration of the face, body and hands; in isolation, and together in a single take.</span><br><span class="line"></span><br><span class="line">Our parametric model captures body and hand pose, body and face shape, and facial expression.</span><br><span class="line"></span><br><span class="line">We can also track tongue articulation and eye gaze.</span><br><span class="line"></span><br><span class="line">Our method achieves state-of-the-art results on a number of 3D reconstruction benchmarks.</span><br><span class="line"></span><br><span class="line">No Hassle</span><br><span class="line">Motion capture shoots typically require specialist hardware, skilled experts and a lot of time to get right. This can make them expensive and challenging to manage in a tight production schedule. Our method aims to eliminate this inconvenience by providing a marker-less, calibration-free solution that can be used with off-the-shelf hardware. This allows for quick and easy capture of high-quality motion data in a variety of environments.</span><br><span class="line"></span><br><span class="line">Using just two uncalibrated mobile-phone cameras we can achieve high quality results in world-space.</span><br><span class="line"></span><br><span class="line">Our method even works with a single, moving camera in an unconstrained environment with arbitrary clothing.</span><br><span class="line"></span><br><span class="line">Synthetic Datasets</span><br><span class="line">Our method is trained exclusively on synthetic data, generated using a conventional computer graphics pipeline. The three datasets used in the paper are available to download here.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SynthBody can be used for tasks such as skeletal tracking and body pose prediction.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SynthFace can be used for tasks such as facial landmark and head pose prediction or face parsing.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SynthHand can be used for tasks such as hand pose prediction or landmark regression.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/facebookresearch/MobileLLM</span><br><span class="line">META</span><br><span class="line">10/30/24</span><br><span class="line"></span><br><span class="line">🚨 Meta released MobileLLM - 125M, 350M, 600M, and 1B model checkpoints! 🔥</span><br><span class="line">Notes on the release:</span><br><span class="line">Depth vs. Width: Contrary to the scaling law (Kaplan et al., 2020), depth is more critical than width for small LLMs, enhancing abstract concept capture and final performance</span><br><span class="line">Embedding Sharing: Revisited and implemented embedding sharing methods ( to maximize weight utilization</span><br><span class="line">Grouped Query Attention: Adopted from Ainslie et al. (2023) to optimize attention mechanisms</span><br><span class="line">Immediate Block-wise Weight Sharing: Reduces latency by avoiding weight movement with minimal overhead</span><br><span class="line">Performance:</span><br><span class="line">&gt; Zero-Shot Tasks: MobileLLM outperforms previous SOTA 125M/350M models by 2.7%/4.3%.</span><br><span class="line">&gt; API Calling: Comparable exact-match score to the larger LLaMA-v2 7B model 7B</span><br><span class="line">Models are available on the Hub &amp; integrated with Transformers! 🔥</span><br><span class="line"></span><br><span class="line">MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases</span><br><span class="line">Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas Chandra</span><br><span class="line">This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight-sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a further accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover, MobileLLM model family shows significant improvements compared to previous sub-billion models on chat benchmarks, and demonstrates close correctness to LLaMA-v2 7B in API calling tasks, highlighting the capability of small models for common on-device use cases.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/facebookresearch/lingua</span><br><span class="line">META</span><br><span class="line">10/25/24</span><br><span class="line">Meta Lingua</span><br><span class="line">Mathurin Videau*, Badr Youbi Idrissi*, Daniel Haziza, Luca Wehrstedt, Jade Copet, Olivier Teytaud, David Lopez-Paz. *Equal and main contribution</span><br><span class="line"></span><br><span class="line">Meta Lingua is a minimal and fast LLM training and inference library designed for research. Meta Lingua uses easy-to-modify PyTorch components in order to try new architectures, losses, data, etc. We aim for this code to enable end to end training, inference and evaluation as well as provide tools to better understand speed and stability. While Meta Lingua is currently under development, we provide you with multiple apps to showcase how to use this codebase.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9</span><br><span class="line">Huggingface</span><br><span class="line">11/1/24</span><br><span class="line">Pushing the boundaries of Small LLMs! We just released SmolLM 2, a new set of small, powerful LLMs optimized for on-device, outperforming Meta Llama 3.2 1B! SmolLM comes in three sizes, 0.1B, 0.3B, and 1.7B and under Apache 2.0. 🚀</span><br><span class="line"></span><br><span class="line">TL;DR;</span><br><span class="line">🔢 Comes in 3 sizes with 135M, 360M, and 1.7B parameters.</span><br><span class="line">📚 Trained on 11 trillion mostly English tokens from FineWeb-Edu, DCLM, the Stack…</span><br><span class="line">🚀 Trained for text rewriting, summarization, and function calling (27% on BFCL Leaderboard)</span><br><span class="line">🥇 Matches or outperforms Llama 3.2 1B and Qwen2.5 1B</span><br><span class="line">🏆 56.7 IFEval; 6.13 MT Bench; 19.3 MMLU-Pro; 48.2 GMS8k</span><br><span class="line">🔧 Post-Training with SFT → DPO</span><br><span class="line">📱 Runs on-device with llama.cpp or in browser with Transformers.js</span><br><span class="line">🤗 Available on Hugging Face and under Apache 2.0</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://maskgct.github.io/</span><br><span class="line">10/24/24</span><br><span class="line">MaskGCT - New open SoTA Text to Speech model! 🔥</span><br><span class="line">&gt; Zero-shot voice cloning</span><br><span class="line">&gt; Emotional TTS</span><br><span class="line">&gt; Trained on 100K hours of data</span><br><span class="line">&gt; Long form synthesis</span><br><span class="line">&gt; Variable speed synthesis</span><br><span class="line">&gt; Bilingual - Chinese &amp; English</span><br><span class="line">&gt; Available on Hugging Face</span><br><span class="line"></span><br><span class="line">Fully non-autoregressive architecture:</span><br><span class="line">&gt; Stage 1: Predicts semantic tokens from text, using tokens extracted from a speech self-supervised learning (SSL) model</span><br><span class="line">&gt; Stage 2: Predicts acoustic tokens conditioned on the semantic tokens.</span><br><span class="line"></span><br><span class="line">Synthesised: &quot;Would you guys personally like to have a fake fireplace, an electric one, in your house? Or would you rather have a real fireplace? Let me know down below. Okay everybody, that&#x27;s all for today&#x27;s video and I hope you guys learned a bunch of furniture vocabulary!&quot;</span><br><span class="line"></span><br><span class="line">TTS scene keeps getting lit! 🐐https://maskgct.github.io/</span><br><span class="line">Zero-Shot Text-to-Speech with Masked Generative Codec Transformer</span><br><span class="line">Abstract The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2410.18417</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Submitted on 24 Oct 2024]</span><br><span class="line">Large Language Models Reflect the Ideology of their Creators</span><br><span class="line">Maarten Buyl, Alexander Rogiers, Sander Noels, Iris Dominguez-Catena, Edith Heiter, Raphael Romero, Iman Johary, Alexandru-Cristian Mara, Jefrey Lijffijt, Tijl De Bie</span><br><span class="line">Large language models (LLMs) are trained on vast amounts of data to generate natural language, enabling them to perform tasks like text summarization and question answering. These models have become popular in artificial intelligence (AI) assistants like ChatGPT and already play an influential role in how humans access information. However, the behavior of LLMs varies depending on their design, training, and use.</span><br><span class="line">In this paper, we uncover notable diversity in the ideological stance exhibited across different LLMs and languages in which they are accessed. We do this by prompting a diverse panel of popular LLMs to describe a large number of prominent and controversial personalities from recent world history, both in English and in Chinese. By identifying and analyzing moral assessments reflected in the generated descriptions, we find consistent normative differences between how the same LLM responds in Chinese compared to English. Similarly, we identify normative disagreements between Western and non-Western LLMs about prominent actors in geopolitical conflicts. Furthermore, popularly hypothesized disparities in political goals among Western models are reflected in significant normative differences related to inclusion, social inequality, and political scandals.</span><br><span class="line">Our results show that the ideological stance of an LLM often reflects the worldview of its creators. This raises important concerns around technological and regulatory efforts with the stated aim of making LLMs ideologically `unbiased&#x27;, and it poses risks for political instrumentalization.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>기술적으로 최대한 자세하게 적어. 13개의 기사가 있고 하나도 빼먹지 말고 적어.</p>
</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-10-25</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/10/25/2024-10-25-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 10월 25일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/11/01/2024-11-1-AI-NEWS/" title="2024년 11월 1일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/10/22/2024-10-15-AI-NEWS/" title="2024년 10월 22일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>