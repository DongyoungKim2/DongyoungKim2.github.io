<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 11월 28일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="알리바바에서는 새로운 추론 AI 모델 QwQ-32B-Preview를 출시하여 OpenAI의 o1 모델에 도전하고 있습니다. 이 모델은 특정 벤치마크에서 우수한 성능을 보이며, 수학 및 논리 퍼즐 해결 능력을 향상시켰습니다.
DeepSeek에서는 DeepSeek-R1-L"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 11월 28일 AI 소식</a></h3></div><div class="post-content"><ul>
<li><strong>알리바바</strong>에서는 새로운 추론 AI 모델 <strong>QwQ-32B-Preview</strong>를 출시하여 OpenAI의 o1 모델에 도전하고 있습니다. 이 모델은 특정 벤치마크에서 우수한 성능을 보이며, 수학 및 논리 퍼즐 해결 능력을 향상시켰습니다.</li>
<li><strong>DeepSeek</strong>에서는 <strong>DeepSeek-R1-Lite-Preview</strong>를 발표하여 AIME 및 MATH 벤치마크에서 OpenAI의 o1-preview 수준의 성능을 달성했습니다. 이 모델은 실시간으로 투명한 사고 과정을 보여주며, 곧 오픈 소스 모델과 API를 출시할 예정입니다.</li>
<li><strong>NVIDIA</strong>는 <strong>Hymba</strong>라는 하이브리드 헤드 아키텍처를 적용한 소형 언어 모델을 발표하여 성능과 효율성을 향상시켰습니다. <strong>Hymba 1.5B</strong> 모델은 유사한 크기의 오픈 소스 모델보다 우수한 성능을 보이며, 상용 사용이 가능합니다.</li>
<li><strong>Hugging Face</strong>에서는 <strong>SmolVLM</strong>이라는 소형이지만 강력한 비전 언어 모델을 공개하였습니다. 이 모델은 메모리 효율성이 뛰어나고 완전한 오픈 소스로 제공되며, 다양한 응용 프로그램에서 사용할 수 있습니다.</li>
<li><strong>Blackforest Labs</strong>는 <strong>FLUX.1 Tools</strong>를 출시하여 FLUX.1 텍스트-이미지 모델에 제어 기능을 추가하였습니다. 이 도구는 실세계 이미지와 생성된 이미지를 수정 및 재생성할 수 있게 해주며, 네 가지 모델이 공개되었습니다.</li>
<li><strong>Apple</strong>에서는 빠른 <strong>CoreML 모델</strong>과 이를 iPhone에서 실행할 수 있는 iOS 앱을 발표하였습니다. 이 모델들은 높은 성능과 효율성을 제공하며, Hugging Face에서 다운로드할 수 있고 GitHub에서 코드를 확인할 수 있습니다.</li>
<li><strong>Alex Garcia</strong>는 <strong>sqlite-vec</strong>의 새로운 업데이트를 발표하여 vec0 가상 테이블에서 메타데이터 열과 필터링을 지원하게 되었습니다. 이를 통해 SQLite 또는 LibSQL 데이터베이스에서의 벡터 검색이 더욱 향상되었습니다.</li>
<li><strong>연구팀</strong>은 <strong>XGrammar</strong>라는 대형 언어 모델을 위한 유연하고 효율적인 구조화 생성 엔진을 제안하였습니다. 이 엔진은 기존 솔루션보다 최대 100배 빠른 속도를 제공하며, 구조화된 생성을 거의 지연 없이 수행할 수 있습니다.</li>
<li><strong>.txt 팀</strong>에서는 ‘Let Me Speak Freely’ 논문에 대한 반박을 통해 구조화된 출력이 올바르게 구현되면 LLM 성능을 실제로 향상시킬 수 있음을 보여주었습니다. 이는 프롬프트, 파서, 생성기의 일치를 통해 가능하다고 강조하였습니다.</li>
</ul>
<h3 id="알리바바-QwQ-Reflect-Deeply-on-the-Boundaries-of-the-Unknown"><a href="#알리바바-QwQ-Reflect-Deeply-on-the-Boundaries-of-the-Unknown" class="headerlink" title="알리바바, QwQ: Reflect Deeply on the Boundaries of the Unknown"></a>알리바바, QwQ: Reflect Deeply on the Boundaries of the Unknown</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/Qwen/QwQ-32B-Preview">링크</a>, 2024년 11월 28일</p>
<ul>
<li>새로운 추론 AI 모델 <strong>QwQ-32B-Preview</strong> 출시</li>
<li>325억 개의 파라미터를 포함하고 약 32,000 단어의 프롬프트를 처리 가능</li>
<li>OpenAI의 o1-preview 및 o1-mini보다 특정 벤치마크에서 우수한 성능</li>
<li><strong>AIME</strong>와 <strong>MATH</strong> 테스트에서 OpenAI의 o1 모델을 능가</li>
<li>논리 퍼즐 해결 및 도전적인 수학 질문 해결 가능</li>
<li>언어 전환 및 루프에 빠지는 현상 등의 한계점 존재</li>
<li>모델 자체의 사실 확인을 통해 잘못된 답변 감소, 그러나 답변 시간이 길어질 수 있음</li>
<li><strong>Hugging Face</strong> 플랫폼에서 다운로드 가능하며 상업적 사용 가능</li>
<li><strong>Apache 2.0</strong> 라이선스 하에 공개</li>
<li>모델의 일부 구성 요소만 공개되어 전체 시스템의 재현이나 심층적인 이해는 어려움</li>
</ul>
<h3 id="DeepSeek-NewsDeepSeek-R1-Lite-Release-2024-11-20"><a href="#DeepSeek-NewsDeepSeek-R1-Lite-Release-2024-11-20" class="headerlink" title="DeepSeek, NewsDeepSeek-R1-Lite Release 2024&#x2F;11&#x2F;20"></a>DeepSeek, NewsDeepSeek-R1-Lite Release 2024&#x2F;11&#x2F;20</h3><p><a target="_blank" rel="noopener" href="https://api-docs.deepseek.com/news/news1120">링크</a>, 2024년 11월 20일</p>
<ul>
<li><strong>DeepSeek-R1-Lite-Preview</strong> 출시, 향상된 추론 능력 제공</li>
<li><strong>AIME</strong> 및 <strong>MATH</strong> 벤치마크에서 OpenAI의 o1-preview 수준의 성능 달성</li>
<li>실시간으로 투명한 사고 과정 제공</li>
<li>곧 오픈 소스 모델과 API 출시 예정</li>
<li>벤치마크 전반에서 인상적인 결과 달성</li>
<li>더 긴 사고 과정을 통해 더 나은 성능 달성</li>
<li>추론 길이가 증가함에 따라 AIME에서 점수가 꾸준히 향상</li>
</ul>
<h3 id="NVIDIA-Hymba-Hybrid-Head-Architecture-Boosts-Small-Language-Model-Performance"><a href="#NVIDIA-Hymba-Hybrid-Head-Architecture-Boosts-Small-Language-Model-Performance" class="headerlink" title="NVIDIA, Hymba Hybrid-Head Architecture Boosts Small Language Model Performance"></a>NVIDIA, Hymba Hybrid-Head Architecture Boosts Small Language Model Performance</h3><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/hymba-hybrid-head-architecture-boosts-small-language-model-performance/">링크</a>, 2024년 11월 22일</p>
<ul>
<li>NVIDIA 연구팀이 <strong>Hymba</strong>라는 하이브리드 헤드 아키텍처를 도입한 소형 언어 모델 발표</li>
<li>Transformer의 어텐션 메커니즘과 SSM을 통합하여 효율성과 성능 향상</li>
<li><strong>Hymba 1.5B</strong> 모델이 유사한 크기의 오픈 소스 모델보다 우수한 성능 달성</li>
<li>더 높은 처리량과 적은 메모리 요구사항 달성</li>
<li><strong>Hymba-1.5B-Base</strong>는 상업적 사용 준비 완료</li>
<li><strong>NVIDIA Open Model License Agreement</strong> 하에 출시</li>
<li>다양한 벤치마크에서 뛰어난 성능을 입증</li>
<li>어텐션 맵 시각화를 통해 모델의 작동 방식 분석</li>
<li>구조적 재매개변수를 통한 모델 효율성 개선</li>
</ul>
<h3 id="Hugging-Face-SmolVLM-small-yet-mighty-Vision-Language-Model"><a href="#Hugging-Face-SmolVLM-small-yet-mighty-Vision-Language-Model" class="headerlink" title="Hugging Face, SmolVLM - small yet mighty Vision Language Model"></a>Hugging Face, SmolVLM - small yet mighty Vision Language Model</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct">링크</a>, 2024년 11월 26일</p>
<ul>
<li><strong>SmolVLM</strong>이라는 20억 규모의 비전 언어 모델 공개</li>
<li>작은 메모리 공간에서도 강력한 성능을 발휘하며, 완전한 오픈 소스로 제공</li>
<li>모델 체크포인트, 데이터셋, 훈련 레시피 및 도구를 <strong>Apache 2.0</strong> 라이선스로 공개</li>
<li>다양한 멀티모달 작업에서 우수한 성능을 보임</li>
<li>메모리 사용량과 처리량 측면에서 효율성 제공</li>
<li>비디오 이해 기능도 제공하며, 제한된 계산 자원에서도 활용 가능</li>
<li><strong>VLMEvalKit</strong>와의 통합을 통해 추가 벤치마크 평가 가능</li>
<li><strong>transformers</strong> 라이브러리를 통해 쉽게 모델 사용 가능</li>
<li>사용자 맞춤형 미세 조정을 위한 튜토리얼 및 도구 제공</li>
</ul>
<h3 id="Blackforest-Labs-Flux1-Tools-Hugging-Face-Transformers"><a href="#Blackforest-Labs-Flux1-Tools-Hugging-Face-Transformers" class="headerlink" title="Blackforest Labs, Flux1 Tools: Hugging Face Transformers"></a>Blackforest Labs, Flux1 Tools: Hugging Face Transformers</h3><p><a target="_blank" rel="noopener" href="https://blackforestlabs.ai/flux-1-tools/">링크</a>, 2024년 11월 21일</p>
<ul>
<li><strong>FLUX.1 Tools</strong> 출시, FLUX.1 텍스트-이미지 모델에 제어 및 조정 기능 추가</li>
<li>실제 및 생성된 이미지를 수정 및 재생성할 수 있는 도구 제공</li>
<li>네 가지 오픈 액세스 모델 공개: <strong>FLUX.1 Fill</strong>, <strong>FLUX.1 Depth</strong>, <strong>FLUX.1 Canny</strong>, <strong>FLUX.1 Redux</strong></li>
<li>각 모델은 고유한 기능과 능력을 제공</li>
<li><strong>FLUX.1 Fill</strong>은 최신 인페인팅 및 아웃페인팅 모델 제공</li>
<li><strong>FLUX.1 Depth</strong>와 <strong>Canny</strong>는 구조적 가이드를 통해 이미지 변환 제어 가능</li>
<li><strong>FLUX.1 Redux</strong>는 입력 이미지와 텍스트 프롬프트를 혼합하여 새로운 이미지를 생성</li>
<li>벤치마크 결과, <strong>FLUX.1</strong> 시리즈가 기존 모델들보다 우수한 성능을 보임</li>
<li><strong>BFL API</strong>를 통해 프로 버전 모델에 접근 가능</li>
</ul>
<h3 id="Apple-Core-ML-Models"><a href="#Apple-Core-ML-Models" class="headerlink" title="Apple, Core ML Models"></a>Apple, Core ML Models</h3><p><a target="_blank" rel="noopener" href="https://developer.apple.com/machine-learning/models/">링크</a>, 2024년 11월 22일</p>
<ul>
<li>Apple이 빠른 <strong>CoreML 모델</strong>과 이를 iPhone에서 실행할 수 있는 iOS 앱 발표</li>
<li><strong>S0</strong>, <strong>S2</strong>, <strong>MobileCLIP-B(LT)</strong> 등 고성능 모델 출시</li>
<li><strong>S0</strong>는 OpenAI의 ViT-B&#x2F;16과 동일한 제로샷 성능을 보이면서 4.8배 빠르고 2.8배 작음</li>
<li><strong>S2</strong>는 SigLIP의 ViT-B&#x2F;16보다 우수한 성능을 보이며, 더 빠르고 작음</li>
<li>모델은 <strong>Hugging Face</strong>에서 다운로드 가능하고, 코드도 <strong>GitHub</strong>에서 확인 가능</li>
<li>다양한 컴퓨터 비전 작업에서 사용 가능</li>
<li><strong>FastViT</strong>, <strong>Depth Anything V2</strong> 등 다양한 모델 제공</li>
<li>각 모델에 대한 상세한 정보와 예제 프로젝트 제공</li>
</ul>
<h3 id="Alex-Garcia-sqlite-vec-now-supports-metadata-columns-and-filtering"><a href="#Alex-Garcia-sqlite-vec-now-supports-metadata-columns-and-filtering" class="headerlink" title="Alex Garcia, sqlite-vec now supports metadata columns and filtering"></a>Alex Garcia, sqlite-vec now supports metadata columns and filtering</h3><p><a target="_blank" rel="noopener" href="https://alexgarcia.xyz/blog/2024/sqlite-vec-metadata-release/index.html">링크</a>, 2024년 11월 20일</p>
<ul>
<li><strong>sqlite-vec</strong>의 새로운 업데이트 버전 0.1.6 발표</li>
<li><strong>vec0</strong> 가상 테이블에서 메타데이터 열과 필터링 지원</li>
<li>메타데이터 컬럼을 통해 벡터 검색에서 조건부 필터링 가능</li>
<li><strong>파티션 키</strong> 도입으로 벡터 인덱스를 샤딩하여 쿼리 속도 향상</li>
<li>보조 열을 사용하여 별도의 조인 없이 메타데이터 저장 및 접근 가능</li>
<li>향후 양자화 지원 개선 계획</li>
<li>ANN 인덱스 지원을 통해 성능 향상 예정</li>
<li><strong>sqlite-lembed</strong> 및 <strong>sqlite-rembed</strong> 등의 관련 프로젝트도 업데이트 예정</li>
</ul>
<h3 id="연구팀-XGrammar-Flexible-and-Efficient-Structured-Generation-Engine-for-Large-Language-Models"><a href="#연구팀-XGrammar-Flexible-and-Efficient-Structured-Generation-Engine-for-Large-Language-Models" class="headerlink" title="연구팀, XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models"></a>연구팀, XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.15100">링크</a>, 2024년 11월 22일</p>
<ul>
<li><strong>XGrammar</strong>라는 유연하고 효율적인 구조화 생성 엔진 제안</li>
<li>문맥 자유 문법을 사용하여 구조화된 생성을 가능하게 함</li>
<li>기존 솔루션보다 최대 100배의 속도 향상 달성</li>
<li>LLM 추론 엔진과 결합하여 거의 지연 없는 구조화 생성 가능</li>
<li><strong>MLC-LLM</strong>, <strong>SGLang</strong>, <strong>WebLLM</strong> 등에 적용됨</li>
<li>곧 <strong>vLLM</strong> 및 <strong>HuggingFace</strong>에서도 지원 예정</li>
<li>정확한 JSON&#x2F;문법 생성 가능</li>
<li>기술 보고서에서 상세한 기술 및 성능 데이터 제공</li>
</ul>
<h3 id="txt-팀-Say-What-You-Mean-A-Response-to-‘Let-Me-Speak-Freely’"><a href="#txt-팀-Say-What-You-Mean-A-Response-to-‘Let-Me-Speak-Freely’" class="headerlink" title=".txt 팀, Say What You Mean: A Response to ‘Let Me Speak Freely’"></a>.txt 팀, Say What You Mean: A Response to ‘Let Me Speak Freely’</h3><p><a target="_blank" rel="noopener" href="https://blog.dottxt.co/say-what-you-mean.html">링크</a>, 2024년 11월 24일</p>
<ul>
<li>‘Let Me Speak Freely’ 논문에 대한 반박을 통해 구조화된 출력이 LLM 성능을 향상시킬 수 있음을 증명</li>
<li>원 논문의 결과는 약한 프롬프트와 구조화된 프롬프트의 잘못된 사용으로 인한 것임을 지적</li>
<li>정확한 프롬프트 설계와 파서, 생성기의 일치가 중요함을 강조</li>
<li>구조화된 출력이 테스트에서 비구조화된 출력보다 우수한 성능을 보임</li>
<li>JSON 생성을 위한 프롬프트 설계의 중요성 강조</li>
<li>예시에서 기대하는 형식과 실제 작업에서의 형식이 일치해야 함</li>
<li>구조화된 생성은 적절하게 구현되면 성능 향상에 기여함을 강조</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br><span class="line">936</span><br><span class="line">937</span><br><span class="line">938</span><br><span class="line">939</span><br><span class="line">940</span><br><span class="line">941</span><br><span class="line">942</span><br><span class="line">943</span><br><span class="line">944</span><br><span class="line">945</span><br><span class="line">946</span><br><span class="line">947</span><br><span class="line">948</span><br><span class="line">949</span><br><span class="line">950</span><br><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br><span class="line">954</span><br><span class="line">955</span><br><span class="line">956</span><br><span class="line">957</span><br><span class="line">958</span><br><span class="line">959</span><br><span class="line">960</span><br><span class="line">961</span><br><span class="line">962</span><br><span class="line">963</span><br><span class="line">964</span><br><span class="line">965</span><br><span class="line">966</span><br><span class="line">967</span><br><span class="line">968</span><br><span class="line">969</span><br><span class="line">970</span><br><span class="line">971</span><br><span class="line">972</span><br><span class="line">973</span><br><span class="line">974</span><br><span class="line">975</span><br><span class="line">976</span><br><span class="line">977</span><br><span class="line">978</span><br><span class="line">979</span><br><span class="line">980</span><br><span class="line">981</span><br><span class="line">982</span><br><span class="line">983</span><br><span class="line">984</span><br><span class="line">985</span><br><span class="line">986</span><br><span class="line">987</span><br><span class="line">988</span><br><span class="line">989</span><br><span class="line">990</span><br><span class="line">991</span><br><span class="line">992</span><br><span class="line">993</span><br><span class="line">994</span><br><span class="line">995</span><br><span class="line">996</span><br><span class="line">997</span><br><span class="line">998</span><br><span class="line">999</span><br><span class="line">1000</span><br><span class="line">1001</span><br><span class="line">1002</span><br><span class="line">1003</span><br><span class="line">1004</span><br><span class="line">1005</span><br><span class="line">1006</span><br><span class="line">1007</span><br><span class="line">1008</span><br><span class="line">1009</span><br><span class="line">1010</span><br><span class="line">1011</span><br><span class="line">1012</span><br><span class="line">1013</span><br><span class="line">1014</span><br><span class="line">1015</span><br><span class="line">1016</span><br><span class="line">1017</span><br><span class="line">1018</span><br><span class="line">1019</span><br><span class="line">1020</span><br><span class="line">1021</span><br><span class="line">1022</span><br><span class="line">1023</span><br><span class="line">1024</span><br><span class="line">1025</span><br><span class="line">1026</span><br><span class="line">1027</span><br><span class="line">1028</span><br><span class="line">1029</span><br><span class="line">1030</span><br><span class="line">1031</span><br><span class="line">1032</span><br><span class="line">1033</span><br><span class="line">1034</span><br><span class="line">1035</span><br><span class="line">1036</span><br><span class="line">1037</span><br><span class="line">1038</span><br><span class="line">1039</span><br><span class="line">1040</span><br><span class="line">1041</span><br><span class="line">1042</span><br><span class="line">1043</span><br><span class="line">1044</span><br><span class="line">1045</span><br><span class="line">1046</span><br><span class="line">1047</span><br><span class="line">1048</span><br><span class="line">1049</span><br><span class="line">1050</span><br><span class="line">1051</span><br><span class="line">1052</span><br><span class="line">1053</span><br><span class="line">1054</span><br><span class="line">1055</span><br><span class="line">1056</span><br><span class="line">1057</span><br><span class="line">1058</span><br><span class="line">1059</span><br><span class="line">1060</span><br><span class="line">1061</span><br><span class="line">1062</span><br><span class="line">1063</span><br><span class="line">1064</span><br><span class="line">1065</span><br><span class="line">1066</span><br><span class="line">1067</span><br><span class="line">1068</span><br><span class="line">1069</span><br><span class="line">1070</span><br><span class="line">1071</span><br><span class="line">1072</span><br><span class="line">1073</span><br><span class="line">1074</span><br><span class="line">1075</span><br><span class="line">1076</span><br><span class="line">1077</span><br><span class="line">1078</span><br><span class="line">1079</span><br><span class="line">1080</span><br><span class="line">1081</span><br><span class="line">1082</span><br><span class="line">1083</span><br><span class="line">1084</span><br><span class="line">1085</span><br><span class="line">1086</span><br><span class="line">1087</span><br><span class="line">1088</span><br><span class="line">1089</span><br><span class="line">1090</span><br><span class="line">1091</span><br><span class="line">1092</span><br><span class="line">1093</span><br><span class="line">1094</span><br><span class="line">1095</span><br><span class="line">1096</span><br><span class="line">1097</span><br><span class="line">1098</span><br><span class="line">1099</span><br><span class="line">1100</span><br><span class="line">1101</span><br><span class="line">1102</span><br><span class="line">1103</span><br><span class="line">1104</span><br><span class="line">1105</span><br><span class="line">1106</span><br><span class="line">1107</span><br><span class="line">1108</span><br><span class="line">1109</span><br><span class="line">1110</span><br><span class="line">1111</span><br><span class="line">1112</span><br><span class="line">1113</span><br><span class="line">1114</span><br><span class="line">1115</span><br><span class="line">1116</span><br><span class="line">1117</span><br><span class="line">1118</span><br><span class="line">1119</span><br><span class="line">1120</span><br><span class="line">1121</span><br><span class="line">1122</span><br><span class="line">1123</span><br><span class="line">1124</span><br><span class="line">1125</span><br><span class="line">1126</span><br><span class="line">1127</span><br><span class="line">1128</span><br><span class="line">1129</span><br><span class="line">1130</span><br><span class="line">1131</span><br><span class="line">1132</span><br><span class="line">1133</span><br><span class="line">1134</span><br><span class="line">1135</span><br><span class="line">1136</span><br><span class="line">1137</span><br><span class="line">1138</span><br><span class="line">1139</span><br><span class="line">1140</span><br><span class="line">1141</span><br><span class="line">1142</span><br><span class="line">1143</span><br><span class="line">1144</span><br><span class="line">1145</span><br><span class="line">1146</span><br><span class="line">1147</span><br><span class="line">1148</span><br><span class="line">1149</span><br><span class="line">1150</span><br><span class="line">1151</span><br><span class="line">1152</span><br><span class="line">1153</span><br><span class="line">1154</span><br><span class="line">1155</span><br><span class="line">1156</span><br><span class="line">1157</span><br><span class="line">1158</span><br><span class="line">1159</span><br><span class="line">1160</span><br><span class="line">1161</span><br><span class="line">1162</span><br><span class="line">1163</span><br><span class="line">1164</span><br><span class="line">1165</span><br><span class="line">1166</span><br><span class="line">1167</span><br><span class="line">1168</span><br><span class="line">1169</span><br><span class="line">1170</span><br><span class="line">1171</span><br><span class="line">1172</span><br><span class="line">1173</span><br><span class="line">1174</span><br><span class="line">1175</span><br><span class="line">1176</span><br><span class="line">1177</span><br><span class="line">1178</span><br><span class="line">1179</span><br><span class="line">1180</span><br><span class="line">1181</span><br><span class="line">1182</span><br><span class="line">1183</span><br><span class="line">1184</span><br><span class="line">1185</span><br><span class="line">1186</span><br><span class="line">1187</span><br><span class="line">1188</span><br><span class="line">1189</span><br><span class="line">1190</span><br><span class="line">1191</span><br><span class="line">1192</span><br><span class="line">1193</span><br><span class="line">1194</span><br><span class="line">1195</span><br><span class="line">1196</span><br><span class="line">1197</span><br><span class="line">1198</span><br><span class="line">1199</span><br><span class="line">1200</span><br><span class="line">1201</span><br><span class="line">1202</span><br><span class="line">1203</span><br><span class="line">1204</span><br><span class="line">1205</span><br><span class="line">1206</span><br><span class="line">1207</span><br><span class="line">1208</span><br><span class="line">1209</span><br><span class="line">1210</span><br><span class="line">1211</span><br><span class="line">1212</span><br><span class="line">1213</span><br><span class="line">1214</span><br><span class="line">1215</span><br><span class="line">1216</span><br><span class="line">1217</span><br><span class="line">1218</span><br><span class="line">1219</span><br><span class="line">1220</span><br><span class="line">1221</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://huggingface.co/Qwen/QwQ-32B-Preview</span><br><span class="line">QwQ: Reflect Deeply on the Boundaries of the Unknown</span><br><span class="line">November 28, 2024</span><br><span class="line"> · 22 min · 4496 words · Qwen Team | Translations:</span><br><span class="line"></span><br><span class="line"> 알리바바가 OpenAI의 o1 reasoning model에 도전하는 &#x27;open&#x27; 모델을 출시한다</span><br><span class="line"></span><br><span class="line">새로운 &quot;추론&quot; AI 모델 QwQ-32B-Preview가 출시됨.</span><br><span class="line">QwQ-32B-Preview는 32.5억 개의 파라미터를 포함하고, 약 32,000 단어 길이의 프롬프트를 처리할 수 있음.</span><br><span class="line">해당 모델은 특정 벤치마크에서 OpenAI의 o1-preview 및 o1-mini보다 더 나은 성능을 보임.</span><br><span class="line">AIME와 MATH 테스트에서 OpenAI의 o1 모델을 능가함.</span><br><span class="line">논리 퍼즐을 풀고, 도전적인 수학 질문을 해결할 수 있음.</span><br><span class="line">그러나 언어 전환, 루프에 빠지는 현상, 상식적 추론을 요구하는 작업에서 약점을 보일 수 있음.</span><br><span class="line">QwQ-32B-Preview는 모델 자체적으로 사실 확인을 수행함으로써 잘못된 답변을 줄이는 반면, 답변 시간이 길어질 수 있음.</span><br><span class="line">Hugging Face 플랫폼에서 다운로드 가능하며 상업적 용도로 사용 가능함.</span><br><span class="line">중국의 규제를 준수하기 위해 정치적 주제에 대해서는 신중한 접근을 취함.</span><br><span class="line">QwQ-32B-Preview는 Apache 2.0 라이선스 하에 공개됨.</span><br><span class="line">모델의 일부 구성 요소만이 공개되어 전체 시스템의 재현이나 내밀한 이해는 어려움.</span><br><span class="line">&quot;스케일링 법칙&quot;의 지속 가능한 효율성에 대한 의문이 제기되면서, AI 연구는 새로운 접근법과 기술 개발에 집중하고 있음.</span><br><span class="line">Google을 포함한 여러 대형 연구소들이 추론 모델에 투자하고 있음.</span><br><span class="line"></span><br><span class="line">简体中文</span><br><span class="line">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</span><br><span class="line"></span><br><span class="line">Note: This is the pronunciation of QwQ: /kwju:/ , similar to the word “quill”.</span><br><span class="line"></span><br><span class="line">What does it mean to think, to question, to understand? These are the deep waters that QwQ (Qwen with Questions) wades into. Like an eternal student of wisdom, it approaches every problem - be it mathematics, code, or knowledge of our world - with genuine wonder and doubt. QwQ embodies that ancient philosophical spirit: it knows that it knows nothing, and that’s precisely what drives its curiosity. Before settling on any answer, it turns inward, questioning its own assumptions, exploring different paths of thought, always seeking deeper truth. Yet, like all seekers of wisdom, QwQ has its limitations. This version is but an early step on a longer journey - a student still learning to walk the path of reasoning. Its thoughts sometimes wander, its answers aren’t always complete, and its wisdom is still growing. But isn’t that the beauty of true learning? To be both capable and humble, knowledgeable yet always questioning? We invite you to explore alongside QwQ, embracing both its insights and its imperfections as part of the endless quest for understanding.</span><br><span class="line"></span><br><span class="line">Limitations</span><br><span class="line">QwQ-32B-Preview is an experimental research model developed by the Qwen Team, focused on advancing AI reasoning capabilities. As a preview release, it demonstrates promising analytical abilities while having several important limitations:</span><br><span class="line"></span><br><span class="line">Language Mixing and Code-Switching: The model may mix languages or switch between them unexpectedly, affecting response clarity.</span><br><span class="line">Recursive Reasoning Loops: The model may enter circular reasoning patterns, leading to lengthy responses without a conclusive answer.</span><br><span class="line">Safety and Ethical Considerations: The model requires enhanced safety measures to ensure reliable and secure performance, and users should exercise caution when deploying it.</span><br><span class="line">Performance and Benchmark Limitations: The model excels in math and coding but has room for improvement in other areas, such as common sense reasoning and nuanced language understanding.</span><br><span class="line">Performance</span><br><span class="line">Through deep exploration and countless trials, we discovered something profound: when given time to ponder, to question, and to reflect, the model’s understanding of mathematics and programming blossoms like a flower opening to the sun. Just as a student grows wiser by carefully examining their work and learning from mistakes, our model achieves deeper insight through patient, thoughtful analysis. This process of careful reflection and self-questioning leads to remarkable breakthroughs in solving complex problems. Our journey of discovery revealed the model’s exceptional ability to tackle some of the most challenging problems in mathematics and programming, including:</span><br><span class="line"></span><br><span class="line">GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark, a challenging benchmark for evaluating scientific problem-solving abilities through grade school level questions.</span><br><span class="line">AIME: American Invitation Mathematics Evaluation, which tests mathematical problem solving with arithmetic, algebra, counting, geometry, number theory, and probability and other secondary school math topics.</span><br><span class="line">MATH-500: The 500 test cases of the MATH benchmark, a comprehensive dataset testing mathematical problem-solving.</span><br><span class="line">LiveCodeBench: A challenging benchmark for evaluating code generation and problem solving abilities in real-world programming scenarios.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Specifically, QwQ demonstrates remarkable performance across these benchmarks, achieving impressive scores of 65.2% on GPQA, showcasing its graduate-level scientific reasoning capabilities; 50.0% on AIME, highlighting its strong mathematical problem-solving skills; 90.6% on MATH-500, demonstrating exceptional mathematical comprehension across diverse topics; and 50.0% on LiveCodeBench, validating its robust programming abilities in real-world scenarios. These results underscore QwQ’s significant advancement in analytical and problem-solving capabilities, particularly in technical domains requiring deep reasoning.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Introduction</span><br><span class="line">QwQ-32B-Preview is an experimental research model developed by the Qwen Team, focused on advancing AI reasoning capabilities. As a preview release, it demonstrates promising analytical abilities while having several important limitations:</span><br><span class="line"></span><br><span class="line">Language Mixing and Code-Switching: The model may mix languages or switch between them unexpectedly, affecting response clarity.</span><br><span class="line">Recursive Reasoning Loops: The model may enter circular reasoning patterns, leading to lengthy responses without a conclusive answer.</span><br><span class="line">Safety and Ethical Considerations: The model requires enhanced safety measures to ensure reliable and secure performance, and users should exercise caution when deploying it.</span><br><span class="line">Performance and Benchmark Limitations: The model excels in math and coding but has room for improvement in other areas, such as common sense reasoning and nuanced language understanding.</span><br><span class="line">Specification:</span><br><span class="line"></span><br><span class="line">Type: Causal Language Models</span><br><span class="line">Training Stage: Pretraining &amp; Post-training</span><br><span class="line">Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias</span><br><span class="line">Number of Parameters: 32.5B</span><br><span class="line">Number of Paramaters (Non-Embedding): 31.0B</span><br><span class="line">Number of Layers: 64</span><br><span class="line">Number of Attention Heads (GQA): 40 for Q and 8 for KV</span><br><span class="line">Context Length: Full 32,768 tokens</span><br><span class="line">For more details, please refer to our blog. You can also check Qwen2.5 GitHub, and Documentation.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://api-docs.deepseek.com/news/news1120</span><br><span class="line">NewsDeepSeek-R1-Lite Release 2024/11/20</span><br><span class="line">🚀 DeepSeek-R1-Lite-Preview is now live: unleashing supercharged reasoning power!</span><br><span class="line">🔍 o1-preview-level performance on AIME &amp; MATH benchmarks.</span><br><span class="line"></span><br><span class="line">💡 Transparent thought process in real-time.</span><br><span class="line"></span><br><span class="line">🛠️ Open-source models &amp; API coming soon!</span><br><span class="line"></span><br><span class="line">🌐 Try it now at http://chat.deepseek.com</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">🌟 Impressive Results of DeepSeek-R1-Lite-Preview Across Benchmarks!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">🌟 Inference Scaling Laws of DeepSeek-R1-Lite-Preview</span><br><span class="line"></span><br><span class="line">Longer Reasoning, Better Performance. DeepSeek-R1-Lite-Preview shows steady score improvements on AIME as thought length increases.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://developer.nvidia.com/blog/hymba-hybrid-head-architecture-boosts-small-language-model-performance/</span><br><span class="line">NVIDIA</span><br><span class="line"></span><br><span class="line">Hymba Hybrid-Head Architecture Boosts Small Language Model Performance</span><br><span class="line">Nov 22, 2024</span><br><span class="line">By Xin Dong, Yonggan Fu, Shizhe Diao and Pavlo Molchanov</span><br><span class="line"></span><br><span class="line">+17</span><br><span class="line">Like</span><br><span class="line"> Discuss (0)</span><br><span class="line"></span><br><span class="line">Transformers, with their attention-based architecture, have become the dominant choice for language models (LMs) due to their strong performance, parallelization capabilities, and long-term recall through key-value (KV) caches. However, their quadratic computational cost and high memory demands pose efficiency challenges. In contrast, state space models (SSMs) like Mamba and Mamba-2 offer constant complexity and efficient hardware optimization but struggle with memory recall tasks, affecting their performance on general benchmarks.</span><br><span class="line"></span><br><span class="line">NVIDIA researchers recently proposed Hymba, a family of small language models (SLMs) featuring a hybrid-head parallel architecture that integrates transformer attention mechanisms with SSMs to achieve both enhanced efficiency and improved performance. In Hymba, attention heads provide high-resolution recall, while SSM heads enable efficient context summarization.</span><br><span class="line"></span><br><span class="line">The novel architecture of Hymba reveals several insights:</span><br><span class="line"></span><br><span class="line">Overhead in attention: Over 50% of attention computation can be replaced by cheaper SSM computation.</span><br><span class="line">Local attention dominance: Most global attention can be replaced by local attention without sacrificing performance on general and recall-intensive tasks, thanks to the global information summarized by SSM heads.</span><br><span class="line">KV cache redundancy: Key-value cache is highly correlated across heads and layers, so it can be shared across heads (group query attention) and layers (cross-layer KV cache sharing).</span><br><span class="line">Softmax attention limitation: Attention mechanisms are constrained to sum to one, limiting sparsity, and flexibility. We introduce learnable meta-tokens that are prepended to prompts, storing critical information and alleviating the “forced-to-attend” burden associated with attention mechanisms.</span><br><span class="line">This post shows that Hymba 1.5B performs favorably against state-of-the-art open-source models of similar size, including Llama 3.2 1B, OpenELM 1B, Phi 1.5, SmolLM2 1.7B, Danube2 1.8B, and Qwen2.5 1.5B. Compared to Transformer models of similar size, Hymba also achieves higher throughput and requires 10x less memory to store cache.</span><br><span class="line"></span><br><span class="line">Hymba 1.5B is released to the Hugging Face collection and GitHub.</span><br><span class="line"></span><br><span class="line">Hymba 1.5B performance</span><br><span class="line">Figure 1 compares Hymba 1.5B against sub-2B models (Llama 3.2 1B, OpenELM 1B, Phi 1.5, SmolLM2 1.7B, Danube2 1.8B, Qwen2.5 1.5B) in terms of average task accuracy, cache size (MB) relative to sequence length, and throughput (tok/sec).</span><br><span class="line"></span><br><span class="line">A figure showing three performance metrics comparing seven different AI language models in terms of average accuracy, cache size (MB) relative to sequence length, and throughput (tok/sec).</span><br><span class="line">Figure 1. Performance comparison of Hymba 1.5B Base against sub-2B models </span><br><span class="line">In this set of experiments, the tasks include MMLU, ARC-C, ARC-E, PIQA, Hellaswag, Winogrande, and SQuAD-C. The throughput is measured on an NVIDIA A100 GPU with a sequence length of 8K and a batch size of 128 using PyTorch. For models encountering out of memory (OOM) issues during throughput measurement, the batch size was halved until the OOM is resolved to measure the maximal achievable throughput without OOM.</span><br><span class="line"></span><br><span class="line">Hymba model design </span><br><span class="line">SSMs such as Mamba were introduced to address the quadratic complexity and large inference-time KV cache issues of transformers. However, due to their low-resolution memory, SSMs struggle with memory recall and performance. To overcome these limitations, we propose a road map for developing efficient and high-performing small LMs in Table 1.</span><br><span class="line"></span><br><span class="line">Configuration	Commonsense reasoning (%) ↑	Recall (%) ↑	Throughput (token/sec) ↑	Cache size (MB) ↓	Design reason</span><br><span class="line">Ablations on 300M model size and 100B training tokens</span><br><span class="line">Transformer (Llama)	44.08	39.98	721.1	414.7	Accurate recall while inefficient </span><br><span class="line">State-space models (Mamba)	42.98	19.23	4720.8	1.9	Efficient while inaccurate recall</span><br><span class="line">A. + Attention heads (sequential)	44.07	45.16	776.3	156.3	Enhance recall capabilities</span><br><span class="line">B. + Multi-head heads (parallel)	45.19	49.90	876.7	148.2	Better balance of two modules</span><br><span class="line">C. + Local / global attention	44.56	48.79	2399.7	41.2	Boost compute/cache efficiency</span><br><span class="line">D. + KV cache sharing	45.16	48.04	2756.5	39.4	Cache efficiency</span><br><span class="line">E. + Meta-tokens 	45.59	51.79	2695.8	40.0	Learned memory initialization</span><br><span class="line">Scaling to 1.5B model size and 1.5T training tokens</span><br><span class="line">F. + Size / data	60.56	64.15	664.1	78.6	Further boost task performance</span><br><span class="line">G. + Extended context length (2K→8K)	60.64	68.79	664.1	78.6	Improve multishot and recall tasks</span><br><span class="line">Table 1. Design road map of the Hymba model</span><br><span class="line">Fused hybrid modules </span><br><span class="line">Fusing attention and SSM heads in parallel within a hybrid-head module outperforms sequential stacking, according to the ablation study. Hymba fuses attention and SSM heads in parallel within a hybrid head module, enabling both heads to process the same information simultaneously. This architecture improves reasoning and recall accuracy.</span><br><span class="line"></span><br><span class="line">A diagram showing the architecture of a dual-path attention mechanism. The flow starts with an Input Projection, leading to Latent Feature extraction which splits into two parallel paths. The upper path (in blue) contains SSM Feature processing through SSM Heads and Gate Normalization. The lower path (in red) processes Attention Features through Attention Heads and Gate Normalization. Both paths converge at a Mean operation before final Output Projection. Arrows indicate the flow of data through the system.</span><br><span class="line">Figure 2. The hybrid-head module in Hymba</span><br><span class="line">Efficiency and KV cache optimization</span><br><span class="line">While attention heads improve task performance, they increase KV cache requirements and reduce throughput. To mitigate this, Hymba optimizes the hybrid-head module by combining local and global attention and employing cross-layer KV cache sharing. This improves throughput by 3x and reduces cache by almost 4x without sacrificing performance. </span><br><span class="line"></span><br><span class="line">A diagram showing the architecture of a neural network model with Hymba Blocks. The model flows from left to right, starting with an Embedding layer, followed by alternating Hymba Blocks with Full Attention (in red) and SWA (in blue). The blocks are connected with KV sharing every 2 layers, shown in dotted green boxes labeled &#x27;Repeat (N-3)/2&#x27;. Below the main flow, there&#x27;s a detailed view of a module containing Layer norm, Hybrid-head module, another Layer norm, and FFN components. The diagram ends with an LM Head layer on the right.</span><br><span class="line">Figure 3. Hymba model architecture</span><br><span class="line">Meta-tokens</span><br><span class="line">A set of 128 pretrained embeddings prepended to inputs, functioning as learned cache initialization to enhance focus on relevant information. These tokens serve a dual purpose: </span><br><span class="line"></span><br><span class="line">Mitigating attention drain by acting as backstop tokens, redistributing attention effectively</span><br><span class="line">Encapsulating compressed world knowledge</span><br><span class="line">A diagram illustrating the Fading Memory architecture from SSM (State Space Model). The image shows three layers: At the top is a blue rectangular box labeled &#x27;Fading Memory (From SSM)&#x27;. Below it are seven gray input tokens arranged horizontally. At the bottom are two sets of memory blocks: on the left are two green blocks labeled &#x27;Meta Memory (Meta Tokens)&#x27;, and on the right are three red blocks labeled &#x27;Snapshot Memory (From Attn)&#x27;. Green arrows connect the Meta Memory to the input tokens, while red arrows connect the Snapshot Memory to the rightmost input tokens. A blue arrow loops back from the Fading Memory box to itself.</span><br><span class="line">Figure 4. Interpretation of Hymba from the memory aspect</span><br><span class="line">Model analysis</span><br><span class="line">This section presents an apples-to-apples comparison across different architectures under the same training settings. We then visualize the attention maps of SSM and Attention in different pretrained models. Finally, we perform head importance analysis for Hymba through pruning. All the analyses in this section help to illustrate how and why the design choices for Hymba are effective. </span><br><span class="line"></span><br><span class="line">Apples-to-apples comparison </span><br><span class="line">We performed an apples-to-apples comparison of Hymba, pure Mamba2, Mamba2 with FFN, Llama3 style, and Samba style (Mamba-FFN-Attn-FFN) architectures. All models have 1 billion parameters and are trained from scratch for 100 billion tokens from SmolLM-Corpus with exactly the same training recipe. All results are obtained through lm-evaluation-harness using a zero-shot setting on Hugging Face models. Hymba performs the best on commonsense reasoning as well as question answering and recall-intensive tasks. </span><br><span class="line"></span><br><span class="line">Table 2 compares various model architectures on language modeling and recall-intensive and commonsense reasoning tasks, with Hymba achieving strong performance across metrics. Hymba demonstrates the lowest perplexity in language tasks (18.62 for Wiki and 10.38 for LMB) and solid results in recall-intensive tasks, particularly in SWDE (54.29) and SQuAD-C (44.71), leading to the highest average score in this category (49.50). </span><br><span class="line"></span><br><span class="line">Model	Language (PPL) ↓	Recall intensive (%) ↑	Commonsense reasoning (%) ↑</span><br><span class="line">Mamba2	15.88	43.34	52.52</span><br><span class="line">Mamba2 w/ FFN	17.43	28.92	51.14</span><br><span class="line">Llama3	16.19	47.33	52.82</span><br><span class="line">Samba	16.28	36.17	52.83</span><br><span class="line">Hymba	14.5	49.5	54.57</span><br><span class="line">Table 2. Comparison of architectures trained on 100 billion tokens under the same settings</span><br><span class="line">In commonsense reasoning and question answering, Hymba outperforms other models in most tasks, such as SIQA (31.76) and TruthfulQA (31.64), with an average score of 54.57, slightly above Llama3 and Mamba2. Overall, Hymba stands out as a balanced model, excelling in both efficiency and task performance across diverse categories.</span><br><span class="line"></span><br><span class="line">Attention map visualization</span><br><span class="line">We further categorized elements in the attention map into four types: </span><br><span class="line"></span><br><span class="line">Meta: Attention scores from all real tokens to meta-tokens. This category reflects the model’s preference for attending to meta-tokens. In attention maps, they are usually located in the first few columns (for example, 128 for Hymba) if a model has meta-tokens. </span><br><span class="line">BOS: Attention scores from all real tokens to the beginning-of-sequence token. In the attention map, they are usually located in the first column right after the meta-tokens. </span><br><span class="line">Self: Attention scores from all real tokens to themselves. In the attention map, they are usually located in the diagonal line. </span><br><span class="line">Cross: Attention scores from all real tokens to other real tokens. In the attention map, they are usually located in the off-diagonal area. </span><br><span class="line">The attention pattern of Hymba is significantly different from that of vanilla Transformers. In vanilla Transformers, attention scores are more concentrated on BOS, which is consistent with the findings in Attention Sink. In addition, vanilla Transformers also have a higher proportion of Self attention scores. In Hymba, meta-tokens, attention heads, and SSM heads work complementary to each other, leading to a more balanced distribution of attention scores across different types of tokens. </span><br><span class="line"></span><br><span class="line">Specifically, meta-tokens offload the attention scores from BOS, enabling the model to focus more on the real tokens. SSM heads summarize the global context, which focuses more on current tokens (Self attention scores). Attention heads, on the other hand, pay less attention to Self and BOS tokens, and more attention to other tokens (that is, Cross attention scores). This suggests that the hybrid-head design of Hymba can effectively balance the attention distribution across different types of tokens, potentially leading to better performance.</span><br><span class="line"></span><br><span class="line">A diagram showing the composition of the Hymba attention mechanism. It consists of three components that are added together: Meta Tokens (shown as a vertical green stripe on the left), Sliding Window Attention (displayed as a diagonal green band), and SSM (Mamba) (represented as a triangular green gradient). These three patterns combine to form the final Hymba pattern on the right, which shows a triangular area filled with green squares of varying intensity. Each component is displayed in a square grid format, and the combination is shown using plus signs between the components and an equals sign before the final result.</span><br><span class="line">Figure 5. Schematics of the attention map of Hymba as a combination of meta-tokens, sliding window attention, and Mamba contributions</span><br><span class="line">A comparative visualization showing attention patterns across different language models. The image consists of three main parts: 1) Three attention heatmaps for Llama 3.2 3B and Hymba 1.5B models, showing diagonal patterns in purple, yellow, and blue colors. 2) A grid diagram showing BOS (Beginning of Sequence) token connections with Meta and Cross sections marked. 3) Three horizontal stacked bar charts comparing percentage distributions of Meta, BOS, Cross, and Self attention patterns across Llama 3.2 3B and two variants of Hymba models, with percentages clearly labeled in different colors.</span><br><span class="line">Figure 6. Sum of the attention score from different categories in Llama 3.2 3B and Hymba 1.5B</span><br><span class="line">Heads importance analysis </span><br><span class="line">We analyzed the relative importance of attention and SSM heads in each layer by removing them and recording the final accuracy. Our analysis reveals the following: </span><br><span class="line"></span><br><span class="line">The relative importance of attention/SSM heads in the same layer is input-adaptive and varies across tasks, suggesting that they can serve different roles when handling various inputs.</span><br><span class="line">The SSM head in the first layer is critical for language modeling, and removing it causes a substantial accuracy drop to random guess levels.</span><br><span class="line">Generally, removing one attention/SSM head results in an average accuracy drop of 0.24%/1.1% on Hellaswag, respectively.</span><br><span class="line">A line graph comparing the Hellswag Accuracy (y-axis ranging from 0.45 to 0.50) across 32 different layers (x-axis). The graph shows three elements: a horizontal dashed line labeled Orig Model at approximately 0.493, and two sets of bars in blue and orange representing Remove Attn and Remove SSM, respectively. The bars fluctuate slightly above and below the original model line, with most values falling between 0.47 and 0.495. The graph compares the impact of removing attention mechanisms versus SSM components at different layers of the model.</span><br><span class="line">Figure 7. The achieved accuracy, measured using 1K samples from Hellaswag, after removing the Attention or SSM heads in each layer</span><br><span class="line">Model architecture and training best practices</span><br><span class="line">This section outlines key architectural decisions and training methodologies for Hymba 1.5B Base and Hymba 1.5B Instruct.</span><br><span class="line"></span><br><span class="line">Model architecture</span><br><span class="line">Hybrid architecture: Mamba is great at summarization and usually closer focuses on the current token, while attention is more precise and acts as snapshot memory. Combining them in parallel merges these benefits, but standard sequential fusion does not. We chose a 5:1 parameter ratio between SSM and attention heads.</span><br><span class="line">Sliding window attention: Full attention heads are preserved in three layers (first, last, and middle), with sliding window attention heads used in the remaining 90% layers.</span><br><span class="line">Cross-layer KV cache sharing: Implemented between every two consecutive attention layers. It is done in addition to GQA KV cache sharing between heads.</span><br><span class="line">Meta-tokens: These 128 tokens are learnable with no supervision, helping to avoid entropy collapse problems in large language models (LLMs) and mitigate the attention sink phenomenon. Additionally, the model stores general knowledge in these tokens. </span><br><span class="line">Training best practices </span><br><span class="line">Pretraining: We opted for two-stage base model training. Stage 1 maintained a constant large learning rate and used less filtered large corpus data. Continuous learning rate decay was then performed to 1e-5 using high-quality data. This approach enables continuous training and resuming of Stage 1.</span><br><span class="line">Instruction fine-tuning: Instruct model tuning is performed in three stages. First, SFT-1 provides the model with strong reasoning abilities by training on code, math, function calling, role play, and other task-specific data. Second, SFT-2 teaches the model to follow human instructions. Finally, DPO is leveraged to align the model with human preferences and improve the model’s safety.</span><br><span class="line">Training pipeline for the Hymba model family divided into five sections that read (left to right) General pretraining, LR annealing, SFT-1, SFT-2, and DPO. </span><br><span class="line">Figure 8. Training pipeline adapted for the Hymba model family</span><br><span class="line">Performance and efficiency evaluation </span><br><span class="line">With only 1.5T pretraining tokens, the Hymba 1.5B model performs the best among all small LMs and achieves better throughput and cache efficiency than all transformer-based LMs. </span><br><span class="line"></span><br><span class="line">For example, when benchmarking against the strongest baseline, Qwen2.5, which is pretrained on 13x more tokens, Hymba 1.5B achieves a 1.55% average accuracy improvement, 1.41x throughput, and 2.90x cache efficiency. Compared to the strongest small LM trained on fewer than 2T tokens, namely h2o-danube2, our method achieves a 5.41% average accuracy improvement, 2.45x throughput, and 6.23x cache efficiency.</span><br><span class="line"></span><br><span class="line">New small hybrid model from NVIDIA has been announced! Hymba is a 1.5B hybrid Mamba x Attention Model that outperforms other small LLMs like Meta 3.2 or SmolLM v2 being trained on only 1.5T Tokens. 🤯</span><br><span class="line">It uses a new hybrid architecture with Mamba and Attention heads running in parallel with additional meta tokens (learnable tokens prepended to every prompt), to improve the efficacy of the model. 👀</span><br><span class="line">It shares the KV cache between 2 layers and between heads in a single layer. It has 16 SSM states and 3 full attention layers; the rest are sliding window attention. It also uses FlexAttention from Pytorch 2.5! 🔥</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Hymba-1.5B-Base</span><br><span class="line">Model Overview</span><br><span class="line">Hymba-1.5B-Base is a base text-to-text model that can be adopted for a variety of natural language generation tasks.</span><br><span class="line"></span><br><span class="line">The model has hybrid architecture with Mamba and Attention heads running in parallel. Meta tokens, a set of learnable tokens prepended to every prompt, help improve the efficacy of the model. The model shares KV cache between 2 layers and between heads in a single layer. 90% of attention layers are sliding window attention.</span><br><span class="line"></span><br><span class="line">This model is ready for commercial use.</span><br><span class="line"></span><br><span class="line">[Caution] During generation, the batch size needs to be 1. Our current implementation does not fully support padding of Meta tokens + SWA; this is a work in progress. Training and pre-filling support any batch size.</span><br><span class="line"></span><br><span class="line">Model Developer: NVIDIA</span><br><span class="line"></span><br><span class="line">Model Dates: Hymba-1.5B-Base was trained between September 1, 2024 and November 10th, 2024.</span><br><span class="line"></span><br><span class="line">License: This model is released under the NVIDIA Open Model License Agreement.</span><br><span class="line"></span><br><span class="line">Model Architecture</span><br><span class="line">Hymba-1.5B-Base has a model embedding size of 1600, 25 attention heads, and an MLP intermediate dimension of 5504, with 32 layers in total, 16 SSM states, 3 full attention layers, the rest are sliding window attention. Unlike the standard Transformer, each attention layer in Hymba has a hybrid combination of standard attention heads and Mamba heads in parallel. Additionally, it uses Grouped-Query Attention (GQA) and Rotary Position Embeddings (RoPE).</span><br><span class="line"></span><br><span class="line">Features of this architecture:</span><br><span class="line"></span><br><span class="line">Fuse attention heads and SSM heads within the same layer, offering parallel and complementary processing of the same inputs.</span><br><span class="line">Hymba Module</span><br><span class="line">Introduce meta tokens that are prepended to the input sequences and interact with all subsequent tokens, thus storing important information and alleviating the burden of &quot;forced-to-attend&quot; in attention.</span><br><span class="line"></span><br><span class="line">Integrate with cross-layer KV sharing and global-local attention to further boost memory and computation efficiency.</span><br><span class="line"></span><br><span class="line">Hymba Model</span><br><span class="line">Performance Highlights</span><br><span class="line">Hymba-1.5B-Base outperforms all sub-2B public models.</span><br><span class="line">Compare with SoTA Small LMs</span><br><span class="line">Compare with SoTA Small LMs</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct</span><br><span class="line">SmolVLM - small yet mighty Vision Language Model</span><br><span class="line">Published November 26, 2024</span><br><span class="line">Andres Marafioti&#x27;s avatar</span><br><span class="line">andito</span><br><span class="line">Andres Marafioti</span><br><span class="line">Merve Noyan&#x27;s avatar</span><br><span class="line">merve</span><br><span class="line">Merve Noyan</span><br><span class="line">Miquel Farré&#x27;s avatar</span><br><span class="line">mfarre</span><br><span class="line">Miquel Farré</span><br><span class="line">Elie Bakouch&#x27;s avatar</span><br><span class="line">eliebak</span><br><span class="line">Elie Bakouch</span><br><span class="line">Pedro Cuenca&#x27;s avatar</span><br><span class="line">pcuenq</span><br><span class="line">Pedro Cuenca</span><br><span class="line">TLDR</span><br><span class="line">This blog post introduces SmolVLM, a 2B VLM, SOTA for its memory footprint. SmolVLM is small, fast, memory-efficient, and fully open-source. All model checkpoints, VLM datasets, training recipes and tools are released under the Apache 2.0 license.</span><br><span class="line"></span><br><span class="line">Image description</span><br><span class="line">What is SmolVLM?</span><br><span class="line">This year has seen a boom in multimodal AI with many large vision language models released. The trends were to initially scale up compute, later scale up the data diversity by generating synthetic data with large models, and, recently, scale down to make these models more efficient. Small open models allow local deployment to browser or edge devices, cut inference costs, and enable user customization. Some notable examples of these models include PaliGemma 3B, moondream2, and Qwen2VL.</span><br><span class="line"></span><br><span class="line">In this blog post, we introduce SmolVLM, a new family of 2B small vision language models that can be used commercially and deployed to smaller local setups, with completely open training pipelines.</span><br><span class="line"></span><br><span class="line">We release three models: SmolVLM-Base, which can be used for downstream fine-tuning, SmolVLM-Synthetic, the fine-tuned variant on synthetic data, and SmolVLM Instruct, the fine-tuned instruction variant, which can be used out of the box for interactive end-user applications.</span><br><span class="line"></span><br><span class="line">This release comes with open-source models integrated into transformers, a demo built on SmolVLM Instruct, and a supervised fine-tuning script. We have used the datasets previously used for Idefics3: the Cauldron and Docmatix, which are also fully open-source.</span><br><span class="line"></span><br><span class="line">Table of Contents</span><br><span class="line">TLDR</span><br><span class="line">What is SmolVLM?</span><br><span class="line">Model capabilities</span><br><span class="line">Architecture</span><br><span class="line">Performance</span><br><span class="line">Benchmarks</span><br><span class="line">Memory</span><br><span class="line">Throughput</span><br><span class="line">Video</span><br><span class="line">VLMEvalKit integration</span><br><span class="line">Use SmolVLM with Transformers</span><br><span class="line">Training Details</span><br><span class="line">Dataset</span><br><span class="line">Context extension</span><br><span class="line">Checkpoint selection</span><br><span class="line">Fine-Tuning</span><br><span class="line">Wrapping up</span><br><span class="line">Model capabilities</span><br><span class="line">Input Image	Input Prompt	Answer</span><br><span class="line">	I&#x27;m planning a visit to this temple, give me travel tips.	The Grand Palace is the most famous landmark in Bangkok and it&#x27;s worth visiting for its stunning architecture and history. Here are some of our favorite things about the palace:</span><br><span class="line">The Main Building: Located at the center of the complex, the main building features intricate tile work with traditional Thai motifs that cover every inch of the structure. This includes patterns such as dragons, mythical creatures, flowers, and geometric shapes. The roofline has an elaborate design featuring multiple layers of tiles adorned with gold accents.</span><br><span class="line"></span><br><span class="line">Towers &amp; Spires: There are several towers within the Grand Palace including the Phra Nakhon Si Ayutthaya National Museum Tower which houses exhibits on Thai culture and heritage. Another notable tower is the Wat Yai Chai Mongkol Tower where you can find beautiful murals depicting scenes from Buddhist mythology.</span><br><span class="line"></span><br><span class="line">Architecture Style: The architectural style combines elements of Hindu and Khmer styles seen commonly throughout Southeast Asia. You&#x27;ll notice influences like stupas (dome-shaped structures), lotus flower carvings, and ornate details typical of these regions&#x27; religious buildings.</span><br><span class="line"></span><br><span class="line">Visitor Experience: While exploring the Grand Palace, visitors should be aware of certain rules due to security measures taken by authorities. These include no photography inside certain areas or using flashbulbs outside. Additionally, there may be queues during peak tourist seasons so plan accordingly if you&#x27;re traveling here.</span><br><span class="line"></span><br><span class="line">Travel Tips For Visiting The Grand Palace:</span><br><span class="line">Best Time To Visit: The best time to visit would likely be early morning before sunrise when temperatures drop significantly making walking around easier.</span><br><span class="line">Transportation: Public transportation options exist but they might not provide direct access to the Grand Palace itself; consider taking a taxi instead.</span><br><span class="line">Food Options: There aren&#x27;t many dining facilities available near the Grand Palace yet local street food vendors do offer delicious meals nearby.</span><br><span class="line">Clothing Guidelines: While not mandatory, wearing traditional Thai clothing (sarongs) could add cultural immersion experience. However, shorts/trousers are allowed provided they don&#x27;t go below knee level.</span><br><span class="line">Safety Measures: Visitors must follow signs indicating restricted zones marked &quot;No Photography&quot; among others. Always carry identification documents just in case.</span><br><span class="line">By following these guidelines, your trip will undoubtedly enhance both your understanding of Thailand’s rich historical tapestry and personal enjoyment!</span><br><span class="line"></span><br><span class="line">	Where do the severe droughts happen according to this diagram?	The severe droughts are happening in eastern and southern Africa.</span><br><span class="line">	What is the due date and the invoice date?	The due date is 26/02/2019, and the invoice date is 11/02/2019.</span><br><span class="line">Architecture</span><br><span class="line">Image description</span><br><span class="line">For SmolVLM, we closely followed the architecture from Idefics3, to the point that we use the same implementation in transformers. There are, however a few key differences:</span><br><span class="line"></span><br><span class="line">We replaced Llama 3.1 8B with SmolLM2 1.7B as the language backbone.</span><br><span class="line">We more aggressively compress the patched visual information by reducing the information 9x using the pixel shuffle strategy, compared to 4x with idefics3.</span><br><span class="line">We use patches of 384*384, instead of 364x364, because 384 is divisible by 3, which is necessary for our pixel shuffle strategy to work.</span><br><span class="line">For this, we change the vision backbone to use shape-optimized SigLIP with patches of 384x384 pixels and inner patches of 14x14.</span><br><span class="line">Performance</span><br><span class="line">Benchmarks</span><br><span class="line">We present benchmarks for the tasks we mention in training details.</span><br><span class="line"></span><br><span class="line">Model	MMMU (val)	MathVista (testmini)	MMStar (val)	DocVQA (test)	TextVQA (val)	Min GPU RAM required (GB)</span><br><span class="line">SmolVLM	38.8	44.6	42.1	81.6	72.7	5.02</span><br><span class="line">Qwen2-VL 2B	41.1	47.8	47.5	90.1	79.7	13.70</span><br><span class="line">InternVL2 2B	34.3	46.3	49.8	86.9	73.4	10.52</span><br><span class="line">PaliGemma 3B 448px	34.9	28.7	48.3	32.2	56.0	6.72</span><br><span class="line">moondream2	32.4	24.3	40.3	70.5	65.2	3.87</span><br><span class="line">MiniCPM-V-2	38.2	39.8	39.1	71.9	74.1	7.88</span><br><span class="line">MM1.5 1B	35.8	37.2	0.0	81.0	72.5	NaN</span><br><span class="line">Memory</span><br><span class="line">Inference GPU memory use for SmolVLM and other models</span><br><span class="line">SmolVLM provides the best memory usage among the existing suite of vision language models in transformers. This allows it to run efficiently on-device, such as a laptop! You can see above the GPU memory usage in GBs for each model, running inference with one or two input images, and using the same images and text prompts in all tests. SmolVLM&#x27;s efficiency in image encoding is built into the model. SmolVLM encodes each 384x384 image patch to 81 tokens. This results in SmolVLM encoding our test prompt and a single image in 1.2k tokens, whereas Qwen2-VL uses 16k tokens. This also explains why the memory consumption increases so much for 2 images with Qwen and InternVL. In contrast, the increase is much more moderate for SmolVLM and PaliGemma, which use a similar approach.</span><br><span class="line"></span><br><span class="line">Throughput</span><br><span class="line">Image description</span><br><span class="line">SmolVLM&#x27;s tiny memory footprint also implies that it requires far fewer computations to prefill the model and generate. Compared to Qwen2-VL, the prefill throughput is 3.3 to 4.5 times faster, and the generation throughput is 7.5 to 16 times faster.</span><br><span class="line"></span><br><span class="line">Video</span><br><span class="line">Given SmolVLM&#x27;s long context and the possibility of tweaking the internal frame resizing of the model, we explored its suitability as an accessible option for basic video analysis tasks, particularly when computational resources are limited.</span><br><span class="line"></span><br><span class="line">In our evaluation of SmolVLM&#x27;s video understanding capabilities, we implemented a straightforward video processing pipeline code, extracting up to 50 evenly sampled frames from each video while avoiding internal frame resizing. This simple approach yielded surprisingly competitive results on the CinePile benchmark, with a score of 27.14%, a performance that positions the model between InterVL2 (2B) and Video LlaVa (7B).</span><br><span class="line"></span><br><span class="line">The quantitative results align with our qualitative testing, looking at an example from the FineVideo dataset:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Question: what is shown in the close-up? </span><br><span class="line">Assistant: In the close-up, there are bottles of Heinz tomato ketchup,</span><br><span class="line">  honey, soy sauce, and garlic on a kitchen counter.</span><br><span class="line"></span><br><span class="line">Question: describe what is happening in the video. </span><br><span class="line">Assistant: The video shows a man cooking in a kitchen.</span><br><span class="line">  He is wearing a black shirt and a black apron.</span><br><span class="line">  He is standing in front of a counter and is holding a bottle of Heinz tomato ketchup,</span><br><span class="line">  a bottle of honey, soy sauce, and a bowl of minced garlic</span><br><span class="line"></span><br><span class="line">While in the second question, we see some temporal understanding limitations (the cook points to one ingredient after the other rather than pointing/holding all of them at the same time) SmolVLM demonstrated great scene understanding and object recognition capabilities.</span><br><span class="line"></span><br><span class="line">VLMEvalKit integration</span><br><span class="line">We integrated SmolVLM with VLMEvalKit to facilitate easy evaluation across additional benchmarks.</span><br><span class="line"></span><br><span class="line">By running the following command, you can evaluate SmolVLM or your fine-tuned SmolVLM model.</span><br><span class="line"></span><br><span class="line">python run.py --data &lt;benchmarks&gt; --model SmolVLM --work-dir &lt;output_directory&gt;</span><br><span class="line"></span><br><span class="line">For example, to evaluate on MMMU dev validation set and MathVista mini and store the results in a folder called smol.</span><br><span class="line"></span><br><span class="line">python run.py --data MMMU_DEV_VAL MathVista_MINI --model SmolVLM --work-dir smol</span><br><span class="line"></span><br><span class="line">Use SmolVLM with Transformers</span><br><span class="line">You can easily load SmolVLM using the Auto classes in transformers. Under the hood, the model and processor are mapped to the same implementations used for Idefics3.</span><br><span class="line"></span><br><span class="line">from transformers import AutoProcessor, AutoModelForVision2Seq</span><br><span class="line">import torch</span><br><span class="line">DEVICE = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;</span><br><span class="line"></span><br><span class="line">processor = AutoProcessor.from_pretrained(&quot;HuggingFaceTB/SmolVLM-Instruct&quot;)</span><br><span class="line">model = AutoModelForVision2Seq.from_pretrained(&quot;HuggingFaceTB/SmolVLM-Instruct&quot;,</span><br><span class="line">                                                torch_dtype=torch.bfloat16,</span><br><span class="line">                                                _attn_implementation=&quot;flash_attention_2&quot; if DEVICE == &quot;cuda&quot; else &quot;eager&quot;).to(DEVICE)</span><br><span class="line"></span><br><span class="line">Image and text can be interleaved arbitrarily, and you can pass in multiple images. Here’s how you can use the chat template and pass in the formatted input to the processor.</span><br><span class="line"></span><br><span class="line">from PIL import Image</span><br><span class="line">from transformers.image_utils import load_image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load images</span><br><span class="line">image1 = load_image(&quot;https://huggingface.co/spaces/HuggingFaceTB/SmolVLM/resolve/main/example_images/rococo.jpg&quot;)</span><br><span class="line">image2 = load_image(&quot;https://huggingface.co/spaces/HuggingFaceTB/SmolVLM/blob/main/example_images/rococo_1.jpg&quot;)</span><br><span class="line"></span><br><span class="line"># Create input messages</span><br><span class="line">messages = [</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;role&quot;: &quot;user&quot;,</span><br><span class="line">        &quot;content&quot;: [</span><br><span class="line">            &#123;&quot;type&quot;: &quot;image&quot;&#125;,</span><br><span class="line">            &#123;&quot;type&quot;: &quot;image&quot;&#125;,</span><br><span class="line">            &#123;&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Can you describe the two images?&quot;&#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"># Prepare inputs</span><br><span class="line">prompt = processor.apply_chat_template(messages, add_generation_prompt=True)</span><br><span class="line">inputs = processor(text=prompt, images=[image1, image2], return_tensors=&quot;pt&quot;)</span><br><span class="line">inputs = inputs.to(DEVICE)</span><br><span class="line"></span><br><span class="line">Start generating with preprocessed input and decode the generated output.</span><br><span class="line"></span><br><span class="line"># Generate outputs</span><br><span class="line">generated_ids = model.generate(**inputs, max_new_tokens=500)</span><br><span class="line">generated_texts = processor.batch_decode(</span><br><span class="line">    generated_ids,</span><br><span class="line">    skip_special_tokens=True,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">print(generated_texts[0])</span><br><span class="line"></span><br><span class="line">Training Details</span><br><span class="line">Dataset</span><br><span class="line">First, we had to train SmolLM2 to extend it context, but we will discuss that in the next subsection. Once we had a long context SmolLM2, we trained SmolVLM using the same data that we used for Idefics3. Mainly, we used The Cauldron and Docmatix. The full list of datasets we used can be consulted here.</span><br><span class="line"></span><br><span class="line">Image description</span><br><span class="line">Context extension</span><br><span class="line">Image description</span><br><span class="line">SmolLM2’s pre-training context window is insufficient for VLMs. Images are encoded into many tokens, and we wanted to support multiple images. To address this, we extended it to 16k tokens by increasing the RoPE base value from 10k to 273k, following the guidelines in “Scaling Laws of RoPE-based Extrapolation”. We fine-tuned the model on a mixture of long- and short-context datasets. For long-context datasets, we used the “books” subset of Dolma (primarily Project Gutenberg) and code documents with 8k+ tokens from The Stack, each contributing 20% to the final mixture. For short-context datasets, we streamlined the original SmolLM2 pre-training mix to include 20% FineWeb-Edu, 20% DCLM, and 20% from our math dataset (to be released soon). The math dataset was upsampled to mitigate a performance drop observed on GSM8k during the context extension process. All experiments were implemented using the EasyContext repository.</span><br><span class="line"></span><br><span class="line">Checkpoint Selection</span><br><span class="line">For our training run, we saved checkpoints every 25 optimization steps, allowing us to evaluate and potentially recover the model&#x27;s state at different points in training. This practice is crucial for identifying the optimal model version, as training longer doesn&#x27;t always guarantee better performance. We evaluated the performance across multiple vision-language benchmarks, each weighted according to their importance. The core benchmarks included the following:</span><br><span class="line"></span><br><span class="line">General multimodal understanding (MMMU and MMStar) which are the most comprehensive benchmark.</span><br><span class="line">Document and text-based visual question answering (DocVQA and TextVQA)</span><br><span class="line">Mathematical Reasoning (MathVista)</span><br><span class="line">Diagram understanding (AI2D)</span><br><span class="line">General multimodal understanding (MMMU and MMStar).</span><br><span class="line">To select the optimal checkpoint, we created a single metric by combining these benchmarks with different manually assigned weights to reflect their relative importance in assessing the model&#x27;s capabilities. We used this single metric to select the best checkpoint. Generally, the models tended to do great on most benchmarks with more training, but their relative performance on DocVQA would decrease considerably.</span><br><span class="line"></span><br><span class="line">Fine-tuning</span><br><span class="line">You can fine-tune SmolVLM using transformers and apply alignment techniques using TRL 🚀</span><br><span class="line"></span><br><span class="line">We provide a notebook to fine-tune it on the VQAv2 dataset, optionally using LoRA, QLoRA or full fine-tuning. In the notebook, you can find some tricks to save up even more memory and have a larger batch size to fit SmolVLM inside consumer GPUs, like L4, for training. With batch sizes of 4, 8-bit loading with QLoRA and gradient checkpointing we can fine-tune in L4, and it consumes around ~16 GBs of VRAM. This makes it possible to fine-tune your SmolVLM using Colab! You can play around with the parameters to get a nice point in training duration-memory trade-off.</span><br><span class="line"></span><br><span class="line">SmolVLM also comes with TRL integration so you can apply Direct Preference Optimization (DPO) easily through the CLI. Get started by running pip install trl accelerate peft and then run the following command to fine-tune on RLAIF-V dataset:</span><br><span class="line"></span><br><span class="line">accelerate launch \</span><br><span class="line">  --config_file examples/accelerate_configs/multi_gpu.yaml examples/scripts/dpo_vlm.py  \</span><br><span class="line">  --dataset_name HuggingFaceH4/rlaif-v_formatted \</span><br><span class="line">  --model_name_or_path HuggingFaceTB/SmolVLM-Instruct \</span><br><span class="line">  --per_device_train_batch_size 8 \</span><br><span class="line">  --gradient_accumulation_steps 32 \</span><br><span class="line">  --dataset_num_proc 32 \</span><br><span class="line">  --output_dir dpo_smolvlm_rlaif-v \</span><br><span class="line">  --bf16 --torch_dtype bfloat16 \</span><br><span class="line">  --use_peft --lora_target_modules=all-linear </span><br><span class="line"></span><br><span class="line">The resulting LoRA adapter weights are SmolVLM-Instruct-DPO. A detailed tutorial on preference tuning vision-based LLM can be found here: dpo_vlm.</span><br><span class="line"></span><br><span class="line">Wrapping Up</span><br><span class="line">We introduced SmolVLM, a fully open, small, and mighty VLM for the community! We also provide tools for the community to use and customize it. We are looking forward to seeing what you will create with SmolVLM.</span><br><span class="line"></span><br><span class="line">Below are some resources if you would like to read more about all things related to SmolVLM.</span><br><span class="line"></span><br><span class="line">Start playing with SmolVLM using this demo.</span><br><span class="line">Learn how to fine-tune SmolVLM on VQAv2 using this notebook</span><br><span class="line">Learn more about vision language models</span><br><span class="line">SmolVLM</span><br><span class="line">SmolVLM is a compact open multimodal model that accepts arbitrary sequences of image and text inputs to produce text outputs. Designed for efficiency, SmolVLM can answer questions about images, describe visual content, create stories grounded on multiple images, or function as a pure language model without visual inputs. Its lightweight architecture makes it suitable for on-device applications while maintaining strong performance on multimodal tasks.</span><br><span class="line"></span><br><span class="line">Model Summary</span><br><span class="line">Developed by: Hugging Face 🤗</span><br><span class="line">Model type: Multi-modal model (image+text)</span><br><span class="line">Language(s) (NLP): English</span><br><span class="line">License: Apache 2.0</span><br><span class="line">Architecture: Based on Idefics3 (see technical summary)</span><br><span class="line">Resources</span><br><span class="line">Demo: SmolVLM Demo</span><br><span class="line">Blog: Blog post</span><br><span class="line">Uses</span><br><span class="line">SmolVLM can be used for inference on multimodal (image + text) tasks where the input comprises text queries along with one or more images. Text and images can be interleaved arbitrarily, enabling tasks like image captioning, visual question answering, and storytelling based on visual content. The model does not support image generation.</span><br><span class="line"></span><br><span class="line">To fine-tune SmolVLM on a specific task, you can follow the fine-tuning tutorial.</span><br><span class="line"></span><br><span class="line">Technical Summary</span><br><span class="line">SmolVLM leverages the lightweight SmolLM2 language model to provide a compact yet powerful multimodal experience. It introduces several changes compared to previous Idefics models:</span><br><span class="line"></span><br><span class="line">Image compression: We introduce a more radical image compression compared to Idefics3 to enable the model to infer faster and use less RAM.</span><br><span class="line">Visual Token Encoding: SmolVLM uses 81 visual tokens to encode image patches of size 384×384. Larger images are divided into patches, each encoded separately, enhancing efficiency without compromising performance.</span><br><span class="line">More details about the training and architecture are available in our technical report.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://blackforestlabs.ai/flux-1-tools/</span><br><span class="line">Blackforest Labs</span><br><span class="line">Flux1 Tools: Hugging Face Transformers</span><br><span class="line">11/21/2024</span><br><span class="line"></span><br><span class="line">🔥📣 Introducing FLUX.1 Tools from Black Forest Labs</span><br><span class="line">👇Learn more and how you can explore them for free on Hugging Face Spaces</span><br><span class="line">FLUX.1 Tools -- Key highlights:</span><br><span class="line">&gt; A suite of models designed to add control and steerability to the base text-to-image model FLUX.1 released a few months ago</span><br><span class="line">&gt; These tools enables the modification / re-creation of real and generated images.</span><br><span class="line">&gt; This release consists of 4 open-access models within the FLUX.1 [dev] model series. Each with distinct feature set or capabilities:</span><br><span class="line">1️⃣ FLUX.1 Fill: SOTA inpainting and outpainting models</span><br><span class="line">2️⃣ FLUX.1 Depth: Models trained to enable structural guidance based on a depth map extracted from an input image and a text prompt.</span><br><span class="line">3️⃣ FLUX.1 Canny: Models trained to enable structural guidance based on canny edges extracted from an input image and a text prompt.</span><br><span class="line">4️⃣ FLUX.1 Redux: An adapter that allows mixing and recreating input images and text prompts.</span><br><span class="line"></span><br><span class="line">Introducing FLUX.1 Tools</span><br><span class="line">Nov 21, 2024</span><br><span class="line">—</span><br><span class="line"></span><br><span class="line">by</span><br><span class="line"></span><br><span class="line">BlackForestLabs</span><br><span class="line">in News.</span><br><span class="line">Today, we are excited to release FLUX.1 Tools, a suite of models designed to add control and steerability to our base text-to-image model FLUX.1, enabling the modification and re-creation of real and generated images. At release, FLUX.1 Tools consists of four distinct features that will be available as open-access models within the FLUX.1 [dev] model series, and in the BFL API supplementing FLUX.1 [pro]: </span><br><span class="line"></span><br><span class="line">FLUX.1 Fill: State-of-the-art inpainting and outpainting models, enabling editing and expansion of real and generated images given a text description and a binary mask.</span><br><span class="line">FLUX.1 Depth: Models trained to enable structural guidance based on a depth map extracted from an input image and a text prompt.</span><br><span class="line">FLUX.1 Canny: Models trained to enable structural guidance based on canny edges extracted from an input image and a text prompt.</span><br><span class="line">FLUX.1 Redux: An adapter that allows mixing and recreating input images and text prompts.</span><br><span class="line">This release reinforces our dual commitment: delivering cutting-edge open-weight models for the research community while offering best-in-class capabilities through our API. We release each tool in the BFL API as FLUX.1 [pro] variants and with inference code and weights available as guidance-distilled, open-access FLUX.1 [dev] variants. Additionally, we are excited that our released models will be available via our partners fal.ai, Replicate, Together.ai, Freepik and krea.ai.</span><br><span class="line"></span><br><span class="line">The following sections contain details on the new models, analyses on their performance and how they can be accessed. We are excited to see how the vibrant Flux ecosystem will be supplemented by our new tools.</span><br><span class="line"></span><br><span class="line">Inpainting and Outpainting with FLUX.1 Fill</span><br><span class="line">FLUX.1 Fill introduces advanced inpainting capabilities that surpass existing tools like Ideogram 2.0 and popular open-source variants such as AlimamaCreative’s FLUX-Controlnet-Inpainting. It allows for seamless edits that integrate naturally with existing images.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Additionally, FLUX.1 Fill supports outpainting, enabling the user to extend images beyond their original borders.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">We conduct a benchmark, publicly available here. The results show that Flux.1 Fill [pro] outperforms all other competing methods, making it the state-of-the-art inpainting model to date. Second is Flux.1 Fill [dev], outperforming proprietary solutions while being more efficient at inference.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Flux.1 Fill [dev] is available under the Flux Dev License, with</span><br><span class="line"></span><br><span class="line">Full model weights available on Hugging Face: [Fill]</span><br><span class="line">Inference code available on GitHub</span><br><span class="line">Flux.1 Fill [pro] is available in the [BFL API]</span><br><span class="line"></span><br><span class="line">Structural Conditioning with FLUX.1 Canny / Depth</span><br><span class="line">Structural conditioning uses canny edge or depth detection to maintain precise control during image transformations. By preserving the original image’s structure through edge or depth maps, users can make text-guided edits while keeping the core composition intact. This is particularly effective for retexturing images.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In our evaluations, benchmark available here, FLUX.1 Depth outperforms proprietary models like Midjourney ReTexture. In particular, FLUX.1 Depth [pro] offers higher output diversity, while the Dev version of FLUX.1 Depth delivers more consistent results in depth-aware tasks. For canny edge models, benchmark here,  FLUX.1 Canny [pro] is the best in class, followed by FLUX.1 Canny [dev].</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">FLUX.1 Canny / Depth are available in two versions: full models for maximum performance, and LoRA versions based on FLUX.1 [dev] for easier development.</span><br><span class="line"></span><br><span class="line">Flux Depth / Canny [dev] are available under the Flux Dev License with</span><br><span class="line"></span><br><span class="line">Full model weights available on Hugging Face: [Depth] [Canny] </span><br><span class="line">LoRA weights available on Hugging Face: [Depth] [Canny] </span><br><span class="line">Inference code available on GitHub</span><br><span class="line">Flux.1 Depth / Canny [pro] are available in the BFL API.</span><br><span class="line"></span><br><span class="line">Image Variation and Restyling with FLUX.1 Redux</span><br><span class="line">FLUX.1 Redux is an adapter for all FLUX.1 base models for image variation generation. Given an input image, FLUX.1 Redux can reproduce the image with slight variation, allowing to refine a given image.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">It naturally integrates into more complex workflows unlocking image restyling via prompt. Restyling is available through our API by providing an image plus a prompt. The feature is supported in our latest model FLUX1.1 [pro] Ultra, allowing for combining input images and text prompts to create high-quality 4-megapixel outputs with flexible aspect ratios.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Our benchmark demonstrates that FLUX.1 Redux achieves state-of-the-art performance in image variation.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Flux.1 Redux [dev] is available under the Flux Dev License with</span><br><span class="line"></span><br><span class="line">Model weights available on Hugging Face: [Redux]</span><br><span class="line">Inference code available on GitHub</span><br><span class="line">Flux.1 Redux supporting FLUX1.1 [pro] Ultra is available in the BFL API.</span><br><span class="line"></span><br><span class="line">We’re excited to see what the community is going to build with our new set of tools. Try our API at [api.bfl.ml].</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://developer.apple.com/machine-learning/models/</span><br><span class="line">Apple</span><br><span class="line">11/22/2024</span><br><span class="line">Pretty cool! Apple released blazingly fast CoreML models AND an iOS app to run them on iPhone! ⚡</span><br><span class="line">&gt; S0 matches OpenAI&#x27;s ViT-B/16 in zero-shot performance but is 4.8x faster and 2.8x smaller</span><br><span class="line">&gt; S2 outperforms SigLIP&#x27;s ViT-B/16 in zero-shot accuracy, being 2.3x faster, 2.1x smaller, and trained with 3x fewer samples</span><br><span class="line">&gt; MobileCLIP-B(LT) achieves 77.2% zero-shot ImageNet accuracy, surpassing DFN, SigLIP, and even OpenAI&#x27;s ViT-L/14@336</span><br><span class="line">&gt; Models on HF and code on GH 🤗</span><br><span class="line"></span><br><span class="line">Core ML Models</span><br><span class="line">Build intelligence into your apps using machine learning models from the research community designed for Core ML.</span><br><span class="line"></span><br><span class="line">Filter by keywords</span><br><span class="line">Models are in Core ML format and can be integrated into Xcode projects. You can select different versions of models to optimize for sizes and architectures.</span><br><span class="line"></span><br><span class="line">image</span><br><span class="line"></span><br><span class="line">FastViT</span><br><span class="line">Image Classification</span><br><span class="line"></span><br><span class="line">A Fast Hybrid Vision Transformer architecture trained to classify the dominant object in a camera frame or image.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Hide details</span><br><span class="line"></span><br><span class="line">Model Info</span><br><span class="line">Summary</span><br><span class="line">FastViT is a general-purpose, hybrid vision transformer model, trained on the ImageNet dataset, that provides a state-of-the-art accuracy/latency trade-off.</span><br><span class="line"></span><br><span class="line">The model&#x27;s high performance, low latency, and robustness against out-of-distribution samples result from three novel architectural strategies:</span><br><span class="line"></span><br><span class="line">Structural reparameterization</span><br><span class="line">Linear training-time overparameterization</span><br><span class="line">Use of large kernel convolutions</span><br><span class="line">FastViT consistently outperforms competing robust architectures on mobile and desktop GPU platforms across a wide range of computer vision tasks such as image classification, object detection, semantic segmentation, and 3D mesh regression.</span><br><span class="line"></span><br><span class="line">Use Cases</span><br><span class="line">Image classification, object detection, semantic segmentation, 3D mesh regression</span><br><span class="line"></span><br><span class="line">Links</span><br><span class="line">Source code in GitHub</span><br><span class="line">FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization</span><br><span class="line">Variants</span><br><span class="line">Model Name	Size	Action</span><br><span class="line">FastViTMA36F16.mlpackage	88.3MB	Download</span><br><span class="line">FastViTT8F16.mlpackage	8.2MB	Download</span><br><span class="line">FastViTMA36F16Headless.mlpackage	85.8MB	Download</span><br><span class="line">FastViTT8F16Headless.mlpackage	6.5MB	Download</span><br><span class="line">Model Details</span><br><span class="line">Variant	Parameters	Size	Weight Precision	Activation Precision</span><br><span class="line">T8	3.6M	7.8	Float16	Float16</span><br><span class="line">MA36	42.7M	84	Float16	Float16</span><br><span class="line">Inference Time</span><br><span class="line">Variant	Device	OS	Inference Time (ms)	Dominant Compute Unit</span><br><span class="line">T8	iPhone 12 Pro Max	17.5	0.79	Neural Engine</span><br><span class="line">T8	M3 Max	14.4	0.62	Neural Engine</span><br><span class="line">MA36	iPhone 12 Pro Max	18.0	4.50	Neural Engine</span><br><span class="line">MA36	M3 Max	15.0	2.99	Neural Engine</span><br><span class="line">Example Projects</span><br><span class="line">Classifying Images with Vision and Core ML</span><br><span class="line">Preprocess photos using the Vision framework and classify them with a Core ML model.</span><br><span class="line"></span><br><span class="line">image</span><br><span class="line"></span><br><span class="line">Depth Anything V2</span><br><span class="line">Depth Estimation</span><br><span class="line"></span><br><span class="line">The Depth Anything model performs monocular depth estimation.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Hide details</span><br><span class="line"></span><br><span class="line">Model Info</span><br><span class="line">Summary</span><br><span class="line">Depth Anything v2 is a foundation model for monocular depth estimation. It maintains the strengths and rectifies the weaknesses of the original Depth Anything by refining the powerful data curation engine and teacher-student pipeline.</span><br><span class="line"></span><br><span class="line">To train a teacher model, Depth Anything v2 uses purely synthetic, computer-generated images. This avoids problems created by using real images, which can limit monocular depth-estimation model performance due to noisy annotations and low resolution. The teacher model predicts depth information on unlabeled real images, and then uses only that new, pseudo-labeled data to train a student model. This helps avoid distribution shift between synthetic and real images.</span><br><span class="line"></span><br><span class="line">On the depth estimation task, the Depth Anything v2 model optimizes and outperforms v1 especially in terms of robustness, inference speed, and image depth properties like fine-grained details, transparent objects, reflections, and complex scenes. Its refined data curation approach results in competitive performance on standard datasets (including KITTI, NYU-D, Sintel, ETH3D, and DIODE) and a more than 9% accuracy improvement over v1 and other community models on the new DA-2k evaluation set built for depth estimation.</span><br><span class="line"></span><br><span class="line">Depth Anything v2 provides varied model scales and inference efficiency to support extensive applications and is generalizable for fine tuning to downstream tasks. It can be used in any application requiring depth estimation, such as 3D reconstruction, navigation, autonomous driving, and image or video generation.</span><br><span class="line"></span><br><span class="line">Use Cases</span><br><span class="line">Depth estimation, semantic segmentation</span><br><span class="line"></span><br><span class="line">Links</span><br><span class="line">Source code in GitHub</span><br><span class="line">Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</span><br><span class="line">Depth Anything V2</span><br><span class="line">Variants</span><br><span class="line">Model Name	Size	Action</span><br><span class="line">DepthAnythingV2SmallF16.mlpackage	49.8MB	Download</span><br><span class="line">DepthAnythingV2SmallF16P6.mlpackage	19MB	Download</span><br><span class="line">Model Details</span><br><span class="line">Variant	Parameters	Size	Weight Precision	Activation Precision</span><br><span class="line">F32	24.8M	99.2	Float32	Float32</span><br><span class="line">F16	24.8M	49.8	Float16	Float16</span><br><span class="line">Inference Time</span><br><span class="line">Variant	Device	OS	Inference Time (ms)	Dominant Compute Unit</span><br><span class="line">Small F16	iPhone 12 Pro Max	18.0	31.10	Neural Engine</span><br><span class="line">Small F16	iPhone 15 Pro Max	17.4	33.90	Neural Engine</span><br><span class="line">Small F16	MacBook Pro (M1 Max)	15.0	32.80	Neural Engine</span><br><span class="line">Small F16	MacBook Pro (M3 Max)	15.0	24.58	Neural Engine</span><br><span class="line">image</span><br><span class="line"></span><br><span class="line">DETR Resnet50 Semantic Segmentation</span><br><span class="line">Semantic Segmentation</span><br><span class="line"></span><br><span class="line">The DEtection TRansformer (DETR) model, trained for object detection and panoptic segmentation, configured to return semantic segmentation masks.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Hide details</span><br><span class="line"></span><br><span class="line">Model Info</span><br><span class="line">Summary</span><br><span class="line">The DETR model is an encoder/decoder transformer with a convolutional backbone trained on the COCO 2017 dataset. It blends a set of proven ML strategies to detect and classify objects in images more elegantly than standard object detectors can, while matching their performance.</span><br><span class="line"></span><br><span class="line">The model is trained with a loss function that performs bipartite matching between predicted and ground-truth objects. At inference time, DETR applies self-attention to an image globally to predict all objects at once. Thanks to global attention, the model outperforms standard object detectors on large objects but underperforms on small objects. Despite this limitation, DETR demonstrates accuracy and run-time performance on par with other highly optimized architectures when evaluated on the challenging COCO dataset.</span><br><span class="line"></span><br><span class="line">DETR can be easily reproduced in any framework that contains standard CNN and transformer classes. It can also be easily generalized to accommodate more complex tasks, such as panoptic segmentation and other tasks requiring a simple segmentation head trained on top of a pre-trained DETR.</span><br><span class="line"></span><br><span class="line">DETR avoids clunky surrogate tasks and hand-designed components that traditional architectures require to achieve acceptable performance and instead provides a conceptually simple, easily reproducible approach that streamlines the object detection pipeline.</span><br><span class="line"></span><br><span class="line">Use Cases</span><br><span class="line">Object detection, panoptic segmentation</span><br><span class="line"></span><br><span class="line">Links</span><br><span class="line">Source code in GitHub</span><br><span class="line">End-to-End Object Detection with Transformers</span><br><span class="line">Variants</span><br><span class="line">Model Name	Size	Action</span><br><span class="line">DETRResnet50SemanticSegmentationF16.mlpackage	85.5MB	Download</span><br><span class="line">DETRResnet50SemanticSegmentationF16P8.mlpackage	43.1MB	Download</span><br><span class="line">Model Details</span><br><span class="line">Variant	Parameters	Size	Weight Precision	Activation Precision</span><br><span class="line">F32	43M	171	Float32	Float32</span><br><span class="line">F16	43M	86	Float16	Float16</span><br><span class="line">Inference Time</span><br><span class="line">Variant	Device	OS	Inference Time (ms)	Dominant Compute Unit</span><br><span class="line">F16	iPhone 15 Pro Max	17.5	40	Neural Engine</span><br><span class="line">F16	MacBook Pro (M1 Max)	14.5	43	Neural Engine</span><br><span class="line">F16	iPhone 12 Pro Max	18.0	52	Neural Engine</span><br><span class="line">F16	MacBook Pro (M3 Max)	15.0	29	Neural Engine</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://alexgarcia.xyz/blog/2024/sqlite-vec-metadata-release/index.html</span><br><span class="line">sqlite-vec now supports metadata columns and filtering</span><br><span class="line">2024-11-20 by Alex Garcia</span><br><span class="line">SQLite is all you need! Big sqlite-vec update! 🚀 sqlite-vec is a plugin to support Vector Search in SQLite or LibSQL databases. v0.1.6 now allows storing non-vector data in vec0 virtual tables, enabling metadata conditioning and filtering! 🤯</span><br><span class="line">TL;DR:</span><br><span class="line">💡 Store metadata like user_id or created_at fields directly within vec0 virtual tables.</span><br><span class="line">🔍 Metadata columns can be used in WHERE clauses of KNN queries for filtering results based on non-vector data.</span><br><span class="line">🛠️ Introducing Partition Keys to shard the vector index and speed up queries</span><br><span class="line">📚 Auxiliary columns, prefixed with a &#x27;+&#x27;, allow storage of unindexed, SELECT-only metadata without requiring separate joins.</span><br><span class="line">🔜 Improve Quantization support with float16, float8, &quot;smarter&quot; binary quantization</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tl;dr — sqlite-vec, a SQLite extension for vector search, now supports metadata columns, auxiliary columns, and partitioning in vec0 virtual tables! You can use these to store metadata like user_id or created_at fields, add additional WHERE clauses in KNN queries, and make certain selective queries much faster. Try it out!</span><br><span class="line"></span><br><span class="line">As of the latest v0.1.6 release of sqlite-vec, you can now store non-vector data in vec0 virtual tables! For example:</span><br><span class="line"></span><br><span class="line">create virtual table vec_articles using vec0(</span><br><span class="line"></span><br><span class="line">  article_id integer primary key,</span><br><span class="line"></span><br><span class="line">  -- Vector text embedding of the `headline` column, with 384 dimensions</span><br><span class="line">  headline_embedding float[384],</span><br><span class="line"></span><br><span class="line">  -- Partition key, internally shard vector index on article published year</span><br><span class="line">  year integer partition key,</span><br><span class="line"></span><br><span class="line">  -- Metadata columns, can appear in `WHERE` clause of KNN queries</span><br><span class="line">  news_desk text,</span><br><span class="line">  word_count integer,</span><br><span class="line">  pub_date text,</span><br><span class="line"></span><br><span class="line">  -- Auxiliary columns, unindexed but fast lookups</span><br><span class="line">  +headline text,</span><br><span class="line">  +url text</span><br><span class="line">);</span><br><span class="line">Here we are storing a New York Time article headlines dataset from the past 30 years, where we embed the headlines with mixedbread-ai/mxbai-embed-xsmall-v1.</span><br><span class="line"></span><br><span class="line">If we wanted to see the closest related headlines to &#x27;pandemic&#x27; on article published in 2020 by the &#x27;Sports&#x27; or &#x27;Business&#x27; new desk with more than 500 but less than 1000 words, we can perform a KNN query like so:</span><br><span class="line"></span><br><span class="line">select</span><br><span class="line">  article_id,</span><br><span class="line">  headline,</span><br><span class="line">  news_desk,</span><br><span class="line">  word_count,</span><br><span class="line">  url,</span><br><span class="line">  pub_date,</span><br><span class="line">  distance</span><br><span class="line">from vec_articles</span><br><span class="line">where headline_embedding match lembed(&#x27;pandemic&#x27;)</span><br><span class="line">  and k = 8</span><br><span class="line">  and year = 2020</span><br><span class="line">  and news_desk in (&#x27;Sports&#x27;, &#x27;Business&#x27;)</span><br><span class="line">  and word_count between 500 and 1000;</span><br><span class="line">┌────────────┬──────────────────────────────────────────────────────────────────────┬───────────┬────────────┬─────────────────────────────┬──────────────────────────┬───────────┐</span><br><span class="line">│ article_id │ headline                                                             │ news_desk │ word_count │ url                         │ pub_date                 │ distance  │</span><br><span class="line">├────────────┼──────────────────────────────────────────────────────────────────────┼───────────┼────────────┼─────────────────────────────┼──────────────────────────┼───────────┤</span><br><span class="line">│    2911716 │ The Pandemic’s Economic Damage Is Growing                            │ Business  │        910 │ https://www.nytimes.com/... │ 2020-07-07T18:12:40+0000 │ 0.8928120 │</span><br><span class="line">│    2892929 │ As Coronavirus Spreads, Olympics Face Ticking Clock and a Tough Call │ Sports    │        987 │ https://www.nytimes.com/... │ 2020-03-06T01:34:36+0000 │ 0.9608180 │</span><br><span class="line">│    2932041 │ The Pandemic Is Already Affecting Next Year’s Sports Schedule        │ Sports    │        620 │ https://www.nytimes.com/... │ 2020-11-11T13:56:25+0000 │ 0.9802038 │</span><br><span class="line">│    2915381 │ The Week in Business: Getting Rich Off the Pandemic                  │ Business  │        814 │ https://www.nytimes.com/... │ 2020-08-02T11:00:03+0000 │ 1.0064692 │</span><br><span class="line">│    2896043 │ The Coronavirus and the Postponement of the Olympics, Explained      │ Sports    │        798 │ https://www.nytimes.com/... │ 2020-03-25T17:45:58+0000 │ 1.0115833 │</span><br><span class="line">│    2898566 │ Robots Welcome to Take Over, as Pandemic Accelerates Automation      │ Business  │        871 │ https://www.nytimes.com/... │ 2020-04-10T09:00:27+0000 │  1.019637 │</span><br><span class="line">│    2898239 │ The Pandemic Feeds Tech Companies’ Power                             │ Business  │        784 │ https://www.nytimes.com/... │ 2020-04-08T16:43:13+0000 │ 1.0200014 │</span><br><span class="line">│    2929224 │ In M.L.S., the Pandemic Changes the Playoff Math                     │ Sports    │        859 │ https://www.nytimes.com/... │ 2020-10-29T17:09:10+0000 │ 1.0238885 │</span><br><span class="line">└────────────┴──────────────────────────────────────────────────────────────────────┴───────────┴────────────┴─────────────────────────────┴──────────────────────────┴───────────┘</span><br><span class="line">Here we used sqlite-lembed to embed our query, but any other embeddings provider could be used!</span><br><span class="line"></span><br><span class="line">We can reference those metadata columns and parition key columns in the WHERE clause of the KNN query, and get the exact results we want!</span><br><span class="line"></span><br><span class="line">Now, what&#x27;s the difference between metadata, partition key, and auxiliary columns?</span><br><span class="line"></span><br><span class="line">¶ Metadata columns for WHERE clause filtering</span><br><span class="line">Metadata columns are declared with normal column declartions in the vec0 constructor. Metadata columns are stored and indexed alongside vectors, and can appear in the WHERE clause of KNN queries.</span><br><span class="line"></span><br><span class="line">create virtual table vec_articles using vec0(</span><br><span class="line">  article_id integer primary key,</span><br><span class="line">  headline_embedding float[384],</span><br><span class="line">  news_desk text,</span><br><span class="line">  word_count integer,</span><br><span class="line">  pub_date text</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">select</span><br><span class="line">  article_id,</span><br><span class="line">  headline,</span><br><span class="line">  news_desk,</span><br><span class="line">  word_count,</span><br><span class="line">  pub_date,</span><br><span class="line">  distance</span><br><span class="line">from vec_articles</span><br><span class="line">where headline_embedding match lembed(&#x27;new york city housing&#x27;)</span><br><span class="line">  and k = 20</span><br><span class="line">  and news_desk = &#x27;Metro&#x27;</span><br><span class="line">  and word_count &lt; 1000</span><br><span class="line">  and pub_date between &#x27;2004-01-20&#x27; and &#x27;2009-01-20&#x27;;</span><br><span class="line">┌────────────┬──────────────────────────────────────────────────────────────────────┬───────────┬────────────┬──────────────────────────┬────────────────────┐</span><br><span class="line">│ article_id │ headline                                                             │ news_desk │ word_count │ pub_date                 │ distance           │</span><br><span class="line">├────────────┼──────────────────────────────────────────────────────────────────────┼───────────┼────────────┼──────────────────────────┼────────────────────┤</span><br><span class="line">│    1717598 │ Manhattan: City to Expand Housing Program                            │ Metro     │         83 │ 2007-02-28T05:00:00+0000 │ 0.7736235857009888 │</span><br><span class="line">│    1607183 │ Manhattan: More Money for Housing                                    │ Metro     │         96 │ 2006-06-16T04:00:00+0000 │ 0.7818768620491028 │</span><br><span class="line">│                                                                                  ...                                                                       │</span><br><span class="line">│    1772158 │ Ask About New York Architecture, On Screen and Off                   │ Metro     │        241 │ 2007-09-17T18:25:57+0000 │  0.930429220199585 │</span><br><span class="line">│    1673007 │ Manhattan: City Balances Budget for 26th Year                        │ Metro     │         87 │ 2006-11-01T05:00:00+0000 │ 0.9327330589294434 │</span><br><span class="line">│    1616702 │ Little Shift in Prices of Manhattan Apartments                       │ Metro     │        615 │ 2006-07-06T04:00:00+0000 │ 0.9354249238967896 │</span><br><span class="line">└────────────┴──────────────────────────────────────────────────────────────────────┴───────────┴────────────┴──────────────────────────┴────────────────────┘</span><br><span class="line">There we retrieved the 20 most related article headlines to &#x27;new york city housing&#x27;, published by the &#x27;Metro&#x27; news desk, with less than 1000 words, published during the George W Bush administration.</span><br><span class="line"></span><br><span class="line">Metadata columns can be boolean, integer, floats, or text values. More types like BLOBs, dates, and UUID/ULIDs are coming soon!</span><br><span class="line"></span><br><span class="line">Only a subset of operators are supported during metadata filtering, including:</span><br><span class="line"></span><br><span class="line">Equality constraints, ie = and !=</span><br><span class="line">Comparison constraints, ie &gt;, &gt;=, &lt;, &lt;=</span><br><span class="line">column in (...) constraints, only on INTEGER and TEXT columns on SQLite 3.38 or above</span><br><span class="line">Notably absent: REGEXP, LIKE, GLOB, and other custom scalar functions. Also NULL values are not supported yet,</span><br><span class="line"></span><br><span class="line">¶ Partition keys for faster WHERE clause filtering</span><br><span class="line">Now the above query was actually a bit slow! There are 3 million rows in the table, and metadata filters need to visit every single row to do a comparison. Metadata comparison are quite fast and built for fast filtering, but they have their limits.</span><br><span class="line"></span><br><span class="line">But notice how we only wanted a small subset of values – between &#x27;2004-01-20&#x27; and &#x27;2009-01-20&#x27; is only 5 years out of 30 years of data. We can tell the vec0 virtual table to internally shard the vector index on a given key, using partition keys!</span><br><span class="line"></span><br><span class="line">create virtual table vec_articles using vec0(</span><br><span class="line">  article_id integer primary key,</span><br><span class="line">  headline_embedding float[384],</span><br><span class="line"></span><br><span class="line">  -- shard the vector index based on published year</span><br><span class="line">  year integer partition key,</span><br><span class="line"></span><br><span class="line">  news_desk text,</span><br><span class="line">  word_count integer,</span><br><span class="line">  pub_date text</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">select</span><br><span class="line">  article_id,</span><br><span class="line">  headline,</span><br><span class="line">  news_desk,</span><br><span class="line">  word_count,</span><br><span class="line">  pub_date,</span><br><span class="line">  distance</span><br><span class="line">from vec_articles</span><br><span class="line">where headline_embedding match lembed(&#x27;new york city housing&#x27;)</span><br><span class="line">  and k = 20</span><br><span class="line">  -- narrow search to these years only</span><br><span class="line">  and year between 2004 and 2009</span><br><span class="line">  and news_desk = &#x27;Metro&#x27;</span><br><span class="line">  and word_count &lt; 1000</span><br><span class="line">  -- finer filtering for exact dates we care about</span><br><span class="line">  and pub_date between &#x27;2004-01-20&#x27; and &#x27;2009-01-20&#x27;;</span><br><span class="line">This KNN query returns the same exact results as the one above - but is 3x faster! This is because internally, vectors are stored based on the year value of its row. In that KNN query, sqlite-vec will recognize constraints on partition keys, and quickly pre-filter rows before any vectors are compared.</span><br><span class="line"></span><br><span class="line">But beware! It&#x27;s easy to accidentally over-shard a vector index on the wrong values and cause performance issues. Partition keys are great for date-based items like year or month, particulary when each unique partition key value has 100&#x27;s or 1000&#x27;s of vectors. They are also great for user IDs or document IDs, for &quot;per-user&quot; or &quot;per-document&quot; vector indexes.</span><br><span class="line"></span><br><span class="line">Partition key columns can only be TEXT or INTEGER values, file an issue if you want to see some other type support. Currently column in (...) constraints are not supported for partition key columns, but will be soon!</span><br><span class="line"></span><br><span class="line">¶ Auxiliary columns</span><br><span class="line">Some columns never need to be indexed! You can always store addtionally SELECT-only metadata in separate tables and do a JOIN yourself, or you can use auxiliary columns:</span><br><span class="line"></span><br><span class="line">create virtual table vec_articles using vec0(</span><br><span class="line">  article_id integer primary key,</span><br><span class="line">  headline_embedding float[384],</span><br><span class="line">  +headline text,</span><br><span class="line">  +url text</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">select</span><br><span class="line">  article_id,</span><br><span class="line">  headline,</span><br><span class="line">  url,</span><br><span class="line">  distance</span><br><span class="line">from vec_articles</span><br><span class="line">where headline_embedding match lembed(&#x27;dodgers game&#x27;)</span><br><span class="line">  and k = 20;</span><br><span class="line">┌────────────┬─────────────────────────────────────────────────────────────────────────────────────┬───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬────────────────────┐</span><br><span class="line">│ article_id │ headline                                                                            │ url                                                                                                                               │ distance           │</span><br><span class="line">├────────────┼─────────────────────────────────────────────────────────────────────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼────────────────────┤</span><br><span class="line">│    1896278 │ Attention Dodgers Fans: There’s a Game Tonight                                      │ https://bats.blogs.nytimes.com/2008/10/15/attention-dodgers-fans-theres-a-game-tonight/                                           │ 0.6733786463737488 │</span><br><span class="line">│    2556896 │ Dodgers, in Flurry of Activity, Move to Revamp Their Infield                        │ https://www.nytimes.com/2014/12/11/sports/baseball/mlb-jimmy-rollins.html                                                         │ 0.7796685099601746 │</span><br><span class="line">│    2382487 │ Keeping Up With the Dodgers                                                         │ https://www.nytimes.com/2012/12/15/sports/angels-keeping-up-with-the-dodgers-leading-off.html                                     │ 0.7849781513214111 │</span><br><span class="line">│    2585169 │ New Life for the Dodgers’ Old Digs                                                  │ https://www.nytimes.com/slideshow/2015/04/19/sports/baseball/20150419DODGERTOWN.html                                              │ 0.7894293665885925 │</span><br><span class="line">│    1032111 │ Not Dodgers II, but It&#x27;s Baseball; The Game Is Back in Brooklyn, on a Smaller Scale │ https://www.nytimes.com/2001/06/23/nyregion/not-dodgers-ii-but-it-s-baseball-the-game-is-back-in-brooklyn-on-a-smaller-scale.html │ 0.7978747487068176 │</span><br><span class="line">└────────────┴─────────────────────────────────────────────────────────────────────────────────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────┘</span><br><span class="line">Auxiliary columns are denoted by a + prefix in the column definition, modeled after the same feature in the SQLite R*Tree extension. These columns are unindex, stored in a separate internal table and JOIN&#x27;ed at SELECT time. They cannot appear in a KNN WHERE query, as performance would worsen dramatically.</span><br><span class="line"></span><br><span class="line">But it saves you from dealing with additional JOINs yourself! They are especially great for longer TEXT or BLOB values.</span><br><span class="line"></span><br><span class="line">¶ Roadmap and the future of sqlite-vec</span><br><span class="line">Metadata column support is the biggest update to sqlite-vec since the initial v0.1.0 launch 3 months ago, but I have a lot planned for the project!</span><br><span class="line"></span><br><span class="line">First off: ANN indexes. The vec0 virtual table is brute-force only, which really slows down KNN queries on larger datasets. There are strategies like binary quantization or Matryoshka embeddings that can help, but sqlite-vec won&#x27;t be fast until ANN indexes are supported.</span><br><span class="line"></span><br><span class="line">I delayed working on ANN indexes until metadata columns were supported, because its much easier to build an ANN index with metaddata filtering on day 1 than it is to retroactively try to support them. I think this was the right call — metadata columns are hard! Follow issue #25 for future update on this!</span><br><span class="line"></span><br><span class="line">Next: Quantizers. Currently sqlite-vec only supported simple binary quantization and scalar quantization with int8 vectors. But I want to support float16, float8, &quot;smarter&quot; binary quantization (ie custom thresholds instead of just &gt; 0), and other techniques that have come about the last few months. This will also help support ANN indexes, as many of them rely on vector compression for fast queries.</span><br><span class="line"></span><br><span class="line">There&#x27;s also a ton of performance work that sqlite-vec needs, especially with these new metadata column features. This initial release was more of a &quot;make it work&quot; and not &quot;make it fast&quot;, so expect much faster metadata filtering in upcoming releases!</span><br><span class="line"></span><br><span class="line">Sister projects sqlite-lembed and sqlite-rembed also need a ton of love, they both have some older PRs that need merging. Expect releases of both of these projects very soon!</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2411.15100</span><br><span class="line">[Submitted on 22 Nov 2024]</span><br><span class="line">XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models</span><br><span class="line">Yixin Dong, Charlie F. Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, Tianqi Chen</span><br><span class="line">The applications of LLM Agents are becoming increasingly complex and diverse, leading to a high demand for structured outputs that can be parsed into code, structured function calls, and embodied agent commands. These developments bring significant demands for structured generation in LLM inference. Context-free grammar is a flexible approach to enable structured generation via constrained decoding. However, executing context-free grammar requires going through several stack states over all tokens in vocabulary during runtime, bringing non-negligible overhead for structured generation. In this paper, we propose XGrammar, a flexible and efficient structure generation engine for large language models. XGrammar accelerates context-free grammar execution by dividing the vocabulary into context-independent tokens that can be prechecked and context-dependent tokens that need to be interpreted during runtime. We further build transformations to expand the grammar context and reduce the number of context-independent tokens. Additionally, we build an efficient persistent stack to accelerate the context-dependent token checks. Finally, we co-design the grammar engine with LLM inference engine to overlap grammar computation with GPU executions. Evaluation results show that XGrammar can achieve up to 100x speedup over existing solutions. Combined with an LLM inference engine, it can generate near-zero overhead structure generation in end-to-end low-LLM serving.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">LFG!! XGrammar: a lightning fast, flexible, and portable engine for structured generation! 🔥</span><br><span class="line">&gt; Accurate JSON/grammar generation</span><br><span class="line">&gt; 3-10x speedup in latency</span><br><span class="line">&gt; 14x faster JSON-schema generation and up to 80x CFG-guided generation</span><br><span class="line">&gt; Now in MLC-LLM, SGLang, WebLLM; vLLM &amp; HuggingFace coming soon!</span><br><span class="line">GG MLC team is literally the best in the game and slept on! ⚡</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://blog.dottxt.co/say-what-you-mean.html</span><br><span class="line">11/24/24</span><br><span class="line"></span><br><span class="line">Does Structured Outputs hurt LLM performance? 🤔 The paper &quot;Let Me Speak Freely&quot; paper claimed that it does, but new experiments by .txt (team behind outlines) show it doesn’t if you do it correctly! 👀</span><br><span class="line">TL;DR;</span><br><span class="line">📈 The &quot;Let Me Speak Freely&quot; poor results came from weak prompts and wrong use of structured prompting</span><br><span class="line">📊 Structured outputs outperform unstructured on the test GSM8K: 0.78 vs 0.77, Last Letter: 0.77 vs 0.73, Shuffle Object: 0.44 vs 0.41</span><br><span class="line">🛠️ Key success criteria is to align your prompt, parser, and generator - it&#x27;s not just about using JSON mode</span><br><span class="line">📌 JSON generation requires careful prompt design, including specifying the desired schema.</span><br><span class="line">📝 Good prompts should contain enough information for a human to understand the task and expected response format</span><br><span class="line">🎯 JSON generation reached 77% accuracy vs the paper&#x27;s reported &lt;10%</span><br><span class="line">🔮 Examples in prompts should match the exact format expected in the actual tasks</span><br><span class="line">🧰 Structured generation works best when implemented as &quot;running our response parser as a generator&quot;</span><br><span class="line"></span><br><span class="line">No-structured outputs can actually improve LLM performance when implemented correctly.</span><br><span class="line"></span><br><span class="line">BY Will Kurt</span><br><span class="line"></span><br><span class="line">Say What You Mean: A Response to &#x27;Let Me Speak Freely&#x27;</span><br><span class="line">A recent paper from the research team at Appier, Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models, made some very serious accusations about the quality of LLM evaluation results when performing structured generation. Their (Tam, et al.) ultimate conclusion was:</span><br><span class="line"></span><br><span class="line">Our study reveals that structured generation constraints significantly impact LLM performance across various tasks.</span><br><span class="line"></span><br><span class="line">The source for this claim was three sets of evaluations they ran that purportedly showed worse performance for structured generation (”JSON-Mode” in the chart) compared with unstructured (”Natural Language”/NL in the chart). This chart (derived and rescaled from the original charts) shows the concerning performance:</span><br><span class="line"></span><br><span class="line">tam_et-al.png</span><br><span class="line"></span><br><span class="line">Figure 1: Original findings of Let Me Speak Freely</span><br><span class="line"></span><br><span class="line">We here at .txt have always seen structured generation outperform unstructured generation in our past experiments. Our past experiments were on problems with clear, LLM-compatible structure, but so were the tasks that Tam, et al focused on (in fact, we had already done a similar experiment with GSM8K using different models). So these results from Tam, et al were as surprising as they were concerning.</span><br><span class="line"></span><br><span class="line">After revisiting the above tasks with the same model (Llama-3-8B-instruct) we found that our results did not match those found in the paper, and reflected what we have previously seen. Diving deeper into the data and source code for the paper, we have determined there are several critical issues that led the authors to a fundamentally flawed conclusion.</span><br><span class="line"></span><br><span class="line">More than a rebuttal, the goal of this article is to share some of the knowledge we have acquired by working daily with structured generation. We&#x27;ll show where Tam, et al. made mistakes, but also provide insights into prompting for structured generation that should improve the response you get from LLMs even when you aren&#x27;t using structured generation.</span><br><span class="line"></span><br><span class="line">The Quick Rebuttal: Structured Generation improves performance.</span><br><span class="line">For those needing a quick answer as to whether or not structured generation hurts performance: the answer is a clear no. In the figure below you can see the results of a quick implementation of JSON generation for all of the concerning eval runs in the article.</span><br><span class="line"></span><br><span class="line">tasks_revisited.png</span><br><span class="line"></span><br><span class="line">Figure 2: Results of re-implementing the concerning evals.</span><br><span class="line"></span><br><span class="line">Task	Unstructured	Structured</span><br><span class="line">GSM8K	0.77	0.78</span><br><span class="line">Last Letter	0.73	0.77</span><br><span class="line">Shuffle Object	0.41	0.44</span><br><span class="line">While our unstructured results are in line with the paper, our structured results directly contradict the findings of the paper by showing that structured generation is an improvement across the board. Notebooks to reproduce these results can be found on github.</span><br><span class="line"></span><br><span class="line">Here are some of the key issues we found in the paper:</span><br><span class="line"></span><br><span class="line">The paper itself finds that structured generation has superior performance on a number of classification tasks.</span><br><span class="line">The prompts used for unstructured (NL) generation are markedly different than the ones used for structured generation, so the comparisons are not apples-to-apples to begin with.</span><br><span class="line">The structured generation prompts do not provide the model with adequate information to solve the task, this leads to particularly poor performance for the ‘json-mode’ examples.</span><br><span class="line">The real meat of the paper is actually about parsing the results of one LLM with a second LLM. The authors refer to this as the “Perfect Text Parser”, we will refer to it as the “AI parser” (for reasons we&#x27;ll clarify soon).</span><br><span class="line">The paper confuses structured generation with JSON-mode1, although independent runs of these evals show that “JSON-mode” yields better results than unstructured generation.</span><br><span class="line">An apt analogy would be to programming language benchmarking: it would be easy to write a paper showing that Rust performs worse than Python simply by writting terrible Rust code. Any sensible readers of such a paper would quickly realize the results reflected the skills of the author much more than the capability of the tool. But it is genuinely challenging to write optimally performant code, just like it is difficult to ensure your evaluations are truly representative of the task you are trying to understand and measure.</span><br><span class="line"></span><br><span class="line">All that said, this paper does provide a great opportunity to dive a bit deeper into what structured generation is and how to get the best performance out of it.</span><br><span class="line"></span><br><span class="line">Task Overview - Last Letter</span><br><span class="line">Here we’ll be focusing on the task that Speak Freely claimed structured generation did the most poorly on: Last Letter.</span><br><span class="line"></span><br><span class="line">In this task the model is presented with a list of 4 names such as:</span><br><span class="line"></span><br><span class="line">Ian Peter Bernard Stephen</span><br><span class="line">And then the model must concatenate the last letter of each. The answer for that example would be: NRDN</span><br><span class="line"></span><br><span class="line">The evaluation consists of 150 questions in the “test” set, and 350 in the &quot;train&quot; set. The paper uses only the 150 “test” question, so we will as well (though all findings here also hold for the full set).</span><br><span class="line"></span><br><span class="line">Why Do We Need Structured Generation? Parsing Results!</span><br><span class="line">One of the more interesting parts of the approach in the paper, that honestly should have been the focus, is the so called “Perfect Text Parser” used to extract the answer from the initial model response. Typically most evaluation frameworks use a simple, clearly defined regular expression when parsing responses, however Tam, et al use claude-3-haiku-20240307 to parse the response from the generated output of model. This means that two models are actually used for each answer. In the paper they refer to this as the “Perfect Text Parser”. It turns out the choice of the word &quot;perfect&quot; is a bit misleading, so we’ll be referring to this as the “AI Parser”.</span><br><span class="line"></span><br><span class="line">It’s important to note this non-standard method for transforming the response because the primary reason we use structured generation when working with LLMs it to guarantee the format of the response for easy parsing. That is, parsing and structured generation go hand-in-hand. While there are quite a few issues with this paper, the use of the AI parser is quite interesting and worth exploring in the context of structured generation. We’ll take a look at AI parsing vs structured generation, which will help us gain a stronger understanding of just how powerful structured generation is.</span><br><span class="line"></span><br><span class="line">Issue #1: The AI Parser</span><br><span class="line">To better understand the impact the AI Parser makes, we’ll take a deep dive into one of their recorded examples. Thankfully, Tam, et al. did provide extensive data (12GB!) from their experiments. These experiments are all sorted by the model and the prompt template used. We’ll be taking a deep dive into the lasterletter-t3-f3 prompt template using meta-llama/Meta-Llama-3-8B-Instruct specifically looking at the 1-shot example.</span><br><span class="line"></span><br><span class="line">Let’s start by looking at what the prompt instructs the model to do in the best performing Natural Language (NL) format:</span><br><span class="line"></span><br><span class="line">Follow the instruction to complete the task:</span><br><span class="line">String manipulation task:</span><br><span class="line">• Given: A sequence of words</span><br><span class="line">• Required: A new string made from the last letter of each word</span><br><span class="line">• Process: Think step by step to solve this challenge</span><br><span class="line">Note: Ensure you&#x27;ve read the question thoroughly before beginning.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Instruct : Provide your output in the following text format:</span><br><span class="line">Answer: &lt;think step by step&gt;. The final answer is &lt;answer&gt;</span><br><span class="line">Notice that the format of the response is explicitly described: The final answer is &lt;answer&gt;. This means, if the model were to adhere to our prompt, we should be able to parse all answers with this simple regex:</span><br><span class="line"></span><br><span class="line">answer_regex = r&#x27;answer is ([A-Za-z]&#123;4&#125;)&#x27;</span><br><span class="line">To see the impact of using the AI Parser we can iterate through the recorded results of the experiment (found in the file text_llama-3-8b-instruct_shots_1.jsonl). To be clear, we aren’t running any models right now, just seeing how different parsing methods impact the final score for an existing experiment.</span><br><span class="line"></span><br><span class="line">We can immediately see there is a discrepancy between the strict regex parsing and the AI parsing:</span><br><span class="line"></span><br><span class="line">recorded_data_strict_vs_ai.png</span><br><span class="line"></span><br><span class="line">Figure 3: Comparing a strict regex parser with the AI parser.</span><br><span class="line"></span><br><span class="line">So what’s happening here?</span><br><span class="line"></span><br><span class="line">It turns out that AI parser is doing a lot of the heavy lifting for the NL format. By going over the results we can see there were many cases where our strict regex failed to capture the (arguably) correct response from the model. Here are a few examples that the AI Parser was able to correctly recover that didn&#x27;t match our regex:</span><br><span class="line"></span><br><span class="line">The answer is e-S-S-E. → ESSE</span><br><span class="line">The answer is AAA R. → AAAR</span><br><span class="line">The answer is &quot;reye&quot;. → REYE</span><br><span class="line">The final answer is: YOOI → YOOI</span><br><span class="line">Clearly our strict regex was indeed a bit too strict. All of these answers seem reasonable to me, there are a few cases in the full data set where one might disagree, but overall these seem acceptable responses. In lieu of an extra call to a more powerful model, a reasonable solution is to simply extend our regular expression so that it covers these cases. We’ll add the following alternate regexes to solve the sample cases we’ve found:</span><br><span class="line"></span><br><span class="line">alt_regex_1 = r&#x27;answer is ([A-Za-z]-[A-Za-z]-[A-Za-z]-[A-Za-z])&#x27;</span><br><span class="line">alt_regex_2 = r&#x27;answer is:? &quot;?([A-Za-z] ?[A-Za-z] ?[A-Za-z] ?[A-Za-z])&quot;?&#x27;</span><br><span class="line">alt_regex_3 = r&#x27;Concatenating them is &quot;([A-Za-z]&#123;4&#125;)&quot;&#x27;</span><br><span class="line">alt_regex_4 = r&quot;answer is:? ([A-Za-z]&#x27;?[A-Za-z]&#x27;?[A-Za-z]&#x27;?[A-Za-z])&quot;</span><br><span class="line">It turns out that these cover the full set of cases we missed, no need for a separate model at all! If we use this combination of regexes we get the following results from parsing:</span><br><span class="line"></span><br><span class="line">strict_vs_ai_vs_flexible.png</span><br><span class="line"></span><br><span class="line">Figure 4: Comparing a strict regex parser, the AI parser and a more flexible regex parser.</span><br><span class="line"></span><br><span class="line">Parser	Accuracy</span><br><span class="line">strict regex	0.35</span><br><span class="line">AI parser	0.57</span><br><span class="line">flexible regex	0.61</span><br><span class="line">Surprisingly, our hand curated list of regexes (which didn’t take too much time to write) outperforms the AI parser for this data set! It’s worth mentioning that the main selling point of structured generation is to not have to worry about this parsing. However it is a useful exercise that shows us that the AI parser is, in fact, not the “perfect” text parser: running our hand crafted flexible regex parser outperforms a call to Claude (and is much faster and cheaper!).</span><br><span class="line"></span><br><span class="line">Reproducing These Results - Unstructured</span><br><span class="line">Now let’s go ahead and run this using outlines.generate.text with the exact same prompt to see what we get. We will make a small modification to the one-shot example used in the prompt. Tam, et al’s examples only use two names, but all the questions use four. In my experience, even without structured generation, it’s always important that your examples match the format you are looking for. So we’ve modified the prompt example to include all four names.</span><br><span class="line"></span><br><span class="line">When running using outlines.generate.text we get the following results compared to the original results reported in the data:</span><br><span class="line"></span><br><span class="line">repro_nl_1.png</span><br><span class="line"></span><br><span class="line">Figure 5: Reproducing the original NL results.</span><br><span class="line"></span><br><span class="line">As you can see our results, while slightly better, are more or less in line with what the recorded results show. We’re also not aiming for perfect replication, just making sure our results are on the same page.</span><br><span class="line"></span><br><span class="line">Since cleaning up the bad one-shot example, the gap in the difference between the parsers is also smaller. We’ve seen in the past that it seems what models are learning from the example cases is in fact the structure of the problem, so it’s not surprising that giving better examples of structure improves adherence to that structure.</span><br><span class="line"></span><br><span class="line">Now we can really test the impact structured generation has on performance.</span><br><span class="line"></span><br><span class="line">Anything You Can Parse, You Can Generate</span><br><span class="line">The reason we focus on the AI parser is that understanding how you parse the response from an LLM is the key to really understanding structured generation. It’s a common misunderstanding (one made by the paper) to think that structured generation is merely another name for JSON-mode (or YAML-mode, XML-mode, etc). A better mental model for structured generation is: running our response parser as a generator.</span><br><span class="line"></span><br><span class="line">To make this clear, when running structured generation on the prompt used, we are simply going to add structure for the reasoning step, and then append our answer regex to that. This allows a unification of the prompt, the parser, and the generator. That’s the secret to why structured generation is so powerful.</span><br><span class="line"></span><br><span class="line">Let’s define our structure and see how it does. Here’s a regex that represents the “chain-of-thought” reasoning structure in the answer (which is also found in the prompt itself):</span><br><span class="line"></span><br><span class="line">cot_regex = r&#x27;Answer: T[\w \\&quot;,\\.]&#123;30,250&#125;. The &#x27;</span><br><span class="line">This will allow the model “think” for between 30 and 250 characters, then start it’s answer. To complete our structure we just use our existing answer_regex to make our generator:</span><br><span class="line"></span><br><span class="line">struct_strict = outlines.generate.regex(</span><br><span class="line">    model,</span><br><span class="line">    cot_regex + answer_regex, </span><br><span class="line">    sampler=greedy())</span><br><span class="line">That’s it! We’re going to use our default strict regex because the entire point of structured generation is not to worry about parsing out output! There’s no need to use the more flexible regex since the model will only output what we want. Let’s see how it does:</span><br><span class="line"></span><br><span class="line">reproducing_nl_results.png</span><br><span class="line"></span><br><span class="line">Figure 6: Reproducing the original NL results with structured generation.</span><br><span class="line"></span><br><span class="line">method	Accuracy</span><br><span class="line">original	0.57</span><br><span class="line">unstructured (strict)	0.65</span><br><span class="line">unstructured (flexible)	0.66</span><br><span class="line">structured	0.68</span><br><span class="line">Consistent with all of our past findings, structured generation outperforms unstructured generation.</span><br><span class="line"></span><br><span class="line">What about JSON?</span><br><span class="line">With some insight into how to properly perform structured generation, let’s try to figure out what went wrong with the JSON results. After all, if correct, the results in this chart are concerning:</span><br><span class="line"></span><br><span class="line">bad_json.png</span><br><span class="line"></span><br><span class="line">Figure 7: Original chart showing poor JSON-mode performance on Last Letter.</span><br><span class="line"></span><br><span class="line">In the chart above, it looks like the structured generation (JSON-mode) is getting an awful &lt; 10% accuracy compared to the unstructured (NL) result that’s getting ~70% accuracy. The NL result is essentially what we reproduced previously, so that result is in line with what we reproduced, but that 10% doesn’t match with what we saw. Maybe it’s an issue with requiring the model to respond in JSON?</span><br><span class="line"></span><br><span class="line">Issue #2: Bad Prompting</span><br><span class="line">As mentioned, a major issue with Speak Freely is that the paper uses different prompts for structured generation and unstructured generation. In our example, to evaluate the performance of structured generation, we compared results with the same prompt. This is the only honest way to make any statements about structured vs unstructured performance.</span><br><span class="line"></span><br><span class="line">Because of this the first place we should look for trouble is in the prompt used for the JSON-mode evaluations. So let’s take a look at the recorded data that matches closely to these results in the chart (found in lastletter-t2-structure/struct_llama-3-8b-instruct_shots_0.jsonl). Here is an example of the prompt used:</span><br><span class="line"></span><br><span class="line">Follow the instruction to complete the task:</span><br><span class="line">Read carefully for each of the last question and think step </span><br><span class="line">by step before answering. You are given a string of words </span><br><span class="line">and you need to take the last letter of each words and concate them</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Instruct : You must use the tool</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Question: Take the last letters of each words in </span><br><span class="line">&quot;Britt Tamara Elvis Nayeli&quot; and concatenate them.</span><br><span class="line">This prompt needs substantial improvement before it can be used to properly evaluate the task at hand! One practice I encourage when writing prompts is to always ask yourself “does this prompt contain enough information that a reasonably well informed human could answer the question correctly?” Reading this prompt is not obvious to me that:</span><br><span class="line"></span><br><span class="line">The answer must be JSON (JSON isn’t mentioned at all)</span><br><span class="line">Even if you did guess JSON for the response, what is the schema you should respond in?</span><br><span class="line">While tool use is mentioned, the prompt doesn’t mention the tools to use! There is no way an LLM could infer what it’s supposed to be doing. Structured generation is incredible, but it can’t magically make a model understand what you want any more than throwing rail road tracks in your backyard will make your home a convenient train stop.</span><br><span class="line"></span><br><span class="line">Proper Prompting for JSON</span><br><span class="line">We discussed earlier that structured generation is not the same thing as JSON-mode, but sometimes we do want JSON. To understand where Speak Freely went wrong, let’s walk through the way to do structured generation correctly. We’ll start by using an instruct prompt such as the following:</span><br><span class="line"></span><br><span class="line">&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;</span><br><span class="line"></span><br><span class="line">You are an expert in solving simple word puzzles using reasoning steps. Your specific</span><br><span class="line">task is going to be to take a list of 4 names and reason about the last letter of each .,</span><br><span class="line">then you will concatenate those letters into a word. The Question will be plaintest from the user</span><br><span class="line">and response will be formatted as JSON below:</span><br><span class="line"></span><br><span class="line">&#123;&quot;reasoning&quot;: &lt;reasoning about the answer&gt;, &quot;answer&quot;: &lt;final answer&gt;&#125;&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</span><br><span class="line"></span><br><span class="line">Question: Take the last letters of each words in &#x27;Ian Peter Bernard Stephen&#x27; and concatenate them.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;</span><br><span class="line"></span><br><span class="line">&#123;&quot;reasoning&quot;: &quot;The last letter of &#x27;Ian&#x27; is &#x27;N&#x27;, the last letter of &#x27;Peter&#x27; is &#x27;R&#x27;, the last letter of &#x27;Bernard&#x27; is &#x27;D&#x27;, and the last letter of &#x27;Stephen&#x27; is &#x27;N&#x27;. Therefore, the answer is &#x27;NRDN&#x27;.&quot;, &quot;answer&quot;: &quot;NRDN&quot;&#125;&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</span><br><span class="line"></span><br><span class="line">Question: Take the last letters of each words in &quot;Britt Tamara Elvis Nayeli&quot; and concatenate them.&quot;,&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;</span><br><span class="line"></span><br><span class="line">&lt;|eot_id|&gt;</span><br><span class="line">Here are a few things that make this prompt good.</span><br><span class="line"></span><br><span class="line">Using the proper instruct format for our model (using apply_chat_template).</span><br><span class="line">Provide an example that uses the correct structure and matches our problem.</span><br><span class="line">End with an empty “assistant” prompt so that the response will start with our structure.</span><br><span class="line">Structured generation works perfectly well with continuation models, but since we’re using an instruct model we should use the instruct prompt format for best results (this is issue 1). The most important thing we’ve done is 2 which is to show the model the structure we want it to follow. Item 3 is a small but important detail: instruct prompts are trained to alternate between the ‘user’ role and the ‘assistant’ role, if we don’t start with the empty assistant response the model will want to start with assistant... rather than our desired structure &#123;&quot;reasoning&quot;: ....</span><br><span class="line"></span><br><span class="line">It can be quite helpful when writing your prompt to also generate unstructured samples to see how well the model is following the desired formatting behavior.</span><br><span class="line"></span><br><span class="line">Defining Our Structure</span><br><span class="line">Next we want to define our structure (though really this should go hand in hand with the prompt). For this task we’ll use a simple Pydantic model:</span><br><span class="line"></span><br><span class="line">class Response(BaseModel):</span><br><span class="line">    reasoning: constr(max_length=250)</span><br><span class="line">    answer: str = Field(pattern=r&#x27;[A-Z]&#123;4&#125;&#x27;)</span><br><span class="line">We’re constraining the reasoning step here to 250 characters to make sure it doesn’t take too long to reason, and also constraining our answer to only valid possible responses of 4 letters.</span><br><span class="line"></span><br><span class="line">A really important step in the process is to verify the prompt contains our structure. The entire point of the prompt is to prime our LLM for success, if we aren’t showing it the exact structure we want, the model will have to work harder to get the correct answer. Here’s the code for our ensuring the prompt matches the structure:</span><br><span class="line"></span><br><span class="line">from outlines.fsm.json_schema import build_regex_from_schema</span><br><span class="line">schema_regex = build_regex_from_schema(Response.schema_json())</span><br><span class="line"></span><br><span class="line">example_prompt = create_prompt(all_evals[5][&#x27;question&#x27;], tokenizer)</span><br><span class="line">re.search(schema_regex, example_prompt)</span><br><span class="line">After verifying this we’re good to go! It’s worth pointing out that doing all these thing will also generally improve your unstructured results.</span><br><span class="line"></span><br><span class="line">JSON Results</span><br><span class="line">Now let’s run a proper apples-to-apples comparison of structured generation to unstructured. Here is the outcome of running this eval:</span><br><span class="line"></span><br><span class="line">json_results.png</span><br><span class="line"></span><br><span class="line">Figure 8: Results of comparing structured and unstrutured JSON generation using the same prompt (compared with NL structured prompt).</span><br><span class="line"></span><br><span class="line">method	Accuracy</span><br><span class="line">NL (struct)	0.68</span><br><span class="line">JSON (unstruct)	0.73</span><br><span class="line">JSON (struct)	0.77</span><br><span class="line">Once again we see that structured generation outperforms unstructured generation. It’s also worth noting that our unstructured JSON result (at 73% accuracy) outperformed our structured natural language (68% accuracy) result, but our overall winner is structured JSON with 77% accuracy!</span><br><span class="line"></span><br><span class="line">Here we can see why it’s essential to compare apples to apples when understanding the performance of structured generation. If, for example, we compared the JSON prompt for unstructured and the NL for structured, we would incorrectly conclude the structured generation is slightly worse, when the issue is really the prompt.</span><br><span class="line"></span><br><span class="line">Conclusion</span><br><span class="line">We here at .txt are passionate about structured generation, and truly believe it has the potential to transform the work being done with LLMs in profound ways. That’s precisely why we take claims that structure generation has adverse effects on LLM output very seriously. As a community we are all still learning how to get the most out of LLMs and working hard to advance this promising area of research. Everyone struggles at times to get LLMs to perform. If you are experiencing problems with structured generation, don’t hesitate to reach out. We would love to understand more about the areas where structured generation might need improvement, and get those areas fixed.</span><br><span class="line"></span><br><span class="line">That is why it is so disappointing when other researchers take less care and end up spreading misinformation, that we then have to do the work to correct. If you’re interested in driving the future of LLMs, we’re more than happy to help you on your journey.</span><br><span class="line"></span><br><span class="line">But, if you are going to publish a damning critique of structured generation, just make sure you put the same effort in to understanding the problem as we would. Or, perhaps better expressed by Omar Little from the Wire:</span><br><span class="line"></span><br><span class="line">&quot;Come at the King, you best not miss.&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>기술적으로 최대한 자세하게 적어. 9개의 기사가 있고 하나도 빼먹지 말고 적어.</p>
</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-11-28</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/11/28/2024-11-28-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 11월 28일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/12/05/2024-12-5-AI-NEWS/" title="2024년 12월 5일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/11/21/2024-11-21-AI-NEWS/" title="2024년 11월 21일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>