<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 12월 5일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="Amazon에서는 새로운 파운데이션 모델 Amazon Nova를 발표하였습니다. 이 모델은 Amazon Bedrock을 통해 독점적으로 제공되며, 텍스트부터 비디오 생성까지 다양한 기능을 지원합니다.
Meta AI와 난양기술대학교 연구팀은 EfficientTAM을 공개"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 12월 5일 AI 소식</a></h3></div><div class="post-content"><ul>
<li><strong>Amazon</strong>에서는 새로운 파운데이션 모델 <strong>Amazon Nova</strong>를 발표하였습니다. 이 모델은 Amazon Bedrock을 통해 독점적으로 제공되며, 텍스트부터 비디오 생성까지 다양한 기능을 지원합니다.</li>
<li><strong>Meta AI</strong>와 <strong>난양기술대학교</strong> 연구팀은 <strong>EfficientTAM</strong>을 공개하였습니다. 이는 경량의 Vision Transformer를 사용하여 비디오 객체 분할 및 추적의 효율성을 높였습니다.</li>
<li><strong>IDEA Research</strong>에서는 <strong>DINO-X</strong>를 발표하였습니다. 이는 개방형 객체 감지와 이해를 위한 세계 최고의 비전 모델로, 다양한 입력 프롬프트와 다중 레벨의 출력 표현을 지원합니다.</li>
<li><strong>베이징교통대학</strong> 연구팀은 <strong>O1-CODER</strong>를 소개하였습니다. 이는 OpenAI의 O1 모델을 복제하여 코딩 작업에 특화된 시스템입니다.</li>
<li><strong>Google DeepMind</strong>에서는 <strong>Generative Omnimatte</strong>를 발표하였습니다. 이 기술은 비디오를 레이어로 분해하여 객체와 그에 따른 효과를 분리합니다.</li>
<li><strong>Tencent</strong> 연구팀은 <strong>HiFiVFS</strong>를 선보였습니다. 이는 고품질의 비디오 얼굴 교체를 가능하게 하는 프레임워크로, 복잡한 조건에서도 우수한 성능을 보여줍니다.</li>
<li><strong>싱가포르 국립대학교</strong>와 <strong>Microsoft</strong> 연구팀은 <strong>ShowUI</strong>를 공개하였습니다. 이는 어떤 UI에서도 동작할 수 있는 작은 종단 간 에이전트입니다.</li>
<li><strong>Pydantic</strong> 팀은 <strong>PydanticAI</strong>를 출시하였습니다. 이는 Pydantic을 활용하여 LLM과 함께 사용할 수 있는 에이전트 프레임워크입니다.</li>
<li><strong>Quivr</strong>에서는 <strong>MegaParse</strong>를 소개하였습니다. 이는 모든 유형의 문서를 처리할 수 있는 강력한 파서입니다.</li>
<li><strong>Google Research</strong>에서는 <strong>LLM 임베딩을 활용한 회귀 분석</strong>에 대한 연구를 발표하였습니다.</li>
<li><strong>Adobe</strong> 연구팀은 <strong>DynaSaur</strong>를 발표하였습니다. 이는 사전 정의된 액션을 넘어서는 대형 언어 모델 에이전트입니다.</li>
<li><strong>ConsisID</strong>는 일관성 있는 아이덴티티 보존 텍스트-비디오 생성 모델을 제안하였습니다.</li>
<li><strong>Hugging Face</strong>에서는 <strong>ShowUI-2B</strong> 모델을 공개하였습니다.</li>
<li><strong>DataLab</strong>은 LLM을 활용한 비즈니스 인텔리전스를 위한 통합 플랫폼을 소개하였습니다.</li>
</ul>
<hr>
<h3 id="Amazon-Amazon-Nova-파운데이션-모델-발표"><a href="#Amazon-Amazon-Nova-파운데이션-모델-발표" class="headerlink" title="Amazon, Amazon Nova 파운데이션 모델 발표"></a>Amazon, Amazon Nova 파운데이션 모델 발표</h3><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/de/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/">링크</a>, 2024년 12월 3일</p>
<ul>
<li>Amazon은 새로운 세대의 최첨단 파운데이션 모델인 <strong>Amazon Nova</strong>를 발표함.</li>
<li><strong>Amazon Nova</strong>는 Amazon Bedrock을 통해 독점적으로 제공되며, 다양한 생성 AI 작업을 지원함.</li>
<li><strong>Understanding Models</strong>와 <strong>Creative Content Generation Models</strong>의 두 가지 카테고리로 구성됨.</li>
<li><strong>Understanding Models</strong>는 텍스트, 이미지, 비디오 입력을 받아 텍스트 출력을 생성하며, <strong>Micro</strong>, <strong>Lite</strong>, <strong>Pro</strong>, <strong>Premier</strong>의 네 가지 모델로 구성됨.<ul>
<li><strong>Amazon Nova Micro</strong>: 텍스트 전용 모델로, 128K 토큰의 컨텍스트 길이를 지원하며, 낮은 지연 시간과 비용으로 텍스트 요약, 번역, 분류 등에 최적화됨.</li>
<li><strong>Amazon Nova Lite</strong>: 멀티모달 모델로, 최대 300K 토큰의 컨텍스트 길이를 지원하며, 이미지, 비디오, 텍스트 입력을 처리하여 텍스트 출력을 생성함.</li>
<li><strong>Amazon Nova Pro</strong>: 고성능 멀티모달 모델로, 복잡한 추론 및 에이전트 워크플로우를 지원하며, 300K 토큰의 컨텍스트 길이를 가짐.</li>
<li><strong>Amazon Nova Premier</strong>: 가장 강력한 멀티모달 모델로, 2025년에 출시 예정임.</li>
</ul>
</li>
<li><strong>Creative Content Generation Models</strong>는 텍스트 및 이미지 입력을 받아 이미지 또는 비디오 출력을 생성함.<ul>
<li><strong>Amazon Nova Canvas</strong>: 스튜디오 품질의 이미지를 생성하며, 인페인팅, 아웃페인팅, 배경 제거 등의 편집 기능을 제공함.</li>
<li><strong>Amazon Nova Reel</strong>: 텍스트 프롬프트와 이미지를 통해 전문적인 품질의 비디오를 생성함.</li>
</ul>
</li>
<li>모든 모델은 내장된 안전 제어 및 워터마크 기능을 포함하여 책임 있는 AI 사용을 촉진함.</li>
<li>Amazon Bedrock을 통해 사용자 지정 및 미세 조정이 가능하며, Retrieval-Augmented Generation(RAG), 함수 호출, 에이전트 응용 프로그램에 탁월한 성능을 보임.</li>
<li>현재 미국 내 AWS 리전에서 제공되며, 가격은 모델과 사용량에 따라 다름.</li>
</ul>
<h3 id="Meta-AI-및-난양기술대학교-EfficientTAM-발표"><a href="#Meta-AI-및-난양기술대학교-EfficientTAM-발표" class="headerlink" title="Meta AI 및 난양기술대학교, EfficientTAM 발표"></a>Meta AI 및 난양기술대학교, EfficientTAM 발표</h3><p><a target="_blank" rel="noopener" href="https://yformer.github.io/efficient-track-anything/">링크</a>, 2024년 12월 5일</p>
<ul>
<li>**EfficientTAM(Efficient Track Anything Model)**을 공개함.</li>
<li>경량의 Vision Transformer(ViT)를 사용하여 비디오 객체 분할 및 추적의 효율성을 향상시킴.</li>
<li>iPhone 15에서 초당 10프레임 이상의 실시간 성능을 달성함.</li>
<li>**SAM 2(Segment Anything Model 2)**와 비교하여 유사한 성능을 유지하면서도 속도와 효율성을 향상시킴.</li>
<li><strong>효율적인 메모리 크로스 어텐션</strong>을 도입하여 메모리 계산을 최적화하고, 메모리 공간 임베딩의 강력한 지역성을 활용함.</li>
<li>여러 비디오 세분화 벤치마크(예: Semi-supervised VOS, Promptable Video Segmentation)에서 우수한 성능을 입증함.</li>
<li>다양한 프롬프트(포인트, 박스, 세그먼트 등)를 지원하며, 이미지 세분화에서도 향상된 성능을 보임.</li>
<li>경량의 ViT와 효율적인 메모리 모듈을 결합하여 모델 크기와 지연 시간을 줄임.</li>
</ul>
<h3 id="IDEA-Research-DINO-X-발표"><a href="#IDEA-Research-DINO-X-발표" class="headerlink" title="IDEA Research, DINO-X 발표"></a>IDEA Research, DINO-X 발표</h3><p><a target="_blank" rel="noopener" href="https://deepdataspace.com/home">링크</a>, 2024년 11월 25일</p>
<ul>
<li><strong>DINO-X</strong>는 개방형 객체 감지 및 이해를 위한 최첨단 비전 모델임.</li>
<li>이전 버전인 <strong>Grounding DINO 1.5</strong> 및 <strong>1.6</strong> 대비 성능을 크게 향상시킴.</li>
<li>COCO, LVIS-minival, LVIS-val 등의 제로샷 객체 감지 벤치마크에서 새로운 최고 성능을 달성함.</li>
<li>특히 LVIS의 희귀 클래스에서 이전 모델 대비 5.8 AP 및 5.0 AP의 성능 향상을 보임.</li>
<li>다양한 입력 프롬프트(텍스트, 비주얼, 사용자 정의)를 지원하며, 바운딩 박스, 세그멘테이션 마스크, 포즈 키포인트, 객체 캡션 등 다중 레벨의 출력 표현을 제공함.</li>
<li>개방형 객체 감지, 프레이즈 그라운딩, 비주얼 카운팅, 포즈 추정, 프롬프트 없는 객체 감지 및 인식 등 다양한 실용적인 작업을 지원함.</li>
<li><strong>Grounding DINO</strong>와 **SAM(Segment Anything Model)**의 조합인 <strong>Grounded SAM</strong>과 비교하여 통합된 비전 모델로서 효율성과 성능을 향상시킴.</li>
</ul>
<h3 id="베이징교통대학-O1-CODER-소개"><a href="#베이징교통대학-O1-CODER-소개" class="headerlink" title="베이징교통대학, O1-CODER 소개"></a>베이징교통대학, O1-CODER 소개</h3><p><a target="_blank" rel="noopener" href="https://github.com/ADaM-BJTU/O1-CODER">링크</a>, 2024년 12월 4일</p>
<ul>
<li><strong>O1-CODER</strong>는 OpenAI의 <strong>O1</strong> 모델을 코딩 작업에 특화하여 복제한 시도임.</li>
<li>강화 학습(RL)과 몬테카를로 트리 탐색(MCTS)을 통합하여 모델의 시스템 2 사고 능력을 향상시킴.</li>
<li>**테스트 케이스 생성기(TCG)**를 훈련하여 표준화된 코드 테스트를 수행함.</li>
<li>자체 플레이와 강화 학습을 통해 모델이 추론 프로세스를 포함한 코드 데이터를 생성하고, 정책 모델을 반복적으로 최적화함.</li>
<li>모델은 초기에는 의사 코드(pseudo-code)를 생성하고, 이후에 전체 코드를 생성하도록 훈련됨.</li>
<li>실제 응용에서의 기회와 도전을 다루며, 시스템 2 패러다임으로의 전환을 제안함.</li>
<li>모든 소스 코드, 데이터셋 및 모델을 공개할 예정임.</li>
</ul>
<h3 id="Google-DeepMind-Generative-Omnimatte-발표"><a href="#Google-DeepMind-Generative-Omnimatte-발표" class="headerlink" title="Google DeepMind, Generative Omnimatte 발표"></a>Google DeepMind, Generative Omnimatte 발표</h3><p><a target="_blank" rel="noopener" href="https://gen-omnimatte.github.io/">링크</a>, 2024년 12월 3일</p>
<ul>
<li><strong>Generative Omnimatte</strong>는 비디오를 레이어로 분해하여 각 객체와 그에 관련된 효과(그림자, 반사 등)를 분리하는 기술임.</li>
<li>기존 방법들은 정적인 배경이나 단일 객체에 제한되었으나, 이 기술은 동적인 배경, 가려짐, 다중 객체의 효과 연관을 처리할 수 있음.</li>
<li>사용자에게 다양한 비디오 편집 기능을 제공하여 객체 제거, 효과 제거 등의 작업을 가능하게 함.</li>
<li>두 단계의 프로세스를 사용함:<ul>
<li><strong>Stage 1</strong>: 객체 및 효과 제거 모델인 <strong>Casper</strong>를 사용하여 클린 플레이트 배경과 단일 객체 비디오를 생성함.</li>
<li><strong>Stage 2</strong>: 테스트 시 최적화를 통해 솔로 비디오와 배경 비디오에서 오미매트 레이어를 재구성함.</li>
</ul>
</li>
<li>자기 주의(attention) 메커니즘을 활용하여 효과 연관을 시각화하고, 모델의 내부 이해를 분석함.</li>
<li>사용자 지정 트리마스크(trimask)를 통해 결과를 개선하고, 다중 객체 시나리오에서 더 정확한 결과를 얻을 수 있음.</li>
</ul>
<h3 id="Tencent-HiFiVFS-공개"><a href="#Tencent-HiFiVFS-공개" class="headerlink" title="Tencent, HiFiVFS 공개"></a>Tencent, HiFiVFS 공개</h3><p><a target="_blank" rel="noopener" href="https://cxcx1996.github.io/HiFiVFS/">링크</a>, 2024년 12월 1일</p>
<ul>
<li><strong>HiFiVFS</strong>는 고품질 비디오 얼굴 교체를 위한 프레임워크임.</li>
<li>**Stable Video Diffusion(SVD)**를 기반으로 하며, 시간적 안정성을 유지하면서 얼굴 교체를 수행함.</li>
<li>**세밀한 속성 학습(FAL)**을 통해 아이덴티티 비민감화와 적대적 학습을 통해 속성을 분리하고 강화함.</li>
<li>**상세한 아이덴티티 학습(DIL)**을 통해 더 얼굴 교체에 적합한 ID 특징을 사용하여 아이덴티티 유사성을 향상시킴.</li>
<li>복잡한 조명 조건, 극단적인 포즈, 가려짐, 다른 얼굴 형태 등의 어려운 시나리오에서도 우수한 성능을 보임.</li>
<li>기존 방법들과 비교하여 새로운 최고 성능(SOTA)을 달성하였으며, 추가적인 후처리 없이도 고품질의 얼굴 교체 영상을 생성함.</li>
</ul>
<h3 id="싱가포르-국립대학교-및-Microsoft-ShowUI-발표"><a href="#싱가포르-국립대학교-및-Microsoft-ShowUI-발표" class="headerlink" title="싱가포르 국립대학교 및 Microsoft, ShowUI 발표"></a>싱가포르 국립대학교 및 Microsoft, ShowUI 발표</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/showlab/ShowUI-2B">링크</a>, 2024년 12월 5일</p>
<ul>
<li><strong>ShowUI</strong>는 어떤 UI에서도 동작할 수 있는 경량의 종단 간 에이전트임.</li>
<li><strong>Qwen-2-VL-2B</strong> 모델을 기반으로 하여 작은 모델로도 GPT-4V와 같은 더 큰 모델을 능가하는 성능을 보임.</li>
<li>UI 스크린샷을 최대한 단순화하여 효율성을 높이고, 패치를 그룹화하여 시각적 복잡성을 줄임.</li>
<li>데스크톱, 안드로이드, 웹 등 다양한 플랫폼에서 작동하며, 추가적인 텍스트 정보 없이도 UI를 탐색하고 작업을 수행함.</li>
<li>다양한 데이터셋(OmniAct, Mind2Web, AMEX 등)을 통합하여 일반적인 UI 에이전트를 훈련함.</li>
<li>에이전트가 자체적으로 도구를 정의하고, 이를 재사용하여 보다 유연한 문제 해결이 가능함.</li>
<li>**OOTB(Out-of-the-box)**를 통해 로컬에서 쉽게 실행할 수 있음.</li>
</ul>
<h3 id="Pydantic-PydanticAI-출시"><a href="#Pydantic-PydanticAI-출시" class="headerlink" title="Pydantic, PydanticAI 출시"></a>Pydantic, PydanticAI 출시</h3><p><a target="_blank" rel="noopener" href="https://ai.pydantic.dev/">링크</a>, 2024년 12월 2일</p>
<ul>
<li><strong>PydanticAI</strong>는 Pydantic을 활용하여 LLM과 함께 사용할 수 있는 에이전트 프레임워크임.</li>
<li>파이썬으로 제어 흐름과 구성을 수행하며, 별도의 DSL이나 복잡한 추상화가 필요 없음.</li>
<li>타입 안전성을 제공하며, IDE 지원과 정적 타입 검사를 통해 개발 생산성을 향상시킴.</li>
<li>OpenAI, Gemini, vLLM, TGI 등 다양한 모델을 지원하며, 모델에 종속되지 않는 구조를 가짐.</li>
<li>구조화된 응답 검증과 스트리밍 응답을 지원하여 안정적인 응용 프로그램 구축이 가능함.</li>
<li>동적 런타임 컨텍스트 및 종속성 주입을 지원하여 테스트와 반복적인 개발을 용이하게 함.</li>
<li><strong>Logfire</strong> 통합을 통해 LLM 기반 응용 프로그램의 디버깅과 성능 모니터링을 지원함.</li>
</ul>
<h3 id="Quivr-MegaParse-소개"><a href="#Quivr-MegaParse-소개" class="headerlink" title="Quivr, MegaParse 소개"></a>Quivr, MegaParse 소개</h3><p><a target="_blank" rel="noopener" href="https://github.com/quivrhq/megaparse">링크</a>, 2024년 12월 4일</p>
<ul>
<li><strong>MegaParse</strong>는 모든 유형의 문서를 처리할 수 있는 강력하고 다재다능한 파서임.</li>
<li>정보 손실 없이 문서를 파싱하는 데 중점을 두며, 텍스트, PDF, 파워포인트, 워드, 엑셀, CSV 등 다양한 파일 형식을 지원함.</li>
<li>빠르고 효율적인 성능을 제공하며, 오픈 소스로 자유롭게 사용할 수 있음.</li>
<li><strong>UnstructuredParser</strong>, <strong>MegaParseVision</strong>, <strong>LlamaParser</strong> 등 다양한 파서 모듈을 제공하여 사용자 필요에 따라 선택 가능함.</li>
<li>MegaParse Vision은 멀티모달 모델(예: GPT-4V, Claude 4)을 활용하여 이미지와 PDF의 파싱 성능을 향상시킴.</li>
<li>API로도 사용할 수 있으며, 간단한 설치와 사용법을 제공함.</li>
<li>벤치마크 결과에서 기존의 파서보다 높은 유사도 비율을 달성하여 정확성을 입증함.</li>
</ul>
<h3 id="Google-Research-LLM-임베딩을-활용한-회귀-분석-연구-발표"><a href="#Google-Research-LLM-임베딩을-활용한-회귀-분석-연구-발표" class="headerlink" title="Google Research, LLM 임베딩을 활용한 회귀 분석 연구 발표"></a>Google Research, LLM 임베딩을 활용한 회귀 분석 연구 발표</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.14708">링크</a>, 2024년 12월 2일</p>
<ul>
<li>LLM 임베딩을 회귀 분석의 특징(feature)으로 사용하는 방법을 종합적으로 조사함.</li>
<li>LLM 임베딩을 특징으로 사용할 경우, 고차원 회귀 작업에서 전통적인 특징 공학보다 더 나은 성능을 보일 수 있음을 실험적으로 보여줌.</li>
<li>LLM 임베딩이 특징 공간에서 리프시츠 연속성(Lipschitz continuity)을 보존한다는 것을 발견함.</li>
<li>모델 크기와 언어 이해 능력이 회귀 성능 향상에 항상 기여하지는 않는다는 것을 관찰함.</li>
<li>다양한 모델 효과(모델 크기, 언어 이해 등)의 기여도를 정량화하여 회귀 분석에서 LLM 임베딩의 활용 가능성을 제시함.</li>
</ul>
<h3 id="Adobe-DynaSaur-발표"><a href="#Adobe-DynaSaur-발표" class="headerlink" title="Adobe, DynaSaur 발표"></a>Adobe, DynaSaur 발표</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.01747">링크</a>, 2024년 12월 2일</p>
<ul>
<li><strong>DynaSaur</strong>는 사전 정의된 액션을 넘어서는 대형 언어 모델(LLM) 에이전트임.</li>
<li>기존의 에이전트 시스템은 고정되고 사전 정의된 액션 세트에서 선택하지만, DynaSaur는 코드 생성을 통해 새로운 액션을 동적으로 생성하고 구성할 수 있음.</li>
<li>에이전트가 파이썬 코드를 작성하여 프로그램을 실행하고, 생성된 액션을 축적하여 향후 재사용 가능함.</li>
<li>코드로 행동함으로써 에이전트가 튜링 완전한 문제를 해결할 수 있으며, 일반성과 구성 가능성을 향상시킴.</li>
<li>GAIA 벤치마크에서 최고 성능을 달성하였으며, 기존의 정해진 액션 세트 기반의 방법보다 유연하고 강력한 문제 해결 능력을 보임.</li>
<li>코드 에이전트의 성능이 크게 향상되었으며, 특히 예기치 않은 엣지 케이스나 새로운 상황에서 회복 능력이 뛰어남.</li>
<li>오픈 소스로 코드가 공개되어 있으며, 다양한 응용 분야에서 활용 가능함.</li>
</ul>
<h3 id="ConsisID-아이덴티티-보존-텍스트-비디오-생성-모델-제안"><a href="#ConsisID-아이덴티티-보존-텍스트-비디오-생성-모델-제안" class="headerlink" title="ConsisID, 아이덴티티 보존 텍스트-비디오 생성 모델 제안"></a>ConsisID, 아이덴티티 보존 텍스트-비디오 생성 모델 제안</h3><p><a target="_blank" rel="noopener" href="https://github.com/PKU-YuanGroup/ConsisID">링크</a>, 2024년 12월 1일</p>
<ul>
<li><strong>ConsisID</strong>는 일관성 있는 아이덴티티 보존 텍스트-비디오 생성 모델임.</li>
<li>주파수 분해를 활용하여 얼굴 특징을 저주파(global features)와 고주파(intrinsic features)로 분리함.</li>
<li><strong>글로벌 얼굴 추출기</strong>를 통해 참조 이미지와 얼굴 키포인트를 인코딩하여 저주파 정보를 포함한 특징을 추출함.</li>
<li><strong>로컬 얼굴 추출기</strong>를 통해 고주파 세부 정보를 캡처하고, 트랜스포머 블록에 주입하여 아이덴티티 보존 능력을 향상시킴.</li>
<li>계층적 학습 전략을 통해 주파수 정보를 활용하여 아이덴티티를 보존함.</li>
<li>튜닝 없이 단일 이미지와 텍스트 프롬프트만으로 고품질의 아이덴티티 일관성을 가진 비디오를 생성함.</li>
<li>다양한 벤치마크에서 우수한 성능을 입증하였으며, Apache 2.0 라이선스로 공개되어 있음.</li>
</ul>
<h3 id="Hugging-Face-Showlab의-ShowUI-2B-모델-공개"><a href="#Hugging-Face-Showlab의-ShowUI-2B-모델-공개" class="headerlink" title="Hugging Face, Showlab의 ShowUI-2B 모델 공개"></a>Hugging Face, Showlab의 ShowUI-2B 모델 공개</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/showlab/ShowUI-2B">링크</a>, 2024년 12월 5일</p>
<ul>
<li><strong>ShowUI-2B</strong>는 경량의 비전-언어-액션 모델로, GUI 에이전트임.</li>
<li><strong>Qwen-2-VL-2B</strong> 모델을 기반으로 하여 다양한 UI에서 동작함.</li>
<li>웹, 데스크톱, 안드로이드 등 다양한 인터페이스에서 작동하며, 추가적인 텍스트 정보 없이도 UI를 탐색하고 작업을 수행함.</li>
<li>UI 스크린샷을 최대한 단순화하여 시각적 복잡성을 줄이고, 패치를 그룹화하여 효율성을 높임.</li>
<li>다양한 데이터셋을 통합하여 일반적인 UI 에이전트를 훈련하였으며, GPT-4V와 같은 더 큰 모델을 능가하는 성능을 보임.</li>
<li>에이전트가 자체적으로 도구를 정의하고, 이를 재사용하여 유연한 문제 해결이 가능함.</li>
<li>**OOTB(Out-of-the-box)**를 통해 로컬에서 쉽게 실행할 수 있음.</li>
</ul>
<h3 id="DataLab-LLM-기반-비즈니스-인텔리전스-플랫폼-발표"><a href="#DataLab-LLM-기반-비즈니스-인텔리전스-플랫폼-발표" class="headerlink" title="DataLab, LLM 기반 비즈니스 인텔리전스 플랫폼 발표"></a>DataLab, LLM 기반 비즈니스 인텔리전스 플랫폼 발표</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.02205">링크</a>, 2024년 12월 3일</p>
<ul>
<li><strong>DataLab</strong>은 LLM 기반 에이전트를 통합한 통합 비즈니스 인텔리전스(BI) 플랫폼임.</li>
<li>자연어 쿼리를 기반으로 자동으로 작업 계획, 추론, 실행을 수행하며, 데이터 분석 업무를 간소화함.</li>
<li>다양한 BI 작업을 단일 환경에서 지원하여 데이터 전문가들이 여러 도구를 전환하지 않고도 작업 가능함.</li>
<li><strong>도메인 지식 통합 모듈</strong>을 설계하여 기업별 BI 작업을 지원하고, LLM이 기업 특유의 용어와 데이터에 적응할 수 있게 함.</li>
<li><strong>에이전트 간 통신 메커니즘</strong>을 통해 BI 워크플로우에서 정보 공유를 촉진하고, 협업을 강화함.</li>
<li><strong>셀 기반 컨텍스트 관리 전략</strong>을 도입하여 노트북 환경에서의 컨텍스트 활용을 최적화하고, 긴 컨텍스트 문제를 해결함.</li>
<li>다양한 BI 작업에서 기존의 방법보다 우수한 성능을 보였으며, 실제 기업 데이터셋에서도 높은 효과와 효율성을 입증함.</li>
<li>Tencent의 실제 데이터에서 최대 58.58%의 정확도 향상과 61.65%의 토큰 비용 절감을 달성함.</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://aws.amazon.com/de/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/</span><br><span class="line">Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance</span><br><span class="line">by Danilo Poccia on 03 DEC 2024 in Amazon Bedrock, Amazon Machine Learning, Announcements, Artificial Intelligence, AWS re:Invent, Featured, Generative AI, Launch, News Permalink  Comments  Share</span><br><span class="line">Voiced by Polly</span><br><span class="line">Today, we’re thrilled to announce Amazon Nova, a new generation of state-of-the-art foundation models (FMs) that deliver frontier intelligence and industry leading price performance, available exclusively in Amazon Bedrock.</span><br><span class="line"></span><br><span class="line">Unexpected. Amazon is back with Foundation Models. As part of re:Invent, they announced 6 new foundation models, from text-only to text-to-video! 👀 Nova models will be exclusively available through Amazon Bedrock.</span><br><span class="line">TL;DR;</span><br><span class="line">🧠 Micro (text-only), Lite (multimodal), Pro (high-capability), and Premier (coming 2025)</span><br><span class="line">🎨 Canvas (image-generation) and Reel (video-generation)</span><br><span class="line">📊 Context length up to 300K tokens and 200+ languages</span><br><span class="line">🥇 Performance on benchmarks similar to Llama 3</span><br><span class="line">🗺️ Models currently only available in AWS Regions in the US</span><br><span class="line">🔒 Includes watermarking capabilities (no details here)</span><br><span class="line">🔧 Can be fine-tuned inside Amazon Bedrock</span><br><span class="line">💰 Micro: $0.035 / $0.14; Lite: $0.06 / $0.24; Pro: $0.80 / $3.20 per million input/output tokens</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">You can use Amazon Nova to lower costs and latency for almost any generative AI task. You can build on Amazon Nova to analyze complex documents and videos, understand charts and diagrams, generate engaging video content, and build sophisticated AI agents, from across a range of intelligence classes optimized for enterprise workloads.</span><br><span class="line"></span><br><span class="line">Whether you’re developing document processing applications that need to process images and text, creating marketing content at scale, or building AI assistants that can understand and act on visual information, Amazon Nova provides the intelligence and flexibility you need with two categories of models: understanding and creative content generation.</span><br><span class="line"></span><br><span class="line">Amazon Nova understanding models accept text, image, or video inputs to generate text output. Amazon creative content generation models accept text and image inputs to generate image or video output.</span><br><span class="line"></span><br><span class="line">Understanding models: Text and visual intelligence</span><br><span class="line">The Amazon Nova models include three understanding models (with a fourth one coming soon) designed to meet different needs:</span><br><span class="line"></span><br><span class="line">Amazon Nova Micro – A text-only model that delivers the lowest latency responses in the Amazon Nova family of models at a very low cost. With a context length of 128K tokens and optimized for speed and cost, Amazon Nova Micro excels at tasks such as text summarization, translation, content classification, interactive chat and brainstorming, and simple mathematical reasoning and coding. Amazon Nova Micro also supports customization on proprietary data using fine-tuning and model distillation to boost accuracy.</span><br><span class="line"></span><br><span class="line">Amazon Nova Lite – A very low-cost multimodal model that is lightning fast for processing image, video, and text inputs to generate text output. Amazon Nova Lite can handle real-time customer interactions, document analysis, and visual question-answering tasks with high accuracy. The model processes inputs up to 300K tokens in length and can analyze multiple images or up to 30 minutes of video in a single request. Amazon Nova Lite also supports text and multimodal fine-tuning and can be optimized to deliver the best quality and costs for your use case with techniques such as model distillation.</span><br><span class="line"></span><br><span class="line">Amazon Nova Pro – A highly capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Pro is capable of processing up to 300K input tokens and sets new standards in multimodal intelligence and agentic workflows that require calling APIs and tools to complete complex workflows. It achieves state-of-the-art performance on key benchmarks including visual question answering (TextVQA) and video understanding (VATEX). Amazon Nova Pro demonstrates strong capabilities in processing both visual and textual information and excels at analyzing financial documents. With an input context of 300K tokens, it can process code bases with over fifteen thousand lines of code. Amazon Nova Pro also serves as a teacher model to distill custom variants of Amazon Nova Micro and Lite.</span><br><span class="line"></span><br><span class="line">Amazon Nova Premier – Our most capable multimodal model for complex reasoning tasks and for use as the best teacher for distilling custom models. Amazon Nova Premier is still in training. We’re targeting availability in early 2025.</span><br><span class="line"></span><br><span class="line">Amazon Nova understanding models excel in Retrieval-Augmented Generation (RAG), function calling, and agentic applications. This is reflected in Amazon Nova model scores in the Comprehensive RAG Benchmark (CRAG) evaluation, Berkeley Function Calling Leaderboard (BFCL), VisualWebBench, and Mind2Web.</span><br><span class="line"></span><br><span class="line">What makes Amazon Nova particularly powerful for enterprises is its customization capabilities. Think of it as tailoring a suit: you start with a high-quality foundation and adjust it to fit your exact needs. You can fine-tune the models with text, image, and video to understand your industry’s terminology, align with your brand voice, and optimize for your specific use cases. For instance, a legal firm might customize Amazon Nova to better understand legal terminology and document structures.</span><br><span class="line"></span><br><span class="line">You can see the latest benchmark scores for these models on the Amazon Nova product page.</span><br><span class="line"></span><br><span class="line">Creative content generation: Bringing concepts to life</span><br><span class="line">The Amazon Nova models also include two creative content generation models:</span><br><span class="line"></span><br><span class="line">Amazon Nova Canvas – A state-of-the-art image generation model producing studio-quality images with precise control over style and content, including rich editing features such as inpainting, outpainting, and background removal. Amazon Nova Canvas excels on human evaluations and key benchmarks such as text-to-image faithfulness evaluation with question answering (TIFA) and ImageReward.</span><br><span class="line"></span><br><span class="line">Amazon Nova Reel – A state-of-the-art video generation model. Using Amazon Nova Reel, you can produce short videos through text prompts and images, control visual style and pacing, and generate professional-quality video content for marketing, advertising, and entertainment. Amazon Nova Reel outperforms existing models on human evaluations of video quality and video consistency.</span><br><span class="line"></span><br><span class="line">All Amazon Nova models include built-in safety controls and creative content generation models include watermarking capabilities to promote responsible AI use.</span><br><span class="line"></span><br><span class="line">Let’s see how these models work in practice for a few use cases.</span><br><span class="line"></span><br><span class="line">Using Amazon Nova Pro for document analysis</span><br><span class="line">To demonstrate the capabilities of document analysis, I downloaded the Choosing a generative AI service decision guide in PDF format from the AWS documentation.</span><br><span class="line"></span><br><span class="line">First, I choose Model access in the Amazon Bedrock console navigation pane and request access to the new Amazon Nova models. Then, I choose Chat/text in the Playground section of the navigation pane and select the Amazon Nova Pro model. In the chat, I upload the decision guide PDF and ask:</span><br><span class="line"></span><br><span class="line">Write a summary of this doc in 100 words. Then, build a decision tree.</span><br><span class="line"></span><br><span class="line">The output follows my instructions producing a structured decision tree that gives me a glimpse of the document before reading it.</span><br><span class="line"></span><br><span class="line">Console screenshot.</span><br><span class="line"></span><br><span class="line">Using Amazon Nova Pro for video analysis</span><br><span class="line">To demonstrate video analysis, I prepared a video by joining two short clips (more on this in the next section):</span><br><span class="line"></span><br><span class="line">This time, I use the AWS SDK for Python (Boto3) to invoke the Amazon Nova Pro model using the Amazon Bedrock Converse API and analyze the video:</span><br><span class="line"></span><br><span class="line">import boto3</span><br><span class="line"></span><br><span class="line">AWS_REGION = &quot;us-east-1&quot;</span><br><span class="line">MODEL_ID = &quot;amazon.nova-pro-v1:0&quot;</span><br><span class="line">VIDEO_FILE = &quot;the-sea.mp4&quot;</span><br><span class="line"></span><br><span class="line">bedrock_runtime = boto3.client(&quot;bedrock-runtime&quot;, region_name=AWS_REGION)</span><br><span class="line">with open(VIDEO_FILE, &quot;rb&quot;) as f:</span><br><span class="line">    video = f.read()</span><br><span class="line"></span><br><span class="line">user_message = &quot;Describe this video.&quot;</span><br><span class="line"></span><br><span class="line">messages = [ &#123; &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [</span><br><span class="line">    &#123;&quot;video&quot;: &#123;&quot;format&quot;: &quot;mp4&quot;, &quot;source&quot;: &#123;&quot;bytes&quot;: video&#125;&#125;&#125;,</span><br><span class="line">    &#123;&quot;text&quot;: user_message&#125;</span><br><span class="line">] &#125; ]</span><br><span class="line"></span><br><span class="line">response = bedrock_runtime.converse(</span><br><span class="line">    modelId=MODEL_ID,</span><br><span class="line">    messages=messages,</span><br><span class="line">    inferenceConfig=&#123;&quot;temperature&quot;: 0.0&#125;</span><br><span class="line"> )</span><br><span class="line"></span><br><span class="line">response_text = response[&quot;output&quot;][&quot;message&quot;][&quot;content&quot;][0][&quot;text&quot;]</span><br><span class="line">print(response_text)</span><br><span class="line">Python</span><br><span class="line">Amazon Nova Pro can analyze videos that are uploaded with the API (as in the previous code) or that are stored in an Amazon Simple Storage Service (Amazon S3) bucket.</span><br><span class="line"></span><br><span class="line">In the script, I ask to describe the video. I run the script from the command line. Here’s the result:</span><br><span class="line"></span><br><span class="line">The video begins with a view of a rocky shore on the ocean, and then transitions to a close-up of a large seashell resting on a sandy beach.</span><br><span class="line"></span><br><span class="line">I can use a more detailed prompt to extract specific information from the video such as objects or text. Note that Amazon Nova currently does not process audio in a video.</span><br><span class="line"></span><br><span class="line">Using Amazon Nova for video creation</span><br><span class="line">Now, let’s create a video using Amazon Nova Reel, starting from a text-only prompt and then providing a reference image.</span><br><span class="line"></span><br><span class="line">Because generating a video takes a few minutes, the Amazon Bedrock API introduced three new operations:</span><br><span class="line"></span><br><span class="line">StartAsyncInvoke – To start an asynchronous invocation</span><br><span class="line"></span><br><span class="line">GetAsyncInvoke – To get the current status of a specific asynchronous invocation</span><br><span class="line"></span><br><span class="line">ListAsyncInvokes – To list the status of all asynchronous invocations with optional filters such as status or date</span><br><span class="line"></span><br><span class="line">Amazon Nova Reel supports camera control actions such as zooming or moving the camera. This Python script creates a video from this text prompt:</span><br><span class="line"></span><br><span class="line">Closeup of a large seashell in the sand. Gentle waves flow all around the shell. Sunset light. Camera zoom in very close.</span><br><span class="line"></span><br><span class="line">After the first invocation, the script periodically checks the status until the creation of the video has been completed. I pass a random seed to get a different result each time the code runs.</span><br><span class="line"></span><br><span class="line">import random</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">import boto3</span><br><span class="line"></span><br><span class="line">AWS_REGION = &quot;us-east-1&quot;</span><br><span class="line">MODEL_ID = &quot;amazon.nova-reel-v1:0&quot;</span><br><span class="line">SLEEP_TIME = 30</span><br><span class="line">S3_DESTINATION_BUCKET = &quot;&lt;BUCKET&gt;&quot;</span><br><span class="line"></span><br><span class="line">video_prompt = &quot;Closeup of a large seashell in the sand. Gentle waves flow all around the shell. Sunset light. Camera zoom in very close.&quot;</span><br><span class="line"></span><br><span class="line">bedrock_runtime = boto3.client(&quot;bedrock-runtime&quot;, region_name=AWS_REGION)</span><br><span class="line">model_input = &#123;</span><br><span class="line">    &quot;taskType&quot;: &quot;TEXT_VIDEO&quot;,</span><br><span class="line">    &quot;textToVideoParams&quot;: &#123;&quot;text&quot;: video_prompt&#125;,</span><br><span class="line">    &quot;videoGenerationConfig&quot;: &#123;</span><br><span class="line">        &quot;durationSeconds&quot;: 6,</span><br><span class="line">        &quot;fps&quot;: 24,</span><br><span class="line">        &quot;dimension&quot;: &quot;1280x720&quot;,</span><br><span class="line">        &quot;seed&quot;: random.randint(0, 2147483648)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">invocation = bedrock_runtime.start_async_invoke(</span><br><span class="line">    modelId=MODEL_ID,</span><br><span class="line">    modelInput=model_input,</span><br><span class="line">    outputDataConfig=&#123;&quot;s3OutputDataConfig&quot;: &#123;&quot;s3Uri&quot;: f&quot;s3://&#123;S3_DESTINATION_BUCKET&#125;&quot;&#125;&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">invocation_arn = invocation[&quot;invocationArn&quot;]</span><br><span class="line">s3_prefix = invocation_arn.split(&#x27;/&#x27;)[-1]</span><br><span class="line">s3_location = f&quot;s3://&#123;S3_DESTINATION_BUCKET&#125;/&#123;s3_prefix&#125;&quot;</span><br><span class="line">print(f&quot;\nS3 URI: &#123;s3_location&#125;&quot;)</span><br><span class="line"></span><br><span class="line">while True:</span><br><span class="line">    response = bedrock_runtime.get_async_invoke(</span><br><span class="line">        invocationArn=invocation_arn</span><br><span class="line">    )</span><br><span class="line">    status = response[&quot;status&quot;]</span><br><span class="line">    print(f&quot;Status: &#123;status&#125;&quot;)</span><br><span class="line">    if status != &quot;InProgress&quot;:</span><br><span class="line">        break</span><br><span class="line">    time.sleep(SLEEP_TIME)</span><br><span class="line"></span><br><span class="line">if status == &quot;Completed&quot;:</span><br><span class="line">    print(f&quot;\nVideo is ready at &#123;s3_location&#125;/output.mp4&quot;)</span><br><span class="line">else:</span><br><span class="line">    print(f&quot;\nVideo generation status: &#123;status&#125;&quot;)</span><br><span class="line">Python</span><br><span class="line">I run the script:</span><br><span class="line"></span><br><span class="line">Status: InProgress</span><br><span class="line">. . .</span><br><span class="line">Status: Completed</span><br><span class="line"></span><br><span class="line">Video is ready at s3://BUCKET/PREFIX/output.mp4</span><br><span class="line">Bash</span><br><span class="line">After a few minutes, the script completes and prints the output Amazon Simple Storage Service (Amazon S3) location. I download the output video using the AWS Command Line Interface (AWS CLI):</span><br><span class="line"></span><br><span class="line">aws s3 cp s3://BUCKET/PREFIX/output.mp4 ./output-from-text.mp4</span><br><span class="line">This is the resulting video. As requested, the camera zooms in on the subject.</span><br><span class="line"></span><br><span class="line">Using Amazon Nova Reel with a reference image</span><br><span class="line">To have better control over the creation of the video, I can provide Amazon Nova Reel a reference image such as the following:</span><br><span class="line"></span><br><span class="line">A seascape image.</span><br><span class="line"></span><br><span class="line">This script uses the reference image and a text prompt with a camera action (drone view flying over a coastal landscape) to create a video:</span><br><span class="line"></span><br><span class="line">import base64</span><br><span class="line">import random</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">import boto3</span><br><span class="line"></span><br><span class="line">S3_DESTINATION_BUCKET = &quot;&lt;BUCKET&gt;&quot;</span><br><span class="line">AWS_REGION = &quot;us-east-1&quot;</span><br><span class="line">MODEL_ID = &quot;amazon.nova-reel-v1:0&quot;</span><br><span class="line">SLEEP_TIME = 30</span><br><span class="line">input_image_path = &quot;seascape.png&quot;</span><br><span class="line">video_prompt = &quot;drone view flying over a coastal landscape&quot;</span><br><span class="line"></span><br><span class="line">bedrock_runtime = boto3.client(&quot;bedrock-runtime&quot;, region_name=AWS_REGION)</span><br><span class="line"></span><br><span class="line"># Load the input image as a Base64 string.</span><br><span class="line">with open(input_image_path, &quot;rb&quot;) as f:</span><br><span class="line">    input_image_bytes = f.read()</span><br><span class="line">    input_image_base64 = base64.b64encode(input_image_bytes).decode(&quot;utf-8&quot;)</span><br><span class="line"></span><br><span class="line">model_input = &#123;</span><br><span class="line">    &quot;taskType&quot;: &quot;TEXT_VIDEO&quot;,</span><br><span class="line">    &quot;textToVideoParams&quot;: &#123;</span><br><span class="line">        &quot;text&quot;: video_prompt,</span><br><span class="line">        &quot;images&quot;: [&#123; &quot;format&quot;: &quot;png&quot;, &quot;source&quot;: &#123; &quot;bytes&quot;: input_image_base64 &#125; &#125;]</span><br><span class="line">        &#125;,</span><br><span class="line">    &quot;videoGenerationConfig&quot;: &#123;</span><br><span class="line">        &quot;durationSeconds&quot;: 6,</span><br><span class="line">        &quot;fps&quot;: 24,</span><br><span class="line">        &quot;dimension&quot;: &quot;1280x720&quot;,</span><br><span class="line">        &quot;seed&quot;: random.randint(0, 2147483648)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">invocation = bedrock_runtime.start_async_invoke(</span><br><span class="line">    modelId=MODEL_ID,</span><br><span class="line">    modelInput=model_input,</span><br><span class="line">    outputDataConfig=&#123;&quot;s3OutputDataConfig&quot;: &#123;&quot;s3Uri&quot;: f&quot;s3://&#123;S3_DESTINATION_BUCKET&#125;&quot;&#125;&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">invocation_arn = invocation[&quot;invocationArn&quot;]</span><br><span class="line">s3_prefix = invocation_arn.split(&#x27;/&#x27;)[-1]</span><br><span class="line">s3_location = f&quot;s3://&#123;S3_DESTINATION_BUCKET&#125;/&#123;s3_prefix&#125;&quot;</span><br><span class="line"></span><br><span class="line">print(f&quot;\nS3 URI: &#123;s3_location&#125;&quot;)</span><br><span class="line"></span><br><span class="line">while True:</span><br><span class="line">    response = bedrock_runtime.get_async_invoke(</span><br><span class="line">        invocationArn=invocation_arn</span><br><span class="line">    )</span><br><span class="line">    status = response[&quot;status&quot;]</span><br><span class="line">    print(f&quot;Status: &#123;status&#125;&quot;)</span><br><span class="line">    if status != &quot;InProgress&quot;:</span><br><span class="line">        break</span><br><span class="line">    time.sleep(SLEEP_TIME)</span><br><span class="line">if status == &quot;Completed&quot;:</span><br><span class="line">    print(f&quot;\nVideo is ready at &#123;s3_location&#125;/output.mp4&quot;)</span><br><span class="line">else:</span><br><span class="line">    print(f&quot;\nVideo generation status: &#123;status&#125;&quot;)</span><br><span class="line">Python</span><br><span class="line">Again, I download the output using the AWS CLI:</span><br><span class="line"></span><br><span class="line">aws s3 cp s3://BUCKET/PREFIX/output.mp4 ./output-from-image.mp4</span><br><span class="line">This is the resulting video. The camera starts from the reference image and moves forward.</span><br><span class="line"></span><br><span class="line">Building AI responsibly</span><br><span class="line">Amazon Nova models are built with a focus on customer safety, security, and trust throughout the model development stages, offering you peace of mind as well as an adequate level of control to enable your unique use cases.</span><br><span class="line"></span><br><span class="line">We’ve built in comprehensive safety features and content moderation capabilities, giving you the controls you need to use AI responsibly. Every generated image and video include digital watermarking.</span><br><span class="line"></span><br><span class="line">The Amazon Nova foundation models are built with protections that match its increased capabilities. Amazon Nova extends our safety measures to combat the spread of misinformation, child sexual abuse material (CSAM), and chemical, biological, radiological, or nuclear (CBRN) risks.</span><br><span class="line"></span><br><span class="line">Things to know</span><br><span class="line">Amazon Nova models are available in Amazon Bedrock in the US East (N. Virginia) AWS region. Amazon Nova Micro, Lite, and Pro are also available in the US West (Oregon), and US East (Ohio) regions via cross-Region inference. As usual with Amazon Bedrock, the pricing follows a pay-as-you-go model. For more information, see Amazon Bedrock pricing.</span><br><span class="line"></span><br><span class="line">The new generation of Amazon Nova understanding models speaks your language. These models understand and generate content in over 200 languages, with particularly strong capabilities in English, German, Spanish, French, Italian, Japanese, Korean, Arabic, Simplified Chinese, Russian, Hindi, Portuguese, Dutch, Turkish, and Hebrew. This means you can build truly global applications without worrying about language barriers or maintaining separate models for different regions. Amazon Nova models for creative content generation support English prompts.</span><br><span class="line"></span><br><span class="line">As you explore Amazon Nova, you’ll discover its ability to handle increasingly complex tasks. You can use these models to process lengthy documents up to 300K tokens, analyze multiple images in a single request, understand up to 30 minutes of video content, and generate images and videos at scale from natural language. This makes these models suitable for a variety of business use cases, from quick customer service interactions to deep analysis of corporate documentation and asset creation for advertising, ecommerce, and social media applications.</span><br><span class="line"></span><br><span class="line">Integration with Amazon Bedrock makes deployment and scaling straightforward. You can leverage features like Amazon Bedrock Knowledge Bases to enhance your model with proprietary information, use Amazon Bedrock Agents to automate complex workflows, and implement Amazon Bedrock Guardrails to promote responsible AI use. The platform supports real-time streaming for interactive applications, batch processing for high-volume workloads, and detailed monitoring to help you optimize performance.</span><br><span class="line"></span><br><span class="line">Ready to start building with Amazon Nova? Give the new models a try in the Amazon Bedrock console today, visit the Amazon Nova models section of the Amazon Bedrock documentation, and send feedback to AWS re:Post for Amazon Bedrock. You can find deep-dive technical content and discover how our Builder communities are using Amazon Bedrock at community.aws. Let us know what you build with these new models!</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://yformer.github.io/efficient-track-anything/</span><br><span class="line">Efficient Track Anything</span><br><span class="line">Yunyang Xiong☨, Chong Zhou, Xiaoyu Xiang, Lemeng Wu, Chenchen Zhu, Zechun Liu, Saksham Suri, Bala Varadarajan, Ramya Akula, Forrest Iandola, Raghuraman Krishnamoorthi☨, Bilge Soran☨, Vikas Chandra☨</span><br><span class="line">12/5/24</span><br><span class="line">▶ Meta AI ▶ Nanyang Technological University</span><br><span class="line">☨Project Lead</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> Efficient Track Anything Model (EfficientTAM) 📣</span><br><span class="line">🔥 Can run &gt;10 frames per second with reasonable video segmentation performance on iPhone 15. Achieves comparable performance with SAM 2 with improved efficiency (⚡ fast!). Uses a vanilla lightweight ViT image encoder.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Abstract</span><br><span class="line">Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video object segmentation and tracking anything. Key components of SAM 2 that drive the impressive video object segmentation performance include a large multistage image encoder for frame feature extraction and a memory mechanism that stores memory contexts from past frames to help current frame segmentation. The high computation complexity of multistage image encoder and memory module has limited its applications in real-world tasks, e.g., video object segmentation on mobile devices. To address this limitation, we propose EfficientTAMs, lightweight track anything models that produce high-quality results with low latency and model size. Our idea is based on revisiting the plain, nonhierarchical Vision Transformer (ViT) as an image encoder for video object segmentation, and introducing an efficient memory module, which reduces the complexity for both frame feature extraction and memory computation for current frame segmentation. We take vanilla lightweight ViTs and efficient memory module to build EfficientTAMs, and train the models on SA-1B and SA-V datasets for video object segmentation and track anything tasks. We evaluate on multiple video segmentation benchmarks including semi-supervised VOS and promptable video segmentation, and find that our proposed EfficientTAM with vanilla ViT perform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and ~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs also perform favorably over original SAM with ~20x speedup on A100 and ~20x parameter reduction. On mobile devices such as iPhone 15 Pro Max, our EfficientTAMs can run at ~10 FPS for performing video object segmentation with reasonable quality, highlighting the capability of small models for on-device video object segmentation applications.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Method</span><br><span class="line">Our proposed EfficientTAM takes a vanilla lightweight ViT image encoder for frame feature extraction. An efficient memory cross-attention is proposed to further improve the efficiency of EfficientTAM by leveraging the strong locality of memory spatial embeddings.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The overview of EfficientTAM framework.</span><br><span class="line"></span><br><span class="line">Quantative Results</span><br><span class="line"></span><br><span class="line">Semi-supervised video object segmentation results across video object segmentation benchmarks.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Speed vs J&amp;F accuracy.</span><br><span class="line"></span><br><span class="line">Qualitative Results</span><br><span class="line"></span><br><span class="line">Video segmentation and tracking: SAM 2 (top), EfficientTAM (bottom).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Image segmentation with point prompt, box prompt, and segment everything: Input (left), SAM, EfficientSAM, SAM 2, EfficientTAM (right).</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://deepdataspace.com/home</span><br><span class="line">DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding</span><br><span class="line">2024/11/25</span><br><span class="line">IDEA Research</span><br><span class="line"></span><br><span class="line">The World&#x27;s Top-Performing Vision Model for Open-World Object Detection</span><br><span class="line"></span><br><span class="line">The project provides examples for using DINO-X, which are hosted on DeepDataSpace.</span><br><span class="line"></span><br><span class="line">IDEA Research</span><br><span class="line"></span><br><span class="line">arXiv preprint Homepage</span><br><span class="line"></span><br><span class="line"> dino_x_intro.mp4</span><br><span class="line">Highlights</span><br><span class="line">Beyond Grounding DINO 1.5, DINO-X has several improvements, taking a step forward towards becoming a more general object-centric vision model. The highlights of the DINO-X are as follows:</span><br><span class="line"></span><br><span class="line">✨ The Strongest Open-Set Detection Performance: DINO-X Pro set new SOTA results on zero-shot transfer detection benchmarks: 56.0 AP on COCO, 59.8 AP on LVIS-minival and 52.4 AP on LVIS-val. Notably, it scores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val benchmarks, improving the previous SOTA performance by 5.8 box AP and 5.0 box AP. Such a result underscores its significantly enhanced capacity for recognizing long-tailed objects.</span><br><span class="line"></span><br><span class="line">🔥 Diverse Input Prompt and Multi-level Output Semantic Representations: DINO-X can accept text prompts, visual prompts, and customized prompts as input, and it outputs representations at various semantic levels, including bounding boxes, segmentation masks, pose keypoints, and object captions, with multiple perception heads.</span><br><span class="line"></span><br><span class="line">🍉 Rich and Practical Capabilities: DINO-X can simultaneously support lots of highly practical tasks, including Open-Set Object Detection and Segmentation, Phrase Grounding, Visual-Prompt Counting, Pose Estimation, and Region Captioning. We further develop a universal object prompt to achieve Prompt-Free Anything Detection and Recognition.</span><br><span class="line"></span><br><span class="line">Latest News</span><br><span class="line">2024/12/04: Release the DINO-X Open-World Detection and Segmentation feature. Please check here for the API usage.</span><br><span class="line">2024/12/03: Support DINO-X with SAM 2 for Open-World Object Segmentation and Tracking. Please check Grounded SAM 2 for more details.</span><br><span class="line">2024/11/25: Release DINO-X API for Open-World Detection.</span><br><span class="line">2024/11/22: Launch DINO-X project and init documentation.</span><br><span class="line">Contents</span><br><span class="line">Model Framework</span><br><span class="line">Model Performance</span><br><span class="line">Side-by-side Performance Comparison with Previous Best Methods</span><br><span class="line">Zero-Shot Performance on Object Detection Benchmarks</span><br><span class="line">Zero-Shot Performance on Segmentation Benchmarks</span><br><span class="line">API Usage</span><br><span class="line">Installation</span><br><span class="line">Registration</span><br><span class="line">Support Demos</span><br><span class="line">Open-World Detection and Segmentation</span><br><span class="line">Related Works</span><br><span class="line">BibTeX</span><br><span class="line">Model Framework</span><br><span class="line">DINO-X can accept text prompts, visual prompts, and customized prompts as input, and it can generate representations at various semantic levels, including bounding boxes, segmentation masks, pose keypoints, and object captions.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Performance</span><br><span class="line">Side-by-Side Performance Comparison with Previous Best Methods</span><br><span class="line"></span><br><span class="line">Zero-Shot Performance on Object Detection Benchmarks</span><br><span class="line">Model	COCO</span><br><span class="line">(AP box)	LVIS-minival</span><br><span class="line">(AP all)	LVIS-minival</span><br><span class="line">(AP rare)	LVIS-val</span><br><span class="line">(AP all)	LVIS-val</span><br><span class="line">(AP rare)</span><br><span class="line">Other Best</span><br><span class="line">Open-Set Model	53.4</span><br><span class="line">(OmDet-Turbo)	47.6</span><br><span class="line">(T-Rex2 visual)	45.4</span><br><span class="line">(T-Rex2 visual)	45.3</span><br><span class="line">(T-Rex2 visual)	43.8</span><br><span class="line">(T-Rex2 visual)</span><br><span class="line">DetCLIPv3	-	48.8	49.9	41.4	41.4</span><br><span class="line">Grounding DINO	52.5	27.4	18.1	-	-</span><br><span class="line">T-Rex2 (text)	52.2	54.9	49.2	45.8	42.7</span><br><span class="line">Grounding DINO 1.5 Pro	54.3	55.7	56.1	47.6	44.6</span><br><span class="line">Grounding DINO 1.6 Pro	55.4	57.7	57.5	51.1	51.5</span><br><span class="line">DINO-X Pro	56.0	59.7	63.3	52.4	56.5</span><br><span class="line">Performance: DINO-X Pro achieves SOTA performance on COCO, LVIS-minival, LVIS-val, zero-shot object detection benchmarks.</span><br><span class="line">Effective Long-tail Object Detection: DINO-X Pro has significantly improved the model&#x27;s performance on LVIS-rare classes, significantly surpassing the previous SOTA Grounding DINO 1.6 Pro model by 5.8 AP and 5.0 AP, respectively, demonstrating the exceptional capability of DINO-X in long-tailed object detection scenarios.</span><br><span class="line">Zero-Shot Performance on Segmentation Benchmarks</span><br><span class="line">Model	COCO</span><br><span class="line">(AP mask)	LVIS-minival</span><br><span class="line">(AP mask)	LVIS-minival</span><br><span class="line">(AP mask rare)	LVIS-val</span><br><span class="line">(AP mask)	LVIS-val</span><br><span class="line">(AP mask rare)</span><br><span class="line">Assembled General Perception Model</span><br><span class="line">Grounded SAM (1.5 Pro + Huge)	44.3	47.7	50.2	41.8	46.0</span><br><span class="line">Grounded SAM 2 (1.5 Pro + Large)	44.7	46.2	50.1	40.5	44.6</span><br><span class="line">DINO-X Pro + SAM-Huge	44.2	51.2	52.2	-	-</span><br><span class="line">Unified Vision Model</span><br><span class="line">DINO-X Pro (Mask Head)	37.9	43.8	46.7	38.5	44.4</span><br><span class="line">Performance: DINO-X achieves mask AP scores of 37.9, 43.8, and 38.5 on the COCO, LVIS-minival, and LVIS-val zero-shot instance segmentation benchmarks, respectively.Compared to Grounded SAM and Grounded SAM 2, there is still a notable performance gap for DINO-X to catch up. We will further optimize the segmentation performance in the future release.</span><br><span class="line">Effeciency: Unlike Grounded SAM series, DINO-X significantly improves the segmentation efficiency by generating corresponding masks for each region without requiring multiple complex inference steps.</span><br><span class="line">Practical Usage: Users can use the mask function of DINO-X based on their actual needs. If the users require simultaneously object segmentation and tracking, we recommend using the latest Grounded SAM 2 (DINO-X + SAM 2), which we have already implemented in here.</span><br><span class="line">API Usage</span><br><span class="line">Installation</span><br><span class="line">Install the required packages</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">Note: If you encounter some errors with API, please install the latest version of dds-cloudapi-sdk:</span><br><span class="line"></span><br><span class="line">pip install dds-cloudapi-sdk --upgrade</span><br><span class="line">Register on Offical Website to Get API Token</span><br><span class="line">First-Time Application: If you are interested in our project and wish to try our algorithm, you will need to apply for the corresponding API Token through our request API token website for your first attempt.</span><br><span class="line"></span><br><span class="line">Request Additional Token Quotas: If you find our project helpful and need more API token quotas, you can request additional tokens by filling out this form. Our team will review your request and allocate more tokens for your use in one or two days. You can also apply for more tokens by sending us an email.</span><br><span class="line"></span><br><span class="line">Run local API demos</span><br><span class="line">Open-World Object Detection and Segmentation</span><br><span class="line">Set your API token in demo.py and run local demo</span><br><span class="line">python demo.py</span><br><span class="line">After running the local demo, the annotated image will be saved at: ./outputs/open_world_detection</span><br><span class="line">Demo Image Visualization</span><br><span class="line">Related Work</span><br><span class="line">Grounding DINO: Strong open-set object detection model.</span><br><span class="line">Grounding DINO 1.5: Previous SOTA open-set detection model.</span><br><span class="line">Grounded-Segment-Anything: Open-set detection and segmentation model by combining Grounding DINO with SAM.</span><br><span class="line">T-Rex/T-Rex2: Generic open-set detection model supporting both text and visual prompts.</span><br><span class="line"></span><br><span class="line">🦖 Dino-X: Unified Object-Centric LVM 🦖🦖</span><br><span class="line">👉IDEA unveils DINO-X: unified object-centric vision model for Open-World Detection/Segmentation, Phrase Grounding, Visual Counting, Pose Estimation, Prompt-Free Detection/Recognition, Dense Region Caption, &amp; much more. Impressive skills! Demo &amp; API announced💙</span><br><span class="line">𝐇𝐢𝐠𝐡𝐥𝐢𝐠𝐡𝐭𝐬:</span><br><span class="line">✅Object-Centric Understanding towards LVM</span><br><span class="line">✅SOTA open-world object detection performance</span><br><span class="line">✅Grounding-100M: 100M+ HQ training samples</span><br><span class="line">✅DINO-X supports textual &amp; visual prompts</span><br><span class="line">✅Universal prompt-free open-world detection</span><br><span class="line">#artificialintelligence #machinelearning #ml #AI #deeplearning #computervision #AIwithPapers #metaverse #LLM</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://github.com/PKU-YuanGroup/ConsisID</span><br><span class="line">2024.12.01</span><br><span class="line">Identity-Preserving Text-to-Video Generation by Frequency Decomposition</span><br><span class="line">Published on Nov 26</span><br><span class="line">·</span><br><span class="line">Submitted by</span><br><span class="line">BestWishYsh</span><br><span class="line">on Nov 27</span><br><span class="line">Authors:</span><br><span class="line"></span><br><span class="line">Shenghai Yuan</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Jinfa Huang</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Xianyi He</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Yunyuan Ge</span><br><span class="line">,</span><br><span class="line">Yujun Shi</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Liuhan Chen</span><br><span class="line">,</span><br><span class="line">Jiebo Luo</span><br><span class="line">,</span><br><span class="line">Li Yuan</span><br><span class="line">Abstract</span><br><span class="line">Identity-preserving text-to-video (IPT2V) generation aims to create high-fidelity videos with consistent human identity. It is an important task in video generation but remains an open problem for generative models. This paper pushes the technical frontier of IPT2V in two directions that have not been resolved in literature: (1) A tuning-free pipeline without tedious case-by-case finetuning, and (2) A frequency-aware heuristic identity-preserving DiT-based control scheme. We propose ConsisID, a tuning-free DiT-based controllable IPT2V model to keep human identity consistent in the generated video. Inspired by prior findings in frequency analysis of diffusion transformers, it employs identity-control signals in the frequency domain, where facial features can be decomposed into low-frequency global features and high-frequency intrinsic features. First, from a low-frequency perspective, we introduce a global facial extractor, which encodes reference images and facial key points into a latent space, generating features enriched with low-frequency information. These features are then integrated into shallow layers of the network to alleviate training challenges associated with DiT. Second, from a high-frequency perspective, we design a local facial extractor to capture high-frequency details and inject them into transformer blocks, enhancing the model&#x27;s ability to preserve fine-grained features. We propose a hierarchical training strategy to leverage frequency information for identity preservation, transforming a vanilla pre-trained video generation model into an IPT2V model. Extensive experiments demonstrate that our frequency-aware heuristic scheme provides an optimal control solution for DiT-based models. Thanks to this scheme, our ConsisID generates high-quality, identity-preserving videos, making strides towards more effective IPT2V.</span><br><span class="line"></span><br><span class="line">ConsisID can generate high-quality Identity-Preserving videos using an input Image and a text prompt! 🤩 🚀</span><br><span class="line">Apache 2.0 Licensed. ConsisID Gradio app is available on Hugging Face</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/pdf/2411.01747</span><br><span class="line">DynaSaur: Large Language Agents Beyond Predefined Actions</span><br><span class="line">Dang Nguyen, Viet Dac Lai, Seunghyun Yoon, Ryan A. Rossi, Handong Zhao, Ruiyi Zhang, Puneet Mathur, Nedim Lipka, Yu Wang, Trung Bui, Franck Dernoncourt, Tianyi Zhou</span><br><span class="line">12/2/24</span><br><span class="line"></span><br><span class="line">Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly-scoped environments, we argue that it presents two major challenges when deploying LLM agents in real-world scenarios: (1) selecting from a fixed set of actions significantly restricts the planning and acting capabilities of LLM agents, and (2) this approach requires substantial human effort to enumerate and implement all possible actions, which becomes impractical in complex environments with a vast number of potential actions. In this work, we propose an LLM agent framework that enables the dynamic creation and composition of actions in an online manner. In this framework, the agent interacts with the environment by generating and executing programs written in a general-purpose programming language at each step. Furthermore, generated actions are accumulated over time for future reuse. Our extensive experiments on the GAIA benchmark demonstrate that this framework offers significantly greater flexibility and outperforms previous methods. Notably, it allows an LLM agent to recover in scenarios where no relevant action exists in the predefined set or when existing actions fail due to unforeseen edge cases. At the time of writing, we hold the top position on the GAIA public leaderboard. Our code can be found in \href&#123;this https URL&#125;&#123;this https URL&#125;.</span><br><span class="line"></span><br><span class="line">𝗔𝗱𝗼𝗯𝗲&#x27;𝘀 𝗰𝗼𝗱𝗲-𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗶𝗻𝗴 𝗮𝗴𝗲𝗻𝘁 𝗿𝗲𝗮𝗰𝗵𝗲𝘀 𝘁𝗵𝗲 𝘁𝗼𝗽 𝗼𝗳 𝗚𝗔𝗜𝗔 𝗹𝗲𝗮𝗱𝗲𝗿𝗯𝗼𝗮𝗿𝗱 - and their paper cites</span><br><span class="line">💡 Reminder: Broadly defined, an &quot;Agent&quot; is a system where a LLM is augmented with the ability to run multiple steps or call tools. This means when a user asks a question, the LLM&#x27;s answer can be grounded in a web search, document or code execution instead of having to rely on the LLM&#x27;s lackuster awareness of the outside world.</span><br><span class="line">In short, Agents are a vehicle in which you put your LLM to allow it access to the outside world!</span><br><span class="line">➡️ The team of researchers at Adobe started from the idea that current agentic systems lack the ability to define their own tools. So they decided to make an agent that writes actions as code, thus allowing it to write python functions that can be re-used later as tools!</span><br><span class="line">Here&#x27;s what the LLM generations can look like with the proper prompt:</span><br><span class="line">Thought: I need to access the excel file using a different method.</span><br><span class="line">Action:</span><br><span class="line">```py</span><br><span class="line">def access_excel_file(file_path)</span><br><span class="line">... # rest of the code (the agent does writes it, but I don&#x27;t have room in this post)</span><br><span class="line">return rows</span><br></pre></td></tr></table></figure>

<p>Then your system executes this and appends the observation to the agent’s memory.<br>Why is this code formulation better than classical tool use formulation as JSON? The paper explains:<br>“Most existing work uses text or JSON as the representation of actions, which significantly lacks the two criteria mentioned earlier: generality and composability. In contrast, DynaSaur can utilize available actions or create new ones if necessary, using code as a unified representation. In principle, acting with code enables agents to solve any Turing-complete problem.”<br>The idea of using code is not new: in fact, we do it in transformers.agents (thus the citation that I got). They implementation adds further refinements, like using RAG to retrieve relevant functions before generating an action, which increases performance further.<br>And they observe that code agents perform much better, reaching the top of GAIA leaderboard! 🥇<br>Go take a look, it’s really clear and informative!</p>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><p><a target="_blank" rel="noopener" href="https://github.com/Francis-Rings/StableAnimator">https://github.com/Francis-Rings/StableAnimator</a></p>
<p>🛟🛟 StableAnimator: HQ ID-aware Humans 🛟🛟<br>👉StableAnimator: first e2e ID-preserving diffusion for HQ videos without any post-processing. Input: single image + sequence of poses. Training, pre-processing, eval-dataset &amp; pro-model announced. Code &amp; base-model released💙<br>𝐇𝐢𝐠𝐡𝐥𝐢𝐠𝐡𝐭𝐬:<br>✅First E2E ID-preserving human animation<br>✅Global content-aware Face Encoder + ID-Adapter<br>✅Incorporate face embeddings maintaining fidelity<br>✅Novel HJB equation-based face optimization<br>✅Authors: CS-FUDAN, CMIC, #Microsoft, Huya &amp; CMU<br>#artificialintelligence #machinelearning #ml #AI #deeplearning #computervision #AIwithPapers #metaverse #LLM #tiktok</p>
<p>StableAnimator</p>
<p>StableAnimator: High-Quality Identity-Preserving Human Image Animation<br>Shuyuan Tu1, Zhen Xing1, Xintong Han3, Zhi-Qi Cheng4, Qi Dai2, Chong Luo2, Zuxuan Wu1<br>[1Fudan University; 2Microsoft Research Asia; 3Huya Inc; 4Carnegie Mellon University]</p>
<p>Pose-driven Human image animations generated by StableAnimator, showing its power to synthesize high-fidelity and ID-preserving videos. All animations are directly synthesized by StableAnimator without the use of any face-related post-processing tools, such as the face-swapping tool FaceFusion or face restoration models like GFP-GAN and CodeFormer.</p>
<p>Comparison results between StableAnimator and state-of-the-art (SOTA) human image animation models highlight the superior performance of StableAnimator in delivering high-fidelity, identity-preserving human image animation.</p>
<p>Overview<br>model architecture<br>The overview of the framework of StableAnimator.</p>
<p>Current diffusion models for human image animation struggle to ensure identity (ID) consistency. This paper presents StableAnimator, the first end-to-end ID-preserving video diffusion framework, which synthesizes high-quality videos without any post-processing, conditioned on a reference image and a sequence of poses. Building upon a video diffusion model, StableAnimator contains carefully designed modules for both training and inference striving for identity consistency. In particular, StableAnimator begins by computing image and face embeddings with off-the-shelf extractors, respectively and face embeddings are further refined by interacting with image embeddings using a global content-aware Face Encoder. Then, StableAnimator introduces a novel distribution-aware ID Adapter that prevents interference caused by temporal layers while preserving ID via alignment. During inference, we propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization to further enhance the face quality. We demonstrate that solving the HJB equation can be integrated into the diffusion denoising process, and the resulting solution constrains the denoising path and thus benefits ID preservation. Experiments on multiple benchmarks show the effectiveness of StableAnimator both qualitatively and quantitatively.</p>
<p>News<br>[2024-12-4]:🔥 We are thrilled to release an interesting dance demo (🔥🔥APT Dance🔥🔥)! The generated video can be seen on YouTube and Bilibili.<br>[2024-11-28]:🔥 The data pre-processing codes (human skeleton extraction) are available! Other codes will be released very soon. Stay tuned!<br>[2024-11-26]:🔥 The project page, code, technical report and a basic model checkpoint are released. Further training codes, data pre-processing codes, the evaluation dataset and StableAnimator-pro will be released very soon. Stay tuned!</p>
<h3 id="-1"><a href="#-1" class="headerlink" title=""></a></h3><p><a target="_blank" rel="noopener" href="https://cxcx1996.github.io/HiFiVFS/">https://cxcx1996.github.io/HiFiVFS/</a><br>HiFiVFS: High Fidelity Video Face Swapping<br>Xu Chen*,1, Keke He*,1, Junwei Zhu†,1, Yanhao Ge2, Wei Li2, Chengjie Wang1<br>1Tencent, 2VIVO<br>12&#x2F;1&#x2F;24<br>*Indicates Equal Contribution †Corresponding Author<br>Special makeup<br>Different face shape<br>Complicated occlusion<br>Complex lighting conditions<br>Extreme poses<br>Special makeup<br>Different face shape<br>Complicated occlusion<br>Complex lighting conditions<br>Extreme poses<br>Special makeup<br>Different face shape<br>Complicated occlusion<br>Abstract<br>Face swapping aims to generate results that combine the identity from the source with attributes from the target. Existing methods primarily focus on image-based face swapping. When processing videos, each frame is handled independently, making it difficult to ensure temporal stability. From a model perspective, face swapping is gradually shifting from generative adversarial networks (GANs) to diffusion models (DMs), as DMs have been shown to possess stronger generative capabilities. Current diffusion-based approaches often employ inpainting techniques, which struggle to preserve fine-grained attributes like lighting and makeup. To address these challenges, we propose a high fidelity video face swapping (HiFiVFS) framework, which leverages the strong generative capability and temporal prior of Stable Video Diffusion (SVD). We build a fine-grained attribute module to extract identity-disentangled and fine-grained attribute features through identity desensitization and adversarial learning. Additionally, We introduce detailed identity injection to further enhance identity similarity. Extensive experiments demonstrate that our method achieves state-of-the-art (SOTA) in video face swapping, both qualitatively and quantitatively.</p>
<p>Comparisons in the wild<br>Method Overview<br>HiFiVFS<br>Pipeline of our proposed HiFiVFS, including training and inference phases. HiFiVFS is primarily trained based on the SVD framework, utilizing multi-frame input and a temporal attention to ensure the stability of the generated videos. In the training phase, HiFiVFS introduces fine-grained attribute learning (FAL) and detailed identity learning (DIL). In FAL, attribute disentanglement and enhancement are achieved through identity desensitization and adversarial learning. DIL uses more face swapping suited ID features to further boost identity similarity. In the inference phase, FAL only retains Eatt for attribute extraction, making the testing process more convenient. It is noted that HiFiVFS is trained and tested in the latent space, but for visualization purposes, we illustrate all processes in the original image space.</p>
<p>👺👺 HiFiVFS: Extreme Face Swapping 👺👺<br>👉#Tencent unveils a novel video face swapping method called HiFiVFS, which can consistently generate HQ face swapping videos even in extremely challenging scenarios (occlusion, makeup, lights, extreme poses, etc.). Impressive results, no code announced😢<br>𝐇𝐢𝐠𝐡𝐥𝐢𝐠𝐡𝐭𝐬:<br>✅HiFiVFS: high-fidelity video face swapping<br>✅Temporal stability within the face swapping<br>✅FAL: Fine-grained Attributes Learning<br>✅DIL: Detailed Identity Learning preservation<br>✅It’s the new SOTA, especially with extreme clips<br>#artificialintelligence #machinelearning #ml #AI #deeplearning #computervision #AIwithPapers #metaverse #LLM</p>
<h3 id="-2"><a href="#-2" class="headerlink" title=""></a></h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/showlab/ShowUI-2B">https://huggingface.co/showlab/ShowUI-2B</a><br>𝗦𝗵𝗼𝘄𝗨𝗜: 𝗮 𝘀𝗺𝗮𝗹𝗹 𝗲𝗻𝗱-𝘁𝗼-𝗲𝗻𝗱 𝗮𝗴𝗲𝗻𝘁 𝘁𝗵𝗮𝘁 𝗰𝗮𝗻 𝗻𝗮𝘃𝗶𝗴𝗮𝘁𝗲 𝗮𝗻𝘆 𝗨𝗜 𝗮𝗻𝗱 𝗼𝘂𝘁𝗽𝗲𝗿𝗳𝗼𝗿𝗺𝘀 𝗺𝘂𝗰𝗵 𝗯𝗶𝗴𝗴𝗲𝗿 𝘀𝘆𝘀𝘁𝗲𝗺𝘀! 📲<br>12&#x2F;5&#x2F;24</p>
<p>A team from National University of Singapore and Microsoft just released an agent that can act on any UI (Desktop, Android, Web) without needing additional text information. It works extremely well : they applied their method on a tiny Qwen2-VL-2B, and they managed to beat methods that use either much more powerful vision models (like GPT-4V) without using any additional info (e.g. leveraging the DOM of a webpage) like previous methods did ! 👏👏<br>🧑‍🏫 Reminder: definition of an “Agent”: this is really a bloated word by now. It would be more precise to talk of “agency”, defined as “the ability to for an LLM to execute actions on its environment”. Just parsing an LLM output and using it to determine the workflow of your code could be called an “agent” already.<br>Anyway, they started from the idea that most existing methods rely heavily on text, which makes them less generalizable, while letting aside rich UI structure that user actually rely on when navigating this interfaces.<br>⚙️ They put several good ideas to work:<br>💡 Simplify screenshots to the max:<br>They prune a lot the heavy visual content of UI screenshots, by removing cloned image patches (like any vast patch of the same color will be reduced to a small patch, while maintaining positional embeddings), then group patches from the same GUI elements together to simplify even further<br>💡 Build a truly generalist dataset:<br>To train a general UI agent, you need trajectories from each possible UI, and express them in a common language. Authors merge datasets like OmniAct for Desktop, Mind2Web for websites, AMEX for Android trajectories to create a high-quality and diverse dataset.<br>➡️ Nice results ensued:<br>They fine-tune a tiny Qwen-2-VL-2B on their method, and it reaches SOTA on several task (element identification, web navigation), even beating methods that either use additional info from the DOM or use much bigger VLMS like GPT-4v! 🏆<br>And performance could certainly jump with a slightly bigger vision model. Let’s hope the community builds this soon! 🚀<br>howUI is a lightweight vision-language-action model for GUI agents.</p>
<p>You can easily run this model on Windows and macOS using OOTB!</p>
<p>model arXiv demo dataset X (formerly Twitter) URL Hits</p>
<p>If you like our project, please give us a star ⭐ for the latest update.</p>
<p>📢 News<br>[2024.11.16] showlab&#x2F;ShowUI-2B is available at huggingface.<br>[2024.11.27] We release the arXiv paper, HF Spaces demo and ShowUI-desktop-8K.<br>[2024.12.1] We support iterative refinement to improve grounding accuracy. Try it at HF Spaces demo.<br>[2024.12.5] Major Update: ShowUI is integrated into OOTB for local run!</p>
<h3 id="-3"><a href="#-3" class="headerlink" title=""></a></h3><p><a target="_blank" rel="noopener" href="https://github.com/ADaM-BJTU/O1-CODER">https://github.com/ADaM-BJTU/O1-CODER</a><br>O1-CODER: AN O1 REPLICATION FOR CODING<br>Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong &amp; Jitao Sang ∗<br>12&#x2F;4&#x2F;24<br>School of Computer Science and Technology<br>Beijing Jiaotong University<br>Beijing, China<br>{yuxiangzhang, wushangxi, yqyang, jiangmingshu, jinlinx, 23120361,<br>jtsang}@bjtu.edu.cn<br>ABSTRACT<br>The technical report introduces O1-CODER, an attempt to replicate OpenAI’s o1<br>model with a focus on coding tasks. It integrates reinforcement learning (RL)<br>and Monte Carlo Tree Search (MCTS) to enhance the model’s System-2 thinking capabilities. The framework includes training a Test Case Generator (TCG)<br>for standardized code testing, using MCTS to generate code data with reasoning<br>processes, and iteratively fine-tuning the policy model to initially produce pseudocode, followed by the generation of the full code. The report also addresses the<br>opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for environment state updates. Updated model progress and experimental<br>results will be reported in subsequent versions. All source code, curated datasets,<br>as well as the derived models will be disclosed at <a target="_blank" rel="noopener" href="https://github.com/ADaMBJTU/O1-CODER">https://github.com/ADaMBJTU/O1-CODER</a> .</p>
<p>Overview<br>O1-CODER is an attempt to replicate OpenAI’s O1 model, focused on coding tasks. The approach combines Reinforcement Learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the model’s System-2 thinking capabilities, aiming to generate more efficient and logical code.</p>
<p>Method<br>The core components of O1-CODER are:</p>
<p>Test Case Generator (TCG): Automatically generates standardized test cases to evaluate the correctness of the generated code.<br>Self-Play and Reinforcement Learning: The model generates reasoning data through self-play, and uses RL and MCTS to iteratively optimize the policy model。 These methods work in an iterative cycle, continuously refining the model to improve systematic reasoning and optimization in coding tasks.</p>
<h3 id="-4"><a href="#-4" class="headerlink" title=""></a></h3><p><a target="_blank" rel="noopener" href="https://gen-omnimatte.github.io/">https://gen-omnimatte.github.io/</a></p>
<p>Generative Omnimatte<br>Learning to Decompose Video into Layers<br>Yao-Chih Lee1,2 Erika Lu1 Sarah Rumbley1 Michal Geyer1,3 Jia-Bin Huang2 Tali Dekel1,3 Forrester Cole1<br>1Google DeepMind 2University of Maryland College Park 3Weizmann Institute of Science</p>
<p>12&#x2F;3&#x2F;24</p>
<p>This AI Learned to Turn a Video Into Layers 🖼️<br>✨ Developed by Google DeepMind, this new method decomposes a video into complete layers, including objects and their associated effects (e.g., shadows, reflections). 🎬<br>🤔 Previous methods coulnd’t handle:</p>
<ul>
<li>dynamic backgrounds</li>
<li>occlusions</li>
<li>effect association for multi objects<br>Video is liquid 💦</li>
</ul>
<p>Input video<br>Omnimatte layers</p>
<p>lego</p>
<p>Our method decomposes a video into a set of RGBA omnimatte layers,<br>where each layer consists of a fully-visible object and its associated effects like shadows and reflections.<br>Our omnimattes enable a wide range of video editing for users.<br>(Scroll to view more videos)</p>
<p>Comparisons on Omnimattes</p>
<p>Input<br>Omnimatte<br>Omnimatte3D<br>OmnimatteRF<br>Ours<br>We compare our method with existing omnimatte methods (Omnimatte, Omnimatte3D, OmnimatteRF, and FactorMatte). Existing methods rely on restrictive motion assumptions, such as stationary background, resulting in dynamic background elements becoming entangled with foreground object layers. Omnimatte3D and OmnimatteRF may also produce blurry background layers (e.g., horses) because their 3D-aware background representations are sensitive to camera pose estimation quality. Furthermore, these methods lack a generative and semantic prior for completing occluded pixels and accurately associating effects with their corresponding objects.</p>
<p>Comparisons on Object and Effect Removal</p>
<p>Input &amp; object to remove<br>ProPainter<br>Lumiere-Inpainting<br>ObjectDrop<br>Ours<br>We compare our object-effect-removal model, Casper, with existing methods for object removal. Video inpainting models (ProPainter and Lumiere-Inpainting) fail to remove soft shadows and reflections outside the input masks. ObjectDrop is an image-based model, and thus, it processes each video frame independently and inpaints regions without global context and temporal consistency. We use the same ratio of mask dilation for all the methods.</p>
<p>Method<br>Given an input video and binary object masks, we first apply our object-effect-removal model, Casper, to generate a clean-plate background and a set of single-object (solo) videos applying different trimask conditions. The trimasks specify regions to preserve (white), remove (black), and regions that potentially contain uncertain object effects (gray). In Stage 2, a test-time optimization reconstructs the omnimatte layers Oi from pairs of solo video and background video.</p>
<p>Object and Effect Removal with Trimask Condition<br>We use different trimask conditions for an input video to obtain a set of single-object (solo) videos and a clean-plate background video (bottom row). Note that we do not cherry pick the random seeds for the Casper model. We use the same random seed (&#x3D;0) for all different input videos.</p>
<p>Input<br>Trimask<br>Output removal</p>
<p>Training data</p>
<p>We collect omnimatte results from existing omnimatte methods (Omnimatte, Omnimatte3D, and OmnimatteRF) to provide examples of cause-and-effect relationships in real videos.</p>
<p>Ablation Study on Training data of Casper</p>
<p>Input<br>Trimask<br>Omnimatte-only</p>
<ul>
<li>Tripod</li>
<li>Kubric</li>
<li>Object-Paste (full)<br>We assess the individual contributions of each dataset category to our model’s performance by incrementally adding each category to the training set. While the Omnimatte data provides basic examples of shadows in real-world videos, it primarily features static backgrounds and single objects. The Tripod data provides additional real-world scenarios to handle better water effects, such as reflections and boat wakes. Our Kubric synthetic data strengthens the models’ ability to handle multi-object scenes. Finally, the Object-Paste data reduces undesired background changes and improves inpainting quality.</li>
</ul>
<p>Ablation Study on Input Condition of Casper</p>
<p>Input<br>Masked RGB + binary mask<br>Unmasked RGB + binary mask<br>Unmasked RGB + Trimask (ours)<br>Our proposed trimask explicitly defines the regions to be removed or preserved, thereby enabling more accurate handling of multi-object scenarios. In contrast, the model trained on binary masks is susceptible to ambiguity, potentially leading to undesired removal of objects meant to be preserved.</p>
<p>Our Limitations</p>
<p>Input<br>Trimask<br>Output removal<br>The removal model may not always produce the desired outcome, particularly in challenging multi-object cases.</p>
<p>User-specified trimask<br>We observe some cases where Casper will associate unrelated dynamic background effects with a foreground layer, such as the waves in the below example. To mitigate this, our system allows the user to modify the trimask by specifying a coarse preservation region to preserve the background waves better.</p>
<p>Visualization of Effect Association in the Self-Attention of Video Generator</p>
<p>Input &amp; target object<br>for visualization metric<br>Lumiere T2V<br>output &amp; attention<br>Lumiere Inpainting<br>output &amp; attention<br>Our Casper<br>output &amp; attention</p>
<p>To investigate the inherent understanding of object-effect associations in the text-to-video (T2V) Lumiere generation model, we analyze its self-attention patterns during the denoising process using SDEdit. We hypothesize that the T2V model possesses an intrinsic understanding of effect associations, allowing us to train an effective object-effect-removal model with a relatively small dataset.</p>
<p>We further compare the attention behaviors of the original T2V model, the Lumiere-Inpainting model, and our Casper model, which is sequentially fine-tuned from the T2V model. To ensure accurate attention measurement, we do not dilate the input mask conditions for both Inpainting and Casper models.</p>
<p>The visualized value of each pixel indicates the strength of association between its query token and the key tokens in the target object mask region. We visualize the first, middle, and final attention blocks of the U-Net architecture at the sampling step t&#x3D;0.125. For a detailed description of the attention visualization metric, please refer to Section 3.3 of our main paper.</p>
<p>We observe that the T2V model’s object query tokens exhibit a strong focus on the object itself, as its primary task is to generate the object and its effects. This tendency may also be present in the Inpainting model when it attempts to fill the mask region with another object to justify shadows. In contrast, Casper’s object query tokens show less self-attention and more attention to the background region, suggesting a focus on background completion rather than object and effect generation.</p>
<p>In multi-object scenarios (boys-beach, five-beagles), the T2V and Inpainting models may associate different, similar objects with the target object. Our Casper model, however, demonstrates a lower attention response (darker) to similar objects, indicating a stronger ability to isolate individual objects.</p>
<p>We also analyzed the attention patterns of the failure case, five-beagles, where our Casper model did not remove the corresponding shadow completely. We hypothesize that the effect association is already weak in the T2V model, and our Casper model, inheriting knowledge from the pretrained models, struggles to handle such challenging cases.</p>
<h3 id="-5"><a href="#-5" class="headerlink" title=""></a></h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.02205">https://arxiv.org/pdf/2412.02205</a><br>[Submitted on 3 Dec 2024 (v1), last revised 4 Dec 2024 (this version, v2)]<br>DataLab: A Unified Platform for LLM-Powered Business Intelligence<br>Luoxuan Weng, Yinghao Tang, Yingchaojie Feng, Zhuo Chang, Peng Chen, Ruiqin Chen, Haozhe Feng, Chen Hou, Danqing Huang, Yang Li, Huaming Rao, Haonan Wang, Canshi Wei, Xiaofeng Yang, Yuhui Zhang, Yifeng Zheng, Xiuqi Huang, Minfeng Zhu, Yuxin Ma, Bin Cui, Wei Chen<br>Business intelligence (BI) transforms large volumes of data within modern organizations into actionable insights for informed decision-making. Recently, large language model (LLM)-based agents have streamlined the BI workflow by automatically performing task planning, reasoning, and actions in executable environments based on natural language (NL) queries. However, existing approaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS. The fragmentation of tasks across different data roles and tools lead to inefficiencies and potential errors due to the iterative and collaborative nature of BI. In this paper, we introduce DataLab, a unified BI platform that integrates a one-stop LLM-based agent framework with an augmented computational notebook interface. DataLab supports a wide range of BI tasks for different data roles by seamlessly combining LLM assistance with user customization within a single environment. To achieve this unification, we design a domain knowledge incorporation module tailored for enterprise-specific BI tasks, an inter-agent communication mechanism to facilitate information sharing across the BI workflow, and a cell-based context management strategy to enhance context utilization efficiency in BI notebooks. Extensive experiments demonstrate that DataLab achieves state-of-the-art performance on various BI tasks across popular research benchmarks. Moreover, DataLab maintains high effectiveness and efficiency on real-world datasets from Tencent, achieving up to a 58.58% increase in accuracy and a 61.65% reduction in token cost on enterprise-specific BI tasks.</p>
<h3 id="-6"><a href="#-6" class="headerlink" title=""></a></h3><p><a target="_blank" rel="noopener" href="https://ai.pydantic.dev/">https://ai.pydantic.dev/</a><br>PydanticAI<br>Agent Framework &#x2F; shim to use Pydantic with LLMs<br>12&#x2F;2&#x2F;24</p>
<p>A New Agent Framework from Pydantic looks like a combination of instructor and the OpenAI swarm concept! It’s simple, model-agnostic, type-safe, and without complex abstractions!<br>TL;DR:<br>🔧 Uses Python for control flow and composition - no special DSL or patterns<br>🔍 Type-safe by design with full IDE support and static type-checking<br>📈 Model-agnostic, supports OpenAI, Gemini, and open models from vLLM or TGI<br>📚 Structured response validation with Pydantic<br>🌊 Supports streaming responses with validation<br>🎮 Dynamic runtime context&#x2F;dependencies, e.g. customer data<br>🧪 Dependency injection for testing and iterative development.<br>🤖 Supports function tools, reflection, and self-correction.</p>
<p>CI Coverage PyPI versions license</p>
<p>When I first found FastAPI, I got it immediately. I was excited to find something so innovative and ergonomic built on Pydantic.</p>
<p>Virtually every Agent Framework and LLM library in Python uses Pydantic, but when we began to use LLMs in Pydantic Logfire, I couldn’t find anything that gave me the same feeling.</p>
<p>PydanticAI is a Python Agent Framework designed to make it less painful to build production grade applications with Generative AI.</p>
<p>Why use PydanticAI<br>Built by the team behind Pydantic (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more)<br>Model-agnostic — currently OpenAI, Gemini, and Groq are supported, Anthropic is coming soon. And there is a simple interface to implement support for other models.<br>Type-safe<br>Control flow and agent composition is done with vanilla Python, allowing you to make use of the same Python development best practices you’d use in any other (non-AI) project<br>Structured response validation with Pydantic<br>Streamed responses, including validation of streamed structured responses with Pydantic<br>Novel, type-safe dependency injection system, useful for testing and eval-driven iterative development<br>Logfire integration for debugging and monitoring the performance and general behavior of your LLM-powered application</p>
<h3 id="-7"><a href="#-7" class="headerlink" title=""></a></h3><p><a target="_blank" rel="noopener" href="https://github.com/quivrhq/megaparse">https://github.com/quivrhq/megaparse</a><br>12&#x2F;4&#x2F;24<br>MegaParse - Your Parser for every type of documents<br>Quivr-logo<br>MegaParse is a powerful and versatile parser that can handle various types of documents with ease. Whether you’re dealing with text, PDFs, Powerpoint presentations, Word documents MegaParse has got you covered. Focus on having no information loss during parsing.</p>
<p>Key Features 🎯<br>Versatile Parser: MegaParse is a powerful and versatile parser that can handle various types of documents with ease.<br>No Information Loss: Focus on having no information loss during parsing.<br>Fast and Efficient: Designed with speed and efficiency at its core.<br>Wide File Compatibility: Supports Text, PDF, Powerpoint presentations, Excel, CSV, Word documents.<br>Open Source: Freedom is beautiful, and so is MegaParse. Open source and free to use.<br>Support<br>Files: ✅ PDF ✅ Powerpoint ✅ Word<br>Content: ✅ Tables ✅ TOC ✅ Headers ✅ Footers ✅ Images<br>Example<br>megaparse.mp4<br>Installation<br>pip install megaparse<br>Usage<br>Add your OpenAI or Anthropic API key to the .env file</p>
<p>Install poppler on your computer (images and PDFs)</p>
<p>Install tesseract on your computer (images and PDFs)</p>
<p>If you have a mac, you also need to install libmagic brew install libmagic</p>
<p>from megaparse import MegaParse<br>from langchain_openai import ChatOpenAI<br>from megaparse.parser.unstructured_parser import UnstructuredParser</p>
<p>parser &#x3D; UnstructuredParser()<br>megaparse &#x3D; MegaParse(parser)<br>response &#x3D; megaparse.load(“.&#x2F;test.pdf”)<br>print(response)<br>megaparse.save(“.&#x2F;test.md”)<br>Use MegaParse Vision<br>Change the parser to MegaParseVision<br>from megaparse import MegaParse<br>from langchain_openai import ChatOpenAI<br>from megaparse.parser.megaparse_vision import MegaParseVision</p>
<p>model &#x3D; ChatOpenAI(model&#x3D;”gpt-4o”, api_key&#x3D;os.getenv(“OPENAI_API_KEY”)) # type: ignore<br>parser &#x3D; MegaParseVision(model&#x3D;model)<br>megaparse &#x3D; MegaParse(parser)<br>response &#x3D; megaparse.load(“.&#x2F;test.pdf”)<br>print(response)<br>megaparse.save(“.&#x2F;test.md”)<br>Note: The model supported by MegaParse Vision are the multimodal ones such as claude 3.5, claude 4, gpt-4o and gpt-4.</p>
<p>(Optional) Use LlamaParse for Improved Results<br>Create an account on Llama Cloud and get your API key.</p>
<p>Change the parser to LlamaParser</p>
<p>from megaparse import MegaParse<br>from langchain_openai import ChatOpenAI<br>from megaparse.parser.llama_parser import LlamaParser</p>
<p>parser &#x3D; LlamaParser(api_key &#x3D; os.getenv(“LLAMA_CLOUD_API_KEY”))<br>megaparse &#x3D; MegaParse(parser)<br>response &#x3D; megaparse.load(“.&#x2F;test.pdf”)<br>print(response)<br>megaparse.save(“.&#x2F;test.md”) #saves the last processed doc in md format<br>Use as an API<br>There is a MakeFile for you, simply use : make dev at the root of the project and you are good to go.</p>
<p>See localhost:8000&#x2F;docs for more info on the different endpoints !</p>
<p>BenchMark<br>Parser similarity_ratio<br>megaparse_vision 0.87<br>unstructured_with_check_table 0.77<br>unstructured 0.59<br>llama_parser 0.33<br>Higher the better</p>
<p>Note: Want to evaluate and compare your Megaparse module with ours ? Please add your config in evaluations&#x2F;script.py and then run python evaluations&#x2F;script.py. If it is better, do a PR, I mean, let’s go higher together .</p>
<p>In Construction 🚧<br>Improve table checker<br>Create Checkers to add modular postprocessing ⚙️<br>Add Structured output, let’s get computer talking 🤖<br>Star History</p>
<h3 id="-8"><a href="#-8" class="headerlink" title=""></a></h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.14708">https://arxiv.org/abs/2411.14708</a></p>
<p>[Submitted on 22 Nov 2024 (v1), last revised 2 Dec 2024 (this version, v2)]<br>Understanding LLM Embeddings for Regression<br>Eric Tang, Bangding Yang, Xingyou Song<br>Google Research</p>
<p>With the rise of large language models (LLMs) for flexibly processing information as strings, a natural application is regression, specifically by preprocessing string representations into LLM embeddings as downstream features for metric prediction. In this paper, we provide one of the first comprehensive investigations into embedding-based regression and demonstrate that LLM embeddings as features can be better for high-dimensional regression tasks than using traditional feature engineering. This regression performance can be explained in part due to LLM embeddings over numeric data inherently preserving Lipschitz continuity over the feature space. Furthermore, we quantify the contribution of different model effects, most notably model size and language understanding, which we find surprisingly do not always improve regression performance.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">기술적으로 최대한 자세하게 적어. 14개의 기사가 있고 하나도 빼먹지 말고 적어.</span><br><span class="line"></span><br><span class="line">&lt;/details&gt;</span><br></pre></td></tr></table></figure>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-12-05</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/12/05/2024-12-5-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 12월 5일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/11/28/2024-11-28-AI-NEWS/" title="2024년 11월 28일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>