<!DOCTYPE html><html lang="ko-KR"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>2024년 12월 20일 AI 소식 · TECH BLOG  by Dongyoung Kim   Ph.D.</title><meta name="description" content="➡️ Cohere에서는 기업용 대형 언어 모델 시리즈의 마지막 모델인 Command R7B를 출시하였습니다.
➡️ Genesis-world에서는 로보틱스 및 물리 AI 애플리케이션을 위한 종합 물리 플랫폼인 Genesis를 공개하였습니다.
➡️ Answer.ai에서는 "><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- Google tag (gtag.js)--><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y36E4JYRDG"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y36E4JYRDG');</script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:157px;"><h3 title=""><a href="/">TECH BLOG  by Dongyoung Kim   Ph.D.</a></h3></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"></a><div class="by_farbox"><a href="https://dykim.dev/" target="_blank">© Dongyoung Kim, Ph.D. </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Curriculum Vitae</a></li><li><a href="/archives">Archive</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>2024년 12월 20일 AI 소식</a></h3></div><div class="post-content"><p>➡️ Cohere에서는 기업용 대형 언어 모델 시리즈의 마지막 모델인 Command R7B를 출시하였습니다.</p>
<p>➡️ Genesis-world에서는 로보틱스 및 물리 AI 애플리케이션을 위한 종합 물리 플랫폼인 Genesis를 공개하였습니다.</p>
<p>➡️ Answer.ai에서는 ModernBERT를 발표하여 BERT 모델의 현대화를 통해 성능과 효율성을 대폭 향상시켰습니다.</p>
<p>➡️ Google DeepMind에서는 Gemini 2.0 Flash Thinking 모델을 출시하여 다중 모달 입력과 강화된 추론 성능을 제공하고 있습니다.</p>
<p>➡️ IBM에서는 Granite 3.1 Language Models를 업데이트하여 더 긴 컨텍스트, 향상된 RAG 및 함수 호출 기능을 도입하였습니다.</p>
<p>➡️ 연구자들은 오픈 소스 LLM의 장점에 대한 논문을 제출하였습니다.</p>
<p>➡️ Hugging Face에서는 Falcon3 오픈 모델을 출시하여 다양한 버전과 향상된 성능을 선보였습니다.</p>
<p>➡️ Google DeepMind와 Google Research는 FACTS Grounding 벤치마크를 발표하여 LLM의 사실 정확성과 문서 기반 응답 능력을 평가하였습니다.</p>
<p>➡️ Tencent에서는 BrushEdit을 발표하여 정밀한 이미지 편집을 지원하는 인페인팅 모델을 소개하였습니다.</p>
<h3 id="Cohere-Command-R7B-출시"><a href="#Cohere-Command-R7B-출시" class="headerlink" title="Cohere, Command R7B 출시"></a>Cohere, Command R7B 출시</h3><p><a target="_blank" rel="noopener" href="https://cohere.com/blog/command-r7b">링크</a>, 2024년 12월 14일</p>
<ul>
<li>Cohere, R 시리즈 마지막 모델인 Command R7B 출시.</li>
<li>Command R7B, 128k 컨텍스트 길이, 다국어 지원, 인용 검증된 검색 보강 생성(RAG), 추론, 도구 사용, 에이전트 행동 기능 탑재.</li>
<li>소형 GPU, 맥북, CPU에서도 서비스 가능, AI 애플리케이션 배포 비용 절감.</li>
<li>HuggingFace Open LLM Leaderboard에서 동급 모델 중 평균 1위 기록.</li>
<li>수학, 코드, 추론 작업에서 다른 오픈 웨이트 모델 능가하는 성능 발휘.</li>
<li>기업용 AI 시스템 배포에 최적화, 높은 처리 속도와 실시간 사용 사례에 적합.</li>
</ul>
<h3 id="Genesis-world-Genesis-플랫폼-공개"><a href="#Genesis-world-Genesis-플랫폼-공개" class="headerlink" title="Genesis-world, Genesis 플랫폼 공개"></a>Genesis-world, Genesis 플랫폼 공개</h3><p><a target="_blank" rel="noopener" href="https://genesis-world.readthedocs.io/en/latest/">링크</a>, 2024년 12월 19일</p>
<ul>
<li>Genesis-world, 로보틱스, 엠보디드 AI, 물리 AI 애플리케이션 위한 종합 물리 플랫폼 Genesis 공개.</li>
<li>Genesis, 범용 물리 엔진, 경량 로보틱스 시뮬레이션 플랫폼, 포토리얼리스틱 렌더링 시스템, 생성 데이터 엔진 통합.</li>
<li>100% 파이썬 개발, 설치 용이, 사용자 친화적 API 제공.</li>
<li>기존 시뮬레이션 플랫폼보다 10~80배 빠른 시뮬레이션 속도, 다양한 물리 솔버 지원.</li>
<li>멀티스레드 시뮬레이션과 차별화된 렌더링 기능으로 고도의 물리적 정확성과 시각적 충실도 제공.</li>
<li>오픈 소스로 제공, 커뮤니티 기여 환영, 로보틱스 연구를 위한 자동화된 데이터 생성 가능.</li>
</ul>
<h3 id="Answer-ai-ModernBERT-발표"><a href="#Answer-ai-ModernBERT-발표" class="headerlink" title="Answer.ai, ModernBERT 발표"></a>Answer.ai, ModernBERT 발표</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/collections/answerdotai/modernbert-67627ad707a4acbf33c41deb">링크</a>, 2024년 12월 18일</p>
<ul>
<li>Answer.ai, ModernBERT 발표하여 BERT 모델 현대화.</li>
<li>ModernBERT, 8192 토큰의 컨텍스트 길이와 플래시 어텐션, RoPE 임베딩, 교대 어텐션 도입, 성능과 효율성 향상.</li>
<li>2조 토큰 대규모 데이터로 훈련, 다양한 분류 작업과 다중 벡터 검색에서 뛰어난 성능 발휘.</li>
<li>OpenAI O1 모델과 비교, 비슷한 크기 모델 중 우수한 성능, 메모리 효율성 뛰어남.</li>
<li>다양한 언어와 코드 데이터 학습, 프로그래밍 관련 작업에서 독보적인 성능 발휘.</li>
<li>Hugging Face와 Transformers에 공개, Apache 2.0 라이선스로 제공.</li>
</ul>
<h3 id="Google-DeepMind-Gemini-2-0-Flash-Thinking-출시"><a href="#Google-DeepMind-Gemini-2-0-Flash-Thinking-출시" class="headerlink" title="Google DeepMind, Gemini 2.0 Flash Thinking 출시"></a>Google DeepMind, Gemini 2.0 Flash Thinking 출시</h3><p><a target="_blank" rel="noopener" href="https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-thinking-exp-1219">링크</a>, 2024년 12월 11일</p>
<ul>
<li>Google DeepMind, Gemini 2.0 Flash Thinking 모델 출시, 향상된 추론 성능 제공.</li>
<li>Gemini 2.0 Flash, 다중 모달 입력(이미지, 비디오, 오디오) 지원, 체인 오브 싱크스(Chain of Thoughts) 노출시켜 강력한 추론 성능 구현.</li>
<li>Gemini API 통해 Google AI Studio와 Vertex AI에서 개발자 사용 가능.</li>
<li>Gemini 2.0 Flash, 빠른 응답 시간과 향상된 성능 제공, Gemini 앱과 AI 어시스턴트에 통합.</li>
<li>Project Astra, Project Mariner, Jules 등 에이전트 기반 연구 프로토타입 통해 다양한 도메인에서 에이전트 활용 가능성 탐구.</li>
<li>안전 및 보안 기준 준수, 사용자 프라이버시 보호 기능 탑재.</li>
</ul>
<h3 id="IBM-Granite-3-1-Language-Models-업데이트"><a href="#IBM-Granite-3-1-Language-Models-업데이트" class="headerlink" title="IBM, Granite 3.1 Language Models 업데이트"></a>IBM, Granite 3.1 Language Models 업데이트</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/collections/ibm-granite/granite-31-language-models-6751dbbf2f3389bec5c6f02d">링크</a>, 2024년 12월 18일</p>
<ul>
<li>IBM, Granite 3.1 Language Models 업데이트, 더 긴 컨텍스트, 향상된 RAG 및 함수 호출 기능 도입.</li>
<li>Granite3.1-8B-Instruct 모델, 8B 파라미터로 다양한 언어와 코드 데이터 특화 성능 발휘.</li>
<li>128K 토큰 긴 시퀀스 길이 지원, 12개 언어 다국어 지원.</li>
<li>Apache 2.0 라이선스로 공개, Hugging Face에서 접근 가능, 다양한 파인튜닝 옵션 제공.</li>
<li>텍스트 분류, 질문 응답, 코드 관련 작업 등 다양한 용도로 활용 가능.</li>
<li>IBM의 슈퍼컴퓨터 Blue Vela 사용, 대규모 데이터로 훈련.</li>
</ul>
<h3 id="오픈-소스-LLM의-장점에-대한-논문"><a href="#오픈-소스-LLM의-장점에-대한-논문" class="headerlink" title="오픈 소스 LLM의 장점에 대한 논문"></a>오픈 소스 LLM의 장점에 대한 논문</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.12004">링크</a>, 2024년 12월 16일</p>
<ul>
<li>연구자들, 오픈 소스와 클로즈드 소스 LLM 장단점 분석 논문 아카이브 제출.</li>
<li>오픈 소스 LLM인 LLaMA와 BLOOM, 커뮤니티 주도 개발과 컴퓨팅 효율성으로 경쟁력 있는 성능 발휘.</li>
<li>클로즈드 소스 모델 GPT-4, 대규모 데이터셋과 계산 자원 활용해 최첨단 성능 유지, 투명성 부족과 접근성 제한 문제 있음.</li>
<li>오픈 소스 모델, 투명성과 재현 가능성 제공, 윤리적 감사 프레임워크 부재로 일관된 윤리적 관리 어려움.</li>
<li>하이브리드 모델 필요성 강조, 접근성, 기술적 성능, 윤리적 배치 충족 가능성 제시.</li>
</ul>
<h3 id="Hugging-Face-Falcon3-오픈-모델-출시"><a href="#Hugging-Face-Falcon3-오픈-모델-출시" class="headerlink" title="Hugging Face, Falcon3 오픈 모델 출시"></a>Hugging Face, Falcon3 오픈 모델 출시</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/falcon3">링크</a>, 2024년 12월 17일</p>
<ul>
<li>Hugging Face, Falcon3 오픈 모델 출시.</li>
<li>Falcon3 모델, 10B 미만 파라미터로 수학, 코드, 과학 지식 분야에서 뛰어난 성능 발휘.</li>
<li>Falcon3-10B-Base 모델, MATH-Lvl5에서 22.9, GSM8K에서 83.0 점수 기록, 수학 추론 능력 입증.</li>
<li>다양한 버전(Instruct, GGUF, GPTQ-Int4 등) 제공, 다양한 애플리케이션에 유연하게 적용 가능.</li>
<li>Llama 아키텍처와 호환, AI 생태계에 쉽게 통합 가능.</li>
<li>Technology Innovation Institute (TII)에서 개발한 Falcon3, 14조 토큰으로 훈련.</li>
</ul>
<h3 id="Google-DeepMind와-Google-Research-FACTS-Grounding-벤치마크-발표"><a href="#Google-DeepMind와-Google-Research-FACTS-Grounding-벤치마크-발표" class="headerlink" title="Google DeepMind와 Google Research, FACTS Grounding 벤치마크 발표"></a>Google DeepMind와 Google Research, FACTS Grounding 벤치마크 발표</h3><p><a target="_blank" rel="noopener" href="https://www.kaggle.com/facts-leaderboard">링크</a>, 2024년 12월 17일</p>
<ul>
<li>Google DeepMind와 Google Research, FACTS Grounding 벤치마크 발표, LLM의 사실 정확성과 문서 기반 응답 능력 평가.</li>
<li>FACTS Grounding, 1,719개 예제로 구성, 860개 공개 데이터셋, 859개 비공개 데이터셋 포함.</li>
<li>다양한 도메인(의료, 법률, 기술, 금융, 소매 등) 포함, 사실 발견, 요약, 영향 분석, 개념 비교 등 작업 유형 포함.</li>
<li>Gemini 2.0 Flash 모델, 83.6% 정확도로 1위, OpenAI GPT-4o 모델, 78.8%로 그 뒤.</li>
<li>FACTS Grounding, 자동화된 LLM 심사 모델 사용, 다중 심사자 통해 편향 최소화.</li>
<li>Kaggle 리더보드 운영, 커뮤니티 참여 통해 지속적으로 업데이트 예정.</li>
</ul>
<h3 id="Tencent-Brush-Edit-출시"><a href="#Tencent-Brush-Edit-출시" class="headerlink" title="Tencent, Brush Edit 출시"></a>Tencent, Brush Edit 출시</h3><p><a target="_blank" rel="noopener" href="https://www.tencent.com/en-us/articles/2024-12-17">링크</a>, 2024년 12월 17일</p>
<ul>
<li>Tencent, BrushEdit 출시, 정밀한 이미지 편집을 지원하는 인페인팅 모델 공개.</li>
<li>BrushEdit, 자연어와 이미지를 기반으로 사용자가 원하는 대로 이미지를 편집할 수 있는 모델.</li>
<li>모델이 이미지를 정확하게 재구성하고, 다양한 물체를 삽입하거나 제거하는 기능 제공.</li>
<li>사용자가 텍스트 프롬프트와 마우스 드래그만으로 이미지를 손쉽게 수정할 수 있음.</li>
<li>고도화된 학습 데이터와 딥러닝 기술을 바탕으로 현실감 있는 이미지 생성 및 편집 가능.</li>
<li>온라인 상에서 쉽게 접근할 수 있는 플랫폼으로 이미지 편집의 새로운 패러다임 제시.</li>
</ul>
<details>
  <summary>Sources</summary>

<p>This GPT assists users by creating a detailed daily newspaper in Korean based on provided links. It follows these steps: read the content, summarize each content with detailed points, and write a report. The report format is:</p>
<h1 id="today’s-date-in-년-월-일-AI-소식"><a href="#today’s-date-in-년-월-일-AI-소식" class="headerlink" title="(today’s date in 년 월 일) AI 소식,"></a>(today’s date in 년 월 일) AI 소식,</h1><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>(overall short summary, make summary with good details. for Summary section, explain the details starting with company name, e.g. OpenAI에서는 ~~~를 발표하였습니다.)</p>
<h3 id="company-name-Title"><a href="#company-name-Title" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)</li>
</ul>
<h3 id="company-name-Title-1"><a href="#company-name-Title-1" class="headerlink" title="company name, Title"></a>company name, Title</h3><p><a href="link">링크</a>, date</p>
<p><a href="link">링크</a>, date,</p>
<ul>
<li>detailed summary1, (개조식 문체 사용)</li>
<li>detailed summary2, (개조식 문체 사용)<br>…</li>
<li>detailed summary N, (개조식 문체 사용)<br>…</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br><span class="line">936</span><br><span class="line">937</span><br><span class="line">938</span><br><span class="line">939</span><br><span class="line">940</span><br><span class="line">941</span><br><span class="line">942</span><br><span class="line">943</span><br><span class="line">944</span><br><span class="line">945</span><br><span class="line">946</span><br><span class="line">947</span><br><span class="line">948</span><br><span class="line">949</span><br><span class="line">950</span><br><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br><span class="line">954</span><br><span class="line">955</span><br><span class="line">956</span><br><span class="line">957</span><br><span class="line">958</span><br><span class="line">959</span><br><span class="line">960</span><br><span class="line">961</span><br><span class="line">962</span><br><span class="line">963</span><br><span class="line">964</span><br><span class="line">965</span><br><span class="line">966</span><br><span class="line">967</span><br><span class="line">968</span><br><span class="line">969</span><br><span class="line">970</span><br><span class="line">971</span><br><span class="line">972</span><br><span class="line">973</span><br><span class="line">974</span><br><span class="line">975</span><br><span class="line">976</span><br><span class="line">977</span><br><span class="line">978</span><br><span class="line">979</span><br><span class="line">980</span><br><span class="line">981</span><br><span class="line">982</span><br><span class="line">983</span><br><span class="line">984</span><br><span class="line">985</span><br><span class="line">986</span><br><span class="line">987</span><br><span class="line">988</span><br><span class="line">989</span><br><span class="line">990</span><br><span class="line">991</span><br><span class="line">992</span><br><span class="line">993</span><br><span class="line">994</span><br><span class="line">995</span><br><span class="line">996</span><br><span class="line">997</span><br><span class="line">998</span><br><span class="line">999</span><br><span class="line">1000</span><br><span class="line">1001</span><br><span class="line">1002</span><br><span class="line">1003</span><br><span class="line">1004</span><br><span class="line">1005</span><br><span class="line">1006</span><br><span class="line">1007</span><br><span class="line">1008</span><br><span class="line">1009</span><br><span class="line">1010</span><br><span class="line">1011</span><br><span class="line">1012</span><br><span class="line">1013</span><br><span class="line">1014</span><br><span class="line">1015</span><br><span class="line">1016</span><br><span class="line">1017</span><br><span class="line">1018</span><br><span class="line">1019</span><br><span class="line">1020</span><br><span class="line">1021</span><br><span class="line">1022</span><br><span class="line">1023</span><br><span class="line">1024</span><br><span class="line">1025</span><br><span class="line">1026</span><br><span class="line">1027</span><br><span class="line">1028</span><br><span class="line">1029</span><br><span class="line">1030</span><br><span class="line">1031</span><br><span class="line">1032</span><br><span class="line">1033</span><br><span class="line">1034</span><br><span class="line">1035</span><br><span class="line">1036</span><br><span class="line">1037</span><br><span class="line">1038</span><br><span class="line">1039</span><br><span class="line">1040</span><br><span class="line">1041</span><br><span class="line">1042</span><br><span class="line">1043</span><br><span class="line">1044</span><br><span class="line">1045</span><br><span class="line">1046</span><br><span class="line">1047</span><br><span class="line">1048</span><br><span class="line">1049</span><br><span class="line">1050</span><br><span class="line">1051</span><br><span class="line">1052</span><br><span class="line">1053</span><br><span class="line">1054</span><br><span class="line">1055</span><br><span class="line">1056</span><br><span class="line">1057</span><br><span class="line">1058</span><br><span class="line">1059</span><br><span class="line">1060</span><br><span class="line">1061</span><br><span class="line">1062</span><br><span class="line">1063</span><br><span class="line">1064</span><br><span class="line">1065</span><br><span class="line">1066</span><br><span class="line">1067</span><br><span class="line">1068</span><br><span class="line">1069</span><br><span class="line">1070</span><br><span class="line">1071</span><br><span class="line">1072</span><br><span class="line">1073</span><br><span class="line">1074</span><br><span class="line">1075</span><br><span class="line">1076</span><br><span class="line">1077</span><br><span class="line">1078</span><br><span class="line">1079</span><br><span class="line">1080</span><br><span class="line">1081</span><br><span class="line">1082</span><br><span class="line">1083</span><br><span class="line">1084</span><br><span class="line">1085</span><br><span class="line">1086</span><br><span class="line">1087</span><br><span class="line">1088</span><br><span class="line">1089</span><br><span class="line">1090</span><br><span class="line">1091</span><br><span class="line">1092</span><br><span class="line">1093</span><br><span class="line">1094</span><br><span class="line">1095</span><br><span class="line">1096</span><br><span class="line">1097</span><br><span class="line">1098</span><br><span class="line">1099</span><br><span class="line">1100</span><br><span class="line">1101</span><br><span class="line">1102</span><br><span class="line">1103</span><br><span class="line">1104</span><br><span class="line">1105</span><br><span class="line">1106</span><br><span class="line">1107</span><br><span class="line">1108</span><br><span class="line">1109</span><br><span class="line">1110</span><br><span class="line">1111</span><br><span class="line">1112</span><br><span class="line">1113</span><br><span class="line">1114</span><br><span class="line">1115</span><br><span class="line">1116</span><br><span class="line">1117</span><br><span class="line">1118</span><br><span class="line">1119</span><br><span class="line">1120</span><br><span class="line">1121</span><br><span class="line">1122</span><br><span class="line">1123</span><br><span class="line">1124</span><br><span class="line">1125</span><br><span class="line">1126</span><br><span class="line">1127</span><br><span class="line">1128</span><br><span class="line">1129</span><br><span class="line">1130</span><br><span class="line">1131</span><br><span class="line">1132</span><br><span class="line">1133</span><br><span class="line">1134</span><br><span class="line">1135</span><br><span class="line">1136</span><br><span class="line">1137</span><br><span class="line">1138</span><br><span class="line">1139</span><br><span class="line">1140</span><br><span class="line">1141</span><br><span class="line">1142</span><br><span class="line">1143</span><br><span class="line">1144</span><br><span class="line">1145</span><br><span class="line">1146</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">https://cohere.com/blog/command-r7b</span><br><span class="line">Introducing Command R7B: Fast and efficient generative AI</span><br><span class="line">Image of Aidan Gomez</span><br><span class="line">Aidan Gomez</span><br><span class="line">12월 14, 2024</span><br><span class="line"></span><br><span class="line">Blog Post Featured Image</span><br><span class="line">The smallest model in our R series delivers top-tier speed, efficiency, and quality to build powerful AI applications on commodity GPUs and edge devices.</span><br><span class="line"></span><br><span class="line">Product</span><br><span class="line">Newsroom</span><br><span class="line">Share:</span><br><span class="line"></span><br><span class="line">X (Formerly Twitter) Icon</span><br><span class="line">LinkedIn Icon</span><br><span class="line">Today, we’re excited to release Command R7B, the smallest, fastest, and final model in our R series of enterprise-focused large language models (LLMs). Command R7B provides state-of-the-art performance in its class of open-weights models across real-world tasks that matter for users. The model is designed for developers and businesses that need to optimize for the speed, cost-performance, and compute resources of their use cases.</span><br><span class="line"></span><br><span class="line">Like our other models in the R series, Command R7B offers a context length of 128k and excels in capabilities important for a wide range of business applications. It delivers a powerful combination of multilingual support, citation verified retrieval-augmented generation (RAG), reasoning, tool use, and agentic behavior. Thanks to its compact size and efficiency it can be served on low-end GPUs, a MacBook, or even CPUs – drastically lowering the cost of deploying AI applications into production.</span><br><span class="line"></span><br><span class="line">High performance in a small package</span><br><span class="line">A well-rounded model</span><br><span class="line">Command R7B excels on standardized and externally verifiable benchmarks such as the HuggingFace Open LLM Leaderboard. Compared to other similarly sized open-weights models, Command R7B ranks first on average with strong performance across all tasks.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">HuggingFace Leaderboard evaluation results. Competitor numbers are taken from the official leaderboard. Command R7B results are calculated by us using the official HuggingFace prompts and evaluation code.</span><br><span class="line">Enhanced efficiency in math, code, and reasoning tasks</span><br><span class="line">A major area of focus for Command R7B has been improving performance on math and reasoning, code, and multilingual tasks. In particular, the model matches or exceeds leading open-weights models in its class across common math and code benchmarks while using fewer parameters.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Model performance on math and code benchmarks. All numbers are from internal evaluations except those marked with an asterisk which are from externally reported results where these are higher. We use the base version of MBPPPlus, LBPP is the average across 6 languages, SQL the average of 3 datasets (SpiderDev and Test - hard and extra hard only, BirdBench, and an internal one) and COBOL is an internally developed dataset.</span><br><span class="line"></span><br><span class="line">Document translation quality evaluated with corpus spBLEU on the NTREX dataset.</span><br><span class="line">Best-in-class RAG, tool use, and agents</span><br><span class="line">Command R7B outperforms the other similarly sized open-weights models when it comes to core business use cases such as RAG, tool use, and AI agents. It is an ideal choice for enterprises looking for a cost-efficient model grounded in their internal documents and data. Like our other R series models, our RAG offering delivers native in-line citations that significantly reduce hallucinations and make fact-checking easier.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Performance evaluated across the ChatRAGBench (10-dataset average), BFCL-v3, StrategyQA, Bamboogle, and Tooltalk-hard. Methodology and further details are provided at the bottom in a footnote [1].</span><br><span class="line">For tool use, we see stronger overall performance than models of similar size on the industry-standard Berkeley Function-Calling Leaderboard. This shows Command R7B is particularly effective at tool use in real-world, diverse, and dynamic environments and avoids calling tools unnecessarily which is an important aspect of tool use in practical applications . Command R7B’s multi-step tool use capabilities allow it to power fast and capable AI agents.</span><br><span class="line"></span><br><span class="line">Optimized for enterprise use cases</span><br><span class="line">Our models are optimized for the capabilities enterprises need for real-world deployment of AI systems. The R series delivers an unmatched balance of efficiency and strong performance. This means ensuring they excel on human evaluation, the gold standard for quality assessment. Command R7B outperforms similarly sized open-weights models in blind head-to-head evaluations by human raters on RAG use cases our customers care about when building AI assistants for functions like customer service, HR, compliance, and IT support.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Head-to-head Human evaluation of vs Gemma 2 9B on a collection of 949 examples of enterprise RAG use-cases. All examples are at least 3-way blind-annotated by specially-trained human annotators, assessing fluency, faithfulness and response utility.</span><br><span class="line">Efficient and fast</span><br><span class="line">Command R7B’s compact size offers a reduced serving footprint that is ideal for rapid prototyping and iteration. It excels at high throughput, real-time use cases like chatbots and code assistants. It also unlocks dramatically cheaper deployment infrastructure such as consumer GPUs and CPUs to unlock on-device inference.</span><br><span class="line"></span><br><span class="line">We achieve this without compromising on our enterprise-grade security and privacy standards to protect customers&#x27; data.</span><br><span class="line"></span><br><span class="line">Get started</span><br><span class="line">Command R7B is available today on the Cohere Platform as well as accessible on HuggingFace. We’re excited to be releasing the weights of this model to provide greater access to cutting-edge technology for the AI research community.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://genesis-world.readthedocs.io/en/latest/</span><br><span class="line">12/19/24</span><br><span class="line"></span><br><span class="line">Genesis</span><br><span class="line">_images/teaser.png</span><br><span class="line">What is Genesis?</span><br><span class="line">Genesis is a physics platform designed for general purpose Robotics/Embodied AI/Physical AI applications. It is simultaneously multiple things:</span><br><span class="line"></span><br><span class="line">A universal physics engine re-built from the ground up, capable of simulating a wide range of materials and physical phenomena.</span><br><span class="line"></span><br><span class="line">A lightweight, ultra-fast, pythonic, and user-friendly robotics simulation platform.</span><br><span class="line"></span><br><span class="line">A powerful and fast photo-realistic rendering system.</span><br><span class="line"></span><br><span class="line">A generative data engine that transforms user-prompted natural language description into various modalities of data.</span><br><span class="line"></span><br><span class="line">Powered by a universal physics engine re-designed and re-built from the ground up, Genesis integrates various physics solvers and their coupling into a unified framework. This core physics engine is further enhanced by a generative agent framework that operates at an upper level, aiming towards fully automated data generation for robotics and beyond. Currently, we are open-sourcing the underlying physics engine and the simulation platform. The generative framework will be released in the near future.</span><br><span class="line"></span><br><span class="line">Genesis is built and will continuously evolve with the following long-term missions:</span><br><span class="line"></span><br><span class="line">Lowering the barrier to using physics simulations and making robotics research accessible to everyone. (See our commitment)</span><br><span class="line"></span><br><span class="line">Unifying a wide spectrum of state-of-the-art physics solvers into a single framework, allowing re-creating the whole physical world in a virtual realm with the highest possible physical, visual and sensory fidelity, using the most advanced simulation techniques.</span><br><span class="line"></span><br><span class="line">Minimizing human effort in collecting and generating data for robotics and other domains, letting the data flywheel spin on its own.</span><br><span class="line"></span><br><span class="line">Project Page: https://genesis-embodied-ai.github.io/</span><br><span class="line"></span><br><span class="line">Key Features</span><br><span class="line">Compared to prior simulation platforms, here we highlight several key features of Genesis:</span><br><span class="line"></span><br><span class="line">🐍 100% Python, both front-end interface and back-end physics engine, all natively developed in python.</span><br><span class="line"></span><br><span class="line">👶 Effortless installation and extremely simple and user-friendly API design.</span><br><span class="line"></span><br><span class="line">🚀 Parallelized simulation with unprecedented speed: Genesis is the world’s fastest physics engine, delivering simulation speeds up to 10~80x (yes, this is a bit sci-fi) faster than existing GPU-accelerated robotic simulators (Isaac Gym/Sim/Lab, Mujoco MJX, etc), without any compromise on simulation accuracy and fidelity.</span><br><span class="line"></span><br><span class="line">💥 A unified framework that supports various state-of-the-art physics solvers, modeling a vast range of materials and physical phenomena.</span><br><span class="line"></span><br><span class="line">📸 Photo-realistic ray-tracing rendering with optimized performance.</span><br><span class="line"></span><br><span class="line">📐 Differentiability: Genesis is designed to be fully compatible with differentiable simulation. Currently, our MPM solver and Tool Solver are differentiable, and differentiability for other solvers will be added soon (starting with rigid-body simulation).</span><br><span class="line"></span><br><span class="line">☝🏻 Physically-accurate and differentiable tactile sensor.</span><br><span class="line"></span><br><span class="line">🌌 Native support for Generative Simulation, allowing language-prompted data generation of various modalities: interactive scenes, task proposals, rewards, assets, character motions, policies, trajectories, camera motions, (physically-accurate) videos, and more.</span><br><span class="line"></span><br><span class="line">Getting Started</span><br><span class="line">Quick Installation</span><br><span class="line">Genesis is available via PyPI:</span><br><span class="line"></span><br><span class="line">pip install genesis-world</span><br><span class="line">You also need to install PyTorch following the official instructions.</span><br><span class="line"></span><br><span class="line">Documentation</span><br><span class="line">Please refer to our documentation site to for detailed installation steps, tutorials and API references.</span><br><span class="line"></span><br><span class="line">Contributing to Genesis</span><br><span class="line">The goal of the Genesis project is to build a fully transparent, user-friendly ecosystem where contributors from both robotics and computer graphics can come together to collaboratively create a high-efficiency, realistic (both physically and visually) virtual world for robotics research and beyond.</span><br><span class="line"></span><br><span class="line">We sincerely welcome any forms of contributions from the community to make the world a better place for robots. From pull requests for new features, bug reports, to even tiny suggestions that will make Genesis API more intuitive, all are wholeheartedly appreciated!</span><br><span class="line"></span><br><span class="line">WTF?! New open-source physics AI engine absolutely insane! 🤯 Genesis is a new physics engine that combines ultra-fast simulation with generative capabilities to create dynamic 4D worlds for robotics and physics.</span><br><span class="line">TL;DR:</span><br><span class="line">🚀 430,000x faster than real-time physics simulation, processes 43M FPS on a single RTX 4090</span><br><span class="line">🐍 Built in pure Python, 10-80x faster than existing GPU solutions like Isaac Gym</span><br><span class="line">🌐 Cross-platform support: Linux, MacOS, Windows, with CPU, NVIDIA, AMD, and Apple Metal backends</span><br><span class="line">🧪 Unified framework combining multiple physics solvers: Rigid body, MPM, SPH, FEM, PBD, Stable Fluid</span><br><span class="line">🤖 Extensive robot support: arms, legged robots, drones, soft robots; supports MJCF, URDF, obj, glb files</span><br><span class="line">🎨 Built-in photorealistic ray-tracing rendering</span><br><span class="line">📐 Differentiable simulation capabilities (currently for MPM and Tool solvers)</span><br><span class="line">🔄 Can generate environments, camera motions, robot policies, character animations from text prompts</span><br><span class="line">⚡ Takes only 26 seconds to train real-world transferrable robot locomotion policies</span><br><span class="line">💻 Simple installation via pip: pip install genesis-world</span><br><span class="line">🤝 Physics engine and simulation platform are fully open-sourced</span><br><span class="line">🔜 ”.generate” method/generative framework coming soon.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/collections/answerdotai/modernbert-67627ad707a4acbf33c41deb</span><br><span class="line">Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference</span><br><span class="line">Published on Dec 18</span><br><span class="line">·</span><br><span class="line">Submitted by</span><br><span class="line">jph00</span><br><span class="line">on Dec 19</span><br><span class="line">Authors:</span><br><span class="line">Benjamin Warner</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Antoine Chaffin</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Benjamin Clavié</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Orion Weller</span><br><span class="line">,</span><br><span class="line">Oskar Hallström</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Said Taghadouini</span><br><span class="line">,</span><br><span class="line">Alexis Gallagher</span><br><span class="line">,</span><br><span class="line">Raja Biswas</span><br><span class="line">,</span><br><span class="line">Faisal Ladhak</span><br><span class="line">,</span><br><span class="line"></span><br><span class="line">Tom Aarsen</span><br><span class="line">,</span><br><span class="line">Nathan Cooper</span><br><span class="line">,</span><br><span class="line">Griffin Adams</span><br><span class="line">,</span><br><span class="line">Jeremy Howard</span><br><span class="line">,</span><br><span class="line">Iacopo Poli</span><br><span class="line">Abstract</span><br><span class="line">Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.</span><br><span class="line"></span><br><span class="line">ModernBERT, BERT revisited in the age of LLMs and Generative AI! LightOn and Answer.ai modernized BERT! Improved architecture with 8192 context length, flash attention, and trained on 2T tokens. ModernBERT outperforms version BERT and RoBERTa versions! 👀</span><br><span class="line">TL;DR;</span><br><span class="line">2️⃣ Comes in 2 sizes base (139M) and large (395M)</span><br><span class="line">🚀 Better performance across all metrics than the original BERT</span><br><span class="line">📏 8,192 token context length (16x longer than BERT)</span><br><span class="line">⚡ Modern architecture with Flash Attention 2, RoPE embeddings, and alternating attention</span><br><span class="line">📚 Trained on 2 trillion tokens, primarily English and Code</span><br><span class="line">💨 2-4x faster than other models with mixed-length inputs</span><br><span class="line">🔓 Released under Apache 2.0</span><br><span class="line">🤗 Available on Hugging Face and Transformers (main)</span><br><span class="line"></span><br><span class="line">Finally, a Replacement for BERT</span><br><span class="line">Published December 19, 2024</span><br><span class="line">Benjamin Warner&#x27;s avatar</span><br><span class="line">bwarner</span><br><span class="line">Benjamin Warner</span><br><span class="line">Answer.AI&#x27;s avatar</span><br><span class="line">answerdotai</span><br><span class="line">Antoine Chaffin&#x27;s avatar</span><br><span class="line">NohTow</span><br><span class="line">Antoine Chaffin</span><br><span class="line">LightOn AI&#x27;s avatar</span><br><span class="line">lightonai</span><br><span class="line">Benjamin Clavié&#x27;s avatar</span><br><span class="line">bclavie</span><br><span class="line">Benjamin Clavié</span><br><span class="line">Answer.AI&#x27;s avatar</span><br><span class="line">answerdotai</span><br><span class="line">Orion Weller&#x27;s avatar</span><br><span class="line">orionweller</span><br><span class="line">Orion Weller</span><br><span class="line">guest</span><br><span class="line">Oskar Hallström&#x27;s avatar</span><br><span class="line">ohallstrom</span><br><span class="line">Oskar Hallström</span><br><span class="line">LightOn AI&#x27;s avatar</span><br><span class="line">lightonai</span><br><span class="line">Said Taghadouini&#x27;s avatar</span><br><span class="line">staghado</span><br><span class="line">Said Taghadouini</span><br><span class="line">LightOn AI&#x27;s avatar</span><br><span class="line">lightonai</span><br><span class="line">Alexis Gallagher&#x27;s avatar</span><br><span class="line">alexisgallagher</span><br><span class="line">Alexis Gallagher</span><br><span class="line">Answer.AI&#x27;s avatar</span><br><span class="line">answerdotai</span><br><span class="line">Raja Biswas&#x27;s avatar</span><br><span class="line">rbiswasfc</span><br><span class="line">Raja Biswas</span><br><span class="line">guest</span><br><span class="line">Faisal Ladhak&#x27;s avatar</span><br><span class="line">fladhak</span><br><span class="line">Faisal Ladhak</span><br><span class="line">guest</span><br><span class="line">Tom Aarsen&#x27;s avatar</span><br><span class="line">tomaarsen</span><br><span class="line">Tom Aarsen</span><br><span class="line">Nathan Cooper&#x27;s avatar</span><br><span class="line">ncoop57</span><br><span class="line">Nathan Cooper</span><br><span class="line">Answer.AI&#x27;s avatar</span><br><span class="line">answerdotai</span><br><span class="line">Griffin Adams&#x27;s avatar</span><br><span class="line">griffin</span><br><span class="line">Griffin Adams</span><br><span class="line">guest</span><br><span class="line">Jeremy Howard&#x27;s avatar</span><br><span class="line">jph00</span><br><span class="line">Jeremy Howard</span><br><span class="line">Answer.AI&#x27;s avatar</span><br><span class="line">answerdotai</span><br><span class="line">Jonathan Whitaker&#x27;s avatar</span><br><span class="line">johnowhitaker</span><br><span class="line">Jonathan Whitaker</span><br><span class="line">Answer.AI&#x27;s avatar</span><br><span class="line">answerdotai</span><br><span class="line">Iacopo Poli&#x27;s avatar</span><br><span class="line">iacolippo</span><br><span class="line">Iacopo Poli</span><br><span class="line">LightOn AI&#x27;s avatar</span><br><span class="line">lightonai</span><br><span class="line">TL;DR</span><br><span class="line">This blog post introduces ModernBERT, a family of state-of-the-art encoder-only models representing improvements over older generation encoders across the board, with a 8192 sequence length, better downstream performance and much faster processing.</span><br><span class="line"></span><br><span class="line">ModernBERT is available as a slot-in replacement for any BERT-like models, with both a base (139M params) and large (395M params) model size.</span><br><span class="line"></span><br><span class="line">Click to see how to use these models with transformers</span><br><span class="line">Introduction</span><br><span class="line">BERT was released in 2018 (millennia ago in AI-years!) and yet it’s still widely used today: in fact, it’s currently the second most downloaded model on the HuggingFace hub, with more than 68 million monthly downloads, only second to another encoder model fine-tuned for retrieval. That’s because its encoder-only architecture makes it ideal for the kinds of real-world problems that come up every day, like retrieval (such as for RAG), classification (such as content moderation), and entity extraction (such as for privacy and regulatory compliance).</span><br><span class="line"></span><br><span class="line">Finally, 6 years later, we have a replacement! Today, we at Answer.AI and LightOn (and friends!) are releasing ModernBERT. ModernBERT is a new model series that is a Pareto improvement over BERT and its younger siblings across both speed and accuracy. This model takes dozens of advances from recent years of work on large language models (LLMs), and applies them to a BERT-style model, including updates to the architecture and the training process.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">We expect to see ModernBERT become the new standard in the numerous applications where encoder-only models are now deployed, such as in RAG pipelines (Retrieval Augmented Generation) and recommendation systems.</span><br><span class="line"></span><br><span class="line">In addition to being faster and more accurate, ModernBERT also increases context length to 8k tokens (compared to just 512 for most encoders), and is the first encoder-only model that includes a large amount of code in its training data. These features open up new application areas that were previously inaccessible through open models, such as large-scale code search, new IDE features, and new types of retrieval pipelines based on full document retrieval rather than small chunks.</span><br><span class="line"></span><br><span class="line">But in order to explain just what we did, let’s first take a step back and look at where we’ve come from.</span><br><span class="line"></span><br><span class="line">Decoder-only models</span><br><span class="line">The recent high-profile advances in LLMs have been in models like GPT, Llama, and Claude. These are decoder-only models, or generative models. Their ability to generate human-like content has enabled astonishing new GenAI application areas like generated art and interactive chat. These striking applications have attracted major investment, funded booming research, and led to rapid technical advances. What we’ve done, essentially, is port these advances back to an encoder-only model.</span><br><span class="line"></span><br><span class="line">Why? Because many practical applications need a model that’s lean and mean! And it doesn’t need to be a generative model.</span><br><span class="line"></span><br><span class="line">More bluntly, decoder-only models are too big, slow, private, and expensive for many jobs. Consider that the original GPT-1 was a 117 million parameter model. The Llama 3.1 model, by contrast, has 405 billion parameters, and its technical report describes a data synthesis and curation recipe that is too complex and expensive for most corporations to reproduce. So to use such a model, like ChatGPT, you pay in cents and wait in seconds to get an API reply back from heavyweight servers outside of your control.</span><br><span class="line"></span><br><span class="line">Of course, the open-ended capabilities of these giant generative models mean that you can, in a pinch, press them into service for non-generative or discriminative tasks, such as classification. This is because you can describe a classification task in plain English and ... just ask the model to classify. But while this workflow is great for prototyping, you don’t want to pay prototype prices once you’re in mass production.</span><br><span class="line"></span><br><span class="line">The popular buzz around GenAI has obscured the role of encoder-only models. These are the workhorses of practical language processing, the models that are actually being used for such workloads right now in many scientific and commercial applications.</span><br><span class="line"></span><br><span class="line">Encoder-only models</span><br><span class="line">The output of an encoder-only model is a list of numerical values (an embedding vector). You might say that instead of answering with text, an encoder model literally encodes its “answer” into this compressed, numerical form. That vector is a compressed representation of the model&#x27;s input, which is why encoder-only models are sometimes referred to as representational models.</span><br><span class="line"></span><br><span class="line">While decoder-only models (like a GPT) can do the work of an encoder-only model (like a BERT), they are hamstrung by a key constraint: since they are generative models, they are mathematically “not allowed” to “peek” at later tokens. They can only ever look backwards. This is in contrast to encoder-only models, which are trained so each token can look forwards and backwards (bi-directionally). They are built for this, and it makes them very efficient at what they do.</span><br><span class="line"></span><br><span class="line">Basically, a frontier model like OpenAI&#x27;s O1 is like a Ferrari SF-23. It’s an obvious triumph of engineering, designed to win races, and that’s why we talk about it. But it takes a special pit crew just to change the tires and you can’t buy one for yourself. In contrast, a BERT model is like a Honda Civic. It’s also an engineering triumph, but more subtly, since it is engineered to be affordable, fuel-efficient, reliable, and extremely useful. And that’s why they’re absolutely everywhere.</span><br><span class="line"></span><br><span class="line">You can see this by looking at it a number of ways.</span><br><span class="line"></span><br><span class="line">Supporting generative models: One way to understand the prevalence of representational models (encoder-only) is to note how frequently they are used in concert with a decoder-only model to make a system which is safe and efficient.</span><br><span class="line"></span><br><span class="line">The obvious example is RAG. Instead of relying on the LLM’s knowledge trained into the model’s parameters, the system uses a document store to furnish the LLM with information relevant to the query. But of course this only defers the problem. If the LLM doesn’t know which documents are relevant to the query, then the system will need some other process to select those documents? It’s going to need a model which is fast and cheap enough that it can be used to encode the large quantities of information needed to make the LLM useful. That model is often a BERT-like encoder-only model.</span><br><span class="line"></span><br><span class="line">Another example is supervision architectures, where a cheap classifier might be used to ensure that generated text does not violate content safety requirements.</span><br><span class="line"></span><br><span class="line">In short, whenever you see a decoder-only model in deployment, there’s a reasonable chance an encoder-only model is also part of the system. But the converse is not true.</span><br><span class="line"></span><br><span class="line">Encoder-based systems: Before there was GPT, there were content recommendations in social media and in platforms like Netflix. There was ad targeting in those venues, in search, and elsewhere. There was content classification for spam detection, abuse detection, etc.. These systems were not built on generative models, but on representational models like encoder-only models. And all these systems are still out there and still running at enormous scale. Imagine how many ads are targeted per second around the world!</span><br><span class="line"></span><br><span class="line">Downloads: On HuggingFace, RoBERTa, one of the leading BERT-based models, has more downloads than the 10 most popular LLMs on HuggingFace combined. In fact, currently, encoder-only models add up to over a billion downloads per month, nearly three times more than decoder-only models with their 397 million monthly downloads. In fact, the `fill-mask` model category, composed of encoder “base models” such as ModernBERT, ready to be fine-tuned for other downstream applications, is the most downloaded model category overall.</span><br><span class="line"></span><br><span class="line">Inference costs: What the above suggests, is that on an inference-per-inference basis, there are many times more inferences performed per year on encoder-only models than on decoder-only or generative models. An interesting example is FineWeb-Edu, where model-based quality filtering had to be performed over 15 trillion tokens. The FineWeb-Edu team chose to generate annotations with a decoder-only model, Llama-3-70b-Instruct, and perform the bulk of the filtering with a fine-tuned BERT-based model. This filtering took 6,000 H100 hours, which, at HuggingFace Inference Points’ pricing of $10/hour, comes to a total of $60,000. On the other hand, feeding 15 trillion tokens to popular decoder-only models, even with the lowest-cost option of using Google’s Gemini Flash and its low inference cost of $0.075/million tokens, would cost over one million dollars!</span><br><span class="line"></span><br><span class="line">Performance</span><br><span class="line">Overview</span><br><span class="line">Here’s a snapshot of the accuracy of ModernBERT and other models across a range of tasks, as measured by standard academic benchmarks – as you can see, ModernBERT is the only model which is a top scorer across every category, which makes it the one model you can use for all your encoder-based tasks:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">If you’ve ever done an NLP competition on Kaggle, then you’ll know that DeBERTaV3 has been the choice of champions for years. But no longer: not only is ModernBERT the first base-size model to beat DeBERTaV3 on GLUE, it also uses less than 1/5th of Deberta’s memory.</span><br><span class="line"></span><br><span class="line">And of course, ModernBERT is fast. It’s twice as fast as DeBERTa – in fact, up to 4x faster in the more common situation where inputs are mixed length. Its long context inference is nearly 3 times faster than other high-quality models such as NomicBERT and GTE-en-MLM.</span><br><span class="line"></span><br><span class="line">ModernBERT’s context length of 8,192 tokens is over 16x larger than most existing encoders. This is critical, for instance, in RAG pipelines, where a small context often makes chunks too small for semantic understanding. ModernBERT is also the state-of-the-art long context retriever with ColBERT, and is 9 percentage points above the other long context models. Even more impressive: this very quickly trained model, simply tuned to compare to other backbones, outperforms even widely-used retrieval models on long-context tasks!</span><br><span class="line"></span><br><span class="line">For code retrieval, ModernBERT is unique. There’s nothing to really compare it to, since there’s never been an encoder model like this trained on a large amount of code data before. For instance, on the StackOverflow-QA dataset (SQA), which is a hybrid dataset mixing both code and natural language, ModernBERT&#x27;s specialized code understanding and long-context capabilities make it the only backbone to score over 80 on this task.</span><br><span class="line"></span><br><span class="line">This means whole new applications are likely to be built on this capability. For instance, imagine an AI-connected IDE which had an entire enterprise codebase indexed with ModernBERT embeddings, providing fast long context retrieval of the relevant code across all repositories. Or a code chat service which described how an application feature worked that integrated dozens of separate projects.</span><br><span class="line"></span><br><span class="line">Compared to the mainstream models, ModernBERT performs better across nearly all three broad task categories of retrieval, natural language understanding, and code retrieval. Whilst it slightly lags DeBERTaV3 in one area (natural language understanding), it is many times faster. Please note that ModernBERT, as any other base model, can only do masked word prediction out-of-the-box. To be able to perform other tasks, the base model should be fine-tuned as done in these boilerplates.</span><br><span class="line"></span><br><span class="line">Compared to the specialized models, ModernBERT is comparable or superior in most tasks. In addition, ModernBERT is faster than most models across most tasks, and can handle inputs up to 8,192 tokens, 16x longer than the mainstream models.</span><br><span class="line"></span><br><span class="line">Efficiency</span><br><span class="line">Here’s the memory (max batch size, BS) and Inference (in thousands of tokens per second) efficiency results on an NVIDIA RTX 4090 for ModernBERT and other decoder models:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The first thing you might notice is that we’re analysing the efficiency on an affordable consumer GPU, rather than the latest unobtainable hyped hardware. First and foremost, ModernBERT is focused on practicality, not hype.</span><br><span class="line"></span><br><span class="line">As part of this focus, it also means we’ve made sure ModernBERT works well for real-world applications, rather than just benchmarks. Models of this kind are normally tested on just the one exact size they’re best at – their maximum context length. That’s what the “fixed” column in the table shows. But input sizes vary in the real world, so that’s the performance we worked hard to optimise – the “variable” column. As you can see, for variable length inputs, ModernBERT is much faster than all other models.</span><br><span class="line"></span><br><span class="line">For long context inputs, which we believe will be the basis for the most valuable and important future applications, ModernBERT is 2-3x faster than the next fastest model. And, on the “practicality” dimension again: ModernBERT doesn’t require the additional heavy “xformers” dependency, but instead only requires the now commonplace Flash Attention as a dependency.</span><br><span class="line"></span><br><span class="line">Furthermore, thanks to ModernBERT’s efficiency, it can use a larger batch size than nearly any other model, and can be used effectively on smaller and cheaper GPUs. The efficiency of the base size, in particular, may enable new applications that run directly in browsers, on phones, and so forth.</span><br><span class="line"></span><br><span class="line">Why is ModernBERT, well, Modern?</span><br><span class="line">Now, we’ve made our case to why we should give some more love to encoder models. As trusted, under-appreciated workhorses, they’ve had surprisingly few updates since 2018&#x27;s BERT!</span><br><span class="line"></span><br><span class="line">Even more surprising: since RoBERTa, there has been no encoder providing overall improvements without tradeoffs (fancily known as “Pareto improvements”): DeBERTaV3 had better GLUE and classification performance, but sacrificed both efficiency and retrieval. Other models, such as AlBERT, or newer ones, like GTE-en-MLM, all improved over the original BERT and RoBERTa in some ways but regressed in others.</span><br><span class="line"></span><br><span class="line">However, since the duo’s original release, we&#x27;ve learned an enormous amount about how to build better language models. If you’ve used LLMs at all, you’re very well aware of it: while they’re rare in the encoder-world, Pareto improvements are constant in decoder-land, where models constantly become better at everything. And as we’ve all learned by now: model improvements are only partially magic, and mostly engineering.</span><br><span class="line"></span><br><span class="line">The goal of the (hopefully aptly named) ModernBERT project was thus fairly simple: bring this modern engineering to encoder models. We did so in three core ways:</span><br><span class="line"></span><br><span class="line">a modernized transformer architecture</span><br><span class="line">particular attention to efficiency</span><br><span class="line">modern data scales &amp; sources</span><br><span class="line">Meet the New Transformer, Same as the Old Transformer</span><br><span class="line">The Transformer architecture has become dominant, and is used by the vast majority of models nowadays. However, it’s important to remember that there isn’t one but many Transformers. The main thing they share in common is their deep belief that attention is indeed all you need, and as such, build various improvements centered around the attention mechanism.</span><br><span class="line"></span><br><span class="line">ModernBERT takes huge inspiration from the Transformer++ (as coined by Mamba), first used by the Llama2 family of models. Namely, we replace older BERT-like building blocks with their improved equivalent, namely, we:</span><br><span class="line"></span><br><span class="line">Replace the old positional encoding with &quot;rotary positional embeddings&quot; (RoPE): this makes the model much better at understanding where words are in relation to each other, and allows us to scale to longer sequence lengths.</span><br><span class="line">Switch out the old MLP layers for GeGLU layers, improving on the original BERT’s GeLU activation function.</span><br><span class="line">Streamline the architecture by removing unnecessary bias terms, letting us spend our parameter budget more effectively</span><br><span class="line">Add an extra normalization layer after embeddings, which helps stabilize training</span><br><span class="line">Upgrading a Honda Civic for the Race Track</span><br><span class="line">We’ve covered this already: encoders are no Ferraris, and ModernBERT is no exception. However, that doesn’t mean it can’t be fast. When you get on the highway, you generally don’t go and trade in your car for a race car, but rather hope that your everyday reliable ride can comfortably hit the speed limit.</span><br><span class="line"></span><br><span class="line">In fact, for all the application cases we mentioned above, speed is essential. Encoders are very popular in uses where they either have to process tons of data, allowing even tiny speed increments to add up very quickly, or where latency is very important, as is the case on RAG. In a lot of situations, encoders are even run on CPU, where efficiency is even more important if we want results in a reasonable amount of time.</span><br><span class="line"></span><br><span class="line">As with most things in research, we build while standing on the shoulders of giants, and heavily leverage Flash Attention 2’s speed improvements. Our efficiency improvements rely on three key components: Alternating Attention, to improve processing efficiency, Unpadding and Sequence Packing, to reduce computational waste, and Hardware-Aware Model Design, to maximise hardware utilization.</span><br><span class="line"></span><br><span class="line">Global and Local Attention</span><br><span class="line">One of ModernBERT’s most impactful features is Alternating Attention, rather than full global attention. In technical terms, this means that our attention mechanism only attends to the full input every 3 layers (global attention), while all other layers use a sliding window where every token only attends to the 128 tokens nearest to itself (local attention).</span><br><span class="line">As attention’s computational complexity balloons up with every additional token, this means ModernBERT can process long input sequences considerably faster than any other model.</span><br><span class="line"></span><br><span class="line">In practice, it looks like this:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Conceptually, the reason this works is pretty simple: Picture yourself reading a book. For every sentence you read, do you need to be fully aware of the entire plot to understand most of it (full global attention)? Or is awareness of the current chapter enough (local attention), as long as you occasionally think back on its significance to the main plot (global attention)? In the vast majority of cases, it’s the latter.</span><br><span class="line"></span><br><span class="line">Unpadding and Sequence Packing</span><br><span class="line">Another core mechanism contributing to ModernBERT’s efficiency is its use for Unpadding and Sequence packing.</span><br><span class="line"></span><br><span class="line">In order to be able to process multiple sequences within the same batch, encoder models require them to be the same length, so they can perform parallel computation. Traditionally, we’ve relied on padding to achieve this: figure out which sentence is the longest, and add meaningless tokens (padding tokens) to fill up every other sequence.</span><br><span class="line"></span><br><span class="line">While padding solves the problem, it doesn’t do so elegantly: a lot of compute ends up being spent and wasted on padding tokens, which do not contribute any semantic information.</span><br><span class="line"></span><br><span class="line">Padding vs sequence packing</span><br><span class="line">Comparing padding with sequence packing. Sequence packing (‘unpadding’) avoids wasting compute on padding tokens and has more consistent non-padding token counts per batch. Samples are still processed individually through careful masking.</span><br><span class="line">Unpadding solves this issue: rather than keeping these padding tokens, we remove them all, and concatenate them into mini-batches with a batch size of one, avoiding all unnecessary computations. If you’re using Flash Attention, our implementation of unpadding is even faster than previous methods, which heavily relied on unpadding and repadding sequences as they went through the model: we go one step further by introducing our own implementation of unpadding, relying heavily on recent developments in Flash Attention’s RoPE support. This allows ModernBERT to only have to unpad once, and optionally repad sequences after processing, resulting in a 10-20% speedup over previous methods.</span><br><span class="line"></span><br><span class="line">To speed up pre-training even further, unpadding is in good company within our model, as we use it in conjunction with sequence packing. Sequence packing here is a logical next step: as we’re concatenating inputs into a single sequence, and GPUs are very good at parallelisation, we want to maximise the computational efficiency we can squeeze out of a single forward model pass. To do so, we use a greedy algorithm to group individual sequences into concatenated ones that are as close to the model’s maximum input length as possible.</span><br><span class="line"></span><br><span class="line">Paying Attention to Hardware</span><br><span class="line">Finally, the third facet of ModernBERT’s efficiency is hardware design.</span><br><span class="line"></span><br><span class="line">We attempted to balance two insights that have been highlighted by previous research:</span><br><span class="line"></span><br><span class="line">Deep &amp; Narrow vs Wide &amp; Shallow: Research shows that deeper models with narrower layers, often perform better than shallow models with fewer, wider layers. However, this is a double-edged sword: the deeper the model, the less parallelizable it becomes, and thus, the slower it runs at identical parameter counts.</span><br><span class="line">Hardware Efficiency: Model dimensions need to align well with GPU hardware for maximum performance, and different target GPUs result in different constraints.</span><br><span class="line">Sadly, there is no magic recipe to make a model run similarly well on a wide range of GPUs, but there is an excellent cookbook: The Case for Co-Designing Model Architectures with Hardware, in which the ways to optimize a model architecture for a given GPU are carefully laid out. We came up with a heuristic to extend their method to a basket of GPUs, while respecting a given set of constraints. Logically, the first step is to define said constraints, in our case:</span><br><span class="line"></span><br><span class="line">Defining our target GPUs as common inference ones (RTX 3090/4090, A10, T4, L4)</span><br><span class="line">Roughly defining our target model sizes at 130-to-150 million parameters for ModernBERT-Base, and 350-to-420 for ModernBERT-Large.</span><br><span class="line">The final embedding sizes must match the original BERT’s dimensions, 768 for base and 1024 for large, to maximize backwards compatibility</span><br><span class="line">Set performance constraints which are common across the basket of GPUs</span><br><span class="line">Afterwards, we experimented with multiple model designs via a constrained grid search, varying both layer counts and layer width. Once we’d identified shapes that appeared to be the most efficient ones, we confirmed that our heuristics matched real-world GPU performance, and settled on the final model designs.</span><br><span class="line"></span><br><span class="line">Training</span><br><span class="line">def data(): return [‘text’, ‘bad_text’, ‘math’, ‘code’]</span><br><span class="line">https://media1.tenor.com/m/xJSM2Ky3WpgAAAAd/steve-ballmer-microsoft.gif</span><br><span class="line">Picture this exact scene, but replace Developers with Data</span><br><span class="line"></span><br><span class="line">Another big aspect in which encoders have been trailing behind is training data. This is often understood to mean solely training data scale, but this is not actually the case: previous encoders, such as DeBERTaV3, were trained for long enough that they might have even breached the trillion tokens scale!</span><br><span class="line"></span><br><span class="line">The issue, rather, has been training data diversity: many of the older models train on limited corpora, generally consisting of Wikipedia and Wikibooks. These data mixtures are very noticeably single text modality: they contain nothing but high-quality natural text.</span><br><span class="line"></span><br><span class="line">In contrast, ModernBERT is trained on data from a variety of English sources, including web documents, code, and scientific articles. It is trained on 2 trillion tokens, of which most are unique, rather than the standard 20-to-40 repetitions common in previous encoders.</span><br><span class="line"></span><br><span class="line">The impact of this is immediately noticeable: out of all the existing open source encoders, ModernBERT is in a class of its own on programming-related tasks. We’re particularly interested in what downstream uses this will lead to, in terms of improving programming assistants.</span><br><span class="line"></span><br><span class="line">Process</span><br><span class="line">We stick to the original BERT’s training recipe, with some slight upgrades inspired by subsequent work: we remove the Next-Sentence Prediction objective, since then shown to add overhead for no clear gains, and increase the masking rate from 15% to 30%.</span><br><span class="line"></span><br><span class="line">Both models are trained with a three-phase process. First, we train on 1.7T tokens at a sequence length of 1024. We then adopt a long-context adaptation phase, training on 250B tokens at a sequence length of 8192, while keeping the total tokens seen per batch more or less consistent by lowering the batch size. Finally, we perform annealing on 50 billion tokens sampled differently, following the long-context extension ideal mix highlighted by ProLong.</span><br><span class="line"></span><br><span class="line">Training in three phases is our way of ensuring our model is good across the board, which is reflected in its results: it is competitive on long-context tasks, at no cost to its ability to process short context…</span><br><span class="line"></span><br><span class="line">… But it has another benefit: for the first two-phases, we train using a constant learning rate once the warmup phase is complete, and only perform learning rate decay on the final 50 billion tokens, following the Trapezoidal (or Warmup-Stable-Decay) learning rate. And what’s more: we will release every single immediate intermediate checkpoints from these stable phases, inspired by Pythia. Our main reason for doing so was supporting future research and applications: anyone is free to restart training from any of our pre-decay checkpoints, and perform annealing on domain-appropriate data for their intended use!</span><br><span class="line"></span><br><span class="line">The tricks, it’s all about the tricks!</span><br><span class="line">If you’ve made it this far into this announcement, you’re probably used to this: of course, we use tricks to make things quicker here too. To be precise, we have two main tricks.</span><br><span class="line"></span><br><span class="line">Let’s start with the first one, which is pretty common: since the initial training steps are updating random weights, we adopt batch-size warmup: we start with a smaller batch size so the same number of tokens update the model weights more often, then gradually increase the batch size to the final training size. This significantly speeds up the initial phase of model training, where the model learns its most basic understanding of language.</span><br><span class="line"></span><br><span class="line">The second trick is far more uncommon: weight initialization via tiling for the larger model size, inspired by Microsoft’s Phi family of models. This one’s based on the following realization: Why initialize the ModernBERT-large’s initial weights with random numbers when we have a perfectly good (if we dare say so ourselves) set of ModernBERT-base weights just sitting there?</span><br><span class="line"></span><br><span class="line">And indeed, it turns out that tiling ModernBERT-base’s weights across ModernBERT-large works better than initializing from random weights. It also has the added benefit of stacking nicely with batch size warmup for even faster initial training.</span><br><span class="line"></span><br><span class="line">Conclusion</span><br><span class="line">In this blog post we introduced the ModernBERT models, a new state-of-the-art family of small and efficient encoder-only models, finally giving BERT a much needed do-over.</span><br><span class="line"></span><br><span class="line">ModernBERT demonstrates that encoder-only models can be improved by modern methods. They continue to offer very strong performance on some tasks, providing an extremely attractive size/performance ratio.</span><br><span class="line"></span><br><span class="line">More than anything, we’re really looking forward to seeing what creative ways to use these models the community will come up with! To encourage this, we’re opening a call for demos until January 10th, 2025: the 5 best ones will get added to this post in a showcase section and win a $100 (or local currency equivalent) Amazon gift card, as well as a 6-month HuggingFace Pro subscription! If you need a hint to get started, here’s a demo we thought about: code similarity HF space! And remember, this is an encoder model, so all the coolest downstream applications will likely require some sort of fine-tuning (on real or perhaps decoder-model synthetic data?). Thankfully, there&#x27;s lots of cool frameworks out there to support fine-tuning encoders: 🤗Transformers itself for various tasks, including classification, GliNER for zero-shot Named Entity Recognition, or Sentence-Transformers for retrieval and similarity tasks!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-thinking-exp-1219</span><br><span class="line">Well played! Google DeepMind just released its Gemini (reasoning) preview of an OpenAI o1-like reasoning mode! And it is not hiding its Chain of Thoughts! 😍</span><br><span class="line">Gemini 2.0 Flash Thinking is an experimental model trained to think out loud, leading to stronger reasoning performance. It is natively multimodal and supports image inputs.</span><br><span class="line">You can try it in AI studio, available for everyone</span><br><span class="line"></span><br><span class="line">focus on model=gemini-2.0-flash-thinking-exp-1219</span><br><span class="line"></span><br><span class="line">Introducing Gemini 2.0: our new AI model for the agentic era</span><br><span class="line">Dec 11, 2024</span><br><span class="line"></span><br><span class="line">·</span><br><span class="line">11 min read</span><br><span class="line"></span><br><span class="line">SundarPichai_2x.jpg</span><br><span class="line">Sundar Pichai</span><br><span class="line">CEO of Google and Alphabet</span><br><span class="line">Demis_headshot</span><br><span class="line">Demis Hassabis</span><br><span class="line">CEO of Google DeepMind</span><br><span class="line">koray</span><br><span class="line">Koray Kavukcuoglu</span><br><span class="line">CTO of Google DeepMind</span><br><span class="line">Share</span><br><span class="line"> Read AI-generated summary</span><br><span class="line">Text &quot;Gemini 2.0&quot; in front of a futuristic blue and black abstract background</span><br><span class="line">In this story</span><br><span class="line">A message from our CEO</span><br><span class="line">Introducing Gemini 2.0</span><br><span class="line">Gemini 2.0 Flash</span><br><span class="line">Project Astra</span><br><span class="line">Project Mariner</span><br><span class="line">Agents for developers</span><br><span class="line">Agents in games</span><br><span class="line">Building responsibly</span><br><span class="line">A note from Google and Alphabet CEO Sundar Pichai:</span><br><span class="line"></span><br><span class="line">Information is at the core of human progress. It’s why we’ve focused for more than 26 years on our mission to organize the world’s information and make it accessible and useful. And it’s why we continue to push the frontiers of AI to organize that information across every input and make it accessible via any output, so that it can be truly useful for you.</span><br><span class="line"></span><br><span class="line">That was our vision when we introduced Gemini 1.0 last December. The first model built to be natively multimodal, Gemini 1.0 and 1.5 drove big advances with multimodality and long context to understand information across text, video, images, audio and code, and process a lot more of it.</span><br><span class="line"></span><br><span class="line">Now millions of developers are building with Gemini. And it’s helping us reimagine all of our products — including all 7 of them with 2 billion users — and to create new ones. NotebookLM is a great example of what multimodality and long context can enable for people, and why it’s loved by so many.</span><br><span class="line"></span><br><span class="line">Over the last year, we have been investing in developing more agentic models, meaning they can understand more about the world around you, think multiple steps ahead, and take action on your behalf, with your supervision.</span><br><span class="line"></span><br><span class="line">Today we’re excited to launch our next era of models built for this new agentic era: introducing Gemini 2.0, our most capable model yet. With new advances in multimodality — like native image and audio output — and native tool use, it will enable us to build new AI agents that bring us closer to our vision of a universal assistant.</span><br><span class="line"></span><br><span class="line">We’re getting 2.0 into the hands of developers and trusted testers today. And we’re working quickly to get it into our products, leading with Gemini and Search. Starting today our Gemini 2.0 Flash experimental model will be available to all Gemini users. We&#x27;re also launching a new feature called Deep Research, which uses advanced reasoning and long context capabilities to act as a research assistant, exploring complex topics and compiling reports on your behalf. It&#x27;s available in Gemini Advanced today.</span><br><span class="line"></span><br><span class="line">No product has been transformed more by AI than Search. Our AI Overviews now reach 1 billion people, enabling them to ask entirely new types of questions — quickly becoming one of our most popular Search features ever. As a next step, we’re bringing the advanced reasoning capabilities of Gemini 2.0 to AI Overviews to tackle more complex topics and multi-step questions, including advanced math equations, multimodal queries and coding. We started limited testing this week and will be rolling it out more broadly early next year. And we’ll continue to bring AI Overviews to more countries and languages over the next year.</span><br><span class="line"></span><br><span class="line">2.0’s advances are underpinned by decade-long investments in our differentiated full-stack approach to AI innovation. It’s built on custom hardware like Trillium, our sixth-generation TPUs. TPUs powered 100% of Gemini 2.0 training and inference, and today Trillium is generally available to customers so they can build with it too.</span><br><span class="line"></span><br><span class="line">If Gemini 1.0 was about organizing and understanding information, Gemini 2.0 is about making it much more useful. I can’t wait to see what this next era brings.</span><br><span class="line"></span><br><span class="line">-Sundar</span><br><span class="line"></span><br><span class="line">Introducing Gemini 2.0: our new AI model for the agentic era</span><br><span class="line">By Demis Hassabis, CEO of Google DeepMind and Koray Kavukcuoglu, CTO of Google DeepMind on behalf of the Gemini team</span><br><span class="line"></span><br><span class="line">Over the past year, we have continued to make incredible progress in artificial intelligence. Today, we are releasing the first model in the Gemini 2.0 family of models: an experimental version of Gemini 2.0 Flash. It’s our workhorse model with low latency and enhanced performance at the cutting edge of our technology, at scale.</span><br><span class="line"></span><br><span class="line">We are also sharing the frontiers of our agentic research by showcasing prototypes enabled by Gemini 2.0’s native multimodal capabilities.</span><br><span class="line"></span><br><span class="line">Gemini 2.0 Flash</span><br><span class="line">Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.</span><br><span class="line"></span><br><span class="line">A chart comparing Gemini models and their capabilities</span><br><span class="line">Our goal is to get our models into people’s hands safely and quickly. Over the past month, we’ve been sharing early, experimental versions of Gemini 2.0, getting great feedback from developers.</span><br><span class="line"></span><br><span class="line">Gemini 2.0 Flash is available now as an experimental model to developers via the Gemini API in Google AI Studio and Vertex AI with multimodal input and text output available to all developers, and text-to-speech and native image generation available to early-access partners. General availability will follow in January, along with more model sizes.</span><br><span class="line"></span><br><span class="line">To help developers build dynamic and interactive applications, we’re also releasing a new Multimodal Live API that has real-time audio, video-streaming input and the ability to use multiple, combined tools. More information about 2.0 Flash and the Multimodal Live API can be found in our developer blog.</span><br><span class="line"></span><br><span class="line">Gemini 2.0 available in Gemini app, our AI assistant</span><br><span class="line">Also starting today, Gemini users globally can access a chat optimized version of 2.0 Flash experimental by selecting it in the model drop-down on desktop and mobile web and it will be available in the Gemini mobile app soon. With this new model, users can experience an even more helpful Gemini assistant.</span><br><span class="line"></span><br><span class="line">Early next year, we’ll expand Gemini 2.0 to more Google products.</span><br><span class="line"></span><br><span class="line">Unlocking agentic experiences with Gemini 2.0</span><br><span class="line">Gemini 2.0 Flash’s native user interface action-capabilities, along with other improvements like multimodal reasoning, long context understanding, complex instruction following and planning, compositional function-calling, native tool use and improved latency, all work in concert to enable a new class of agentic experiences.</span><br><span class="line"></span><br><span class="line">The practical application of AI agents is a research area full of exciting possibilities. We’re exploring this new frontier with a series of prototypes that can help people accomplish tasks and get things done. These include an update to Project Astra, our research prototype exploring future capabilities of a universal AI assistant; the new Project Mariner, which explores the future of human-agent interaction, starting with your browser; and Jules, an AI-powered code agent that can help developers.</span><br><span class="line"></span><br><span class="line">We’re still in the early stages of development, but we’re excited to see how trusted testers use these new capabilities and what lessons we can learn, so we can make them more widely available in products in the future.</span><br><span class="line"></span><br><span class="line">Gemini 2.0 supercut video</span><br><span class="line">2:53</span><br><span class="line">Project Astra: agents using multimodal understanding in the real world</span><br><span class="line">Since we introduced Project Astra at I/O, we’ve been learning from trusted testers using it on Android phones. Their valuable feedback has helped us better understand how a universal AI assistant could work in practice, including implications for safety and ethics. Improvements in the latest version built with Gemini 2.0 include:</span><br><span class="line"></span><br><span class="line">Better dialogue: Project Astra now has the ability to converse in multiple languages and in mixed languages, with a better understanding of accents and uncommon words.</span><br><span class="line">New tool use: With Gemini 2.0, Project Astra can use Google Search, Lens and Maps, making it more useful as an assistant in your everyday life.</span><br><span class="line">Better memory: We’ve improved Project Astra’s ability to remember things while keeping you in control. It now has up to 10 minutes of in-session memory and can remember more conversations you had with it in the past, so it is better personalized to you.</span><br><span class="line">Improved latency: With new streaming capabilities and native audio understanding, the agent can understand language at about the latency of human conversation.</span><br><span class="line">We’re working to bring these types of capabilities to Google products like Gemini app, our AI assistant, and to other form factors like glasses. And we’re starting to expand our trusted tester program to more people, including a small group that will soon begin testing Project Astra on prototype glasses.</span><br><span class="line"></span><br><span class="line">Project Astra demo video</span><br><span class="line">4:32</span><br><span class="line">Project Mariner: agents that can help you accomplish complex tasks</span><br><span class="line">Project Mariner is an early research prototype built with Gemini 2.0 that explores the future of human-agent interaction, starting with your browser. As a research prototype, it’s able to understand and reason across information in your browser screen, including pixels and web elements like text, code, images and forms, and then uses that information via an experimental Chrome extension to complete tasks for you.</span><br><span class="line"></span><br><span class="line">When evaluated against the WebVoyager benchmark, which tests agent performance on end-to-end real world web tasks, Project Mariner achieved a state-of-the-art result of 83.5% working as a single agent setup.</span><br><span class="line"></span><br><span class="line">It’s still early, but Project Mariner shows that it’s becoming technically possible to navigate within a browser, even though it’s not always accurate and slow to complete tasks today, which will improve rapidly over time.</span><br><span class="line"></span><br><span class="line">To build this safely and responsibly, we’re conducting active research on new types of risks and mitigations, while keeping humans in the loop. For example, Project Mariner can only type, scroll or click in the active tab on your browser and it asks users for final confirmation before taking certain sensitive actions, like purchasing something.</span><br><span class="line"></span><br><span class="line">Trusted testers are starting to test Project Mariner using an experimental Chrome extension now, and we’re beginning conversations with the web ecosystem in parallel.</span><br><span class="line"></span><br><span class="line">Mariner demo video</span><br><span class="line">2:15</span><br><span class="line">Jules: agents for developers</span><br><span class="line">Next, we’re exploring how AI agents can assist developers with Jules — an experimental AI-powered code agent that integrates directly into a GitHub workflow. It can tackle an issue, develop a plan and execute it, all under a developer’s direction and supervision. This effort is part of our long-term goal of building AI agents that are helpful in all domains, including coding.</span><br><span class="line"></span><br><span class="line">More information about this ongoing experiment can be found in our developer blog post.</span><br><span class="line"></span><br><span class="line">Agents in games and other domains</span><br><span class="line">Google DeepMind has a long history of using games to help AI models become better at following rules, planning and logic. Just last week, for example, we introduced Genie 2, our AI model that can create an endless variety of playable 3D worlds — all from a single image. Building on this tradition, we’ve built agents using Gemini 2.0 that can help you navigate the virtual world of video games. It can reason about the game based solely on the action on the screen, and offer up suggestions for what to do next in real time conversation.</span><br><span class="line"></span><br><span class="line">We&#x27;re collaborating with leading game developers like Supercell to explore how these agents work, testing their ability to interpret rules and challenges across a diverse range of games, from strategy titles like “Clash of Clans” to farming simulators like “Hay Day.”</span><br><span class="line"></span><br><span class="line">Beyond acting as virtual gaming companions, these agents can even tap into Google Search to connect you with the wealth of gaming knowledge on the web.</span><br><span class="line"></span><br><span class="line">Navi demo video</span><br><span class="line">2:26</span><br><span class="line">In addition to exploring agentic capabilities in the virtual world, we’re experimenting with agents that can help in the physical world by applying Gemini 2.0&#x27;s spatial reasoning capabilities to robotics. While it’s still early, we’re excited about the potential of agents that can assist in the physical environment.</span><br><span class="line"></span><br><span class="line">You can learn more about these research prototypes and experiments at labs.google.</span><br><span class="line"></span><br><span class="line">Building responsibly in the agentic era</span><br><span class="line">Gemini 2.0 Flash and our research prototypes allow us to test and iterate on new capabilities at the forefront of AI research that will eventually make Google products more helpful.</span><br><span class="line"></span><br><span class="line">As we develop these new technologies, we recognize the responsibility it entails, and the many questions AI agents open up for safety and security. That is why we are taking an exploratory and gradual approach to development, conducting research on multiple prototypes, iteratively implementing safety training, working with trusted testers and external experts and performing extensive risk assessments and safety and assurance evaluations.</span><br><span class="line"></span><br><span class="line">For example:</span><br><span class="line"></span><br><span class="line">As part of our safety process, we’ve worked with our Responsibility and Safety Committee (RSC), our longstanding internal review group, to identify and understand potential risks.</span><br><span class="line">Gemini 2.0&#x27;s reasoning capabilities have enabled major advancements in our AI-assisted red teaming approach, including the ability to go beyond simply detecting risks to now automatically generating evaluations and training data to mitigate them. This means we can more efficiently optimize the model for safety at scale.</span><br><span class="line">As Gemini 2.0’s multimodality increases the complexity of potential outputs, we’ll continue to evaluate and train the model across image and audio input and output to help improve safety.</span><br><span class="line">With Project Astra, we’re exploring potential mitigations against users unintentionally sharing sensitive information with the agent, and we’ve already built in privacy controls that make it easy for users to delete sessions. We’re also continuing to research ways to ensure AI agents act as reliable sources of information and don’t take unintended actions on your behalf.</span><br><span class="line">With Project Mariner, we’re working to ensure the model learns to prioritize user instructions over 3rd party attempts at prompt injection, so it can identify potentially malicious instructions from external sources and prevent misuse. This prevents users from being exposed to fraud and phishing attempts through things like malicious instructions hidden in emails, documents or websites.</span><br><span class="line">We firmly believe that the only way to build AI is to be responsible from the start and we&#x27;ll continue to prioritize making safety and responsibility a key element of our model development process as we advance our models and agents.</span><br><span class="line"></span><br><span class="line">Gemini 2.0, AI agents and beyond</span><br><span class="line">Today’s releases mark a new chapter for our Gemini model. With the release of Gemini 2.0 Flash, and the series of research prototypes exploring agentic possibilities, we have reached an exciting milestone in the Gemini era. And we’re looking forward to continuing to safely explore all the new possibilities within reach as we build towards AGI.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/collections/ibm-granite/granite-31-language-models-6751dbbf2f3389bec5c6f02d</span><br><span class="line"></span><br><span class="line">Granite 3.1 Update! IBM just updated their Granite LLM family with longer context, better RAG, and function calling! Here are the highlights: ✨</span><br><span class="line">📏 Extended context from 32K to 128K</span><br><span class="line">🔄 Custom &lt;document&gt; role for improved RAG</span><br><span class="line">🛠️ Improved function-calling with &lt;tool_call&gt; schema</span><br><span class="line">🌐 Maintains support for 12 languages while offering finetuning flexibility</span><br><span class="line">🤗 Apache 2.0 license and available on Hugging Face</span><br><span class="line">🌎 Multilingual 12 languages, including English, German, Spanish, French…</span><br><span class="line">4️⃣ 8 new checkpoints base and instruct for 8b, 2b, 3B-A0.8B, and 1B-A0.4B</span><br><span class="line">❌ No official updated evals or benchmarks found</span><br><span class="line"></span><br><span class="line">Granite-3.1-8B-Instruct</span><br><span class="line">Model Summary: Granite-3.1-8B-Instruct is a 8B parameter long-context instruct model finetuned from Granite-3.1-8B-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets tailored for solving long context problems. This model is developed using a diverse set of techniques with a structured chat format, including supervised finetuning, model alignment using reinforcement learning, and model merging.</span><br><span class="line"></span><br><span class="line">Developers: Granite Team, IBM</span><br><span class="line">GitHub Repository: ibm-granite/granite-3.1-language-models</span><br><span class="line">Website: Granite Docs</span><br><span class="line">Paper: Granite 3.1 Language Models (coming soon)</span><br><span class="line">Release Date: December 18th, 2024</span><br><span class="line">License: Apache 2.0</span><br><span class="line">Supported Languages: English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. Users may finetune Granite 3.1 models for languages beyond these 12 languages.</span><br><span class="line"></span><br><span class="line">Intended Use: The model is designed to respond to general instructions and can be used to build AI assistants for multiple domains, including business applications.</span><br><span class="line"></span><br><span class="line">Capabilities</span><br><span class="line"></span><br><span class="line">Summarization</span><br><span class="line">Text classification</span><br><span class="line">Text extraction</span><br><span class="line">Question-answering</span><br><span class="line">Retrieval Augmented Generation (RAG)</span><br><span class="line">Code related tasks</span><br><span class="line">Function-calling tasks</span><br><span class="line">Multilingual dialog use cases</span><br><span class="line">Long-context tasks including long document/meeting summarization, long document QA, etc.</span><br><span class="line">Generation: This is a simple example of how to use Granite-3.1-8B-Instruct model.</span><br><span class="line"></span><br><span class="line">Install the following libraries:</span><br><span class="line"></span><br><span class="line">pip install torch torchvision torchaudio</span><br><span class="line">pip install accelerate</span><br><span class="line">pip install transformers</span><br><span class="line"></span><br><span class="line">Then, copy the snippet from the section that is relevant for your use case.</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">from transformers import AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">device = &quot;auto&quot;</span><br><span class="line">model_path = &quot;ibm-granite/granite-3.1-8b-instruct&quot;</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_path)</span><br><span class="line"># drop device_map if running on CPU</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)</span><br><span class="line">model.eval()</span><br><span class="line"># change input text as desired</span><br><span class="line">chat = [</span><br><span class="line">    &#123; &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Please list one IBM Research laboratory located in the United States. You should only output its name and location.&quot; &#125;,</span><br><span class="line">]</span><br><span class="line">chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)</span><br><span class="line"># tokenize the text</span><br><span class="line">input_tokens = tokenizer(chat, return_tensors=&quot;pt&quot;).to(device)</span><br><span class="line"># generate output tokens</span><br><span class="line">output = model.generate(**input_tokens,</span><br><span class="line">                        max_new_tokens=100)</span><br><span class="line"># decode output tokens into text</span><br><span class="line">output = tokenizer.batch_decode(output)</span><br><span class="line"># print output</span><br><span class="line">print(output)</span><br><span class="line"></span><br><span class="line">Model Architecture: Granite-3.1-8B-Instruct is based on a decoder-only dense transformer architecture. Core components of this architecture are: GQA and RoPE, MLP with SwiGLU, RMSNorm, and shared input/output embeddings.</span><br><span class="line"></span><br><span class="line">Model	2B Dense	8B Dense	1B MoE	3B MoE</span><br><span class="line">Embedding size	2048	4096	1024	1536</span><br><span class="line">Number of layers	40	40	24	32</span><br><span class="line">Attention head size	64	128	64	64</span><br><span class="line">Number of attention heads	32	32	16	24</span><br><span class="line">Number of KV heads	8	8	8	8</span><br><span class="line">MLP hidden size	8192	12800	512	512</span><br><span class="line">MLP activation	SwiGLU	SwiGLU	SwiGLU	SwiGLU</span><br><span class="line">Number of experts	—	—	32	40</span><br><span class="line">MoE TopK	—	—	8	8</span><br><span class="line">Initialization std	0.1	0.1	0.1	0.1</span><br><span class="line">Sequence length	128K	128K	128K	128K</span><br><span class="line">Position embedding	RoPE	RoPE	RoPE	RoPE</span><br><span class="line"># Parameters	2.5B	8.1B	1.3B	3.3B</span><br><span class="line"># Active parameters	2.5B	8.1B	400M	800M</span><br><span class="line"># Training tokens	12T	12T	10T	10T</span><br><span class="line">Training Data: Overall, our SFT data is largely comprised of three key sources: (1) publicly available datasets with permissive license, (2) internal synthetic data targeting specific capabilities including long-context tasks, and (3) very small amounts of human-curated data. A detailed attribution of datasets can be found in the Granite 3.0 Technical Report, Granite 3.1 Technical Report (coming soon), and Accompanying Author List.</span><br><span class="line"></span><br><span class="line">Infrastructure: We train Granite 3.1 Language Models using IBM&#x27;s super computing cluster, Blue Vela, which is outfitted with NVIDIA H100 GPUs. This cluster provides a scalable and efficient infrastructure for training our models over thousands of GPUs.</span><br><span class="line"></span><br><span class="line">Ethical Considerations and Limitations: Granite 3.1 Instruct Models are primarily finetuned using instruction-response pairs mostly in English, but also multilingual data covering eleven languages. Although this model can handle multilingual dialog use cases, its performance might not be similar to English tasks. In such case, introducing a small number of examples (few-shot) can help the model in generating more accurate outputs. While this model has been aligned by keeping safety in consideration, the model may in some cases produce inaccurate, biased, or unsafe responses to user prompts. So we urge the community to use this model with proper safety testing and tuning tailored for their specific tasks.</span><br><span class="line"></span><br><span class="line">Resources</span><br><span class="line"></span><br><span class="line">⭐️ Learn about the latest updates with Granite: https://www.ibm.com/granite</span><br><span class="line">📄 Get started with tutorials, best practices, and prompt engineering advice: https://www.ibm.com/granite/docs/</span><br><span class="line">💡 Learn about the latest Granite learning resources: https://ibm.biz/granite-learning-resources</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://arxiv.org/abs/2412.12004</span><br><span class="line">[Submitted on 16 Dec 2024]</span><br><span class="line">The Open Source Advantage in Large Language Models (LLMs)</span><br><span class="line">Jiya Manchanda, Laura Boettcher, Matheus Westphalen, Jasser Jasser</span><br><span class="line">Large language models (LLMs) mark a key shift in natural language processing (NLP), having advanced text generation, translation, and domain-specific reasoning. Closed-source models like GPT-4, powered by proprietary datasets and extensive computational resources, lead with state-of-the-art performance today. However, they face criticism for their &quot;black box&quot; nature and for limiting accessibility in a manner that hinders reproducibility and equitable AI development. By contrast, open-source initiatives like LLaMA and BLOOM prioritize democratization through community-driven development and computational efficiency. These models have significantly reduced performance gaps, particularly in linguistic diversity and domain-specific applications, while providing accessible tools for global researchers and developers. Notably, both paradigms rely on foundational architectural innovations, such as the Transformer framework by Vaswani et al. (2017). Closed-source models excel by scaling effectively, while open-source models adapt to real-world applications in underrepresented languages and domains. Techniques like Low-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source models to achieve competitive results despite limited resources. To be sure, the tension between closed-source and open-source approaches underscores a broader debate on transparency versus proprietary control in AI. Ethical considerations further highlight this divide. Closed-source systems restrict external scrutiny, while open-source models promote reproducibility and collaboration but lack standardized auditing documentation frameworks to mitigate biases. Hybrid approaches that leverage the strengths of both paradigms are likely to shape the future of LLM innovation, ensuring accessibility, competitive technical performance, and ethical deployment.</span><br><span class="line">The Open-Source Advantage in Large Language Models (LLMs)</span><br><span class="line">Jiya Manchanda</span><br><span class="line">Department of Philosophy and Religion Rollins College Winter Park jmanchanda@rollins.edu</span><br><span class="line">Equal contribution</span><br><span class="line">Laura Boettcher</span><br><span class="line">Department of Mathematics and Computer Science Rollins College Winter Park lboettcher@rollins.edu</span><br><span class="line">Equal contribution</span><br><span class="line">Matheus Westphalen</span><br><span class="line">Department of Mathematics and Computer Science Rollins College Winter Park mwestphalen@rollins.edu</span><br><span class="line">Equal contribution</span><br><span class="line">Jasser Jasser</span><br><span class="line">Department of Mathematics and Computer Science Rollins College Winter Park jjasser@rollins.edu</span><br><span class="line">Abstract</span><br><span class="line">Large language models (LLMs) mark a key shift in natural language processing (NLP), having advanced text generation, translation, and domain-specific reasoning. Closed-source models like GPT-4, powered by proprietary datasets and extensive computational resources, lead with state-of-the-art performance today. However, they face criticism for their &quot;black box&quot; nature and for limiting accessibility in a manner that hinders reproducibility and equitable AI development. By contrast, open-source initiatives like LLaMA and BLOOM prioritize democratization through community-driven development and computational efficiency. These models have significantly reduced performance gaps, particularly in linguistic diversity and domain-specific applications, while providing accessible tools for global researchers and developers. Notably, both paradigms rely on foundational architectural innovations, such as the Transformer framework by Vaswani et al. (2017). Closed-source models excel by scaling effectively, while open-source models adapt to real-world applications in underrepresented languages and domains. Techniques like Low-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source models to achieve competitive results despite limited resources. To be sure, the tension between closed-source and open-source approaches underscores a broader debate on transparency versus proprietary control in AI. Ethical considerations further highlight this divide. Closed-source systems restrict external scrutiny, while open-source models promote reproducibility and collaboration but lack standardized auditing documentation frameworks to mitigate biases. Hybrid approaches that leverage the strengths of both paradigms are likely to shape the future of LLM innovation, ensuring accessibility, competitive technical performance, and ethical deployment.</span><br><span class="line"></span><br><span class="line">Keywords Open-source models</span><br><span class="line">⋅</span><br><span class="line"> Large Language Models (LLMs)</span><br><span class="line">⋅</span><br><span class="line"> Transparency</span><br><span class="line"></span><br><span class="line">1Introduction</span><br><span class="line">Large language models (LLMs) stand at the forefront of connectionist artificial intelligence (AI) today, having revolutionized the realm of natural language processing (NLP) and driven advancements in areas such as text generation, translation, domain-specific inferencing, and sentiment analysis [1]. These models, built on cutting-edge neural architectures and trained on expansive datasets, have not only become indispensable tools in industry but also focal points of academic investigation. However, their rapid development has brought critical issues into focus—chief among them is the tension between open-source and closed-source approaches [2]. This underexplored division (in part, of labor) presents urgent questions about transparency, accessibility, and the equitable distribution of the benefits of AI, and will be the subject of much consideration in this paper.</span><br><span class="line"></span><br><span class="line">The closed-source approach, typified by models like OpenAI’s GPT-4, has thus far dominated benchmarks for LLMs [3]. Their ability to excel stems squarely from the proprietary datasets that they are trained on (in addition to significant computational investments), which have enabled advanced capabilities in reasoning, text synthesis, and conversational AI [4]. Despite their technical achievements, though, these models are often criticized for their lack of transparency [5]. The proprietary nature of their design, development, and data restricts access to methodologies and findings, and has raised concerns about the reproducibility of their outcomes. Moreover, the concentration of these resources within a small number of organizations is claimed to have exacerbated inequities in global AI development. Needless to say, this dynamic has restricted many researchers and practitioners from competing effectively or even building upon these systems [6].</span><br><span class="line"></span><br><span class="line">It is in light of the aforementioned trajectory of development that open-source LLMs, including LLaMA and BLOOM, have emerged as powerful alternatives. These models operate on the ethos of accessibility and community-driven innovation, and now offer researchers and developers the tools to advance NLP without vast computational resources [7]. By way of innovations in fine-tuning, such as low-rank adaptation (LoRA) and domain-specific optimization, open-source models have significantly narrowed performance gaps in recent months [8]. Projects like BLOOM’s multilingual framework demonstrate the capacity of open-source efforts to address linguistic diversity and real-world complexity, while models like LLaMA highlight the feasibility of achieving high performance with computational efficiency. These initiatives underscore the role of open collaboration in broadening the scope of AI research and ensuring a more equitable distribution of its benefits.</span><br><span class="line"></span><br><span class="line">The divide between open- and closed-source models is best framed within the historical development of LLMs, which provides essential context for understanding their current capabilities. Early statistical language models, inspired by Shannon’s Information Theory, relied on probabilistic methods to predict word sequences [9]. While these models laid the groundwork for computational linguistics, they were limited in handling long-range dependencies and capturing the semantic properties of pragmatic language use [10]. The transition to neural network-based models in the 2000s marked a significant advancement. Word embeddings like Word2Vec and GloVe enabled dense vector representations of words, which improved our ability to model such semantic relationships [11]. The development of Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks further expanded this capacity for processing sequential data and capturing temporal dependencies [12].</span><br><span class="line"></span><br><span class="line">However, these architectures struggled with scalability when handling large datasets. The introduction of the Transformer architecture in 2017 by Vaswani et al. shifted the paradigm of NLP insofar as it overcame these limitations to a statistically significant degree [13]. Transformers, with their self-attention mechanisms, allowed for parallel sequence processing and the corresponding modeling of long-range dependencies, which have since become the backbone of modern LLMs. Subsequent innovations built upon this Transformer framework. Simply look to BERT’s bidirectional training approach, which enhanced performance on tasks like question answering and natural language inference, as well as GPT’s autoregressive design, which excelled in text generation and summarization [14]. The release of the closed-source GPT-3, which came bearing 175 billion parameters, demonstrated how scaling model size could improve generalization abilities and enable few-shot learning across highly varied tasks [15]. It was not long before open-source models such as BLOOM and LLaMA met those same benchmarks, though. This achievement quickly showcased how high performance can be attained without dependence on massive scale or proprietary frameworks.</span><br><span class="line"></span><br><span class="line">In examining this dynamic between open- and closed-source LLMs, this paper seeks to elucidate the core issues that will shape the trajectory of developments in computational linguistics. Section 2 explores the innovation and development processes underpinning open-source and closed-source models, highlighting key breakthroughs and limitations. Section 3 evaluates the comparative performance of these models, focusing on benchmarks and task-specific outcomes. Section 4 addresses accessibility and use cases, specifically assessing how the availability and practical applications of these systems impact various stakeholders. Section 5 delves into ethical considerations concerning transparency, scrutinizing the implications of proprietary practices and open collaboration. Section 6 discusses the broader implications of the open-versus-closed divide, integrating findings from previous sections. Finally, Section 7 outlines potential directions for future research, proposing a mechanism of how to best foster innovation while ensuring equitable AI deployment and governance.</span><br><span class="line"></span><br><span class="line">2Innovation and Development</span><br><span class="line">The innovation and development of Large Language Models (LLMs) are marked by (a) foundational architectural changes and (b) refined training methodologies. As noted, the Transformer architecture, introduced by Vaswani et al. in 2017, fundamentally changed the way machine learning models process sequences of data. Previous models, such as recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, operated sequentially. This means that they processed one word or token at a time, with each step depending on the previous output. While effective for short-term dependencies, these architectures struggled to efficiently handle long-range relationships in data. As a result, they were challenged by vanishing gradients and slow training times. Transformers addressed these limitations by introducing a mechanism called self-attention [16]. This mechanism allows the model to evaluate the relationship between all tokens in a sequence simultaneously, rather than step-by-step. So, for example, when processing a sentence, a Transformer can determine the importance of every word relative to the others in a single compute. This parallel processing capability reduces computational bottlenecks by allowing for faster training and, by extension, inference. Moreover, self-attention enables Transformers to model long-range dependencies. In natural language, the meaning of a word or phrase often depends on context from distant parts of the text. Transformers excel in capturing these relationships, which is critical for tasks like summarization, translation, and complex reasoning.</span><br><span class="line"></span><br><span class="line">Of course, the architecture’s scalability naturally facilitated its adoption in large-scale closed-source models like GPT-3 and GPT-4. Yet, it was not lost on open-source models including BLOOM and LLaMA to leverage the Transformer framework for the purpose of achieving competitive performance via computational efficiency. Subsidiary incremental developments upon this foundation like LLaMA’s grouped query attention (GQA) reduced memory demands by sharing attention weights, which allowed for performance gains without exorbitant resource requirements. In a similar fashion, Flash Attention optimized for training speed and energy efficiency by reducing the computational complexity of self-attention operations [17]. These advancements sufficiently demonstrate how open-source models may meet closed-source models on the question of scalability by innovating within architectural constraints.</span><br><span class="line"></span><br><span class="line">The evolution of training techniques has been equally pivotal. Closed-source models often utilize proprietary datasets encompassing billions of tokens, which renders them able to achieve comprehensive pretraining objectives. A key instance of this phenomenon would be autoregressive modeling, which, employed in GPT, trains models to predict the next token in a sequence [18]. In so doing, it reinforces the coherence and fluency of its outcomes. Similarly, masked language modeling, which, employed in BERT, predicts missing tokens in a sentence, fostering a deeper understanding of bidirectional context [19]. These methods allow models to capture the nuanced language patterns and semantic relationships that form the basis of their strong generalization capabilities [20]. It is important to note that Reinforcement Learning from Human Feedback (RLHF) enhances this process by incorporating human evaluative feedback directly into training loops. In RLHF, the models are fine-tuned to align their outputs with human-defined preferences, thus improving both accuracy and alignment with values [21]. This approach is particularly impactful in domains where value-based considerations stand at the forefront of decision-making, such as healthcare and governance.</span><br><span class="line"></span><br><span class="line">In response to these advancements, open-source models have introduced training approaches that effectively balance computational efficiency with performance optimization. Low-Rank Adaptation (LoRA), for example, reduces the computational burden during fine-tuning by updating only a subset of task-specific parameters. This method enables smaller organizations to tailor large models to specific applications without incurring the substantial costs associated with full model retraining. Moreover, publicly available instruction-tuning datasets provide open-source models with an alternative to Reinforcement Learning from Human Feedback (RLHF). These datasets allow models to adapt to highly varied tasks by leveraging clearly structured instructions. Quantization techniques further enhance their efficiency by reducing the precision of model weights. Now, model distillation complements this by compressing large models into smaller, more efficient versions that retain their essential capabilities. These compressed models are particularly valuable for edge-case applications where computational resources are limited.</span><br><span class="line"></span><br><span class="line">Notably, it is the real-world applications of LLMs that illustrate the distinct trajectories of open- and closed-source models. Closed-source LLMs dominate high-stakes domains such as conversational AI, creative content generation, and advanced reasoning. For example, GPT-3 was widely adopted for automated content creation, customer support, and other on-demand tasks. Conversely, open-source models rely on their adaptive capabilities to meet inclusivity metrics given their access to expansive training. A case in point is BLOOM’s multilingual framework that supports over 40 languages. As such, open-source models have the potential to, and already are, surpassing closed-source models on the question of diverse datasets and how those contribute to global NLP research. Collaboration between LLMs and external tools has further expanded their utility. Retrieval-Augmented Generation (RAG), for instance, integrates information retrieval systems with generative models, enabling real-time access to updated knowledge [22]. Such systems are particularly effective in fields like healthcare, law, and finance, where timely and accurate information is critical [23]. By building on modular architectures and publicly accessible datasets, open-source models have been able to achieve comparable functionality to closed-source systems.</span><br><span class="line"></span><br><span class="line">3Performance</span><br><span class="line">The performance of open-source and closed-source Large Language Models (LLMs) is indicative of their underlying architecture, pre-training datasets, and optimization strategies, but understanding their comparative strengths requires a closer evaluation. Closed-source models, such as GPT-4, dominate state-of-the-art benchmarks like HumanEval and GSM8K [24]. This is due in part to their ability to leverage proprietary datasets and in part a result of their expansive pre-training corpora. The datasets on which these models are trained span hundreds of terabytes, which allows them to excel at a wide range of domain-general tasks with minimal fine-tuning [25]. This is precisely the reason that closed-source models possess superior generalization capabilities when it comes to generative tasks, such as creative writing and textual summarization. Look also to GPT-4’s parameter count of one trillion. It is this critical component, coupled with chain-of-thought prompting, that underlies o1’s ability to outperform the vast majority of open-source models on multi-step problem-solving and step-by-step reasoning [26]. However, the dominance of closed-source models is constrained by marked methodological challenges. Data contamination, i.e., overlaps between training and evaluation datasets, has been claimed to compromise the reliability of benchmarks for closed-source models [27]. The lack of transparency in proprietary datasets compounds this issue, leaving many results difficult to validate independently [28].</span><br><span class="line"></span><br><span class="line">Despite these advantages, open-source LLMs have made significant progress in closing the performance gap. Techniques like Low-Rank Adaptation (LoRA) and Conditioned Reinforcement Learning Fine-Tuning (C-RLFT) have been instrumental in this advancement. LoRA’s capacity to selectively fine-tune parameters, for example, minimizes computational costs while maintaining high domain-specific accuracy with competitive results on benchmarks such as GSM8K. NVIDIA’s NVLM 1.0D 72B model exemplifies domain-specific excellence, achieving a 4.3-point improvement in mathematical reasoning and coding tasks through multimodal training [29]. Unlike models such as InternVL2-Llama3-76B, which exhibit degraded text-based performance after multimodal training, NVLM not only preserves but also enhances its textual capabilities. This robustness enables NVLM to handle complex domain-specific inputs, such as handwritten pseudocode or location-specific queries, underscoring its precision within specialized contexts. Its performance provides a proof-of-concept for domain-specific models to complement domain-general systems.</span><br><span class="line"></span><br><span class="line">Similar outcomes in performance hold true for domain-specific models like StarCoder, which underwent targeted optimization for programming tasks, and ClinicalBERT, which underwent targeted optimization for medical analyses. These platforms often outperform general-purpose closed-source models in benchmarks like HumanEval. Techniques like knowledge distillation and model compression are salient instances of the aforementioned open-source optimization strategies [30]. Knowledge distillation creates smaller, efficient versions of larger models by transferring essential performance traits, while model compression reduces the requirements of computational resources. To be sure, these models underscore how open-source frameworks not only rival but increasingly exceed closed-source models in specialized applications, while maintaining their stronghold on accessibility to resource-constrained communities.</span><br><span class="line"></span><br><span class="line">Now although open-source models’ transparency facilitates reproducibility of outcomes, their benchmarking potential is limited by narrower dataset scopes and fewer resources. Current benchmarks often prioritize narrow task-specific metrics, overlooking the complexity and diversity of real-world applications. Resource disparities further skew these outcomes in performance, as closed-source models benefit from high-performance distributed systems, whereas open-source models must optimize performance through collaborative resource pooling and parameter-efficient strategies. Overcoming these limitations would require the development of parameter-normalized and task-agnostic evaluation frameworks to enable more comprehensive comparisons.</span><br><span class="line"></span><br><span class="line">4Accessibility</span><br><span class="line">The question of how accessible Large Language Models (LLMs) are varies substantially between open-source and closed-source systems in a way that influences their deployment on a range of applications. Open-source initiatives such as LLaMA and BLOOM have been instrumental in democratizing LLM technology. LLaMA achieves this by enabling researchers to run advanced NLP tasks on single GPUs [31]. Optimizing for smaller model sizes lowers computational barriers while maintaining high benchmark performance, particularly in reasoning and mathematical tasks, and allows LLaMa to often surpass larger closed-source counterparts like GPT-3 and Chinchilla. Similarly, BLOOM was developed collaboratively by over 1,000 researchers; it supports 46 natural languages and 13 programming languages [32]. Its public release forefronted the potential of open science to expand LLM applications globally and foster inclusion for underrepresented linguistic communities.</span><br><span class="line"></span><br><span class="line">Techniques like Low-Rank Adaptation (LoRA) further enhance such accessibility by significantly reducing computational requirements for fine-tuning. LoRA achieves this by freezing most model parameters and optimizing a low-rank adaptation of the model weights, enabling developers to tailor LLMs such as GPT-3 for specific tasks with considerably lower resource demands. This mechanism has proven versatile and found applications in areas such as textual summarization and SQL query generation, which are critical for both academic research and industry use [33]. Additionally, smaller and distilled models, such as DistilBERT, play a key role in bridging the gap between cutting-edge NLP capabilities and practical deployment. By reducing the size of BERT by 40% while retaining 97% of its language understanding ability, DistilBERT facilitates real-time, on-device applications [34].</span><br><span class="line"></span><br><span class="line">Domain-specific LLMs have also begun to make solutions tailored for specialized fields more accessible. ClinicalBERT, fine-tuned on medical datasets, improves performance in tasks such as named entity recognition (NER) and natural language inference (NLI) for healthcare applications [35]. By releasing ClinicalBERT publicly, researchers have ensured that even organizations with limited resources can access advanced NLP tools to enhance clinical decision-making and patient care. Likewise, LEGAL-BERT and FinBERT address specific needs in the legal and financial sectors. LEGAL-BERT outperforms general-purpose models in tasks such as contract analysis and case law classification [36], while FinBERT excels in sentiment analysis for finance-related tasks, aiding in market trend predictions and supporting strategic financial decisions [37]. These domain-specific models illustrate the versatility of open-source systems in addressing niche requirements while ensuring accessibility for smaller organizations.</span><br><span class="line"></span><br><span class="line">Yet, it must be acknowledged that closed-source models like Codex can also expand accessibility within targeted use cases. Integrated into GitHub Copilot, Codex translates natural language inputs into code. This significantly lowers the barrier to entry for non-programmers and enhances efficiency for experienced developers by 55% [38]. By streamlining the software development process and offering educational tools, Codex exemplifies how closed-source models can drive adoption in specific domains despite their proprietary nature. However, the lack of transparency and limited availability of these models often restrict customization, which are hallmarks of open-source initiatives.</span><br><span class="line"></span><br><span class="line">5Transparency</span><br><span class="line">The ethical dimensions of Large Language Models (LLMs) lie at the heart of their societal impact, with transparency emerging as a pivotal factor in evaluating their fairness, accountability, and trustworthiness. The contrasting approaches taken by open-source and closed-source LLMs reveal a fundamental trade-off between visibility and proprietary control. Open-source models offer unmatched access to internal mechanisms but often lack the governance structures necessary for consistent ethical rigor. Conversely, closed-source models protect intellectual property at the expense of public trust and external accountability. This ongoing debate underscores the challenge of balancing innovation with moral standards in the development and deployment of LLMs.</span><br><span class="line"></span><br><span class="line">By definition, open-source LLMs promote transparency by providing unrestricted access to their architectures, weights, and training methodologies [39]. This openness allows researchers and developers to scrutinize these models at granular levels, auditing for biases, testing adherence to fairness metrics, and identifying vulnerabilities within their decision-making processes. The communal nature of open-source ecosystems democratizes ethical oversight to a certain extent. Platforms like GitHub and Hugging Face not only disseminate open-source models but also provide accompanying documentation, such as model cards, that outline ethical considerations, known limitations, and appropriate usage contexts. As such, distributed networks of researchers and practitioners can collaboratively review and improve these models, often uncovering latent issues such as algorithmic bias, adversarial weaknesses, or dual-use risks [40]. For instance, the open scrutiny of datasets and fine-tuning protocols has led to significant advancements in understanding and mitigating biases in multilingual models like BLOOM.</span><br><span class="line"></span><br><span class="line">Yet, this distributed accountability relies heavily on the quality of available data, and the effectiveness of this transparency is often undermined by inconsistencies in documentation quality. It has been noted that templates are frequently reused, which leads to superficial descriptions of ethical risks and gaps in actionable insights for addressing them. For these reasons, establishing rigorous, standardized frameworks for ethical auditing—including detailed model cards with metrics for bias auditing, fairness evaluations, potential misuse scenarios presented in a standardized schema, and testing of interpretability—is crucial to fully realize the transparency potential of open-source LLMs. Moreover, tools for automated ethical assessments, such as explainability algorithms and adversarial robustness tests, could supplement human-led audits and provide baseline evaluations that align with broader governance standards.</span><br><span class="line"></span><br><span class="line">By contrast, closed-source LLMs operate within proprietary frameworks that limit visibility into their internal mechanisms [41]. The lack of access to training datasets, preprocessing pipelines, and decision-making logic restricts third-party audits and independent evaluations to a degree that has effectively rendered them “black boxes.” This opacity exacerbates the difficulty of identifying and addressing biases embedded in these systems. For example, when closed-source models produce outputs that reinforce harmful stereotypes, as was the case with Google’s Gemini model, it remains unclear whether the issue stems from biased training data, flawed objective functions, or other systemic deficiencies [42]. Worse still is the fact that in virtue of the models being closed-source, their developers are not obligated to disclose ethical risks, mitigation strategies, or even the fundamental design principles guiding their models. This lack of disclosure creates a trust deficit, particularly in high-stakes domains such as healthcare diagnostics, legal decision-making, or autonomous transportation systems like self-driving cars, where the consequences of errors or biases can be profound. Moreover, the absence of external oversight often means that ethical considerations are deprioritized in favor of performance optimization or market demands.</span><br><span class="line"></span><br><span class="line">For these reasons, regulatory frameworks serve as critical mediators of closed-source LLMs [43]. Governments and industry consortia should establish mandates for transparency in high-risk applications. These could include requirements for disclosing anonymized datasets, publishing high-level explanations of decision-making processes, or engaging external (third-party, independent, unaffiliated) ethics oversight boards for pre-deployment risk assessments. While such measures may not match the transparency of open-source systems models, they can provide critical visibility into the ethically salient dimensions of proprietary systems without compromising trade secrets.</span><br><span class="line"></span><br><span class="line">In short, bridging this divide between open- and closed-source LLMs will require hybrid solutions that combine the strengths of both paradigms. For instance, closed-source developers could adopt modular transparency—releasing anonymized components or high-level abstractions of decision-making logic to facilitate third-party evaluations. Simultaneously, open-source ecosystems could leverage advancements in ethical assessment tools to enhance their auditing processes. By integrating these complementary approaches, the ML community can create systems that not only achieve state-of-the-art performance but also uphold principles of fairness, accountability, and trustworthiness. This kind of interdisciplinary collaboration between computer science, ethics, policy, and sociology will be sure to play a decisive role in defining the trajectory of ethical LLM development and its alignment with our values.</span><br><span class="line"></span><br><span class="line">6Discussion</span><br><span class="line">Our comparative analysis of open-source and closed-source Large Language Models (LLMs) offers critical insights into the differing trajectories of innovation, accessibility, and collaboration in natural language processing (NLP). Both paradigms share foundational technologies, such as Transformer architectures, but their distinct underlying philosophical commitments have led to varied impacts on the field. This discussion evaluates both trajectories, clarifying the strengths and limitations of each, while advancing a clear and grounded position on the transformative potential of open-source models.</span><br><span class="line"></span><br><span class="line">Closed-source LLMs continue to lead in performance, leveraging proprietary datasets and significant computational investments to excel in tasks requiring advanced generative abilities, multi-step reasoning, and broad generalization. However, their success comes at the cost of limited transparency and restricted accessibility, which creates challenges for external validation and replication. The closed-source approach also consolidates resources and technological power within a few institutions. In so doing, it poses barriers to equitable AI development and raising concerns about reproducibility of outcomes and organizational accountability. By contrast, open-source LLMs emphasize accessibility and collaborative development. While these models often trail closed-source systems in absolute performance, they have made significant progress in narrowing the gap through methods such as Low-Rank Adaptation (LoRA) and quantization. These strategies enable efficient, competitive outcomes even in resource-constrained environments. By utilizing diverse datasets across languages and contexts, open-source models demonstrate their capacity to address real-world challenges with inclusivity. This democratic ethos has already empowered researchers and developers globally, and is likely to continue to do so.</span><br><span class="line"></span><br><span class="line">The scalability of closed-source models is evident in their ability to set performance benchmarks, leveraging extensive datasets and robust infrastructure. However, their reliance on proprietary resources limits their adaptability to niche or underrepresented use cases. By contrast, open-source models, though constrained by resource limitations, possess the potential to surpass these benchmarks by leveraging their access to diverse and evolving datasets. Their adaptability allows them to address specialized challenges and emerging contexts, but their success hinges on sustained contributions from the global community. Collaborative efforts are essential to overcoming resource gaps and achieving continuous improvement.</span><br><span class="line"></span><br><span class="line">Accessibility remains a key distinction between the two paradigms. Closed-source systems have improved accessibility within specific domains through tools like GitHub Copilot, which streamline adoption for non-expert users. However, these tools often lack the flexibility needed for personal customization [44]. Open-source models excel in this regard, offering modular architectures and reduced computational barriers that enable widespread experimentation and deployment. This flexibility supports innovation across academic, industrial, and grassroots initiatives and serves to highlight the potential for integrating the strengths of both approaches to optimize usability and adaptability.</span><br><span class="line"></span><br><span class="line">Notably, ethical considerations further differentiate these paradigms. The opacity of closed-source models exacerbates deficits in trust, particularly in critical applications such as healthcare and legal decision-making. Their restricted access to internal mechanisms limits external auditing and accountability, which, by extension, raises concerns about fairness and safety. In contrast, open-source models prioritize transparency by providing unrestricted access to architectures, datasets, and methodologies. However, inconsistencies in documentation and the absence of standardized ethical frameworks pose challenges for ensuring reliable oversight. A hybrid approach that combines the transparency of open-source models with regulatory measures could address these concerns, ensuring more responsible AI deployment.</span><br><span class="line"></span><br><span class="line">One promising avenue for future research lies in addressing the phenomenon of hallucinations in LLMs, which manifests differently depending on the context. When these models generate incorrect outputs, they are often labeled as mistakes, yet when their outputs are creative and contextually aligned, they are celebrated as innovation [45]. This tension is particularly pronounced in reasoning tasks, where precision and coherence are of paramount importance. Understanding and mitigating hallucinations requires a systematic approach to model evaluation, focusing on distinguishing productive creativity from erroneous reasoning. Open-source models can play a pivotal role in this research by fostering collaborative experiments with diverse datasets and benchmarking strategies that illuminate the mechanisms underlying hallucinations.</span><br><span class="line"></span><br><span class="line">Another direction for future work involves enhancing the reasoning capabilities of LLMs through interdisciplinary contributions. By integrating insights from cognitive science and formal logic, researchers can develop frameworks to improve reasoning fidelity and robustness in LLMs. Open-source ecosystems are uniquely positioned to drive progress in this area, offering the transparency and flexibility needed to experiment with novel architectures, training methods, and evaluation protocols. Collaborative efforts could enable the design of more reliable and context-aware reasoning systems, pushing the boundaries of what LLMs can achieve in tasks requiring deep understanding and logical coherence.</span><br><span class="line"></span><br><span class="line">To be sure, closed-source LLMs currently dominate performance benchmarks due to their resource-intensive strategies, while open-source models offer unparalleled accessibility and the potential for equitable AI advancement. The future of open-source LLMs depends on fostering a robust ecosystem of contributors to drive innovation and refinement. By leveraging diverse datasets and emphasizing collaboration, open-source models have the capacity to address global challenges and redefine the NLP landscape in a way that is thus far unclear whether closed-source models can.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://huggingface.co/blog/falcon3</span><br><span class="line">Welcome to the Falcon 3 Family of Open Models!</span><br><span class="line">Published December 17, 2024</span><br><span class="line"></span><br><span class="line">WHOA! The Falcon has landed 🔥</span><br><span class="line">TL;DR: Meet Falcon3, the game-changing family of LLMs advancing open and accessible large foundation models.</span><br><span class="line">Here are the top highlights:</span><br><span class="line">📉 Smaller but Mighty: Falcon3 models achieve state-of-the-art performance with fewer parameters (under 10B)</span><br><span class="line">🤓 Math Whiz: Falcon3-10B-Base scores 22.9 on MATH-Lvl5 and 83.0 on GSM8K, showcasing enhanced math reasoning</span><br><span class="line">💻 Coding Master: Falcon3-10B-Base achieves 73.8 on MBPP and Falcon3-10B-Instruct scores 45.8 on Multipl-E, demonstrating coding prowess</span><br><span class="line">📚 Long Context Support: Falcon3 models support up to 32k tokens, with Falcon3-10B-Instruct scoring 86.3 on BFCL</span><br><span class="line">💡 Reasoning Superstar: Falcon3-7B-Base and Falcon3-10B-Base achieve 51.0 and 59.7 on BBH, showcasing improved reasoning capabilities</span><br><span class="line">🎯 Scientific Knowledge Expansion: Falcon3 models demonstrate advances in specialized knowledge, with scores of 67.4 and 73.1 on MMLU benchmarks</span><br><span class="line">🤝 Compatibility: All Falcon3 models are compatible with Llama architecture, ensuring seamless integration into the AI ecosystem</span><br><span class="line">🎉 Variants Galore: Falcon3 models come in various flavors, including Instruct, GGUF, GPTQ-Int4, and more, offering flexibility for diverse applications</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">We introduce Falcon3, a family of decoder-only large language models under 10 billion parameters, developed by Technology Innovation Institute (TII) in Abu Dhabi. By pushing the boundaries of performance and training efficiency, this release reflects our ongoing commitment to advancing open and accessible large foundation models.</span><br><span class="line">Falcon3 represents a natural evolution from previous releases, emphasizing expanding the models&#x27; science, math, and code capabilities.</span><br><span class="line"></span><br><span class="line">This iteration includes five base models:</span><br><span class="line"></span><br><span class="line">Falcon3-1B-Base</span><br><span class="line">Falcon3-3B-Base</span><br><span class="line">Falcon3-Mamba-7B-Base</span><br><span class="line">Falcon3-7B-Base</span><br><span class="line">Falcon3-10B-Base</span><br><span class="line">In developing these models, we incorporated several key innovations aimed at improving the models&#x27; performances while reducing training costs:</span><br><span class="line"></span><br><span class="line">One pre-training for transformer-based models: We conducted a single large-scale pretraining run on the 7B model, using 1024 H100 GPU chips, leveraging 14 trillion tokens featuring web, code, STEM, and curated high-quality and multilingual data.</span><br><span class="line">Depth up-scaling for improved reasoning: Building on recent studies on the effects of model depth, we upscaled the 7B model to a 10B parameters model by duplicating the redundant layers and continuing pre-training with 2 trillion tokens of high-quality data. This yielded Falcon3-10B-Base which achieves state-of-the-art zero-shot and few-shot performance for models under 13B parameters.</span><br><span class="line">Knowledge distillation for better tiny models: To provide compact and efficient alternatives, we developed Falcon3-1B-Base and Falcon3-3B-Base by leveraging pruning and knowledge distillation techniques, using less than 100GT of curated high-quality data, thereby redefining pre-training efficiency.</span><br><span class="line">Pure SSM: We have further enhanced Falcon Mamba 7B by training on an additional 1.5 trillion tokens of high-quality data, resulting in Falcon3-Mamba-7B-Base. Notably, the updated model offers significantly improved reasoning and mathematical capabilities.</span><br><span class="line">Other variants: All models in the Falcon3 family are available in variants such as Instruct, GGUF, GPTQ-Int4, GPTQ-Int8, AWQ, and 1.58-bit, offering flexibility for a wide range of applications.</span><br><span class="line">Key Highlights</span><br><span class="line">Falcon3 featured the limits within the small and medium scales of large language models by demonstrating high performance on common benchmarks:</span><br><span class="line"></span><br><span class="line">Falcon3-1B-Base surpasses SmolLM2-1.7B and is on par with gemma-2-2b.</span><br><span class="line">Falcon3-3B-Base outperforms larger models like Llama-3.1-8B and Minitron-4B-Base, highlighting the benefits of pre-training with knowledge distillation.</span><br><span class="line">Falcon3-7B-Base demonstrates top performance, on par with Qwen2.5-7B, among models under the 9B scale.</span><br><span class="line">Falcon3-10B-Base stands as the state-of-the-art achieving strong results in the under-13B category.</span><br><span class="line">All the transformer-based Falcon3 models are compatible with Llama architecture allowing better integration in the AI ecosystem.</span><br><span class="line">Falcon3-Mamba-7B continues to lead as the top-performing State Space Language Model (SSLM), matching or even surpassing leading transformer-based LLMs at the 7B scale, along with support for a longer 32K context length. Having the same architecture as the original Falcon Mamba 7B, users can integrate Falcon3-Mamba-7B seamlessly without any additional effort.</span><br><span class="line">The instruct versions of our collection of base models further show remarkable performance across various benchmarks with Falcon3-7B-Instruct and Falcon3-10B-Instruct outperforming all instruct models under the 13B scale on the open leaderboard.</span><br><span class="line">Enhanced Capabilities</span><br><span class="line">We evaluated models with our internal evaluation pipeline (based on lm-evaluation-harness) and we report raw scores. Our evaluations highlight key areas where the Falcon3 family of models excel, reflecting the emphasis on enhancing performance in scientific domains, reasoning, and general knowledge capabilities:</span><br><span class="line"></span><br><span class="line">Math Capabilities: Falcon3-10B-Base achieves 22.9 on MATH-Lvl5 and 83.0 on GSM8K, showcasing enhanced reasoning in complex math-focused tasks.</span><br><span class="line">Coding Capabilities: Falcon3-10B-Base achieves 73.8 on MBPP, while Falcon3-10B-Instruct scores 45.8 on Multipl-E, reflecting their abilities to generalize across programming-related tasks.</span><br><span class="line">Extended Context Length: Models in the Falcon3 family support up to 32k tokens (except the 1B supporting up to 8k context), with functional improvements such as scoring 86.3 on BFCL (Falcon3-10B-Instruct).</span><br><span class="line">Improved Reasoning: Falcon3-7B-Base and Falcon3-10B-Base achieve 51.0 and 59.7 on BBH, reflecting enhanced reasoning capabilities, with the 10B model showing improved reasoning performance over the 7B.</span><br><span class="line">Scientific Knowledge Expansion: Performance on MMLU benchmarks demonstrates advances in specialized knowledge, with scores of 67.4/39.2 (MMLU/MMLU-PRO) for Falcon3-7B-Base and 73.1/42.5 (MMLU/MMLU-PRO) for Falcon3-10B-Base respectively.</span><br><span class="line">Models&#x27; Specs and Benchmark Results</span><br><span class="line">Detailed specifications of the Falcon3 family of models are summarized in the following table. The architecture of Falcon3-7B-Base is characterized by a head dimension of 256 which yields high throughput when using FlashAttention-3 as it is optimized for this dimension. These decoder-only models span 18 to 40 layers for the transformer-based ones, and 64 layers for the mamba one, all models share the SwiGLU activation function, with vocabulary size of 131K tokens (65Kfor Mamba-7B). The Falcon3-7B-Base is trained on the largest amount of data ensuring comprehensive coverage of concepts and knowledge, the other variants require way less data.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Training efficiency</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The table below highlights the performances of Falcon3-7B-Base and Falcon3-10B-Base on key benchmarks showing competitive performances in general, math, reasoning, and common sense understanding domains. Feel free to take a look at models&#x27; cards where we provide additional evaluation results (e.g. MT-Bench, Alpaca, etc).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Training efficiency</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The instruct models also demonstrate competitive and super performances with equivalent and small-size models as highlighted in the tables below.</span><br><span class="line"></span><br><span class="line">Instruct models</span><br><span class="line">Falcon3-1B-Instruct and Falcon3-3B-Instruct achieve robust performance across the evaluated benchmarks. Specifically, Falcon3-1B attains competitive results in IFEval (54.4), MUSR (40.7), and SciQ (86.8), while Falcon3-3B exhibits further gains—particularly in MMLU-PRO (29.7) and MATH (19.9)—demonstrating clear scaling effects. Although they do not surpass all competing models on every metric, Falcon models show strong performances in reasoning and common-sense understanding relative to both Qwen and Llama. In our internal evaluation pipeline:</span><br><span class="line"></span><br><span class="line">We use lm-evaluation harness.</span><br><span class="line">We report raw scores obtained by applying chat template without fewshot_as_multiturn (unlike Llama3.1).</span><br><span class="line">We use same batch-size across all models.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Training efficiency</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Furthermore, Falcon3-7B and Falcon3-10B show robust performance across the evaluated benchmarks. Falcon3-7B achieves competitive scores on reasoning (Arc Challenge: 65.9, MUSR: 46.4) and math (GSM8K: 79.1), while Falcon3-10B demonstrates further improvements, notably in GSM8K (83.1) and IFEval (78), indicating clear scaling benefits.</span><br><span class="line"></span><br><span class="line">Training efficiency</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Open Source Commitment</span><br><span class="line">In line with our mission to foster AI accessibility and collaboration, all models in the Falcon3 family are released under the Falcon LLM license. We hope the AI community finds these models valuable for research, application development, and further experimentation. Falcon3 is not a culmination but a continuation of our efforts to create more capable, efficient, specialized foundation models. In January 2025, we will further release other models of the Falcon3 family featuring enhanced multi-modal capabilities including image, video, and audio support, as well as a full technical report covering our methodologies. We welcome feedback and collaboration from the community as we continue to refine and advance these technologies.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://www.kaggle.com/facts-leaderboard</span><br><span class="line">FACTS Leaderboard</span><br><span class="line">FACTS is a novel benchmark from Google DeepMind and Google Research designed to evaluate the factual accuracy and grounding of AI models.</span><br><span class="line"></span><br><span class="line">Introduction</span><br><span class="line">The FACTS Grounding benchmark evaluates the ability of Large Language Models (LLMs) to generate factually accurate responses grounded in provided long-form documents, encompassing a variety of domains. FACTS Grounding moves beyond simple factual question-answering by assessing whether LLM responses are fully grounded to the provided context and correctly synthesize information from a long context document. By providing a standardized evaluation framework, FACTS Grounding aims to promote the development of LLMs that are both knowledgeable and trustworthy, facilitating their responsible deployment in real-world applications.</span><br><span class="line"></span><br><span class="line">Dataset</span><br><span class="line">Starter Code</span><br><span class="line">Technical Report</span><br><span class="line">Blog Post</span><br><span class="line">Model</span><br><span class="line">Factuality Score</span><br><span class="line">95% CI</span><br><span class="line">Organization</span><br><span class="line">License</span><br><span class="line">Knowledge Cutoff</span><br><span class="line">1</span><br><span class="line">gemini-2.0-flash-exp</span><br><span class="line">83.6%</span><br><span class="line">±1.8%</span><br><span class="line">Google</span><br><span class="line">Proprietary</span><br><span class="line">8/2024</span><br><span class="line">2</span><br><span class="line">gemini-1.5-flash-002</span><br><span class="line">82.9%</span><br><span class="line">±1.8%</span><br><span class="line">Google</span><br><span class="line">Proprietary</span><br><span class="line">11/2023</span><br><span class="line">3</span><br><span class="line">gemini-1.5-pro-002</span><br><span class="line">80.0%</span><br><span class="line">±1.9%</span><br><span class="line">Google</span><br><span class="line">Proprietary</span><br><span class="line">11/2023</span><br><span class="line">4</span><br><span class="line">claude-3-5-sonnet-20241022</span><br><span class="line">79.4%</span><br><span class="line">±1.9%</span><br><span class="line">Anthropic</span><br><span class="line">Proprietary</span><br><span class="line">4/2024</span><br><span class="line">5</span><br><span class="line">gpt-4o</span><br><span class="line">78.8%</span><br><span class="line">±1.9%</span><br><span class="line">OpenAI</span><br><span class="line">Proprietary</span><br><span class="line">10/2023</span><br><span class="line">6</span><br><span class="line">claude-3-5-haiku-20241022</span><br><span class="line">74.2%</span><br><span class="line">±2.1%</span><br><span class="line">Anthropic</span><br><span class="line">Proprietary</span><br><span class="line">4/2024</span><br><span class="line">7</span><br><span class="line">gpt-4o-mini</span><br><span class="line">71.0%</span><br><span class="line">±2.1%</span><br><span class="line">OpenAI</span><br><span class="line">Proprietary</span><br><span class="line">10/2023</span><br><span class="line">8</span><br><span class="line">o1-mini</span><br><span class="line">62.0%</span><br><span class="line">±2.3%</span><br><span class="line">OpenAI</span><br><span class="line">Proprietary</span><br><span class="line">10/2023</span><br><span class="line">9</span><br><span class="line">o1-preview</span><br><span class="line">61.7%</span><br><span class="line">±2.3%</span><br><span class="line">OpenAI</span><br><span class="line">Proprietary</span><br><span class="line">10/2023</span><br><span class="line">About FACTS Grounding</span><br><span class="line"></span><br><span class="line">FACTS Grounding is based on a novel set of factual grounding examples collected from human raters. Each example consists of a system instruction, user request and a context document (maximum of 32k tokens), and requires a long-form response. AI generated responses to these examples are evaluated by an ensemble of automated judge models.</span><br><span class="line"></span><br><span class="line">For more details, please refer to the Examples Section or Technical Report.</span><br><span class="line"></span><br><span class="line">Grounding Example Distribution</span><br><span class="line"></span><br><span class="line">The full FACTS Grounding benchmark is comprised of 1,719 examples. This includes 860 public examples available in the FACTS Grounding Public Examples Dataset. The remaining 859 examples comprise a private set that will be held out to mitigate risks of benchmark contamination. Leaderboard results on this page are the results across both public and private sets.</span><br><span class="line">Task Distribution</span><br><span class="line">Fact Finding</span><br><span class="line">Find &amp; Summarize</span><br><span class="line">Effect Analysis</span><br><span class="line">Explain/Define</span><br><span class="line">Concept Comparison</span><br><span class="line">Pros &amp; Cons</span><br><span class="line">Summarize &amp; Format</span><br><span class="line">Summarize</span><br><span class="line">Summarize &amp; Simplify</span><br><span class="line">31.6%</span><br><span class="line">4.4%</span><br><span class="line">6.1%</span><br><span class="line">7.5%</span><br><span class="line">8.9%</span><br><span class="line">29.6%</span><br><span class="line">Task	Count</span><br><span class="line">Fact Finding	543</span><br><span class="line">Find &amp; Summarize	509</span><br><span class="line">Effect Analysis	153</span><br><span class="line">Explain/Define	129</span><br><span class="line">Concept Comparison	105</span><br><span class="line">Pros &amp; Cons	76</span><br><span class="line">Summarize &amp; Format	75</span><br><span class="line">Summarize	65</span><br><span class="line">Summarize &amp; Simplify	64</span><br><span class="line">Domain Distribution</span><br><span class="line">Medical</span><br><span class="line">Legal</span><br><span class="line">Internet/Technology</span><br><span class="line">Financial</span><br><span class="line">Retail/Product</span><br><span class="line">29%</span><br><span class="line">11.4%</span><br><span class="line">18.2%</span><br><span class="line">19.2%</span><br><span class="line">22.2%</span><br><span class="line">Task	Count</span><br><span class="line">Medical	499</span><br><span class="line">Legal	382</span><br><span class="line">Internet/Technology	330</span><br><span class="line">Financial	312</span><br><span class="line">Retail/Product	196</span><br><span class="line"></span><br><span class="line">Running FACTS Grounding</span><br><span class="line">Starter Code</span><br><span class="line"></span><br><span class="line">If you’d like to test your own model’s performance on FACTS Grounding, you can generate your own responses on the set of public examples with the methodology described in the Technical Report.</span><br><span class="line"></span><br><span class="line">Computing the Factuality Score</span><br><span class="line"></span><br><span class="line">The factuality score in the FACTS Grounding benchmark is calculated by first using three different frontier LLM judges to determine if a response is grounded to the provided context. A response is labeled &quot;accurate&quot; if all its claims are directly supported or don&#x27;t require support from the context; otherwise, it&#x27;s marked &quot;inaccurate.&quot; Each judge calculates a factuality score individually as the percentage of accurate responses. To mitigate bias, the final score is an average across all three judges. Responses deemed ineligible are disqualified from the factuality scoring process and are treated as factually inaccurate. The factuality score reported in this leaderboard is the average across both the public and private example sets.</span><br><span class="line"></span><br><span class="line">Quality Filtering</span><br><span class="line"></span><br><span class="line">To prevent models from &quot;gaming&quot; the factuality score by providing short, evasive responses, FACTS Grounding employs a quality filtering step. This process uses the same three LLM judges, but with different prompt templates designed to identify responses that don&#x27;t sufficiently address the user&#x27;s request. A response is disqualified only if all three judges agree that a response is &quot;ineligible&quot;. In this way, low-quality responses are filtered out from the final score shown in the leaderboard.</span><br><span class="line"></span><br><span class="line">Adding New Models</span><br><span class="line"></span><br><span class="line">The FACTS Grounding leaderboard will be actively maintained so suggestions from the community on new models to evaluate are welcome! To begin, we will focus on expanding coverage to more frontier language models.</span><br><span class="line"></span><br><span class="line">As the FACTS Grounding benchmark includes a set of private held out prompts, official results on the leaderboard will be run by the Kaggle team.</span><br><span class="line"></span><br><span class="line">To request a model for evaluation, please fill out this form.</span><br><span class="line"></span><br><span class="line">Limitations</span><br><span class="line"></span><br><span class="line">While this benchmark represents a step forward in evaluating factual accuracy, more work remains to be done. First, this benchmark relies on potentially noisy automated LLM judge models for evaluation. By ensembling a range of frontier LLMs and averaging judge outputs, we attempt to mitigate this. Second, the FACTS benchmark focuses only on evaluating grounded responses to long-form text input and could potentially be extended.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Questions, comments, or issues? Share your thoughts with us in the discussion forum.</span><br><span class="line"></span><br><span class="line">What is the best LLM for RAG and grounded long-form inputs? Google DeepMind FACTS is a new benchmark for how well LLMs can generate factually accurate, long-form responses while remaining faithful to provided source documents. 👀</span><br><span class="line">TL;DR;</span><br><span class="line">📊 1,719 examples (860 public, 859 private) covering prompts up to 32k tokens</span><br><span class="line">🎯 Focuses grounding, excluding creativity, mathematics, and complex reasoning</span><br><span class="line">🏢 Covers medical (29%), legal (22.2%), tech (19.2%), financial (18.1%), retail (11.4%)</span><br><span class="line">📝 Task types include fact-finding (31.6%), find &amp; summarize (29.7%), effect analysis (8.9%), and more</span><br><span class="line">👥 Use three LLM as a judge (Gemini 1.5 Pro, GPT-4o, Claude 3.5 Sonnet) to reduce bias</span><br><span class="line">🏆 Gemini 2.0 Flash outperforms all other models with 83.6% accuracy</span><br><span class="line">🛡️ Comprehensive quality assurance process to ensure high-quality, non-trivial examples</span><br><span class="line">🔢 Judge models tend to rate their own outputs higher, showing the importance of using multiple judge models.</span><br><span class="line">📝 Models can be submitted on Kaggle for evaluation</span><br><span class="line"></span><br><span class="line">Responsibility &amp; Safety</span><br><span class="line"></span><br><span class="line">FACTS Grounding: A new benchmark for evaluating the factuality of large language models</span><br><span class="line">Published</span><br><span class="line">17 December 2024</span><br><span class="line">Authors</span><br><span class="line">FACTS team</span><br><span class="line"></span><br><span class="line">Share</span><br><span class="line"></span><br><span class="line">Our comprehensive benchmark and online leaderboard offer a much-needed measure of how accurately LLMs ground their responses in provided source material and avoid hallucinations</span><br><span class="line"></span><br><span class="line">Large language models (LLMs) are transforming how we access information, yet their grip on factual accuracy remains imperfect. They can “hallucinate” false information, particularly when given complex inputs. In turn, this can erode trust in LLMs and limit their applications in the real world.</span><br><span class="line"></span><br><span class="line">Today, we’re introducing FACTS Grounding, a comprehensive benchmark for evaluating the ability of LLMs to generate responses that are not only factually accurate with respect to given inputs, but also sufficiently detailed to provide satisfactory answers to user queries.</span><br><span class="line"></span><br><span class="line">We hope our benchmark will spur industry-wide progress on factuality and grounding. To track progress, we’re also launching the FACTS leaderboard on Kaggle. We’ve already tested leading LLMs using FACTS Grounding and have populated the initial leaderboard with their grounding scores. We will maintain and update the leaderboard as the field advances.</span><br><span class="line"></span><br><span class="line">A table showing the current leaderboard ranking on the FACTS leaderboard</span><br><span class="line">Current leaderboard ranking</span><br><span class="line"></span><br><span class="line">FACTS Grounding dataset</span><br><span class="line">To accurately evaluate the factuality and grounding of any given LLM, the FACTS Grounding dataset comprises 1,719 examples, each carefully crafted to require long-form responses grounded in the context document provided. Each example comprises a document, a system instruction requiring the LLM to exclusively reference the provided document, and an accompanying user request.</span><br><span class="line"></span><br><span class="line">A table showing an example of a question and response based on the FACTS grounding dataset</span><br><span class="line">An example from the FACTS Grounding dataset</span><br><span class="line"></span><br><span class="line">All examples are divided into a &quot;public&quot; set (860) and a &quot;private&quot; (859) held out set. We are releasing the public set today so anyone can use it to evaluate an LLM. Of course, we know that issues of benchmark contamination and leaderboard hacking are important to protect against, so following standard industry practice, we are keeping the private evaluation set held out. The FACTS leaderboard scores are the average performance across both public and private sets.</span><br><span class="line"></span><br><span class="line">To ensure a diversity of inputs, the FACTS Grounding examples include documents with a variety of lengths, up to a maximum of 32,000 tokens (roughly 20,000 words), covering domains such as finance, technology, retail, medicine, and law. The user requests are similarly wide ranging, including requests for summarization, Q&amp;A generation, and rewriting tasks. We did not include any examples that could require creativity, mathematics, or complex reasoning – capabilities which might require the model to apply more advanced reasoning in addition to grounding.</span><br><span class="line"></span><br><span class="line">Two pie charts showing the domain and task distribution of the prompts</span><br><span class="line">Prompt distribution</span><br><span class="line"></span><br><span class="line">Collective judgement by leading LLMs</span><br><span class="line">To succeed on a given example, an LLM must synthesize the complex information in the document and generate a long-form response that is both a comprehensive answer to the user request and fully attributable to that document.</span><br><span class="line"></span><br><span class="line">FACTS Grounding evaluates model responses automatically using three frontier LLM judges — namely Gemini 1.5 Pro, GPT-4o, and Claude 3.5 Sonnet. We selected a combination of different judges to mitigate any potential bias of a judge giving higher scores to the responses produced by a member of its own model family. The automatic judge models were comprehensively evaluated against a held-out test set to find the best performing judging prompt templates and to verify agreement with human raters.</span><br><span class="line"></span><br><span class="line">Each FACTS Grounding example is judged in two phases. First, responses are evaluated for eligibility, and disqualified if they don’t sufficiently address the user’s request. Second, responses are judged as factually accurate if they are fully grounded in information contained in the provided document, with no hallucinations.</span><br><span class="line"></span><br><span class="line">With the eligibility and grounding accuracy of a given LLM response evaluated separately by multiple AI judge models, the results are then aggregated to determine if the LLM has dealt with the example successfully. The final score for the overall grounding task is the average of all judge models’ scores across all examples. Find more details of our FACTS Grounding evaluation methodology in our paper.</span><br><span class="line"></span><br><span class="line">An illustration of how a factuality score is assigned</span><br><span class="line">A factually correct response that fails to properly address the user’s request fails the benchmarking example. Here we see three instances of model responses that the automated LLM judges considered ineligible</span><br><span class="line"></span><br><span class="line">FACTS Grounding will continue to evolve</span><br><span class="line">We are mindful that benchmarks can be quickly overtaken by progress, so this launch of our FACTS Grounding benchmark and leaderboard is just the beginning. Factuality and grounding are among the key factors that will shape the future success and usefulness of LLMs and broader AI systems, and we aim to grow and iterate FACTS Grounding as the field progresses, continually raising the bar.</span><br><span class="line"></span><br><span class="line">We encourage the AI community to engage with FACTS Grounding, evaluate their models on the open set of examples or to submit their models for evaluation. We believe that comprehensive benchmarking methods, coupled with continuous research and development will continue to improve AI systems.</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">https://liyaowei-stu.github.io/project/BrushEdit/</span><br><span class="line">12/17/24</span><br><span class="line">Tencent&#x27;s new BrushEdit enables precise image editing via an inpainting model. Learn more ⬇️</span><br><span class="line">Supports both, Instruction-based editing and tools based interactive editing.🔥</span><br><span class="line">- Add Objects in Image</span><br><span class="line">- Edit Image Backgrounds</span><br><span class="line">- Edit Image Objects</span><br><span class="line">- Remove Something from Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BrushEdit: All-In-One Image Inpainting and Editing</span><br><span class="line">Yaowei Li1*, Yuxuan Bian3*, Xuan Ju3*, Zhaoyang Zhang2‡, Junhao Zhuang4, Ying Shan2✉,  Yuexian Zou1✉ Qiang Xu3✉</span><br><span class="line">1Peking University 2ARC Lab, Tencent PCG</span><br><span class="line">3The Chinese University of Hong Kong</span><br><span class="line">4Tsinghua University</span><br><span class="line">✉ Corresponding Author ‡ Project Lead</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Demonstration Video</span><br><span class="line"></span><br><span class="line">😎 We recommend watching in full screen and with sound on. 😎</span><br><span class="line">BGM and YouTube.</span><br><span class="line">TL;DR</span><br><span class="line">BrushEdit is an advanced, unified AI agent for image inpainting and editing.</span><br><span class="line"></span><br><span class="line">Main Elements: 🛠️ Fully automated / 🤠 Interactive editing.</span><br><span class="line">Abstract</span><br><span class="line">Image editing has advanced significantly with the development of both inversion-based and instruction-based methods. However, current inversion-based approaches struggle with major modifications (e.g., adding or removing objects) due to the structured nature of inversion noise, which hinders substantial changes. Meanwhile, instruction-based methods often constrain users to black-box operations, limiting direct interaction for specifying editing regions and intensity. To address these limitations, we propose BrushEdit, a novel inpainting-based instruction-guided image editing paradigm, which leverages multimodal large language models (MLLMs) and image inpainting models to enable autonomous, user-friendly, and interactive free-form instruction editing. Specifically, we devise a system enabling free-form instruction editing by integrating MLLMs and a dual-branch image inpainting model in an agent-cooperative framework to perform editing category classification, main object identification, mask acquisition, and editing area inpainting. Extensive experiments show that our framework effectively combines MLLMs and inpainting models, achieving superior performance across seven key metrics, including mask region preservation, and editing effect coherence.</span><br><span class="line"></span><br><span class="line">Pipeline Overview</span><br><span class="line">Our approach consists of four main steps: (i) Editing category classification: determine the type of editing required. (ii) Identification of the primary editing object: Identify the main object to be edited. (iii) Acquisition of the editing mask and target Caption: Generate the editing mask and corresponding target caption. (iv) Image inpainting: Perform the actual image editing. Steps (i) to (iii) utilize pre-trained MLLMs and detection models to ascertain the editing type, target object, editing masks, and target caption. Step (iv) involves image editing using the dual-branch inpainting model improved BrushNet. This model inpaints the target areas based on the target caption and editing masks, leveraging the generative potential and background preservation capabilities of inpainting models.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>기술적으로 최대한 자세하게 적어. 9개의 기사가 있고 하나도 빼먹지 말고 적어.</p>
</details>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2024-12-20</span><i class="fa fa-tag"></i><a class="tag" href="/tags/AI-NEWS/" title="AI_NEWS">AI_NEWS </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://dongyoungkim2.github.io/2024/12/20/2024-12-30-AI-NEWS/,TECH BLOG  by Dongyoung Kim   Ph.D.,2024년 12월 20일 AI 소식,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2024/12/20/2024-12-20-AI-NEWS/" title="2024년 12월 20일 AI 소식">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2024/12/17/2024-12-17-AI-NEWS/" title="2024년 12월 17일 AI 소식">Next</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>